README.md

---

- orphan

# Scrapy documentation quick start guide

This file provides a quick guide on how to compile the Scrapy documentation.

## Setup the environment

To compile the documentation you need Sphinx Python library. To install it and all its dependencies run the following command from this dir

    pip install -r requirements.txt

## Compile the documentation

To compile the documentation (to classic HTML output) run the following command from this dir:

    make html

Documentation will be generated (in HTML format) inside the `build/html` dir.

## View the documentation

To view the documentation run the following command:

    make htmlview

This command will fire up your default browser and open the main page of your (previously generated) HTML documentation.

## Start over

To clean up all generated documentation files and start from scratch run:

    make clean

Keep in mind that this command won't touch any documentation source files.

## Recreating documentation on the fly

There is a way to recreate the doc automatically when you make changes, you need to install watchdog (`pip install watchdog`) and then use:

    make watch

## Alternative method using tox

To compile the documentation to HTML run the following command:

    tox -e docs

Documentation will be generated (in HTML format) inside the `.tox/docs/tmp/html` dir.

---

contributing.md

---

# Contributing to Scrapy

\> **Important** \> Double check that you are reading the most recent version of this document at <https://docs.scrapy.org/en/master/contributing.html>

There are many ways to contribute to Scrapy. Here are some of them:

  - Report bugs and request features in the [issue tracker](https://github.com/scrapy/scrapy/issues), trying to follow the guidelines detailed in [Reporting bugs](#reporting-bugs) below.
  - Submit patches for new functionalities and/or bug fixes. Please read \[writing-patches\](\#writing-patches) and [Submitting patches](#submitting-patches) below for details on how to write and submit a patch.
  - Blog about Scrapy. Tell the world how you're using Scrapy. This will help newcomers with more examples and will help the Scrapy project to increase its visibility.
  - Join the [Scrapy subreddit](https://reddit.com/r/scrapy) and share your ideas on how to improve Scrapy. We're always open to suggestions.
  - Answer Scrapy questions at [Stack Overflow](https://stackoverflow.com/questions/tagged/scrapy).

## Reporting bugs

\> **Note** \> Please report security issues **only** to <scrapy-security@googlegroups.com>. This is a private list only open to trusted Scrapy developers, and its archives are not public.

Well-written bug reports are very helpful, so keep in mind the following guidelines when you're going to report a new bug.

  - check the \[FAQ \<faq\>\](\#faq-\<faq\>) first to see if your issue is addressed in a well-known question
  - if you have a general question about Scrapy usage, please ask it at [Stack Overflow](https://stackoverflow.com/questions/tagged/scrapy) (use "scrapy" tag).
  - check the [open issues](https://github.com/scrapy/scrapy/issues) to see if the issue has already been reported. If it has, don't dismiss the report, but check the ticket history and comments. If you have additional useful information, please leave a comment, or consider \[sending a pull request \<writing-patches\>\](\#sending-a-pull-request-\<writing-patches\>) with a fix.
  - search the [scrapy-users](https://groups.google.com/forum/#!forum/scrapy-users) list and [Scrapy subreddit](https://reddit.com/r/scrapy) to see if it has been discussed there, or if you're not sure if what you're seeing is a bug. You can also ask in the `#scrapy` IRC channel.
  - write **complete, reproducible, specific bug reports**. The smaller the test case, the better. Remember that other developers won't have your project to reproduce the bug, so please include all relevant files required to reproduce it. See for example StackOverflow's guide on creating a [Minimal, Complete, and Verifiable example](https://stackoverflow.com/help/mcve) exhibiting the issue.
  - the most awesome way to provide a complete reproducible example is to send a pull request which adds a failing test case to the Scrapy testing suite (see \[submitting-patches\](\#submitting-patches)). This is helpful even if you don't have an intention to fix the issue yourselves.
  - include the output of `scrapy version -v` so developers working on your bug know exactly which version and platform it occurred on, which is often very helpful for reproducing it, or knowing if it was already fixed.

## Writing patches

Scrapy has a list of [good first issues](https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help wanted issues](https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22) that you can work on. These issues are a great way to get started with contributing to Scrapy. If you're new to the codebase, you may want to focus on documentation or testing-related issues, as they are always useful and can help you get more familiar with the project. You can also check Scrapy's [test coverage](https://app.codecov.io/gh/scrapy/scrapy) to see which areas may benefit from more tests.

The better a patch is written, the higher the chances that it'll get accepted and the sooner it will be merged.

Well-written patches should:

  - contain the minimum amount of code required for the specific change. Small patches are easier to review and merge. So, if you're doing more than one change (or bug fix), please consider submitting one patch per change. Do not collapse multiple changes into a single patch. For big changes consider using a patch queue.

  - pass all unit-tests. See [Running tests](#running-tests) below.

  - include one (or more) test cases that check the bug fixed or the new functionality added. See [Writing tests](#writing-tests) below.

  - if you're adding or changing a public (documented) API, please include the documentation changes in the same patch. See [Documentation policies](#documentation-policies) below.

  - if you're adding a private API, please add a regular expression to the `coverage_ignore_pyobjects` variable of `docs/conf.py` to exclude the new private API from documentation coverage checks.
    
    To see if your private API is skipped properly, generate a documentation coverage report as follows:
    
        tox -e docs-coverage

  - if you are removing deprecated code, first make sure that at least 1 year (12 months) has passed since the release that introduced the deprecation. See \[deprecation-policy\](\#deprecation-policy).

## Submitting patches

The best way to submit a patch is to issue a [pull request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request) on GitHub, optionally creating a new issue first.

Remember to explain what was fixed or the new functionality (what it is, why it's needed, etc). The more info you include, the easier will be for core developers to understand and accept your patch.

You can also discuss the new functionality (or bug fix) before creating the patch, but it's always good to have a patch ready to illustrate your arguments and show that you have put some additional thought into the subject. A good starting point is to send a pull request on GitHub. It can be simple enough to illustrate your idea, and leave documentation/tests for later, after the idea has been validated and proven useful. Alternatively, you can start a conversation in the [Scrapy subreddit](https://reddit.com/r/scrapy) to discuss your idea first.

Sometimes there is an existing pull request for the problem you'd like to solve, which is stalled for some reason. Often the pull request is in a right direction, but changes are requested by Scrapy maintainers, and the original pull request author hasn't had time to address them. In this case consider picking up this pull request: open a new pull request with all commits from the original pull request, as well as additional changes to address the raised issues. Doing so helps a lot; it is not considered rude as long as the original author is acknowledged by keeping his/her commits.

You can pull an existing pull request to a local branch by running `git fetch upstream pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE` (replace 'upstream' with a remote name for scrapy repository, `$PR_NUMBER` with an ID of the pull request, and `$BRANCH_NAME_TO_CREATE` with a name of the branch you want to create locally). See also: <https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/reviewing-changes-in-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally>.

When writing GitHub pull requests, try to keep titles short but descriptive. E.g. For bug \#411: "Scrapy hangs if an exception raises in start\_requests" prefer "Fix hanging when exception occurs in start\_requests (\#411)" instead of "Fix for \#411". Complete titles make it easy to skim through the issue tracker.

Finally, try to keep aesthetic changes (`8` compliance, unused imports removal, etc) in separate commits from functional changes. This will make pull requests easier to review and more likely to get merged.

## Coding style

Please follow these coding conventions when writing code for inclusion in Scrapy:

  - We use [black](https://black.readthedocs.io/en/stable/) for code formatting. There is a hook in the pre-commit config that will automatically format your code before every commit. You can also run black manually with `tox -e pre-commit`.
  - Don't put your name in the code you contribute; git provides enough metadata to identify author of the code. See <https://docs.github.com/en/get-started/getting-started-with-git/setting-your-username-in-git> for setup instructions.

## Pre-commit

We use [pre-commit](https://pre-commit.com/) to automatically address simple code issues before every commit.

After your create a local clone of your fork of the Scrapy repository:

1.  [Install pre-commit](https://pre-commit.com/#installation).
2.  On the root of your local clone of the Scrapy repository, run the following command:
      - \`\`\`bash  
        pre-commit install

Now pre-commit will check your changes every time you create a Git commit. Upon `` ` finding issues, pre-commit aborts your commit, and either fixes those issues automatically, or only reports them to you. If it fixes those issues automatically, creating your commit again should succeed. Otherwise, you may need to address the corresponding issues manually first.  .. _documentation-policies:  Documentation policies ======================  For reference documentation of API members (classes, methods, etc.) use docstrings and make sure that the Sphinx documentation uses the :mod:`~sphinx.ext.autodoc` extension to pull the docstrings. API reference documentation should follow docstring conventions (`PEP 257`_) and be IDE-friendly: short, to the point, and it may provide short examples.  Other types of documentation, such as tutorials or topics, should be covered in files within the ``docs/``directory. This includes documentation that is specific to an API member, but goes beyond API reference documentation.  In any case, if something is covered in a docstring, use the :mod:`~sphinx.ext.autodoc` extension to pull the docstring into the documentation instead of duplicating the docstring in files within the``docs/``directory.  Documentation updates that cover new or modified features must use Sphinx’s :rst:dir:`versionadded` and :rst:dir:`versionchanged` directives. Use``VERSION`as version, we will replace it with the actual version right before the corresponding release. When we release a new major or minor version of Scrapy, we remove these directives if they are older than 3 years.  Documentation about deprecated features must be removed as those features are deprecated, so that new readers do not run into it. New deprecations and deprecation removals are documented in the [release notes <news>](#release-notes-<news>).   Tests =====  Tests are implemented using the [Twisted unit-testing framework <twisted:development/test-standard>](Twisted unit-testing framework <twisted:development/test-standard>.md). Running tests requires [tox <tox:index>](tox <tox:index>.md).  .. _running-tests:  Running tests -------------  To run all tests::      tox  To run a specific test (say`tests/test\_loader.py`) use:`tox -- tests/test\_loader.py`To run the tests on a specific [tox <tox:index>](tox <tox:index>.md) environment, use`-e \<name\>`with an environment name from`tox.ini`. For example, to run the tests with Python 3.10 use::      tox -e py310  You can also specify a comma-separated list of environments, and use [tox’s parallel mode <tox:parallel_mode>](#tox’s parallel-mode-<tox:parallel_mode>) to run the tests on multiple environments in parallel::      tox -e py39,py310 -p auto  To pass command-line options to [pytest <pytest:index>](pytest <pytest:index>.md), add them after`--`in your call to [tox <tox:index>](tox <tox:index>.md). Using`--`overrides the default positional arguments defined in`tox.ini`, so you must include those default positional arguments (`scrapy tests`) after`--``as well::      tox -- scrapy tests -x  # stop after first failure  You can also use the `pytest-xdist`_ plugin. For example, to run all tests on the Python 3.10 [tox <tox:index>](tox <tox:index>.md) environment using all your CPU cores::      tox -e py310 -- scrapy tests -n auto  To see coverage report install [coverage <coverage:index>](coverage <coverage:index>.md) (``pip install coverage`) and run:`coverage report`see output of`coverage --help\`\` for more options like html or xml report.

### Writing tests

All functionality (including new features and bug fixes) must include a test case to check that it works as expected, so please include tests for your patches if you want them to get accepted sooner.

Scrapy uses unit-tests, which are located in the [tests/](https://github.com/scrapy/scrapy/tree/master/tests) directory. Their module name typically resembles the full path of the module they're testing. For example, the item loaders code is in:

    scrapy.loader

And their unit-tests are in:

    tests/test_loader.py

---

faq.md

---

# Frequently Asked Questions

## How does Scrapy compare to BeautifulSoup or lxml?

[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) and [lxml](https://lxml.de/) are libraries for parsing HTML and XML. Scrapy is an application framework for writing web spiders that crawl web sites and extract data from them.

Scrapy provides a built-in mechanism for extracting data (called \[selectors \<topics-selectors\>\](\#selectors-\<topics-selectors\>)) but you can easily use [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) (or [lxml](https://lxml.de/)) instead, if you feel more comfortable working with them. After all, they're just parsing libraries which can be imported and used from any Python code.

In other words, comparing [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) (or [lxml](https://lxml.de/)) to Scrapy is like comparing [jinja2](https://palletsprojects.com/projects/jinja/) to [Django](https://www.djangoproject.com/).

## Can I use Scrapy with BeautifulSoup?

Yes, you can. As mentioned \[above \<faq-scrapy-bs-cmp\>\](\#above-\<faq-scrapy-bs-cmp\>), [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) can be used for parsing HTML responses in Scrapy callbacks. You just have to feed the response's body into a `BeautifulSoup` object and extract whatever data you need from it.

Here's an example spider using BeautifulSoup API, with `lxml` as the HTML parser:

`` `python     from bs4 import BeautifulSoup     import scrapy       class ExampleSpider(scrapy.Spider):         name = "example"         allowed_domains = ["example.com"]         start_urls = ("http://www.example.com/",)          def parse(self, response):             # use lxml to get decent HTML parsing speed             soup = BeautifulSoup(response.text, "lxml")             yield {"url": response.url, "title": soup.h1.string}  > **Note** > ``BeautifulSoup``supports several HTML/XML parsers.     See `BeautifulSoup's official documentation`_ on which ones are available.     Did Scrapy "steal" X from Django?``\` ---------------------------------

Probably, but we don't like that word. We think [Django](https://www.djangoproject.com/) is a great open source project and an example to follow, so we've used it as an inspiration for Scrapy.

We believe that, if something is already done well, there's no need to reinvent it. This concept, besides being one of the foundations for open source and free software, not only applies to software but also to documentation, procedures, policies, etc. So, instead of going through each problem ourselves, we choose to copy ideas from those projects that have already solved them properly, and focus on the real problems we need to solve.

We'd be proud if Scrapy serves as an inspiration for other projects. Feel free to steal from us\!

## Does Scrapy work with HTTP proxies?

Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP Proxy downloader middleware. See <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span>.

## How can I scrape an item with attributes in different pages?

See \[topics-request-response-ref-request-callback-arguments\](\#topics-request-response-ref-request-callback-arguments).

## How can I simulate a user login in my spider?

See \[topics-request-response-ref-request-userlogin\](\#topics-request-response-ref-request-userlogin).

## Does Scrapy crawl in breadth-first or depth-first order?

By default, Scrapy uses a [LIFO](https://en.wikipedia.org/wiki/Stack_\(abstract_data_type\)) queue for storing pending requests, which basically means that it crawls in [DFO order](https://en.wikipedia.org/wiki/Depth-first_search). This order is more convenient in most cases.

If you do want to crawl in true [BFO order](https://en.wikipedia.org/wiki/Breadth-first_search), you can do it by setting the following settings:

`` `python     DEPTH_PRIORITY = 1     SCHEDULER_DISK_QUEUE = "scrapy.squeues.PickleFifoDiskQueue"     SCHEDULER_MEMORY_QUEUE = "scrapy.squeues.FifoMemoryQueue"  While pending requests are below the configured values of ``<span class="title-ref"> :setting:\`CONCURRENT\_REQUESTS</span>, `CONCURRENT_REQUESTS_PER_DOMAIN` or `CONCURRENT_REQUESTS_PER_IP`, those requests are sent concurrently. As a result, the first few requests of a crawl rarely follow the desired order. Lowering those settings to `1` enforces the desired order, but it significantly slows down the crawl as a whole.

## My Scrapy crawler has memory leaks. What can I do?

See \[topics-leaks\](\#topics-leaks).

Also, Python has a builtin memory leak issue which is described in \[topics-leaks-without-leaks\](\#topics-leaks-without-leaks).

## How can I make Scrapy consume less memory?

See previous question.

## How can I prevent memory errors due to many allowed domains?

If you have a spider with a long list of <span class="title-ref">\~scrapy.Spider.allowed\_domains</span> (e.g. 50,000+), consider replacing the default <span class="title-ref">\~scrapy.downloadermiddlewares.offsite.OffsiteMiddleware</span> downloader middleware with a \[custom downloader middleware \<topics-downloader-middleware-custom\>\](\#custom-downloader-middleware \<topics-downloader-middleware-custom\>) that requires less memory. For example:

  - If your domain names are similar enough, use your own regular expression instead joining the strings in <span class="title-ref">\~scrapy.Spider.allowed\_domains</span> into a complex regular expression.
  - If you can meet the installation requirements, use [pyre2](https://github.com/andreasvc/pyre2) instead of Python’s [re]() to compile your URL-filtering regular expression. See `1908`.

See also [other suggestions at StackOverflow](https://stackoverflow.com/q/36440681).

<div class="note">

<div class="title">

Note

</div>

Remember to disable <span class="title-ref">scrapy.downloadermiddlewares.offsite.OffsiteMiddleware</span> when you enable your custom implementation:

  - \`\`\`python
    
      - DOWNLOADER\_MIDDLEWARES = {  
        "scrapy.downloadermiddlewares.offsite.OffsiteMiddleware": None, "myproject.middlewares.CustomOffsiteMiddleware": 50,
    
    }

</div>

  - `` ` .. _re: https://docs.python.org/3/library/re.html  Can I use Basic HTTP Authentication in my spiders? --------------------------------------------------  Yes, see `~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware`.  Why does Scrapy download pages in English instead of my native language? ------------------------------------------------------------------------  Try changing the default `Accept-Language`_ request header by overriding the :setting:`DEFAULT_REQUEST_HEADERS` setting.    Where can I find some example Scrapy projects? ----------------------------------------------  See [intro-examples](#intro-examples).  Can I run a spider without creating a project? ----------------------------------------------  Yes. You can use the :command:`runspider` command. For example, if you have a spider written in a ``my\_spider.py``file you can run it with::      scrapy runspider my_spider.py  See :command:`runspider` command for more info.  I get "Filtered offsite request" messages. How can I fix them? --------------------------------------------------------------  Those messages (logged with``DEBUG``level) don't necessarily mean there is a problem, so you may not need to fix them.  Those messages are thrown by `~scrapy.downloadermiddlewares.offsite.OffsiteMiddleware`, which is a downloader middleware (enabled by default) whose purpose is to filter out requests to domains outside the ones covered by the spider.  What is the recommended way to deploy a Scrapy crawler in production? ---------------------------------------------------------------------  See [topics-deploy](#topics-deploy).  Can I use JSON for large exports? ---------------------------------  It'll depend on how large your output is. See [this warning <json-with-large-data>](#this-warning <json-with-large-data>) in `~scrapy.exporters.JsonItemExporter` documentation.  Can I return (Twisted) deferreds from signal handlers? ------------------------------------------------------  Some signals support returning deferreds from their handlers, others don't. See the [topics-signals-ref](#topics-signals-ref) to know which ones.  What does the response status code 999 mean? --------------------------------------------  999 is a custom response status code used by Yahoo sites to throttle requests. Try slowing down the crawling speed by using a download delay of``2`(or higher) in your spider:`\`python  
    from scrapy.spiders import CrawlSpider
    
      - class MySpider(CrawlSpider):  
        name = "myspider"
        
        download\_delay = 2
        
        \# \[ ... rest of the spider code ... \]

Or by setting a global download delay in your project with the `` ` :setting:`DOWNLOAD_DELAY` setting.  Can I call ``pdb.set\_trace()`from my spiders to debug them? -------------------------------------------------------------  Yes, but you can also use the Scrapy shell which allows you to quickly analyze (and even modify) the response being processed by your spider, which is, quite often, more useful than plain old`pdb.set\_trace()`.  For more info see [topics-shell-inspect-response](#topics-shell-inspect-response).  Simplest way to dump all my scraped items into a JSON/CSV/XML file? -------------------------------------------------------------------  To dump into a JSON file::      scrapy crawl myspider -O items.json  To dump into a CSV file::      scrapy crawl myspider -O items.csv  To dump into an XML file::      scrapy crawl myspider -O items.xml  For more information see [topics-feed-exports](#topics-feed-exports)  What's this huge cryptic`\_\_VIEWSTATE`parameter used in some forms? ----------------------------------------------------------------------  The`\_\_VIEWSTATE``parameter is used in sites built with ASP.NET/VB.NET. For more info on how it works see `this page`_. Also, here's an `example spider`_ which scrapes one of these sites.     What's the best way to parse big XML/CSV data feeds? ----------------------------------------------------  Parsing big feeds with XPath selectors can be problematic since they need to build the DOM of the entire feed in memory, and this can be quite slow and consume a lot of memory.  In order to avoid parsing all the entire feed at once in memory, you can use the `~scrapy.utils.iterators.xmliter_lxml` and `~scrapy.utils.iterators.csviter` functions. In fact, this is what `~scrapy.spiders.XMLFeedSpider` uses.  .. autofunction:: scrapy.utils.iterators.xmliter_lxml  .. autofunction:: scrapy.utils.iterators.csviter  Does Scrapy manage cookies automatically? -----------------------------------------  Yes, Scrapy receives and keeps track of cookies sent by servers, and sends them back on subsequent requests, like any regular web browser does.  For more info see [topics-request-response](#topics-request-response) and [cookies-mw](#cookies-mw).  How can I see the cookies being sent and received from Scrapy? --------------------------------------------------------------  Enable the :setting:`COOKIES_DEBUG` setting.  How can I instruct a spider to stop itself? -------------------------------------------  Raise the `~scrapy.exceptions.CloseSpider` exception from a callback. For more info see: `~scrapy.exceptions.CloseSpider`.  How can I prevent my Scrapy bot from getting banned? ----------------------------------------------------  See [bans](#bans).  Should I use spider arguments or settings to configure my spider? -----------------------------------------------------------------  Both [spider arguments <spiderargs>](#spider-arguments-<spiderargs>) and [settings <topics-settings>](#settings-<topics-settings>) can be used to configure your spider. There is no strict rule that mandates to use one or the other, but settings are more suited for parameters that, once set, don't change much, while spider arguments are meant to change more often, even on each spider run and sometimes are required for the spider to run at all (for example, to set the start url of a spider).  To illustrate with an example, assuming you have a spider that needs to log into a site to scrape data, and you only want to scrape data from a certain section of the site (which varies each time). In that case, the credentials to log in would be settings, while the url of the section to scrape would be a spider argument.  I'm scraping a XML document and my XPath selector doesn't return any items --------------------------------------------------------------------------  You may need to remove namespaces. See [removing-namespaces](#removing-namespaces).   .. _faq-split-item:  How to split an item into multiple items in an item pipeline? -------------------------------------------------------------  [Item pipelines <topics-item-pipeline>](#item-pipelines-<topics-item-pipeline>) cannot yield multiple items per input item. [Create a spider middleware <custom-spider-middleware>](#create-a-spider-middleware-<custom-spider-middleware>) instead, and use its `~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output` method for this purpose. For example:``\`python from copy import deepcopy

> from itemadapter import is\_item, ItemAdapter
> 
>   - class MultiplyItemsMiddleware:
>     
>       - def process\_spider\_output(self, response, result, spider):
>         
>           - for item in result:
>             
>               - if is\_item(item):  
>                 adapter = ItemAdapter(item) for \_ in range(adapter\["multiply\_by"\]): yield deepcopy(item)

Does Scrapy support IPv6 addresses? `` ` -----------------------------------  Yes, by setting :setting:`DNS_RESOLVER` to ``scrapy.resolver.CachingHostnameResolver``. Note that by doing so, you lose the ability to set a specific timeout for DNS requests (the value of the :setting:`DNS_TIMEOUT` setting is ignored).   .. _faq-specific-reactor:  How to deal with``\<class 'ValueError'\>: filedescriptor out of range in select()``exceptions? ----------------------------------------------------------------------------------------------  This issue `has been reported`_ to appear when running broad crawls in macOS, where the default Twisted reactor is `twisted.internet.selectreactor.SelectReactor`. Switching to a different reactor is possible by using the :setting:`TWISTED_REACTOR` setting.   .. _faq-stop-response-download:  How can I cancel the download of a given response? --------------------------------------------------  In some situations, it might be useful to stop the download of a certain response. For instance, sometimes you can determine whether or not you need the full contents of a response by inspecting its headers or the first bytes of its body. In that case, you could save resources by attaching a handler to the `~scrapy.signals.bytes_received` or `~scrapy.signals.headers_received` signals and raising a `~scrapy.exceptions.StopDownload` exception. Please refer to the [topics-stop-response-download](#topics-stop-response-download) topic for additional information and examples.   .. _faq-blank-request:  How can I make a blank request? -------------------------------``\`python from scrapy import Request

> blank\_request = Request("data:,")

In this case, the URL is set to a data URI scheme. Data URLs allow you to include data `` ` inline within web pages, similar to external resources. The "data:" scheme with an empty content (",") essentially creates a request to a data URL without any specific content.   Running ``runspider`I get`error: No spider found in file: \<filename\>``--------------------------------------------------------------------------  This may happen if your Scrapy project has a spider module with a name that conflicts with the name of one of the `Python standard library modules`_, such as``csv.py`or`os.py\`<span class="title-ref">, or any \`Python package</span>\_ that you have installed. See `2680`.

---

index.md

---

# Scrapy documentation

Scrapy is a fast high-level [web crawling](https://en.wikipedia.org/wiki/Web_crawler) and [web scraping](https://en.wikipedia.org/wiki/Web_scraping) framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.

## Getting help

Having trouble? We'd like to help\!

  - Try the \[FAQ \<faq\>\](FAQ \<faq\>.md) -- it's got answers to some common questions.
  - Looking for specific information? Try the \[genindex\](\#genindex) or \[modindex\](\#modindex).
  - Ask or search questions in [StackOverflow using the scrapy tag](https://stackoverflow.com/tags/scrapy).
  - Ask or search questions in the [Scrapy subreddit](https://www.reddit.com/r/scrapy/).
  - Search for questions on the archives of the [scrapy-users mailing list](https://groups.google.com/forum/#!forum/scrapy-users).
  - Ask a question in the [\#scrapy IRC channel](irc://irc.freenode.net/scrapy),
  - Report bugs with Scrapy in our [issue tracker](https://github.com/scrapy/scrapy/issues).
  - Join the Discord community [Scrapy Discord](https://discord.com/invite/mv3yErfpvq).

## First steps

<div class="toctree" data-caption="First steps" hidden="">

intro/overview intro/install intro/tutorial intro/examples

</div>

  - \[intro/overview\](intro/overview.md)  
    Understand what Scrapy is and how it can help you.

  - \[intro/install\](intro/install.md)  
    Get Scrapy installed on your computer.

  - \[intro/tutorial\](intro/tutorial.md)  
    Write your first Scrapy project.

  - \[intro/examples\](intro/examples.md)  
    Learn more by playing with a pre-made Scrapy project.

## Basic concepts

<div class="toctree" data-caption="Basic concepts" hidden="">

topics/commands topics/spiders topics/selectors topics/items topics/loaders topics/shell topics/item-pipeline topics/feed-exports topics/request-response topics/link-extractors topics/settings topics/exceptions

</div>

  - \[topics/commands\](topics/commands.md)  
    Learn about the command-line tool used to manage your Scrapy project.

  - \[topics/spiders\](topics/spiders.md)  
    Write the rules to crawl your websites.

  - \[topics/selectors\](topics/selectors.md)  
    Extract the data from web pages using XPath.

  - \[topics/shell\](topics/shell.md)  
    Test your extraction code in an interactive environment.

  - \[topics/items\](topics/items.md)  
    Define the data you want to scrape.

  - \[topics/loaders\](topics/loaders.md)  
    Populate your items with the extracted data.

  - \[topics/item-pipeline\](topics/item-pipeline.md)  
    Post-process and store your scraped data.

  - \[topics/feed-exports\](topics/feed-exports.md)  
    Output your scraped data using different formats and storages.

  - \[topics/request-response\](topics/request-response.md)  
    Understand the classes used to represent HTTP requests and responses.

  - \[topics/link-extractors\](topics/link-extractors.md)  
    Convenient classes to extract links to follow from pages.

  - \[topics/settings\](topics/settings.md)  
    Learn how to configure Scrapy and see all \[available settings \<topics-settings-ref\>\](\#available-settings-\<topics-settings-ref\>).

  - \[topics/exceptions\](topics/exceptions.md)  
    See all available exceptions and their meaning.

## Built-in services

<div class="toctree" data-caption="Built-in services" hidden="">

topics/logging topics/stats topics/email topics/telnetconsole

</div>

  - \[topics/logging\](topics/logging.md)  
    Learn how to use Python's builtin logging on Scrapy.

  - \[topics/stats\](topics/stats.md)  
    Collect statistics about your scraping crawler.

  - \[topics/email\](topics/email.md)  
    Send email notifications when certain events occur.

  - \[topics/telnetconsole\](topics/telnetconsole.md)  
    Inspect a running crawler using a built-in Python console.

## Solving specific problems

<div class="toctree" data-caption="Solving specific problems" hidden="">

faq topics/debug topics/contracts topics/practices topics/broad-crawls topics/developer-tools topics/dynamic-content topics/leaks topics/media-pipeline topics/deploy topics/autothrottle topics/benchmarking topics/jobs topics/coroutines topics/asyncio

</div>

  - \[faq\](faq.md)  
    Get answers to most frequently asked questions.

  - \[topics/debug\](topics/debug.md)  
    Learn how to debug common problems of your Scrapy spider.

  - \[topics/contracts\](topics/contracts.md)  
    Learn how to use contracts for testing your spiders.

  - \[topics/practices\](topics/practices.md)  
    Get familiar with some Scrapy common practices.

  - \[topics/broad-crawls\](topics/broad-crawls.md)  
    Tune Scrapy for crawling a lot domains in parallel.

  - \[topics/developer-tools\](topics/developer-tools.md)  
    Learn how to scrape with your browser's developer tools.

  - \[topics/dynamic-content\](topics/dynamic-content.md)  
    Read webpage data that is loaded dynamically.

  - \[topics/leaks\](topics/leaks.md)  
    Learn how to find and get rid of memory leaks in your crawler.

  - \[topics/media-pipeline\](topics/media-pipeline.md)  
    Download files and/or images associated with your scraped items.

  - \[topics/deploy\](topics/deploy.md)  
    Deploying your Scrapy spiders and run them in a remote server.

  - \[topics/autothrottle\](topics/autothrottle.md)  
    Adjust crawl rate dynamically based on load.

  - \[topics/benchmarking\](topics/benchmarking.md)  
    Check how Scrapy performs on your hardware.

  - \[topics/jobs\](topics/jobs.md)  
    Learn how to pause and resume crawls for large spiders.

  - \[topics/coroutines\](topics/coroutines.md)  
    Use the \[coroutine syntax \<async\>\](\#coroutine-syntax-\<async\>).

  - \[topics/asyncio\](topics/asyncio.md)  
    Use `asyncio` and `asyncio`-powered libraries.

## Extending Scrapy

<div class="toctree" data-caption="Extending Scrapy" hidden="">

topics/architecture topics/addons topics/downloader-middleware topics/spider-middleware topics/extensions topics/signals topics/scheduler topics/exporters topics/components topics/api

</div>

  - \[topics/architecture\](topics/architecture.md)  
    Understand the Scrapy architecture.

  - \[topics/addons\](topics/addons.md)  
    Enable and configure third-party extensions.

  - \[topics/downloader-middleware\](topics/downloader-middleware.md)  
    Customize how pages get requested and downloaded.

  - \[topics/spider-middleware\](topics/spider-middleware.md)  
    Customize the input and output of your spiders.

  - \[topics/extensions\](topics/extensions.md)  
    Extend Scrapy with your custom functionality

  - \[topics/signals\](topics/signals.md)  
    See all available signals and how to work with them.

  - \[topics/scheduler\](topics/scheduler.md)  
    Understand the scheduler component.

  - \[topics/exporters\](topics/exporters.md)  
    Quickly export your scraped items to a file (XML, CSV, etc).

  - \[topics/components\](topics/components.md)  
    Learn the common API and some good practices when building custom Scrapy components.

  - \[topics/api\](topics/api.md)  
    Use it on extensions and middlewares to extend Scrapy functionality.

## All the rest

<div class="toctree" data-caption="All the rest" hidden="">

news contributing versioning

</div>

  - \[news\](news.md)  
    See what has changed in recent Scrapy versions.

  - \[contributing\](contributing.md)  
    Learn how to contribute to the Scrapy project.

  - \[versioning\](versioning.md)  
    Understand Scrapy versioning and API stability.

---

examples.md

---

# Examples

The best way to learn is with examples, and Scrapy is no exception. For this reason, there is an example Scrapy project named [quotesbot](https://github.com/scrapy/quotesbot), that you can use to play and learn more about Scrapy. It contains two spiders for <https://quotes.toscrape.com>, one using CSS selectors and another one using XPath expressions.

The [quotesbot](https://github.com/scrapy/quotesbot) project is available at: <https://github.com/scrapy/quotesbot>. You can find more information about it in the project's README.

If you're familiar with git, you can checkout the code. Otherwise you can download the project as a zip file by clicking [here](https://github.com/scrapy/quotesbot/archive/master.zip).

---

install.md

---

# Installation guide

## Supported Python versions

Scrapy requires Python 3.9+, either the CPython implementation (default) or the PyPy implementation (see \[python:implementations\](\#python:implementations)).

## Installing Scrapy

If you're using [Anaconda](https://docs.anaconda.com/anaconda/) or [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html), you can install the package from the [conda-forge](https://conda-forge.org/) channel, which has up-to-date packages for Linux, Windows and macOS.

To install Scrapy using `conda`, run:

    conda install -c conda-forge scrapy

Alternatively, if you’re already familiar with installation of Python packages, you can install Scrapy and its dependencies from PyPI with:

    pip install Scrapy

We strongly recommend that you install Scrapy in \[a dedicated virtualenv \<intro-using-virtualenv\>\](\#a-dedicated-virtualenv-\<intro-using-virtualenv\>), to avoid conflicting with your system packages.

Note that sometimes this may require solving compilation issues for some Scrapy dependencies depending on your operating system, so be sure to check the \[intro-install-platform-notes\](\#intro-install-platform-notes).

For more detailed and platform-specific instructions, as well as troubleshooting information, read on.

### Things that are good to know

Scrapy is written in pure Python and depends on a few key Python packages (among others):

  - [lxml](https://lxml.de/index.html), an efficient XML and HTML parser
  - [parsel](https://pypi.org/project/parsel/), an HTML/XML data extraction library written on top of lxml,
  - [w3lib](https://pypi.org/project/w3lib/), a multi-purpose helper for dealing with URLs and web page encodings
  - [twisted](https://twisted.org/), an asynchronous networking framework
  - [cryptography](https://cryptography.io/en/latest/) and [pyOpenSSL](https://pypi.org/project/pyOpenSSL/), to deal with various network-level security needs

Some of these packages themselves depend on non-Python packages that might require additional installation steps depending on your platform. Please check \[platform-specific guides below \<intro-install-platform-notes\>\](\#platform-specific-guides-below-\<intro-install-platform-notes\>).

In case of any trouble related to these dependencies, please refer to their respective installation instructions:

  - [lxml installation](https://lxml.de/installation.html)
  - \[cryptography installation \<cryptography:installation\>\](cryptography installation \<cryptography:installation\>.md)

### Using a virtual environment (recommended)

TL;DR: We recommend installing Scrapy inside a virtual environment on all platforms.

Python packages can be installed either globally (a.k.a system wide), or in user-space. We do not recommend installing Scrapy system wide.

Instead, we recommend that you install Scrapy within a so-called "virtual environment" (`venv`). Virtual environments allow you to not conflict with already-installed Python system packages (which could break some of your system tools and scripts), and still install packages normally with `pip` (without `sudo` and the likes).

See \[tut-venv\](\#tut-venv) on how to create your virtual environment.

Once you have created a virtual environment, you can install Scrapy inside it with `pip`, just like any other Python package. (See \[platform-specific guides \<intro-install-platform-notes\>\](\#platform-specific-guides-\<intro-install-platform-notes\>) below for non-Python dependencies that you may need to install beforehand).

## Platform specific installation notes

### Windows

Though it's possible to install Scrapy on Windows using pip, we recommend you install [Anaconda](https://docs.anaconda.com/anaconda/) or [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) and use the package from the [conda-forge](https://conda-forge.org/) channel, which will avoid most installation issues.

Once you've installed [Anaconda](https://docs.anaconda.com/anaconda/) or [Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html), install Scrapy with:

    conda install -c conda-forge scrapy

To install Scrapy on Windows using `pip`:

<div class="warning">

<div class="title">

Warning

</div>

This installation method requires “Microsoft Visual C++” for installing some Scrapy dependencies, which demands significantly more disk space than Anaconda.

</div>

1.  Download and execute [Microsoft C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/) to install the Visual Studio Installer.

2.  Run the Visual Studio Installer.

3.  Under the Workloads section, select **C++ build tools**.

4.  Check the installation details and make sure following packages are selected as optional components:
    
    >   - **MSVC** (e.g MSVC v142 - VS 2019 C++ x64/x86 build tools (v14.23) )
    >   - **Windows SDK** (e.g Windows 10 SDK (10.0.18362.0))

5.  Install the Visual Studio Build Tools.

Now, you should be able to \[install Scrapy \<intro-install-scrapy\>\](\#install-scrapy-\<intro-install-scrapy\>) using `pip`.

### Ubuntu 14.04 or above

Scrapy is currently tested with recent-enough versions of lxml, twisted and pyOpenSSL, and is compatible with recent Ubuntu distributions. But it should support older versions of Ubuntu too, like Ubuntu 14.04, albeit with potential issues with TLS connections.

**Don't** use the `python-scrapy` package provided by Ubuntu, they are typically too old and slow to catch up with the latest Scrapy release.

To install Scrapy on Ubuntu (or Ubuntu-based) systems, you need to install these dependencies:

    sudo apt-get install python3 python3-dev python3-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev

  - `python3-dev`, `zlib1g-dev`, `libxml2-dev` and `libxslt1-dev` are required for `lxml`
  - `libssl-dev` and `libffi-dev` are required for `cryptography`

Inside a \[virtualenv \<intro-using-virtualenv\>\](\#virtualenv-\<intro-using-virtualenv\>), you can install Scrapy with `pip` after that:

    pip install scrapy

<div class="note">

<div class="title">

Note

</div>

The same non-Python dependencies can be used to install Scrapy in Debian Jessie (8.0) and above.

</div>

### macOS

Building Scrapy's dependencies requires the presence of a C compiler and development headers. On macOS this is typically provided by Apple’s Xcode development tools. To install the Xcode command-line tools, open a terminal window and run:

    xcode-select --install

There's a [known issue](https://github.com/pypa/pip/issues/2468) that prevents `pip` from updating system packages. This has to be addressed to successfully install Scrapy and its dependencies. Here are some proposed solutions:

  - *(Recommended)* **Don't** use system Python. Install a new, updated version that doesn't conflict with the rest of your system. Here's how to do it using the [homebrew](https://brew.sh/) package manager:
      - Install [homebrew](https://brew.sh/) following the instructions in <https://brew.sh/>
    
      - Update your `PATH` variable to state that homebrew packages should be used before system packages (Change `.bashrc` to `.zshrc` accordingly if you're using [zsh](https://www.zsh.org/) as default shell):
        
            echo "export PATH=/usr/local/bin:/usr/local/sbin:$PATH" >> ~/.bashrc
    
      - Reload `.bashrc` to ensure the changes have taken place:
        
            source ~/.bashrc
    
      - Install python:
        
            brew install python

<!-- end list -->

  - \* *(Optional)* \[Install Scrapy inside a Python virtual environment  
    \<intro-using-virtualenv\>\](\#install-scrapy-inside-a-python-virtual-environment

\----\<intro-using-virtualenv\>).

> This method is a workaround for the above macOS issue, but it's an overall good practice for managing dependencies and can complement the first method.

After any of these workarounds you should be able to install Scrapy:

    pip install Scrapy

### PyPy

We recommend using the latest PyPy version. For PyPy3, only Linux installation was tested.

Most Scrapy dependencies now have binary wheels for CPython, but not for PyPy. This means that these dependencies will be built during installation. On macOS, you are likely to face an issue with building the Cryptography dependency. The solution to this problem is described [here](https://github.com/pyca/cryptography/issues/2692#issuecomment-272773481), that is to `brew install openssl` and then export the flags that this command recommends (only needed when installing Scrapy). Installing on Linux has no special issues besides installing build dependencies. Installing Scrapy with PyPy on Windows is not tested.

You can check that Scrapy is installed correctly by running `scrapy bench`. If this command gives errors such as `TypeError: ... got 2 unexpected keyword arguments`, this means that setuptools was unable to pick up one PyPy-specific dependency. To fix this issue, run `pip install 'PyPyDispatcher>=2.1.0'`.

## Troubleshooting

### AttributeError: 'module' object has no attribute 'OP\_NO\_TLSv1\_1'

After you install or upgrade Scrapy, Twisted or pyOpenSSL, you may get an exception with the following traceback:

    […]
      File "[…]/site-packages/twisted/protocols/tls.py", line 63, in <module>
        from twisted.internet._sslverify import _setAcceptableProtocols
      File "[…]/site-packages/twisted/internet/_sslverify.py", line 38, in <module>
        TLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,
    AttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'

The reason you get this exception is that your system or virtual environment has a version of pyOpenSSL that your version of Twisted does not support.

To install a version of pyOpenSSL that your version of Twisted supports, reinstall Twisted with the `tls` extra option:

    pip install twisted[tls]

For details, see [Issue \#2473](https://github.com/scrapy/scrapy/issues/2473).

---

overview.md

---

# Scrapy at a glance

Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.

Even though Scrapy was originally designed for [web scraping](https://en.wikipedia.org/wiki/Web_scraping), it can also be used to extract data using APIs (such as [Amazon Associates Web Services](https://affiliate-program.amazon.com/welcome/ecs)) or as a general purpose web crawler.

## Walk-through of an example spider

In order to show you what Scrapy brings to the table, we'll walk you through an example of a Scrapy Spider using the simplest way to run a spider.

Here's the code for a spider that scrapes famous quotes from website <https://quotes.toscrape.com>, following the pagination:

`` `python     import scrapy       class QuotesSpider(scrapy.Spider):         name = "quotes"         start_urls = [             "https://quotes.toscrape.com/tag/humor/",         ]          def parse(self, response):             for quote in response.css("div.quote"):                 yield {                     "author": quote.xpath("span/small/text()").get(),                     "text": quote.css("span.text::text").get(),                 }              next_page = response.css('li.next a::attr("href")').get()             if next_page is not None:                 yield response.follow(next_page, self.parse)  Put this in a text file, name it something like ``quotes\_spider.py`  `<span class="title-ref"> and run the spider using the :command:\`runspider</span> command:

    scrapy runspider quotes_spider.py -o quotes.jsonl

When this finishes you will have in the `quotes.jsonl` file a list of the quotes in JSON Lines format, containing the text and author, which will look like this:

    {"author": "Jane Austen", "text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"}
    {"author": "Steve Martin", "text": "\u201cA day without sunshine is like, you know, night.\u201d"}
    {"author": "Garrison Keillor", "text": "\u201cAnyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.\u201d"}
    ...

### What just happened?

When you ran the command `scrapy runspider quotes_spider.py`, Scrapy looked for a Spider definition inside it and ran it through its crawler engine.

The crawl started by making requests to the URLs defined in the `start_urls` attribute (in this case, only the URL for quotes in the *humor* category) and called the default callback method `parse`, passing the response object as an argument. In the `parse` callback, we loop through the quote elements using a CSS Selector, yield a Python dict with the extracted quote text and author, look for a link to the next page and schedule another request using the same `parse` method as callback.

Here you will notice one of the main advantages of Scrapy: requests are \[scheduled and processed asynchronously \<topics-architecture\>\](\#scheduled-and-processed-asynchronously-\<topics-architecture\>). This means that Scrapy doesn't need to wait for a request to be finished and processed, it can send another request or do other things in the meantime. This also means that other requests can keep going even if a request fails or an error happens while handling it.

While this enables you to do very fast crawls (sending multiple concurrent requests at the same time, in a fault-tolerant way) Scrapy also gives you control over the politeness of the crawl through \[a few settings \<topics-settings-ref\>\](\#a-few-settings \<topics-settings-ref\>). You can do things like setting a download delay between each request, limiting the amount of concurrent requests per domain or per IP, and even \[using an auto-throttling extension \<topics-autothrottle\>\](\#using-an-auto-throttling-extension-\<topics-autothrottle\>) that tries to figure these settings out automatically.

\> **Note** \> This is using \[feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) to generate the JSON file, you can easily change the export format (XML or CSV, for example) or the storage backend (FTP or [Amazon S3](https://aws.amazon.com/s3/), for example). You can also write an \[item pipeline \<topics-item-pipeline\>\](\#item-pipeline-\<topics-item-pipeline\>) to store the items in a database.

## What else?

You've seen how to extract and store items from a website using Scrapy, but this is just the surface. Scrapy provides a lot of powerful features for making scraping easy and efficient, such as:

  - Built-in support for \[selecting and extracting \<topics-selectors\>\](\#selecting-and-extracting-\<topics-selectors\>) data from HTML/XML sources using extended CSS selectors and XPath expressions, with helper methods for extraction using regular expressions.
  - An \[interactive shell console \<topics-shell\>\](\#interactive-shell-console-\<topics-shell\>) (IPython aware) for trying out the CSS and XPath expressions to scrape data, which is very useful when writing or debugging your spiders.
  - Built-in support for \[generating feed exports \<topics-feed-exports\>\](\#generating-feed-exports-\<topics-feed-exports\>) in multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP, S3, local filesystem)
  - Robust encoding support and auto-detection, for dealing with foreign, non-standard and broken encoding declarations.
  - \[Strong extensibility support \<extending-scrapy\>\](\#strong-extensibility-support-\<extending-scrapy\>), allowing you to plug in your own functionality using \[signals \<topics-signals\>\](\#signals-\<topics-signals\>) and a well-defined API (middlewares, \[extensions \<topics-extensions\>\](\#extensions-\<topics-extensions\>), and \[pipelines \<topics-item-pipeline\>\](\#pipelines-\<topics-item-pipeline\>)).
  - A wide range of built-in extensions and middlewares for handling:
      - cookies and session handling
      - HTTP features like compression, authentication, caching
      - user-agent spoofing
      - robots.txt
      - crawl depth restriction
      - and more
  - A \[Telnet console \<topics-telnetconsole\>\](\#telnet-console-\<topics-telnetconsole\>) for hooking into a Python console running inside your Scrapy process, to introspect and debug your crawler

<!-- end list -->

  - \* Plus other goodies like reusable spiders to crawl sites from [Sitemaps](https://www.sitemaps.org/index.html) and  
    XML/CSV feeds, a media pipeline for \[automatically downloading images \<topics-media-pipeline\>\](\#automatically-downloading-images

  - \--\<topics-media-pipeline\>) (or any other media) associated with the scraped  
    items, a caching DNS resolver, and much more\!

## What's next?

The next steps for you are to \[install Scrapy \<intro-install\>\](\#install-scrapy-\<intro-install\>), \[follow through the tutorial \<intro-tutorial\>\](\#follow-through-the-tutorial-\<intro-tutorial\>) to learn how to create a full-blown Scrapy project and [join the community](https://scrapy.org/community/). Thanks for your interest\!

---

tutorial.md

---

# Scrapy Tutorial

In this tutorial, we'll assume that Scrapy is already installed on your system. If that's not the case, see \[intro-install\](\#intro-install).

We are going to scrape [quotes.toscrape.com](https://quotes.toscrape.com/), a website that lists quotes from famous authors.

This tutorial will walk you through these tasks:

1.  Creating a new Scrapy project
2.  Writing a \[spider \<topics-spiders\>\](\#spider-\<topics-spiders\>) to crawl a site and extract data
3.  Exporting the scraped data using the command line
4.  Changing spider to recursively follow links
5.  Using spider arguments

Scrapy is written in [Python](https://www.python.org/). The more you learn about Python, the more you can get out of Scrapy.

If you're already familiar with other languages and want to learn Python quickly, the [Python Tutorial](https://docs.python.org/3/tutorial) is a good resource.

If you're new to programming and want to start with Python, the following books may be useful to you:

  - [Automate the Boring Stuff With Python](https://automatetheboringstuff.com/)
  - [How To Think Like a Computer Scientist](http://openbookproject.net/thinkcs/python/english3e/)
  - [Learn Python 3 The Hard Way](https://learnpythonthehardway.org/python3/)

You can also take a look at [this list of Python resources for non-programmers](https://wiki.python.org/moin/BeginnersGuide/NonProgrammers), as well as the [suggested resources in the learnpython-subreddit](https://www.reddit.com/r/learnpython/wiki/index#wiki_new_to_python.3F).

## Creating a project

Before you start scraping, you will have to set up a new Scrapy project. Enter a directory where you'd like to store your code and run:

    scrapy startproject tutorial

This will create a `tutorial` directory with the following contents:

    tutorial/
        scrapy.cfg            # deploy configuration file
    
        tutorial/             # project's Python module, you'll import your code from here
            __init__.py
    
            items.py          # project items definition file
    
            middlewares.py    # project middlewares file
    
            pipelines.py      # project pipelines file
    
            settings.py       # project settings file
    
            spiders/          # a directory where you'll later put your spiders
                __init__.py

## Our first Spider

Spiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass <span class="title-ref">\~scrapy.Spider</span> and define the initial requests to be made, and optionally, how to follow links in pages and parse the downloaded page content to extract data.

This is the code for our first Spider. Save it in a file named `quotes_spider.py` under the `tutorial/spiders` directory in your project:

`` `python     from pathlib import Path      import scrapy       class QuotesSpider(scrapy.Spider):         name = "quotes"          def start_requests(self):             urls = [                 "https://quotes.toscrape.com/page/1/",                 "https://quotes.toscrape.com/page/2/",             ]             for url in urls:                 yield scrapy.Request(url=url, callback=self.parse)          def parse(self, response):             page = response.url.split("/")[-2]             filename = f"quotes-{page}.html"             Path(filename).write_bytes(response.body)             self.log(f"Saved file {filename}")   As you can see, our Spider subclasses `scrapy.Spider <scrapy.Spider>` ``\` and defines some attributes and methods:

  - \`\~scrapy.Spider.name\`: identifies the Spider. It must be unique within a project, that is, you can't set the same name for different Spiders.

  - \`\~scrapy.Spider.start\_requests\`: must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.

  - \`\~scrapy.Spider.parse\`: a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of <span class="title-ref">\~scrapy.http.TextResponse</span> that holds the page content and has further helpful methods to handle it.
    
    The <span class="title-ref">\~scrapy.Spider.parse</span> method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests (<span class="title-ref">\~scrapy.Request</span>) from them.

### How to run our spider

To put our spider to work, go to the project's top level directory and run:

    scrapy crawl quotes

This command runs the spider named `quotes` that we've just added, that will send some requests for the `quotes.toscrape.com` domain. You will get an output similar to this:

    ... (omitted for brevity)
    2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
    2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
    2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)
    2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)
    2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)
    2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
    2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
    2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
    ...

Now, check the files in the current directory. You should notice that two new files have been created: *quotes-1.html* and *quotes-2.html*, with the content for the respective URLs, as our `parse` method instructs.

<div class="note">

<div class="title">

Note

</div>

If you are wondering why we haven't parsed the HTML yet, hold on, we will cover that soon.

</div>

#### What just happened under the hood?

Scrapy schedules the <span class="title-ref">scrapy.Request \<scrapy.Request\></span> objects returned by the `start_requests` method of the Spider. Upon receiving a response for each one, it instantiates <span class="title-ref">\~scrapy.http.Response</span> objects and calls the callback method associated with the request (in this case, the `parse` method) passing the response as an argument.

### A shortcut to the start\_requests method

Instead of implementing a <span class="title-ref">\~scrapy.Spider.start\_requests</span> method that generates <span class="title-ref">scrapy.Request \<scrapy.Request\></span> objects from URLs, you can just define a <span class="title-ref">\~scrapy.Spider.start\_urls</span> class attribute with a list of URLs. This list will then be used by the default implementation of <span class="title-ref">\~scrapy.Spider.start\_requests</span> to create the initial requests for your spider.

`` `python     from pathlib import Path      import scrapy       class QuotesSpider(scrapy.Spider):         name = "quotes"         start_urls = [             "https://quotes.toscrape.com/page/1/",             "https://quotes.toscrape.com/page/2/",         ]          def parse(self, response):             page = response.url.split("/")[-2]             filename = f"quotes-{page}.html"             Path(filename).write_bytes(response.body)  The `~scrapy.Spider.parse` method will be called to handle each ``<span class="title-ref"> of the requests for those URLs, even though we haven't explicitly told Scrapy to do so. This happens because </span>\~scrapy.Spider.parse\` is Scrapy's default callback method, which is called for requests without an explicitly assigned callback.

### Extracting data

The best way to learn how to extract data with Scrapy is trying selectors using the \[Scrapy shell \<topics-shell\>\](\#scrapy-shell-\<topics-shell\>). Run:

    scrapy shell 'https://quotes.toscrape.com/page/1/'

\> **Note** \> Remember to always enclose URLs in quotes when running Scrapy shell from the command line, otherwise URLs containing arguments (i.e. `&` character) will not work.

> On Windows, use double quotes instead:
> 
>     scrapy shell "https://quotes.toscrape.com/page/1/"

You will see something like:

    [ ... Scrapy log here ... ]
    2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)
    [s] Available Scrapy objects:
    [s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
    [s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
    [s]   item       {}
    [s]   request    <GET https://quotes.toscrape.com/page/1/>
    [s]   response   <200 https://quotes.toscrape.com/page/1/>
    [s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
    [s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
    [s] Useful shortcuts:
    [s]   shelp()           Shell help (print this help)
    [s]   fetch(req_or_url) Fetch request (or URL) and update local objects
    [s]   view(response)    View response in a browser

Using the shell, you can try selecting elements using [CSS](https://www.w3.org/TR/selectors) with the response object:

`` `pycon     >>> response.css("title")     [<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]  The result of running ``response.css('title')`is a list-like object called`<span class="title-ref"> </span>\~scrapy.selector.SelectorList\`, which represents a list of <span class="title-ref">\~scrapy.Selector</span> objects that wrap around XML/HTML elements and allow you to run further queries to refine the selection or extract the data.

To extract the text from the title above, you can do:

`` `pycon     >>> response.css("title::text").getall()     ['Quotes to Scrape']  There are two things to note here: one is that we've added ``::text`to the`<span class="title-ref"> CSS query, to mean we want to select only the text elements directly inside </span><span class="title-ref">\<title\></span><span class="title-ref"> element. If we don't specify </span><span class="title-ref">::text</span>\`, we'd get the full title element, including its tags:

`` `pycon     >>> response.css("title").getall()     ['<title>Quotes to Scrape</title>']  The other thing is that the result of calling ``.getall()`is a list: it is`\` possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do:

`` `pycon     >>> response.css("title::text").get()     'Quotes to Scrape'  As an alternative, you could've written:  .. code-block:: pycon      >>> response.css("title::text")[0].get()     'Quotes to Scrape'  Accessing an index on a `~scrapy.selector.SelectorList` instance will ``<span class="title-ref"> raise an \`IndexError</span> exception if there are no results:

`` `pycon     >>> response.css("noelement")[0].get()     Traceback (most recent call last):     ...     IndexError: list index out of range  You might want to use ``.get()`directly on the`<span class="title-ref"> </span>\~scrapy.selector.SelectorList\` instance instead, which returns `None` if there are no results:

`` `pycon     >>> response.css("noelement").get()  There's a lesson here: for most scraping code, you want it to be resilient to ``\` errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get **some** data.

Besides the <span class="title-ref">\~scrapy.selector.SelectorList.getall</span> and <span class="title-ref">\~scrapy.selector.SelectorList.get</span> methods, you can also use the <span class="title-ref">\~scrapy.selector.SelectorList.re</span> method to extract using \[regular expressions \<library/re\>\](regular expressions \<library/re\>.md):

`` `pycon     >>> response.css("title::text").re(r"Quotes.*")     ['Quotes to Scrape']     >>> response.css("title::text").re(r"Q\w+")     ['Quotes']     >>> response.css("title::text").re(r"(\w+) to (\w+)")     ['Quotes', 'Scrape']  In order to find the proper CSS selectors to use, you might find it useful to open ``<span class="title-ref"> the response page from the shell in your web browser using </span><span class="title-ref">view(response)</span>\`. You can use your browser's developer tools to inspect the HTML and come up with a selector (see \[topics-developer-tools\](\#topics-developer-tools)).

[Selector Gadget](https://selectorgadget.com/) is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers.

#### XPath: a brief intro

Besides [CSS](https://www.w3.org/TR/selectors), Scrapy selectors also support using [XPath](https://www.w3.org/TR/xpath-10/) expressions:

`` `pycon     >>> response.xpath("//title")     [<Selector query='//title' data='<title>Quotes to Scrape</title>'>]     >>> response.xpath("//title/text()").get()     'Quotes to Scrape'  XPath expressions are very powerful, and are the foundation of Scrapy ``\` Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read the text representation of the selector objects in the shell closely.

While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you're able to select things like: *the link that contains the text "Next Page"*. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier.

We won't cover much of XPath here, but you can read more about \[using XPath with Scrapy Selectors here \<topics-selectors\>\](\#using-xpath with-scrapy-selectors-here-\<topics-selectors\>). To learn more about XPath, we recommend [this tutorial to learn XPath through examples](http://zvon.org/comp/r/tut-XPath_1.html), and [this tutorial to learn "how to think in XPath"](http://plasmasturm.org/log/xpath101/).

#### Extracting quotes and authors

Now that you know a bit about selection and extraction, let's complete our spider by writing the code to extract the quotes from the web page.

Each quote in <https://quotes.toscrape.com> is represented by HTML elements that look like this:

`` `html     <div class="quote">         <span class="text">“The world as we have created it is a process of our         thinking. It cannot be changed without changing our thinking.”</span>         <span>             by <small class="author">Albert Einstein</small>             <a href="/author/Albert-Einstein">(about)</a>         </span>         <div class="tags">             Tags:             <a class="tag" href="/tag/change/page/1/">change</a>             <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>             <a class="tag" href="/tag/thinking/page/1/">thinking</a>             <a class="tag" href="/tag/world/page/1/">world</a>         </div>     </div>  Let's open up scrapy shell and play a bit to find out how to extract the data ``\` we want:

    scrapy shell 'https://quotes.toscrape.com'

We get a list of selectors for the quote HTML elements with:

`` `pycon     >>> response.css("div.quote")     [<Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,     <Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,     ...]  Each of the selectors returned by the query above allows us to run further ``\` queries over their sub-elements. Let's assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote:

`` `pycon     >>> quote = response.css("div.quote")[0]  Now, let's extract the ``text`,`author`and`tags`from that quote`<span class="title-ref"> using the </span><span class="title-ref">quote</span>\` object we just created:

`` `pycon     >>> text = quote.css("span.text::text").get()     >>> text     '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'     >>> author = quote.css("small.author::text").get()     >>> author     'Albert Einstein'  Given that the tags are a list of strings, we can use the ``.getall()`method`\` to get all of them:

`` `pycon     >>> tags = quote.css("div.tags a.tag::text").getall()     >>> tags     ['change', 'deep-thoughts', 'thinking', 'world']  .. invisible-code-block: python    from sys import version_info  Having figured out how to extract each bit, we can now iterate over all the ``\` quote elements and put them together into a Python dictionary:

`` `pycon     >>> for quote in response.css("div.quote"):     ...     text = quote.css("span.text::text").get()     ...     author = quote.css("small.author::text").get()     ...     tags = quote.css("div.tags a.tag::text").getall()     ...     print(dict(text=text, author=author, tags=tags))     ...     {'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}     {'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}     ...  Extracting data in our spider ``\` -----------------------------

Let's get back to our spider. Until now, it hasn't extracted any data in particular, just saving the whole HTML page to a local file. Let's integrate the extraction logic above into our spider.

A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the `yield` Python keyword in the callback, as you can see below:

`` `python     import scrapy       class QuotesSpider(scrapy.Spider):         name = "quotes"         start_urls = [             "https://quotes.toscrape.com/page/1/",             "https://quotes.toscrape.com/page/2/",         ]          def parse(self, response):             for quote in response.css("div.quote"):                 yield {                     "text": quote.css("span.text::text").get(),                     "author": quote.css("small.author::text").get(),                     "tags": quote.css("div.tags a.tag::text").getall(),                 }  To run this spider, exit the scrapy shell by entering::      quit()  Then, run::     scrapy crawl quotes  Now, it should output the extracted data with the log::      2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>     {'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}     2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>     {'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': "“I have not failed. I've just found 10,000 ways that won't work.”"}   .. _storing-data:  Storing the scraped data ``\` ========================

The simplest way to store the scraped data is by using \[Feed exports \<topics-feed-exports\>\](\#feed-exports \<topics-feed-exports\>), with the following command:

    scrapy crawl quotes -O quotes.json

That will generate a `quotes.json` file containing all scraped items, serialized in [JSON](https://en.wikipedia.org/wiki/JSON).

The `-O` command-line switch overwrites any existing file; use `-o` instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as [JSON Lines](https://jsonlines.org):

    scrapy crawl quotes -o quotes.jsonl

The [JSON Lines](https://jsonlines.org) format is useful because it's stream-like, so you can easily append new records to it. It doesn't have the same problem as JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like [JQ](https://stedolan.github.io/jq) to help do that at the command-line.

In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an \[Item Pipeline \<topics-item-pipeline\>\](\#item-pipeline-\<topics-item-pipeline\>). A placeholder file for Item Pipelines has been set up for you when the project is created, in `tutorial/pipelines.py`. Though you don't need to implement any item pipelines if you just want to store the scraped items.

## Following links

Let's say, instead of just scraping the stuff from the first two pages from <https://quotes.toscrape.com>, you want quotes from all the pages in the website.

Now that you know how to extract data from pages, let's see how to follow links from them.

The first thing to do is extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup:

`` `html     <ul class="pager">         <li class="next">             <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>         </li>     </ul>  We can try extracting it in the shell:  >>> response.css('li.next a').get() ``\` '\<a href="/page/2/"\>Next \<span aria-hidden="true"\>→\</span\>\</a\>'

This gets the anchor element, but we want the attribute `href`. For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this:

`` `pycon     >>> response.css("li.next a::attr(href)").get()     '/page/2/'  There is also an ``attrib`property available`\` (see \[selecting-attributes\](\#selecting-attributes) for more):

`` `pycon     >>> response.css("li.next a").attrib["href"]     '/page/2/'  Now let's see our spider, modified to recursively follow the link to the next ``\` page, extracting data from it:

`` `python     import scrapy       class QuotesSpider(scrapy.Spider):         name = "quotes"         start_urls = [             "https://quotes.toscrape.com/page/1/",         ]          def parse(self, response):             for quote in response.css("div.quote"):                 yield {                     "text": quote.css("span.text::text").get(),                     "author": quote.css("small.author::text").get(),                     "tags": quote.css("div.tags a.tag::text").getall(),                 }              next_page = response.css("li.next a::attr(href)").get()             if next_page is not None:                 next_page = response.urljoin(next_page)                 yield scrapy.Request(next_page, callback=self.parse)   Now, after extracting the data, the ``parse()`method looks for the link to`<span class="title-ref"> the next page, builds a full absolute URL using the </span>\~scrapy.http.Response.urljoin\` method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages.

What you see here is Scrapy's mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes.

Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it's visiting.

In our example, it creates a sort of loop, following all the links to the next page until it doesn't find one -- handy for crawling blogs, forums and other sites with pagination.

### A shortcut for creating Requests

As a shortcut for creating Request objects you can use \`response.follow \<scrapy.http.TextResponse.follow\>\`:

`` `python     import scrapy       class QuotesSpider(scrapy.Spider):         name = "quotes"         start_urls = [             "https://quotes.toscrape.com/page/1/",         ]          def parse(self, response):             for quote in response.css("div.quote"):                 yield {                     "text": quote.css("span.text::text").get(),                     "author": quote.css("span small::text").get(),                     "tags": quote.css("div.tags a.tag::text").getall(),                 }              next_page = response.css("li.next a::attr(href)").get()             if next_page is not None:                 yield response.follow(next_page, callback=self.parse)  Unlike scrapy.Request, ``response.follow`supports relative URLs directly - no`<span class="title-ref"> need to call urljoin. Note that </span><span class="title-ref">response.follow</span>\` just returns a Request instance; you still have to yield this Request.

You can also pass a selector to `response.follow` instead of a string; this selector should extract necessary attributes:

`` `python     for href in response.css("ul.pager a::attr(href)"):         yield response.follow(href, callback=self.parse)  For ``\<a\>`elements there is a shortcut:`response.follow`uses their href`\` attribute automatically. So the code can be shortened further:

`` `python     for a in response.css("ul.pager a"):         yield response.follow(a, callback=self.parse)  To create multiple requests from an iterable, you can use ``<span class="title-ref"> \`response.follow\_all \<scrapy.http.TextResponse.follow\_all\></span> instead:

`` `python     anchors = response.css("ul.pager a")     yield from response.follow_all(anchors, callback=self.parse)  or, shortening it further:  .. code-block:: python      yield from response.follow_all(css="ul.pager a", callback=self.parse)  .. skip: end   More examples and patterns ``\` --------------------------

Here is another spider that illustrates callbacks and following links, this time for scraping author information:

`` `python     import scrapy       class AuthorSpider(scrapy.Spider):         name = "author"          start_urls = ["https://quotes.toscrape.com/"]          def parse(self, response):             author_page_links = response.css(".author + a")             yield from response.follow_all(author_page_links, self.parse_author)              pagination_links = response.css("li.next a")             yield from response.follow_all(pagination_links, self.parse)          def parse_author(self, response):             def extract_with_css(query):                 return response.css(query).get(default="").strip()              yield {                 "name": extract_with_css("h3.author-title::text"),                 "birthdate": extract_with_css(".author-born-date::text"),                 "bio": extract_with_css(".author-description::text"),             }  This spider will start from the main page, it will follow all the links to the ``<span class="title-ref"> authors pages calling the </span><span class="title-ref">parse\_author</span><span class="title-ref"> callback for each of them, and also the pagination links with the </span><span class="title-ref">parse</span>\` callback as we saw before.

Here we're passing callbacks to <span class="title-ref">response.follow\_all \<scrapy.http.TextResponse.follow\_all\></span> as positional arguments to make the code shorter; it also works for <span class="title-ref">\~scrapy.Request</span>.

The `parse_author` callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data.

Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don't need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured in the `DUPEFILTER_CLASS` setting.

Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy.

As yet another example spider that leverages the mechanism of following links, check out the <span class="title-ref">\~scrapy.spiders.CrawlSpider</span> class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it.

Also, a common pattern is to build an item with data from more than one page, using a \[trick to pass additional data to the callbacks \<topics-request-response-ref-request-callback-arguments\>\](\#trick-to-pass-additional-data-to-the-callbacks \<topics-request-response-ref-request-callback-arguments\>).

## Using spider arguments

You can provide command line arguments to your spiders by using the `-a` option when running them:

    scrapy crawl quotes -O quotes-humor.json -a tag=humor

These arguments are passed to the Spider's `__init__` method and become spider attributes by default.

In this example, the value provided for the `tag` argument will be available via `self.tag`. You can use this to make your spider fetch only quotes with a specific tag, building the URL based on the argument:

`` `python     import scrapy       class QuotesSpider(scrapy.Spider):         name = "quotes"          def start_requests(self):             url = "https://quotes.toscrape.com/"             tag = getattr(self, "tag", None)             if tag is not None:                 url = url + "tag/" + tag             yield scrapy.Request(url, self.parse)          def parse(self, response):             for quote in response.css("div.quote"):                 yield {                     "text": quote.css("span.text::text").get(),                     "author": quote.css("small.author::text").get(),                 }              next_page = response.css("li.next a::attr(href)").get()             if next_page is not None:                 yield response.follow(next_page, self.parse)   If you pass the ``tag=humor`argument to this spider, you'll notice that it`<span class="title-ref"> will only visit URLs from the </span><span class="title-ref">humor</span><span class="title-ref"> tag, such as </span><span class="title-ref">https://quotes.toscrape.com/tag/humor</span>\`.

You can \[learn more about handling spider arguments here \<spiderargs\>\](\#learn-more-about-handling-spider-arguments-here-\<spiderargs\>).

## Next steps

This tutorial covered only the basics of Scrapy, but there's a lot of other features not mentioned here. Check the \[topics-whatelse\](\#topics-whatelse) section in the \[intro-overview\](\#intro-overview) chapter for a quick overview of the most important ones.

You can continue from the section \[section-basics\](\#section-basics) to know more about the command-line tool, spiders, selectors and other things the tutorial hasn't covered like modeling the scraped data. If you'd prefer to play with an example project, check the \[intro-examples\](\#intro-examples) section.

---

news.md

---

# Release notes

## Scrapy VERSION (YYYY-MM-DD)

### New features

  - If `SPIDER_LOADER_WARN_ONLY` is set to `True`, `SpiderLoader` does not raise <span class="title-ref">SyntaxError</span> but emits a warning instead.

### Deprecations

  - <span class="title-ref">scrapy.core.downloader.Downloader.\_get\_slot\_key</span> is deprecated, use <span class="title-ref">scrapy.core.downloader.Downloader.get\_slot\_key</span> instead. (`6340`)

## Scrapy 2.11.2 (2024-05-14)

### Security bug fixes

  - Redirects to non-HTTP protocols are no longer followed. Please, see the [23j4-mw76-5v7h security advisory]() for more information. (`457`)
  - The `Authorization` header is now dropped on redirects to a different scheme (`http://` or `https://`) or port, even if the domain is the same. Please, see the [4qqq-9vqf-3h3f security advisory]() for more information.
  - When using system proxy settings that are different for `http://` and `https://`, redirects to a different URL scheme will now also trigger the corresponding change in proxy settings for the redirected request. Please, see the [jm3v-qxmh-hxwv security advisory]() for more information. (`767`)
  - <span class="title-ref">Spider.allowed\_domains \<scrapy.Spider.allowed\_domains\></span> is now enforced for all requests, and not only requests from spider callbacks. (`1042`, `2241`, `6358`)
  - <span class="title-ref">\~scrapy.utils.iterators.xmliter\_lxml</span> no longer resolves XML entities. (`6265`)
  - [defusedxml]() is now used to make <span class="title-ref">scrapy.http.request.rpc.XmlRpcRequest</span> more secure. (`6250`, `6251`)

### Bug fixes

  - Restored support for [brotlipy](https://github.com/python-hyper/brotlipy/), which had been dropped in Scrapy 2.11.1 in favor of [brotli](). (`6261`)
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    brotlipy is deprecated, both in Scrapy and upstream. Use brotli instead if you can.
    
    </div>

  - Make `METAREFRESH_IGNORE_TAGS` `["noscript"]` by default. This prevents <span class="title-ref">\~scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware</span> from following redirects that would not be followed by web browsers with JavaScript enabled. (`6342`, `6347`)

<!-- end list -->

  - \- During \[feed export \<topics-feed-exports\>\](\#feed-export-\<topics-feed-exports\>), do not close the  
    underlying file from \[built-in post-processing plugins \<builtin-plugins\>\](\#built-in-post-processing-plugins

  - \----\<builtin-plugins\>).  
    (`5932`, `6178`, `6239`)

  - \- <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span>  
    now properly applies the `unique` and `canonicalize` parameters. (`3273`, `6221`)

  - \- Do not initialize the scheduler disk queue if `JOBDIR` is an empty  
    string. (`6121`, `6124`)

  - \- Fix <span class="title-ref">Spider.logger \<scrapy.Spider.logger\></span> not logging custom extra  
    information. (`6323`, `6324`)

  - \- `robots.txt` files with a non-UTF-8 encoding no longer prevent parsing  
    the UTF-8-compatible (e.g. ASCII) parts of the document. (`6292`, `6298`)

  - \- <span class="title-ref">scrapy.http.cookies.WrappedRequest.get\_header</span> no longer raises an  
    exception if `default` is `None`. (`6308`, `6310`)

  - \- <span class="title-ref">\~scrapy.selector.Selector</span> now uses  
    <span class="title-ref">scrapy.utils.response.get\_base\_url</span> to determine the base URL of a given <span class="title-ref">\~scrapy.http.Response</span>. (`6265`)

  - \- The <span class="title-ref">media\_to\_download</span> method of \[media pipelines  
    \<topics-media-pipeline\>\](\#media-pipelines

  - \----\<topics-media-pipeline\>) now logs exceptions before stripping them.  
    (`5067`, `5068`)

  - \- When passing a callback to the `parse` command, build the callback  
    callable with the right signature. (`6182`)

### Documentation

  - Add a FAQ entry about \[creating blank requests \<faq-blank-request\>\](\#creating-blank-requests-\<faq-blank-request\>). (`6203`, `6208`)
  - Document that <span class="title-ref">scrapy.selector.Selector.type</span> can be `"json"`. (`6328`, `6334`)

### Quality assurance

  - Make builds reproducible. (`5019`, `6322`)
  - Packaging and test fixes. (`6286`, `6290`, `6312`, `6316`, `6344`)

## Scrapy 2.11.1 (2024-02-14)

Highlights:

  - Security bug fixes.
  - Support for Twisted \>= 23.8.0.
  - Documentation improvements.

### Security bug fixes

  - Addressed [ReDoS vulnerabilities]():
    
      - `scrapy.utils.iterators.xmliter` is now deprecated in favor of <span class="title-ref">\~scrapy.utils.iterators.xmliter\_lxml</span>, which <span class="title-ref">\~scrapy.spiders.XMLFeedSpider</span> now uses.
        
        To minimize the impact of this change on existing code, <span class="title-ref">\~scrapy.utils.iterators.xmliter\_lxml</span> now supports indicating the node namespace with a prefix in the node name, and big files with highly nested trees when using libxml2 2.7+.
    
      - Fixed regular expressions in the implementation of the <span class="title-ref">\~scrapy.utils.response.open\_in\_browser</span> function.
    
    Please, see the [cc65-xxvf-f7r9 security advisory]() for more information.

  - `DOWNLOAD_MAXSIZE` and `DOWNLOAD_WARNSIZE` now also apply to the decompressed response body. Please, see the [7j7m-v7m3-jqm7 security advisory]() for more information.

  - Also in relation with the [7j7m-v7m3-jqm7 security advisory](), the deprecated `scrapy.downloadermiddlewares.decompression` module has been removed.

  - The `Authorization` header is now dropped on redirects to a different domain. Please, see the [cw9j-q3vf-hrrv security advisory]() for more information.

### Modified requirements

  - The Twisted dependency is no longer restricted to \< 23.8.0. (`6024`, `6064`, `6142`)

### Bug fixes

  - The OS signal handling code was refactored to no longer use private Twisted functions. (`6024`, `6064`, `6112`)

### Documentation

  - Improved documentation for <span class="title-ref">\~scrapy.crawler.Crawler</span> initialization changes made in the 2.11.0 release. (`6057`, `6147`)
  - Extended documentation for <span class="title-ref">Request.meta \<scrapy.http.Request.meta\></span>. (`5565`)
  - Fixed the `dont_merge_cookies` documentation. (`5936`, `6077`)

<!-- end list -->

  - \- Added a link to Zyte's export guides to the \[feed exports  
    \<topics-feed-exports\>\](\#feed-exports

\----\<topics-feed-exports\>) documentation. (`6183`)

  - Added a missing note about backward-incompatible changes in <span class="title-ref">\~scrapy.exporters.PythonItemExporter</span> to the 2.11.0 release notes. (`6060`, `6081`)
  - Added a missing note about removing the deprecated `scrapy.utils.boto.is_botocore()` function to the 2.8.0 release notes. (`6056`, `6061`)
  - Other documentation improvements. (`6128`, `6144`, `6163`, `6190`, `6192`)

### Quality assurance

  - Added Python 3.12 to the CI configuration, re-enabled tests that were disabled when the pre-release support was added. (`5985`, `6083`, `6098`)
  - Fixed a test issue on PyPy 7.3.14. (`6204`, `6205`)

## Scrapy 2.11.0 (2023-09-18)

Highlights:

  - \- Spiders can now modify \[settings \<topics-settings\>\](\#settings-\<topics-settings\>) in their  
    <span class="title-ref">\~scrapy.Spider.from\_crawler</span> methods, e.g. based on \[spider arguments \<spiderargs\>\](\#spider

\----arguments-\<spiderargs\>).

  - Periodic logging of stats.

### Backward-incompatible changes

  - Most of the initialization of <span class="title-ref">scrapy.crawler.Crawler</span> instances is now done in <span class="title-ref">\~scrapy.crawler.Crawler.crawl</span>, so the state of instances before that method is called is now different compared to older Scrapy versions. We do not recommend using the <span class="title-ref">\~scrapy.crawler.Crawler</span> instances before <span class="title-ref">\~scrapy.crawler.Crawler.crawl</span> is called. (`6038`)
  - <span class="title-ref">scrapy.Spider.from\_crawler</span> is now called before the initialization of various components previously initialized in <span class="title-ref">scrapy.crawler.Crawler.\_\_init\_\_</span> and before the settings are finalized and frozen. This change was needed to allow changing the settings in <span class="title-ref">scrapy.Spider.from\_crawler</span>. If you want to access the final setting values and the initialized <span class="title-ref">\~scrapy.crawler.Crawler</span> attributes in the spider code as early as possible you can do this in <span class="title-ref">\~scrapy.Spider.start\_requests</span> or in a handler of the `engine_started` signal. (`6038`)
  - The <span class="title-ref">TextResponse.json \<scrapy.http.TextResponse.json\></span> method now requires the response to be in a valid JSON encoding (UTF-8, UTF-16, or UTF-32). If you need to deal with JSON documents in an invalid encoding, use `json.loads(response.text)` instead. (`6016`)
  - <span class="title-ref">\~scrapy.exporters.PythonItemExporter</span> used the binary output by default but it no longer does. (`6006`, `6007`)

### Deprecation removals

  - Removed the binary export mode of <span class="title-ref">\~scrapy.exporters.PythonItemExporter</span>, deprecated in Scrapy 1.1.0. (`6006`, `6007`)
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    If you are using this Scrapy version on Scrapy Cloud with a stack that includes an older Scrapy version and get a "TypeError: Unexpected options: binary" error, you may need to add `scrapinghub-entrypoint-scrapy >= 0.14.1` to your project requirements or switch to a stack that includes Scrapy 2.11.
    
    </div>

  - Removed the `CrawlerRunner.spiders` attribute, deprecated in Scrapy 1.0.0, use <span class="title-ref">CrawlerRunner.spider\_loader \<scrapy.crawler.CrawlerRunner.spider\_loader\></span> instead. (`6010`)

  - The <span class="title-ref">scrapy.utils.response.response\_httprepr</span> function, deprecated in Scrapy 2.6.0, has now been removed. (`6111`)

### Deprecations

  - Running <span class="title-ref">\~scrapy.crawler.Crawler.crawl</span> more than once on the same <span class="title-ref">scrapy.crawler.Crawler</span> instance is now deprecated. (`1587`, `6040`)

### New features

  - \- Spiders can now modify settings in their  
    <span class="title-ref">\~scrapy.Spider.from\_crawler</span> method, e.g. based on \[spider arguments \<spiderargs\>\](\#spider

  - \----arguments-\<spiderargs\>). (`1305`, `1580`, `2392`,  
    `3663`, `6038`)

  - \- Added the <span class="title-ref">\~scrapy.extensions.periodic\_log.PeriodicLog</span> extension  
    which can be enabled to log stats and/or their differences periodically. (`5926`)

  - \- Optimized the memory usage in <span class="title-ref">TextResponse.json \<scrapy.http.TextResponse.json\></span> by removing unnecessary body decoding.  
    (`5968`, `6016`)

  - \- Links to `.webp` files are now ignored by \[link extractors  
    \<topics-link-extractors\>\](\#link-extractors

\----\<topics-link-extractors\>). (`6021`)

### Bug fixes

  - Fixed logging enabled add-ons. (`6036`)
  - Fixed <span class="title-ref">\~scrapy.mail.MailSender</span> producing invalid message bodies when the `charset` argument is passed to <span class="title-ref">\~scrapy.mail.MailSender.send</span>. (`5096`, `5118`)
  - Fixed an exception when accessing `self.EXCEPTIONS_TO_RETRY` from a subclass of <span class="title-ref">\~scrapy.downloadermiddlewares.retry.RetryMiddleware</span>. (`6049`, `6050`)
  - <span class="title-ref">scrapy.settings.BaseSettings.getdictorlist</span>, used to parse `FEED_EXPORT_FIELDS`, now handles tuple values. (`6011`, `6013`)
  - Calls to `datetime.utcnow()`, no longer recommended to be used, have been replaced with calls to `datetime.now()` with a timezone. (`6014`)

### Documentation

  - Updated a deprecated function call in a pipeline example. (`6008`, `6009`)

### Quality assurance

  - Extended typing hints. (`6003`, `6005`, `6031`, `6034`)
  - Pinned [brotli]() to 1.0.9 for the PyPy tests as 1.1.0 breaks them. (`6044`, `6045`)
  - Other CI and pre-commit improvements. (`6002`, `6013`, `6046`)

## Scrapy 2.10.1 (2023-08-30)

Marked `Twisted >= 23.8.0` as unsupported. (`6024`, `6026`)

## Scrapy 2.10.0 (2023-08-04)

Highlights:

  - Added Python 3.12 support, dropped Python 3.7 support.
  - The new add-ons framework simplifies configuring 3rd-party components that support it.
  - Exceptions to retry can now be configured.
  - Many fixes and improvements for feed exports.

### Modified requirements

  - Dropped support for Python 3.7. (`5953`)
  - Added support for the upcoming Python 3.12. (`5984`)
  - Minimum versions increased for these dependencies:
      - [lxml](https://lxml.de/): 4.3.0 → 4.4.1
      - [cryptography](https://cryptography.io/en/latest/): 3.4.6 → 36.0.0
  - `pkg_resources` is no longer used. (`5956`, `5958`)
  - [boto3](https://github.com/boto/boto3) is now recommended instead of [botocore](https://github.com/boto/botocore) for exporting to S3. (`5833`).

### Backward-incompatible changes

  - The value of the `FEED_STORE_EMPTY` setting is now `True` instead of `False`. In earlier Scrapy versions empty files were created even when this setting was `False` (which was a bug that is now fixed), so the new default should keep the old behavior. (`872`, `5847`)

### Deprecation removals

  - When a function is assigned to the `FEED_URI_PARAMS` setting, returning `None` or modifying the `params` input parameter, deprecated in Scrapy 2.6, is no longer supported. (`5994`, `5996`)
  - The `scrapy.utils.reqser` module, deprecated in Scrapy 2.6, is removed. (`5994`, `5996`)
  - The `scrapy.squeues` classes `PickleFifoDiskQueueNonRequest`, `PickleLifoDiskQueueNonRequest`, `MarshalFifoDiskQueueNonRequest`, and `MarshalLifoDiskQueueNonRequest`, deprecated in Scrapy 2.6, are removed. (`5994`, `5996`)
  - The property `open_spiders` and the methods `has_capacity` and `schedule` of <span class="title-ref">scrapy.core.engine.ExecutionEngine</span>, deprecated in Scrapy 2.6, are removed. (`5994`, `5998`)
  - Passing a `spider` argument to the <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.spider\_is\_idle</span>, <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.crawl</span> and <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.download</span> methods of <span class="title-ref">scrapy.core.engine.ExecutionEngine</span>, deprecated in Scrapy 2.6, is no longer supported. (`5994`, `5998`)

### Deprecations

  - <span class="title-ref">scrapy.utils.datatypes.CaselessDict</span> is deprecated, use <span class="title-ref">scrapy.utils.datatypes.CaseInsensitiveDict</span> instead. (`5146`)
  - Passing the `custom` argument to <span class="title-ref">scrapy.utils.conf.build\_component\_list</span> is deprecated, it was used in the past to merge `FOO` and `FOO_BASE` setting values but now Scrapy uses <span class="title-ref">scrapy.settings.BaseSettings.getwithbase</span> to do the same. Code that uses this argument and cannot be switched to `getwithbase()` can be switched to merging the values explicitly. (`5726`, `5923`)

### New features

  - Added support for \[Scrapy add-ons \<topics-addons\>\](\#scrapy-add-ons-\<topics-addons\>). (`5950`)
  - Added the `RETRY_EXCEPTIONS` setting that configures which exceptions will be retried by <span class="title-ref">\~scrapy.downloadermiddlewares.retry.RetryMiddleware</span>. (`2701`, `5929`)
  - Added the possiiblity to close the spider if no items were produced in the specified time, configured by `CLOSESPIDER_TIMEOUT_NO_ITEM`. (`5979`)
  - Added support for the `AWS_REGION_NAME` setting to feed exports. (`5980`)
  - Added support for using <span class="title-ref">pathlib.Path</span> objects that refer to absolute Windows paths in the `FEEDS` setting. (`5939`)

### Bug fixes

  - Fixed creating empty feeds even with `FEED_STORE_EMPTY=False`. (`872`, `5847`)
  - Fixed using absolute Windows paths when specifying output files. (`5969`, `5971`)
  - Fixed problems with uploading large files to S3 by switching to multipart uploads (requires [boto3](https://github.com/boto/boto3)). (`960`, `5735`, `5833`)
  - Fixed the JSON exporter writing extra commas when some exceptions occur. (`3090`, `5952`)
  - Fixed the "read of closed file" error in the CSV exporter. (`5043`, `5705`)
  - Fixed an error when a component added by the class object throws <span class="title-ref">\~scrapy.exceptions.NotConfigured</span> with a message. (`5950`, `5992`)
  - Added the missing <span class="title-ref">scrapy.settings.BaseSettings.pop</span> method. (`5959`, `5960`, `5963`)
  - Added <span class="title-ref">\~scrapy.utils.datatypes.CaseInsensitiveDict</span> as a replacement for <span class="title-ref">\~scrapy.utils.datatypes.CaselessDict</span> that fixes some API inconsistencies. (`5146`)

### Documentation

  - Documented <span class="title-ref">scrapy.Spider.update\_settings</span>. (`5745`, `5846`)
  - Documented possible problems with early Twisted reactor installation and their solutions. (`5981`, `6000`)
  - Added examples of making additional requests in callbacks. (`5927`)
  - Improved the feed export docs. (`5579`, `5931`)
  - Clarified the docs about request objects on redirection. (`5707`, `5937`)

### Quality assurance

  - Added support for running tests against the installed Scrapy version. (`4914`, `5949`)
  - Extended typing hints. (`5925`, `5977`)
  - Fixed the `test_utils_asyncio.AsyncioTest.test_set_asyncio_event_loop` test. (`5951`)
  - Fixed the `test_feedexport.BatchDeliveriesTest.test_batch_path_differ` test on Windows. (`5847`)
  - Enabled CI runs for Python 3.11 on Windows. (`5999`)
  - Simplified skipping tests that depend on `uvloop`. (`5984`)
  - Fixed the `extra-deps-pinned` tox env. (`5948`)
  - Implemented cleanups. (`5965`, `5986`)

## Scrapy 2.9.0 (2023-05-08)

Highlights:

  - Per-domain download settings.
  - Compatibility with new [cryptography](https://cryptography.io/en/latest/) and new [parsel](https://github.com/scrapy/parsel).
  - JMESPath selectors from the new [parsel](https://github.com/scrapy/parsel).
  - Bug fixes.

### Deprecations

  - <span class="title-ref">scrapy.extensions.feedexport.\_FeedSlot</span> is renamed to <span class="title-ref">scrapy.extensions.feedexport.FeedSlot</span> and the old name is deprecated. (`5876`)

### New features

  - Settings corresponding to `DOWNLOAD_DELAY`, `CONCURRENT_REQUESTS_PER_DOMAIN` and `RANDOMIZE_DOWNLOAD_DELAY` can now be set on a per-domain basis via the new `DOWNLOAD_SLOTS` setting. (`5328`)
  - Added <span class="title-ref">TextResponse.jmespath</span>, a shortcut for JMESPath selectors available since [parsel](https://github.com/scrapy/parsel) 1.8.1. (`5894`, `5915`)
  - Added `feed_slot_closed` and `feed_exporter_closed` signals. (`5876`)
  - Added <span class="title-ref">scrapy.utils.request.request\_to\_curl</span>, a function to produce a curl command from a <span class="title-ref">\~scrapy.Request</span> object. (`5892`)
  - Values of `FILES_STORE` and `IMAGES_STORE` can now be <span class="title-ref">pathlib.Path</span> instances. (`5801`)

### Bug fixes

  - Fixed a warning with Parsel 1.8.1+. (`5903`, `5918`)
  - Fixed an error when using feed postprocessing with S3 storage. (`5500`, `5581`)
  - Added the missing <span class="title-ref">scrapy.settings.BaseSettings.setdefault</span> method. (`5811`, `5821`)
  - Fixed an error when using [cryptography](https://cryptography.io/en/latest/) 40.0.0+ and `DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING` is enabled. (`5857`, `5858`)
  - The checksums returned by <span class="title-ref">\~scrapy.pipelines.files.FilesPipeline</span> for files on Google Cloud Storage are no longer Base64-encoded. (`5874`, `5891`)
  - <span class="title-ref">scrapy.utils.request.request\_from\_curl</span> now supports $-prefixed string values for the curl `--data-raw` argument, which are produced by browsers for data that includes certain symbols. (`5899`, `5901`)
  - The `parse` command now also works with async generator callbacks. (`5819`, `5824`)
  - The `genspider` command now properly works with HTTPS URLs. (`3553`, `5808`)
  - Improved handling of asyncio loops. (`5831`, `5832`)
  - <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> now skips certain malformed URLs instead of raising an exception. (`5881`)
  - <span class="title-ref">scrapy.utils.python.get\_func\_args</span> now supports more types of callables. (`5872`, `5885`)
  - Fixed an error when processing non-UTF8 values of `Content-Type` headers. (`5914`, `5917`)
  - Fixed an error breaking user handling of send failures in <span class="title-ref">scrapy.mail.MailSender.send()</span>. (`1611`, `5880`)

### Documentation

  - Expanded contributing docs. (`5109`, `5851`)
  - Added [blacken-docs](https://github.com/adamchainz/blacken-docs) to pre-commit and reformatted the docs with it. (`5813`, `5816`)
  - Fixed a JS issue. (`5875`, `5877`)
  - Fixed `make htmlview`. (`5878`, `5879`)
  - Fixed typos and other small errors. (`5827`, `5839`, `5883`, `5890`, `5895`, `5904`)

### Quality assurance

  - Extended typing hints. (`5805`, `5889`, `5896`)
  - Tests for most of the examples in the docs are now run as a part of CI, found problems were fixed. (`5816`, `5826`, `5919`)
  - Removed usage of deprecated Python classes. (`5849`)
  - Silenced `include-ignored` warnings from coverage. (`5820`)
  - Fixed a random failure of the `test_feedexport.test_batch_path_differ` test. (`5855`, `5898`)
  - Updated docstrings to match output produced by [parsel](https://github.com/scrapy/parsel) 1.8.1 so that they don't cause test failures. (`5902`, `5919`)
  - Other CI and pre-commit improvements. (`5802`, `5823`, `5908`)

## Scrapy 2.8.0 (2023-02-02)

This is a maintenance release, with minor features, bug fixes, and cleanups.

### Deprecation removals

  - The `scrapy.utils.gz.read1` function, deprecated in Scrapy 2.0, has now been removed. Use the <span class="title-ref">\~io.BufferedIOBase.read1</span> method of <span class="title-ref">\~gzip.GzipFile</span> instead. (`5719`)
  - The `scrapy.utils.python.to_native_str` function, deprecated in Scrapy 2.0, has now been removed. Use <span class="title-ref">scrapy.utils.python.to\_unicode</span> instead. (`5719`)
  - The `scrapy.utils.python.MutableChain.next` method, deprecated in Scrapy 2.0, has now been removed. Use <span class="title-ref">\~scrapy.utils.python.MutableChain.\_\_next\_\_</span> instead. (`5719`)
  - The `scrapy.linkextractors.FilteringLinkExtractor` class, deprecated in Scrapy 2.0, has now been removed. Use <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> instead. (`5720`)
  - Support for using environment variables prefixed with `SCRAPY_` to override settings, deprecated in Scrapy 2.0, has now been removed. (`5724`)
  - Support for the `noconnect` query string argument in proxy URLs, deprecated in Scrapy 2.0, has now been removed. We expect proxies that used to need it to work fine without it. (`5731`)
  - The `scrapy.utils.python.retry_on_eintr` function, deprecated in Scrapy 2.3, has now been removed. (`5719`)
  - The `scrapy.utils.python.WeakKeyCache` class, deprecated in Scrapy 2.4, has now been removed. (`5719`)
  - The `scrapy.utils.boto.is_botocore()` function, deprecated in Scrapy 2.4, has now been removed. (`5719`)

### Deprecations

  - <span class="title-ref">scrapy.pipelines.images.NoimagesDrop</span> is now deprecated. (`5368`, `5489`)
  - <span class="title-ref">ImagesPipeline.convert\_image \<scrapy.pipelines.images.ImagesPipeline.convert\_image\></span> must now accept a `response_body` parameter. (`3055`, `3689`, `4753`)

### New features

  - Applied [black]() coding style to files generated with the `genspider` and `startproject` commands. (`5809`, `5814`)
  - `FEED_EXPORT_ENCODING` is now set to `"utf-8"` in the `settings.py` file that the `startproject` command generates. With this value, JSON exports won’t force the use of escape sequences for non-ASCII characters. (`5797`, `5800`)
  - The <span class="title-ref">\~scrapy.extensions.memusage.MemoryUsage</span> extension now logs the peak memory usage during checks, and the binary unit MiB is now used to avoid confusion. (`5717`, `5722`, `5727`)
  - The `callback` parameter of <span class="title-ref">\~scrapy.http.Request</span> can now be set to <span class="title-ref">scrapy.http.request.NO\_CALLBACK</span>, to distinguish it from `None`, as the latter indicates that the default spider callback (<span class="title-ref">\~scrapy.Spider.parse</span>) is to be used. (`5798`)

### Bug fixes

  - Enabled unsafe legacy SSL renegotiation to fix access to some outdated websites. (`5491`, `5790`)
  - Fixed STARTTLS-based email delivery not working with Twisted 21.2.0 and better. (`5386`, `5406`)

<!-- end list -->

  - \- Fixed the <span class="title-ref">finish\_exporting</span> method of \[item exporters  
    \<topics-exporters\>\](\#item-exporters

  - \----\<topics-exporters\>) not being called for empty files.  
    (`5537`, `5758`)

  - \- Fixed HTTP/2 responses getting only the last value for a header when  
    multiple headers with the same name are received. (`5777`)

  - \- Fixed an exception raised by the `shell` command on some cases  
    when \[using asyncio \<using-asyncio\>\](\#using-asyncio-\<using-asyncio\>). (`5740`, `5742`, `5748`, `5759`, `5760`, `5771`)

  - \- When using <span class="title-ref">\~scrapy.spiders.CrawlSpider</span>, callback keyword arguments  
    (`cb_kwargs`) added to a request in the `process_request` callback of a <span class="title-ref">\~scrapy.spiders.Rule</span> will no longer be ignored. (`5699`)

  - \- The \[images pipeline \<images-pipeline\>\](\#images-pipeline-\<images-pipeline\>) no longer re-encodes JPEG  
    files. (`3055`, `3689`, `4753`)

  - \- Fixed the handling of transparent WebP images by the \[images pipeline  
    \<images-pipeline\>\](\#images-pipeline

  - \----\<images-pipeline\>).  
    (`3072`, `5766`, `5767`)

  - \- <span class="title-ref">scrapy.shell.inspect\_response</span> no longer inhibits `SIGINT`  
    (Ctrl+C). (`2918`)

  - \- <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span>  
    with `unique=False` no longer filters out links that have identical URL *and* text. (`3798`, `3799`, `4695`, `5458`)

  - \- <span class="title-ref">\~scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware</span> now  
    ignores URL protocols that do not support `robots.txt` (`data://`, `file://`). (`5807`)

  - \- Silenced the `filelock` debug log messages introduced in Scrapy 2.6.  
    (`5753`, `5754`)

  - \- Fixed the output of `scrapy -h` showing an unintended `**commands**`  
    line. (`5709`, `5711`, `5712`)

  - \- Made the active project indication in the output of \[commands  
    \<topics-commands\>\](\#commands

  - \----\<topics-commands\>) more clear.  
    (`5715`)

### Documentation

  - \- Documented how to \[debug spiders from Visual Studio Code  
    \<debug-vscode\>\](\#debug-spiders-from-visual-studio-code

  - \----\<debug-vscode\>).  
    (`5721`)

  - \- Documented how `DOWNLOAD_DELAY` affects per-domain concurrency.  
    (`5083`, `5540`)

  - \- Improved consistency.  
    (`5761`)

  - \- Fixed typos.  
    (`5714`, `5744`, `5764`)

### Quality assurance

  - Applied \[black coding style \<coding-style\>\](\#black-coding-style-\<coding-style\>), sorted import statements, and introduced \[pre-commit \<scrapy-pre-commit\>\](\#pre-commit-\<scrapy-pre-commit\>). (`4654`, `4658`, `5734`, `5737`, `5806`, `5810`)
  - Switched from `os.path` to `pathlib`. (`4916`, `4497`, `5682`)
  - Addressed many issues reported by Pylint. (`5677`)
  - Improved code readability. (`5736`)
  - Improved package metadata. (`5768`)
  - Removed direct invocations of `setup.py`. (`5774`, `5776`)
  - Removed unnecessary <span class="title-ref">\~collections.OrderedDict</span> usages. (`5795`)
  - Removed unnecessary `__str__` definitions. (`5150`)
  - Removed obsolete code and comments. (`5725`, `5729`, `5730`, `5732`)
  - Fixed test and CI issues. (`5749`, `5750`, `5756`, `5762`, `5765`, `5780`, `5781`, `5782`, `5783`, `5785`, `5786`)

## Scrapy 2.7.1 (2022-11-02)

### New features

  - Relaxed the restriction introduced in 2.6.2 so that the `Proxy-Authorization` header can again be set explicitly, as long as the proxy URL in the `proxy` metadata has no other credentials, and for as long as that proxy URL remains the same; this restores compatibility with scrapy-zyte-smartproxy 2.1.0 and older (`5626`).

### Bug fixes

  - Using `-O`/`--overwrite-output` and `-t`/`--output-format` options together now produces an error instead of ignoring the former option (`5516`, `5605`).
  - Replaced deprecated `asyncio` APIs that implicitly use the current event loop with code that explicitly requests a loop from the event loop policy (`5685`, `5689`).
  - Fixed uses of deprecated Scrapy APIs in Scrapy itself (`5588`, `5589`).
  - Fixed uses of a deprecated Pillow API (`5684`, `5692`).
  - Improved code that checks if generators return values, so that it no longer fails on decorated methods and partial methods (`5323`, `5592`, `5599`, `5691`).

### Documentation

  - Upgraded the Code of Conduct to Contributor Covenant v2.1 (`5698`).
  - Fixed typos (`5681`, `5694`).

### Quality assurance

  - Re-enabled some erroneously disabled flake8 checks (`5688`).
  - Ignored harmless deprecation warnings from `typing` in tests (`5686`, `5697`).
  - Modernized our CI configuration (`5695`, `5696`).

## Scrapy 2.7.0 (2022-10-17)

Highlights:

  - Added Python 3.11 support, dropped Python 3.6 support
  - Improved support for \[asynchronous callbacks \<topics-coroutines\>\](\#asynchronous-callbacks-\<topics-coroutines\>)
  - \[Asyncio support \<using-asyncio\>\](\#asyncio-support-\<using-asyncio\>) is enabled by default on new projects
  - Output names of item fields can now be arbitrary strings
  - Centralized \[request fingerprinting \<request-fingerprints\>\](\#request-fingerprinting-\<request-fingerprints\>) configuration is now possible

### Modified requirements

Python 3.7 or greater is now required; support for Python 3.6 has been dropped. Support for the upcoming Python 3.11 has been added.

The minimum required version of some dependencies has changed as well:

  - [lxml](https://lxml.de/): 3.5.0 → 4.3.0
  - [Pillow](https://python-pillow.org/) (\[images pipeline \<images-pipeline\>\](\#images-pipeline-\<images-pipeline\>)): 4.0.0 → 7.1.0
  - [zope.interface](https://zopeinterface.readthedocs.io/en/latest/): 5.0.0 → 5.1.0

(`5512`, `5514`, `5524`, `5563`, `5664`, `5670`, `5678`)

### Deprecations

  - <span class="title-ref">ImagesPipeline.thumb\_path \<scrapy.pipelines.images.ImagesPipeline.thumb\_path\></span> must now accept an `item` parameter (`5504`, `5508`).
  - The `scrapy.downloadermiddlewares.decompression` module is now deprecated (`5546`, `5547`).

### New features

  - The <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_spider\_output</span> method of \[spider middlewares \<topics-spider-middleware\>\](\#spider-middlewares-\<topics-spider-middleware\>) can now be defined as an `asynchronous generator` (`4978`).
  - The output of <span class="title-ref">\~scrapy.Request</span> callbacks defined as \[coroutines \<topics-coroutines\>\](\#coroutines-\<topics-coroutines\>) is now processed asynchronously (`4978`).

<!-- end list -->

  - \- <span class="title-ref">\~scrapy.spiders.crawl.CrawlSpider</span> now supports \[asynchronous  
    callbacks \<topics-coroutines\>\](\#asynchronous

\----callbacks-\<topics-coroutines\>) (`5657`).

  - New projects created with the `startproject` command have \[asyncio support \<using-asyncio\>\](\#asyncio-support-\<using-asyncio\>) enabled by default (`5590`, `5679`).
  - The `FEED_EXPORT_FIELDS` setting can now be defined as a dictionary to customize the output name of item fields, lifting the restriction that required output names to be valid Python identifiers, e.g. preventing them to have whitespace (`1008`, `3266`, `3696`).
  - You can now customize \[request fingerprinting \<request-fingerprints\>\](\#request-fingerprinting-\<request-fingerprints\>) through the new `REQUEST_FINGERPRINTER_CLASS` setting, instead of having to change it on every Scrapy component that relies on request fingerprinting (`900`, `3420`, `4113`, `4762`, `4524`).
  - `jsonl` is now supported and encouraged as a file extension for [JSON Lines]() files (`4848`).
  - <span class="title-ref">ImagesPipeline.thumb\_path \<scrapy.pipelines.images.ImagesPipeline.thumb\_path\></span> now receives the source \[item \<topics-items\>\](\#item-\<topics-items\>) (`5504`, `5508`).

### Bug fixes

  - \- When using Google Cloud Storage with a \[media pipeline  
    \<topics-media-pipeline\>\](\#media-pipeline

  - \----\<topics-media-pipeline\>), `FILES_EXPIRES` now also works when  
    `FILES_STORE` does not point at the root of your Google Cloud Storage bucket (`5317`, `5318`).

  - \- The `parse` command now supports \[asynchronous callbacks  
    \<topics-coroutines\>\](\#asynchronous-callbacks

\----\<topics-coroutines\>) (`5424`, `5577`).

  - When using the `parse` command with a URL for which there is no available spider, an exception is no longer raised (`3264`, `3265`, `5375`, `5376`, `5497`).
  - <span class="title-ref">\~scrapy.http.TextResponse</span> now gives higher priority to the [byte order mark]() when determining the text encoding of the response body, following the [HTML living standard]() (`5601`, `5611`).
  - MIME sniffing takes the response body into account in FTP and HTTP/1.0 requests, as well as in cached requests (`4873`).
  - MIME sniffing now detects valid HTML 5 documents even if the `html` tag is missing (`4873`).
  - An exception is now raised if `ASYNCIO_EVENT_LOOP` has a value that does not match the asyncio event loop actually installed (`5529`).
  - Fixed <span class="title-ref">Headers.getlist \<scrapy.http.headers.Headers.getlist\></span> returning only the last header (`5515`, `5526`).
  - Fixed <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> not ignoring the `tar.gz` file extension by default (`1837`, `2067`, `4066`)

### Documentation

  - Clarified the return type of <span class="title-ref">Spider.parse \<scrapy.Spider.parse\></span> (`5602`, `5608`).
  - To enable <span class="title-ref">\~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware</span> to do [brotli compression](), installing [brotli]() is now recommended instead of installing [brotlipy](https://github.com/python-hyper/brotlipy/), as the former provides a more recent version of brotli.

<!-- end list -->

  - \- \[Signal documentation \<topics-signals\>\](\#signal-documentation-\<topics-signals\>) now mentions \[coroutine  
    support \<topics-coroutines\>\](\#coroutine

  - \----support-\<topics-coroutines\>) and uses it in code examples (`4852`,  
    `5358`).

  - \- \[bans\](\#bans) now recommends [Common Crawl]() instead of [Google cache]()  
    (`3582`, `5432`).

  - \- The new \[topics-components\](\#topics-components) topic covers enforcing requirements on  
    Scrapy components, like \[downloader middlewares \<topics-downloader-middleware\>\](\#downloader-middlewares

  - \----\<topics-downloader-middleware\>), \[extensions \<topics-extensions\>\](\#extensions-\<topics-extensions\>),  
    \[item pipelines \<topics-item-pipeline\>\](\#item-pipelines-\<topics-item-pipeline\>), \[spider middlewares \<topics-spider-middleware\>\](\#spider-middlewares

  - \----\<topics-spider-middleware\>), and more; \[enforce-asyncio-requirement\](\#enforce-asyncio-requirement)  
    has also been added (`4978`).

  - \- \[topics-settings\](\#topics-settings) now indicates that setting values must be  
    \[picklable \<pickle-picklable\>\](\#picklable-\<pickle-picklable\>) (`5607`, `5629`).

  - \- Removed outdated documentation (`5446`, `5373`,  
    `5369`, `5370`, `5554`).

  - \- Fixed typos (`5442`, `5455`, `5457`, `5461`,  
    `5538`, `5553`, `5558`, `5624`, `5631`).

  - \- Fixed other issues (`5283`, `5284`, `5559`,  
    `5567`, `5648`, `5659`, `5665`).

### Quality assurance

  - Added a continuous integration job to run [twine check]() (`5655`, `5656`).
  - Addressed test issues and warnings (`5560`, `5561`, `5612`, `5617`, `5639`, `5645`, `5662`, `5671`, `5675`).
  - Cleaned up code (`4991`, `4995`, `5451`, `5487`, `5542`, `5667`, `5668`, `5672`).
  - Applied minor code improvements (`5661`).

## Scrapy 2.6.3 (2022-09-27)

  - Added support for [pyOpenSSL](https://www.pyopenssl.org/en/stable/) 22.1.0, removing support for SSLv3 (`5634`, `5635`, `5636`).

  - Upgraded the minimum versions of the following dependencies:
    
      - [cryptography](https://cryptography.io/en/latest/): 2.0 → 3.3
      - [pyOpenSSL](https://www.pyopenssl.org/en/stable/): 16.2.0 → 21.0.0
      - [service\_identity](https://service-identity.readthedocs.io/en/stable/): 16.0.0 → 18.1.0
      - [Twisted](https://twisted.org/): 17.9.0 → 18.9.0
      - [zope.interface](https://zopeinterface.readthedocs.io/en/latest/): 4.1.3 → 5.0.0
    
    (`5621`, `5632`)

  - Fixes test and documentation issues (`5612`, `5617`, `5631`).

## Scrapy 2.6.2 (2022-07-25)

**Security bug fix:**

  - When <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> processes a request with `proxy` metadata, and that `proxy` metadata includes proxy credentials, <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> sets the `Proxy-Authorization` header, but only if that header is not already set.
    
    There are third-party proxy-rotation downloader middlewares that set different `proxy` metadata every time they process a request.
    
    Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> and any third-party proxy-rotation downloader middleware.
    
    These third-party proxy-rotation downloader middlewares could change the `proxy` metadata of a request to a new value, but fail to remove the `Proxy-Authorization` header from the previous value of the `proxy` metadata, causing the credentials of one proxy to be sent to a different proxy.
    
    To prevent the unintended leaking of proxy credentials, the behavior of <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> is now as follows when processing a request:
    
      - If the request being processed defines `proxy` metadata that includes credentials, the `Proxy-Authorization` header is always updated to feature those credentials.
    
      - If the request being processed defines `proxy` metadata without credentials, the `Proxy-Authorization` header is removed *unless* it was originally defined for the same proxy URL.
        
        To remove proxy credentials while keeping the same proxy URL, remove the `Proxy-Authorization` header.
    
      - If the request has no `proxy` metadata, or that metadata is a falsy value (e.g. `None`), the `Proxy-Authorization` header is removed.
        
        It is no longer possible to set a proxy URL through the `proxy` metadata but set the credentials through the `Proxy-Authorization` header. Set proxy credentials through the `proxy` metadata instead.

Also fixes the following regressions introduced in 2.6.0:

  - <span class="title-ref">\~scrapy.crawler.CrawlerProcess</span> supports again crawling multiple spiders (`5435`, `5436`)
  - Installing a Twisted reactor before Scrapy does (e.g. importing `twisted.internet.reactor` somewhere at the module level) no longer prevents Scrapy from starting, as long as a different reactor is not specified in `TWISTED_REACTOR` (`5525`, `5528`)
  - Fixed an exception that was being logged after the spider finished under certain conditions (`5437`, `5440`)
  - The `--output`/`-o` command-line parameter supports again a value starting with a hyphen (`5444`, `5445`)
  - The `scrapy parse -h` command no longer throws an error (`5481`, `5482`)

## Scrapy 2.6.1 (2022-03-01)

Fixes a regression introduced in 2.6.0 that would unset the request method when following redirects.

## Scrapy 2.6.0 (2022-03-01)

Highlights:

  - \[Security fixes for cookie handling \<2.6-security-fixes\>\](\#security-fixes-for-cookie-handling-\<2.6-security-fixes\>)
  - Python 3.10 support
  - \[asyncio support \<using-asyncio\>\](\#asyncio-support-\<using-asyncio\>) is no longer considered experimental, and works out-of-the-box on Windows regardless of your Python version
  - Feed exports now support <span class="title-ref">pathlib.Path</span> output paths and per-feed \[item filtering \<item-filter\>\](\#item-filtering-\<item-filter\>) and \[post-processing \<post-processing\>\](\#post-processing-\<post-processing\>)

### Security bug fixes

  - When a <span class="title-ref">\~scrapy.http.Request</span> object with cookies defined gets a redirect response causing a new <span class="title-ref">\~scrapy.http.Request</span> object to be scheduled, the cookies defined in the original <span class="title-ref">\~scrapy.http.Request</span> object are no longer copied into the new <span class="title-ref">\~scrapy.http.Request</span> object.
    
    If you manually set the `Cookie` header on a <span class="title-ref">\~scrapy.http.Request</span> object and the domain name of the redirect URL is not an exact match for the domain of the URL of the original <span class="title-ref">\~scrapy.http.Request</span> object, your `Cookie` header is now dropped from the new <span class="title-ref">\~scrapy.http.Request</span> object.
    
    The old behavior could be exploited by an attacker to gain access to your cookies. Please, see the [cjvr-mfj7-j4j8 security advisory]() for more information.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    It is still possible to enable the sharing of cookies between different domains with a shared domain suffix (e.g. `example.com` and any subdomain) by defining the shared domain suffix (e.g. `example.com`) as the cookie domain when defining your cookies. See the documentation of the <span class="title-ref">\~scrapy.http.Request</span> class for more information.
    
    </div>

  - When the domain of a cookie, either received in the `Set-Cookie` header of a response or defined in a <span class="title-ref">\~scrapy.http.Request</span> object, is set to a [public suffix](https://publicsuffix.org/), the cookie is now ignored unless the cookie domain is the same as the request domain.
    
    The old behavior could be exploited by an attacker to inject cookies from a controlled domain into your cookiejar that could be sent to other domains not controlled by the attacker. Please, see the [mfjm-vh54-3f96 security advisory]() for more information.

### Modified requirements

  - The [h2]() dependency is now optional, only needed to \[enable HTTP/2 support \<http2\>\](\#enable-http/2-support-\<http2\>). (`5113`)

### Backward-incompatible changes

  - The `formdata` parameter of <span class="title-ref">\~scrapy.FormRequest</span>, if specified for a non-POST request, now overrides the URL query string, instead of being appended to it. (`2919`, `3579`)

  - When a function is assigned to the `FEED_URI_PARAMS` setting, now the return value of that function, and not the `params` input parameter, will determine the feed URI parameters, unless that return value is `None`. (`4962`, `4966`)

  - In <span class="title-ref">scrapy.core.engine.ExecutionEngine</span>, methods <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.crawl</span>, <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.download</span>, <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.schedule</span>, and <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.spider\_is\_idle</span> now raise <span class="title-ref">RuntimeError</span> if called before <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.open\_spider</span>. (`5090`)
    
    These methods used to assume that <span class="title-ref">ExecutionEngine.slot \<scrapy.core.engine.ExecutionEngine.slot\></span> had been defined by a prior call to <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.open\_spider</span>, so they were raising <span class="title-ref">AttributeError</span> instead.

  - If the API of the configured \[scheduler \<topics-scheduler\>\](\#scheduler-\<topics-scheduler\>) does not meet expectations, <span class="title-ref">TypeError</span> is now raised at startup time. Before, other exceptions would be raised at run time. (`3559`)

  - The `_encoding` field of serialized <span class="title-ref">\~scrapy.http.Request</span> objects is now named `encoding`, in line with all other fields (`5130`)

### Deprecation removals

  - `scrapy.http.TextResponse.body_as_unicode`, deprecated in Scrapy 2.2, has now been removed. (`5393`)
  - `scrapy.item.BaseItem`, deprecated in Scrapy 2.2, has now been removed. (`5398`)
  - `scrapy.item.DictItem`, deprecated in Scrapy 1.8, has now been removed. (`5398`)
  - `scrapy.Spider.make_requests_from_url`, deprecated in Scrapy 1.4, has now been removed. (`4178`, `4356`)

### Deprecations

  - When a function is assigned to the `FEED_URI_PARAMS` setting, returning `None` or modifying the `params` input parameter is now deprecated. Return a new dictionary instead. (`4962`, `4966`)
  - `scrapy.utils.reqser` is deprecated. (`5130`)
      - Instead of <span class="title-ref">\~scrapy.utils.reqser.request\_to\_dict</span>, use the new <span class="title-ref">Request.to\_dict \<scrapy.http.Request.to\_dict\></span> method.
      - Instead of <span class="title-ref">\~scrapy.utils.reqser.request\_from\_dict</span>, use the new <span class="title-ref">scrapy.utils.request.request\_from\_dict</span> function.
  - In `scrapy.squeues`, the following queue classes are deprecated: <span class="title-ref">\~scrapy.squeues.PickleFifoDiskQueueNonRequest</span>, <span class="title-ref">\~scrapy.squeues.PickleLifoDiskQueueNonRequest</span>, <span class="title-ref">\~scrapy.squeues.MarshalFifoDiskQueueNonRequest</span>, and <span class="title-ref">\~scrapy.squeues.MarshalLifoDiskQueueNonRequest</span>. You should instead use: <span class="title-ref">\~scrapy.squeues.PickleFifoDiskQueue</span>, <span class="title-ref">\~scrapy.squeues.PickleLifoDiskQueue</span>, <span class="title-ref">\~scrapy.squeues.MarshalFifoDiskQueue</span>, and <span class="title-ref">\~scrapy.squeues.MarshalLifoDiskQueue</span>. (`5117`)
  - Many aspects of <span class="title-ref">scrapy.core.engine.ExecutionEngine</span> that come from a time when this class could handle multiple <span class="title-ref">\~scrapy.Spider</span> objects at a time have been deprecated. (`5090`)
      - The <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.has\_capacity</span> method is deprecated.
    
      - The <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.schedule</span> method is deprecated, use <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.crawl</span> or <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.download</span> instead.
    
      - The <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.open\_spiders</span> attribute is deprecated, use <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.spider</span> instead.
    
      - The `spider` parameter is deprecated for the following methods:
        
          - <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.spider\_is\_idle</span>
          - <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.crawl</span>
          - <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.download</span>
        
        Instead, call <span class="title-ref">\~scrapy.core.engine.ExecutionEngine.open\_spider</span> first to set the <span class="title-ref">\~scrapy.Spider</span> object.
  - <span class="title-ref">scrapy.utils.response.response\_httprepr</span> is now deprecated. (`4972`)

### New features

  - You can now use \[item filtering \<item-filter\>\](\#item-filtering-\<item-filter\>) to control which items are exported to each output feed. (`4575`, `5178`, `5161`, `5203`)
  - You can now apply \[post-processing \<post-processing\>\](\#post-processing-\<post-processing\>) to feeds, and \[built-in post-processing plugins \<builtin-plugins\>\](\#built-in-post-processing-plugins-\<builtin-plugins\>) are provided for output file compression. (`2174`, `5168`, `5190`)
  - The `FEEDS` setting now supports <span class="title-ref">pathlib.Path</span> objects as keys. (`5383`, `5384`)
  - Enabling \[asyncio \<using-asyncio\>\](\#asyncio-\<using-asyncio\>) while using Windows and Python 3.8 or later will automatically switch the asyncio event loop to one that allows Scrapy to work. See \[asyncio-windows\](\#asyncio-windows). (`4976`, `5315`)
  - The `genspider` command now supports a start URL instead of a domain name. (`4439`)

<!-- end list -->

  - \- `scrapy.utils.defer` gained 2 new functions,  
    <span class="title-ref">\~scrapy.utils.defer.deferred\_to\_future</span> and <span class="title-ref">\~scrapy.utils.defer.maybe\_deferred\_to\_future</span>, to help \[await on Deferreds when using the asyncio reactor \<asyncio-await-dfd\>\](\#await

  - \----on-deferreds-when-using-the-asyncio-reactor-\<asyncio-await-dfd\>).  
    (`5288`)

  - \- \[Amazon S3 feed export storage \<topics-feed-storage-s3\>\](\#amazon-s3-feed-export-storage-\<topics-feed-storage-s3\>) gained  
    support for [temporary security credentials]() (`AWS_SESSION_TOKEN`) and endpoint customization (`AWS_ENDPOINT_URL`). (`4998`, `5210`)

  - \- New `LOG_FILE_APPEND` setting to allow truncating the log file.  
    (`5279`)

  - \- <span class="title-ref">Request.cookies \<scrapy.Request.cookies\></span> values that are  
    <span class="title-ref">bool</span>, <span class="title-ref">float</span> or <span class="title-ref">int</span> are cast to <span class="title-ref">str</span>. (`5252`, `5253`)

  - \- You may now raise <span class="title-ref">\~scrapy.exceptions.CloseSpider</span> from a handler of  
    the `spider_idle` signal to customize the reason why the spider is stopping. (`5191`)

  - \- When using  
    <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span>, the proxy URL for non-HTTPS HTTP/1.1 requests no longer needs to include a URL scheme. (`4505`, `4649`)

  - \- All built-in queues now expose a `peek` method that returns the next  
    queue object (like `pop`) but does not remove the returned object from the queue. (`5112`)
    
    If the underlying queue does not support peeking (e.g. because you are not using `queuelib` 1.6.1 or later), the `peek` method raises <span class="title-ref">NotImplementedError</span>.

  - \- <span class="title-ref">\~scrapy.http.Request</span> and <span class="title-ref">\~scrapy.http.Response</span> now have  
    an `attributes` attribute that makes subclassing easier. For <span class="title-ref">\~scrapy.http.Request</span>, it also allows subclasses to work with <span class="title-ref">scrapy.utils.request.request\_from\_dict</span>. (`1877`, `5130`, `5218`)

  - \- The <span class="title-ref">\~scrapy.core.scheduler.BaseScheduler.open</span> and  
    <span class="title-ref">\~scrapy.core.scheduler.BaseScheduler.close</span> methods of the \[scheduler \<topics-scheduler\>\](\#scheduler-\<topics-scheduler\>) are now optional. (`3559`)

  - \- HTTP/1.1 <span class="title-ref">\~scrapy.core.downloader.handlers.http11.TunnelError</span>  
    exceptions now only truncate response bodies longer than 1000 characters, instead of those longer than 32 characters, making it easier to debug such errors. (`4881`, `5007`)

  - \- <span class="title-ref">\~scrapy.loader.ItemLoader</span> now supports non-text responses.  
    (`5145`, `5269`)

### Bug fixes

  - The `TWISTED_REACTOR` and `ASYNCIO_EVENT_LOOP` settings are no longer ignored if defined in <span class="title-ref">\~scrapy.Spider.custom\_settings</span>. (`4485`, `5352`)
  - Removed a module-level Twisted reactor import that could prevent \[using the asyncio reactor \<using-asyncio\>\](\#using-the-asyncio-reactor-\<using-asyncio\>). (`5357`)
  - The `startproject` command works with existing folders again. (`4665`, `4676`)
  - The `FEED_URI_PARAMS` setting now behaves as documented. (`4962`, `4966`)
  - <span class="title-ref">Request.cb\_kwargs \<scrapy.Request.cb\_kwargs\></span> once again allows the `callback` keyword. (`5237`, `5251`, `5264`)
  - Made <span class="title-ref">scrapy.utils.response.open\_in\_browser</span> support more complex HTML. (`5319`, `5320`)
  - Fixed <span class="title-ref">CSVFeedSpider.quotechar \<scrapy.spiders.CSVFeedSpider.quotechar\></span> being interpreted as the CSV file encoding. (`5391`, `5394`)
  - Added missing [setuptools]() to the list of dependencies. (`5122`)
  - <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> now also works as expected with links that have comma-separated `rel` attribute values including `nofollow`. (`5225`)

<!-- end list -->

  - \- Fixed a <span class="title-ref">TypeError</span> that could be raised during \[feed export  
    \<topics-feed-exports\>\](\#feed-export

\----\<topics-feed-exports\>) parameter parsing. (`5359`)

### Documentation

  - \[asyncio support \<using-asyncio\>\](\#asyncio-support-\<using-asyncio\>) is no longer considered experimental. (`5332`)
  - Included \[Windows-specific help for asyncio usage \<asyncio-windows\>\](\#windows-specific-help-for-asyncio-usage-\<asyncio-windows\>). (`4976`, `5315`)
  - Rewrote \[topics-headless-browsing\](\#topics-headless-browsing) with up-to-date best practices. (`4484`, `4613`)

<!-- end list -->

  - \- Documented \[local file naming in media pipelines  
    \<topics-file-naming\>\](\#local-file-naming-in-media-pipelines

\----\<topics-file-naming\>). (`5069`, `5152`)

  - \[faq\](\#faq) now covers spider file name collision issues. (`2680`, `3669`)
  - Provided better context and instructions to disable the `URLLENGTH_LIMIT` setting. (`5135`, `5250`)
  - Documented that Reppy parser does not support Python 3.9+. (`5226`, `5231`)
  - Documented \[the scheduler component \<topics-scheduler\>\](\#the-scheduler-component-\<topics-scheduler\>). (`3537`, `3559`)

<!-- end list -->

  - \- Documented the method used by \[media pipelines  
    \<topics-media-pipeline\>\](\#media-pipelines

  - \----\<topics-media-pipeline\>) to \[determine if a file has expired  
    \<file-expiration\>\](\#determine-if-a-file-has-expired

\----\<file-expiration\>). (`5120`, `5254`)

  - \[run-multiple-spiders\](\#run-multiple-spiders) now features <span class="title-ref">scrapy.utils.project.get\_project\_settings</span> usage. (`5070`)
  - \[run-multiple-spiders\](\#run-multiple-spiders) now covers what happens when you define different per-spider values for some settings that cannot differ at run time. (`4485`, `5352`)
  - Extended the documentation of the <span class="title-ref">\~scrapy.extensions.statsmailer.StatsMailer</span> extension. (`5199`, `5217`)
  - Added `JOBDIR` to \[topics-settings\](\#topics-settings). (`5173`, `5224`)
  - Documented <span class="title-ref">Spider.attribute \<scrapy.Spider.attribute\></span>. (`5174`, `5244`)
  - Documented <span class="title-ref">TextResponse.urljoin \<scrapy.http.TextResponse.urljoin\></span>. (`1582`)
  - Added the `body_length` parameter to the documented signature of the `headers_received` signal. (`5270`)
  - Clarified <span class="title-ref">SelectorList.get \<scrapy.selector.SelectorList.get\></span> usage in the \[tutorial \<intro-tutorial\>\](\#tutorial-\<intro-tutorial\>). (`5256`)
  - The documentation now features the shortest import path of classes with multiple import paths. (`2733`, `5099`)
  - `quotes.toscrape.com` references now use HTTPS instead of HTTP. (`5395`, `5396`)
  - Added a link to [our Discord server](https://discord.com/invite/mv3yErfpvq) to \[getting-help\](\#getting-help). (`5421`, `5422`)

<!-- end list -->

  - \- The pronunciation of the project name is now \[officially  
    \<intro-overview\>\](\#officially

\----\<intro-overview\>) /ˈskreɪpaɪ/. (`5280`, `5281`)

  - Added the Scrapy logo to the README. (`5255`, `5258`)
  - Fixed issues and implemented minor improvements. (`3155`, `4335`, `5074`, `5098`, `5134`, `5180`, `5194`, `5239`, `5266`, `5271`, `5273`, `5274`, `5276`, `5347`, `5356`, `5414`, `5415`, `5416`, `5419`, `5420`)

### Quality Assurance

  - Added support for Python 3.10. (`5212`, `5221`, `5265`)
  - Significantly reduced memory usage by <span class="title-ref">scrapy.utils.response.response\_httprepr</span>, used by the <span class="title-ref">\~scrapy.downloadermiddlewares.stats.DownloaderStats</span> downloader middleware, which is enabled by default. (`4964`, `4972`)
  - Removed uses of the deprecated `optparse` module. (`5366`, `5374`)
  - Extended typing hints. (`5077`, `5090`, `5100`, `5108`, `5171`, `5215`, `5334`)
  - Improved tests, fixed CI issues, removed unused code. (`5094`, `5157`, `5162`, `5198`, `5207`, `5208`, `5229`, `5298`, `5299`, `5310`, `5316`, `5333`, `5388`, `5389`, `5400`, `5401`, `5404`, `5405`, `5407`, `5410`, `5412`, `5425`, `5427`)
  - Implemented improvements for contributors. (`5080`, `5082`, `5177`, `5200`)
  - Implemented cleanups. (`5095`, `5106`, `5209`, `5228`, `5235`, `5245`, `5246`, `5292`, `5314`, `5322`)

## Scrapy 2.5.1 (2021-10-05)

  - **Security bug fix:**
    
    If you use <span class="title-ref">\~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware</span> (i.e. the `http_user` and `http_pass` spider attributes) for HTTP authentication, any request exposes your credentials to the request target.
    
    To prevent unintended exposure of authentication credentials to unintended domains, you must now additionally set a new, additional spider attribute, `http_auth_domain`, and point it to the specific domain to which the authentication credentials must be sent.
    
    If the `http_auth_domain` spider attribute is not set, the domain of the first request will be considered the HTTP authentication target, and authentication credentials will only be sent in requests targeting that domain.
    
    If you need to send the same HTTP authentication credentials to multiple domains, you can use <span class="title-ref">w3lib.http.basic\_auth\_header</span> instead to set the value of the `Authorization` header of your requests.
    
    If you *really* want your spider to send the same HTTP authentication credentials to any domain, set the `http_auth_domain` spider attribute to `None`.
    
    Finally, if you are a user of [scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash), know that this version of Scrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will need to upgrade scrapy-splash to a greater version for it to continue to work.

## Scrapy 2.5.0 (2021-04-06)

Highlights:

  - Official Python 3.9 support
  - Experimental \[HTTP/2 support \<http2\>\](\#http/2-support-\<http2\>)
  - New <span class="title-ref">\~scrapy.downloadermiddlewares.retry.get\_retry\_request</span> function to retry requests from spider callbacks
  - New <span class="title-ref">\~scrapy.signals.headers\_received</span> signal that allows stopping downloads early
  - New <span class="title-ref">Response.protocol \<scrapy.http.Response.protocol\></span> attribute

### Deprecation removals

  - Removed all code that \[was deprecated in 1.7.0 \<1.7-deprecations\>\](\#was-deprecated-in-1.7.0-\<1.7-deprecations\>) and had not \[already been removed in 2.4.0 \<2.4-deprecation-removals\>\](\#already-been-removed-in-2.4.0-\<2.4-deprecation-removals\>). (`4901`)
  - Removed support for the `SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE` environment variable, \[deprecated in 1.8.0 \<1.8-deprecations\>\](\#deprecated-in-1.8.0-\<1.8-deprecations\>). (`4912`)

### Deprecations

  - The `scrapy.utils.py36` module is now deprecated in favor of `scrapy.utils.asyncgen`. (`4900`)

### New features

  - Experimental \[HTTP/2 support \<http2\>\](\#http/2-support-\<http2\>) through a new download handler that can be assigned to the `https` protocol in the `DOWNLOAD_HANDLERS` setting. (`1854`, `4769`, `5058`, `5059`, `5066`)
  - The new <span class="title-ref">scrapy.downloadermiddlewares.retry.get\_retry\_request</span> function may be used from spider callbacks or middlewares to handle the retrying of a request beyond the scenarios that <span class="title-ref">\~scrapy.downloadermiddlewares.retry.RetryMiddleware</span> supports. (`3590`, `3685`, `4902`)

<!-- end list -->

  - \- The new <span class="title-ref">\~scrapy.signals.headers\_received</span> signal gives early access  
    to response headers and allows \[stopping downloads \<topics-stop-response-download\>\](\#stopping-downloads

  - \----\<topics-stop-response-download\>).  
    (`1772`, `4897`)

  - \- The new <span class="title-ref">Response.protocol \<scrapy.http.Response.protocol\></span>  
    attribute gives access to the string that identifies the protocol used to download a response. (`4878`)

  - \- \[Stats \<topics-stats\>\](\#stats-\<topics-stats\>) now include the following entries that indicate  
    the number of successes and failures in storing \[feeds \<topics-feed-exports\>\](\#feeds-\<topics-feed-exports\>):
    
        feedexport/success_count/<storage type>
        feedexport/failed_count/<storage type>
    
    Where `<storage type>` is the feed storage backend class name, such as <span class="title-ref">\~scrapy.extensions.feedexport.FileFeedStorage</span> or <span class="title-ref">\~scrapy.extensions.feedexport.FTPFeedStorage</span>.
    
    (`3947`, `4850`)

  - \- The <span class="title-ref">\~scrapy.spidermiddlewares.urllength.UrlLengthMiddleware</span> spider  
    middleware now logs ignored URLs with `INFO` \[logging level \<levels\>\](\#logging-level

  - \----\<levels\>) instead of `DEBUG`, and it now includes the following entry  
    into \[stats \<topics-stats\>\](\#stats-\<topics-stats\>) to keep track of the number of ignored URLs:
    
        urllength/request_ignored_count
    
    (`5036`)

  - \- The  
    <span class="title-ref">\~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware</span> downloader middleware now logs the number of decompressed responses and the total count of resulting bytes:
    
        httpcompression/response_bytes
        httpcompression/response_count
    
    (`4797`, `4799`)

### Bug fixes

  - Fixed installation on PyPy installing PyDispatcher in addition to PyPyDispatcher, which could prevent Scrapy from working depending on which package got imported. (`4710`, `4814`)
  - When inspecting a callback to check if it is a generator that also returns a value, an exception is no longer raised if the callback has a docstring with lower indentation than the following code. (`4477`, `4935`)
  - The [Content-Length](https://datatracker.ietf.org/doc/html/rfc2616#section-14.13) header is no longer omitted from responses when using the default, HTTP/1.1 download handler (see `DOWNLOAD_HANDLERS`). (`5009`, `5034`, `5045`, `5057`, `5062`)
  - Setting the `handle_httpstatus_all` request meta key to `False` now has the same effect as not setting it at all, instead of having the same effect as setting it to `True`. (`3851`, `4694`)

### Documentation

  - \- Added instructions to \[install Scrapy in Windows using pip  
    \<intro-install-windows\>\](\#install-scrapy-in-windows-using-pip

  - \----\<intro-install-windows\>).  
    (`4715`, `4736`)

  - \- Logging documentation now includes \[additional ways to filter logs  
    \<topics-logging-advanced-customization\>\](\#additional-ways-to-filter-logs

  - \----\<topics-logging-advanced-customization\>).  
    (`4216`, `4257`, `4965`)

  - \- Covered how to deal with long lists of allowed domains in the \[FAQ  
    \<faq\>\](\#faq

\----\<faq\>). (`2263`, `3667`)

  - Covered [scrapy-bench](https://github.com/scrapy/scrapy-bench) in \[benchmarking\](\#benchmarking). (`4996`, `5016`)
  - Clarified that one \[extension \<topics-extensions\>\](\#extension-\<topics-extensions\>) instance is created per crawler. (`5014`)
  - Fixed some errors in examples. (`4829`, `4830`, `4907`, `4909`, `5008`)
  - Fixed some external links, typos, and so on. (`4892`, `4899`, `4936`, `4942`, `5005`, `5063`)
  - The \[list of Request.meta keys \<topics-request-meta\>\](\#list-of-request.meta-keys-\<topics-request-meta\>) is now sorted alphabetically. (`5061`, `5065`)
  - Updated references to Scrapinghub, which is now called Zyte. (`4973`, `5072`)
  - Added a mention to contributors in the README. (`4956`)
  - Reduced the top margin of lists. (`4974`)

### Quality Assurance

  - Made Python 3.9 support official (`4757`, `4759`)
  - Extended typing hints (`4895`)
  - Fixed deprecated uses of the Twisted API. (`4940`, `4950`, `5073`)
  - Made our tests run with the new pip resolver. (`4710`, `4814`)
  - Added tests to ensure that \[coroutine support \<coroutine-support\>\](\#coroutine-support-\<coroutine-support\>) is tested. (`4987`)
  - Migrated from Travis CI to GitHub Actions. (`4924`)
  - Fixed CI issues. (`4986`, `5020`, `5022`, `5027`, `5052`, `5053`)
  - Implemented code refactorings, style fixes and cleanups. (`4911`, `4982`, `5001`, `5002`, `5076`)

## Scrapy 2.4.1 (2020-11-17)

  - Fixed \[feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) overwrite support (`4845`, `4857`, `4859`)
  - Fixed the AsyncIO event loop handling, which could make code hang (`4855`, `4872`)
  - Fixed the IPv6-capable DNS resolver <span class="title-ref">\~scrapy.resolver.CachingHostnameResolver</span> for download handlers that call <span class="title-ref">reactor.resolve \<twisted.internet.interfaces.IReactorCore.resolve\></span> (`4802`, `4803`)
  - Fixed the output of the `genspider` command showing placeholders instead of the import path of the generated spider module (`4874`)
  - Migrated Windows CI from Azure Pipelines to GitHub Actions (`4869`, `4876`)

## Scrapy 2.4.0 (2020-10-11)

Highlights:

  - Python 3.5 support has been dropped.

  - The `file_path` method of \[media pipelines \<topics-media-pipeline\>\](\#media-pipelines-\<topics-media-pipeline\>) can now access the source \[item \<topics-items\>\](\#item-\<topics-items\>).
    
    This allows you to set a download file path based on item data.

<!-- end list -->

  - \* The new `item_export_kwargs` key of the `FEEDS` setting allows  
    to define keyword parameters to pass to \[item exporter classes \<topics-exporters\>\](\#item-exporter-classes

\----\<topics-exporters\>)

  - You can now choose whether \[feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) overwrite or append to the output file.
    
    For example, when using the `crawl` or `runspider` commands, you can use the `-O` option instead of `-o` to overwrite the output file.

  - Zstd-compressed responses are now supported if [zstandard](https://pypi.org/project/zstandard/) is installed.

  - In settings, where the import path of a class is required, it is now possible to pass a class object instead.

### Modified requirements

  - \* Python 3.6 or greater is now required; support for Python 3.5 has been  
    dropped
    
    As a result:
    
      - When using PyPy, PyPy 7.2.0 or greater \[is now required \<faq-python-versions\>\](\#is-now-required

\--------\<faq-python-versions\>)

>   - For Amazon S3 storage support in \[feed exports \<topics-feed-storage-s3\>\](\#feed-exports

  - \--------\<topics-feed-storage-s3\>) or \[media pipelines  
    \<media-pipelines-s3\>\](\#media-pipelines

\--------\<media-pipelines-s3\>), [botocore](https://github.com/boto/botocore) 1.4.87 or greater is now required

>   - To use the \[images pipeline \<images-pipeline\>\](\#images-pipeline-\<images-pipeline\>), [Pillow](https://python-pillow.org/) 4.0.0 or greater is now required
> 
> (`4718`, `4732`, `4733`, `4742`, `4743`, `4764`)

### Backward-incompatible changes

  - <span class="title-ref">\~scrapy.downloadermiddlewares.cookies.CookiesMiddleware</span> once again discards cookies defined in <span class="title-ref">Request.headers \<scrapy.http.Request.headers\></span>.
    
    We decided to revert this bug fix, introduced in Scrapy 2.2.0, because it was reported that the current implementation could break existing code.
    
    If you need to set cookies for a request, use the <span class="title-ref">Request.cookies \<scrapy.http.Request\></span> parameter.
    
    A future version of Scrapy will include a new, better implementation of the reverted bug fix.
    
    (`4717`, `4823`)

### Deprecation removals

  - <span class="title-ref">scrapy.extensions.feedexport.S3FeedStorage</span> no longer reads the values of `access_key` and `secret_key` from the running project settings when they are not passed to its `__init__` method; you must either pass those parameters to its `__init__` method or use <span class="title-ref">S3FeedStorage.from\_crawler \<scrapy.extensions.feedexport.S3FeedStorage.from\_crawler\></span> (`4356`, `4411`, `4688`)
  - <span class="title-ref">Rule.process\_request \<scrapy.spiders.crawl.Rule.process\_request\></span> no longer admits callables which expect a single `request` parameter, rather than both `request` and `response` (`4818`)

### Deprecations

  - In custom \[media pipelines \<topics-media-pipeline\>\](\#media-pipelines-\<topics-media-pipeline\>), signatures that do not accept a keyword-only `item` parameter in any of the methods that \[now support this parameter \<media-pipeline-item-parameter\>\](\#now-support-this-parameter-\<media-pipeline-item-parameter\>) are now deprecated (`4628`, `4686`)
  - In custom \[feed storage backend classes \<topics-feed-storage\>\](\#feed-storage-backend-classes-\<topics-feed-storage\>), `__init__` method signatures that do not accept a keyword-only `feed_options` parameter are now deprecated (`547`, `716`, `4512`)
  - The <span class="title-ref">scrapy.utils.python.WeakKeyCache</span> class is now deprecated (`4684`, `4701`)
  - The <span class="title-ref">scrapy.utils.boto.is\_botocore</span> function is now deprecated, use <span class="title-ref">scrapy.utils.boto.is\_botocore\_available</span> instead (`4734`, `4776`)

### New features

<div id="media-pipeline-item-parameter">

  - The following methods of \[media pipelines \<topics-media-pipeline\>\](\#media-pipelines-\<topics-media-pipeline\>) now accept an `item` keyword-only parameter containing the source \[item \<topics-items\>\](\#item-\<topics-items\>):
    
      - In \`scrapy.pipelines.files.FilesPipeline\`:
          - <span class="title-ref">\~scrapy.pipelines.files.FilesPipeline.file\_downloaded</span>
          - <span class="title-ref">\~scrapy.pipelines.files.FilesPipeline.file\_path</span>
          - <span class="title-ref">\~scrapy.pipelines.files.FilesPipeline.media\_downloaded</span>
          - <span class="title-ref">\~scrapy.pipelines.files.FilesPipeline.media\_to\_download</span>
      - In \`scrapy.pipelines.images.ImagesPipeline\`:
          - <span class="title-ref">\~scrapy.pipelines.images.ImagesPipeline.file\_downloaded</span>
          - <span class="title-ref">\~scrapy.pipelines.images.ImagesPipeline.file\_path</span>
          - <span class="title-ref">\~scrapy.pipelines.images.ImagesPipeline.get\_images</span>
          - <span class="title-ref">\~scrapy.pipelines.images.ImagesPipeline.image\_downloaded</span>
          - <span class="title-ref">\~scrapy.pipelines.images.ImagesPipeline.media\_downloaded</span>
          - <span class="title-ref">\~scrapy.pipelines.images.ImagesPipeline.media\_to\_download</span>
    
    (`4628`, `4686`)

</div>

  - \* The new `item_export_kwargs` key of the `FEEDS` setting allows  
    to define keyword parameters to pass to \[item exporter classes \<topics-exporters\>\](\#item-exporter-classes

\----\<topics-exporters\>) (`4606`, `4768`)

\* \[Feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) gained overwrite support:

>   - When using the `crawl` or `runspider` commands, you can use the `-O` option instead of `-o` to overwrite the output file
>   - You can use the `overwrite` key in the `FEEDS` setting to configure whether to overwrite the output file (`True`) or append to its content (`False`)
>   - The `__init__` and `from_crawler` methods of \[feed storage backend classes \<topics-feed-storage\>\](\#feed-storage

  - \--------backend-classes-\<topics-feed-storage\>) now receive a new keyword-only  
    parameter, `feed_options`, which is a dictionary of \[feed options \<feed-options\>\](\#feed

\--------options-\<feed-options\>)

> (`547`, `716`, `4512`)

  - Zstd-compressed responses are now supported if [zstandard](https://pypi.org/project/zstandard/) is installed (`4831`)

  - In settings, where the import path of a class is required, it is now possible to pass a class object instead (`3870`, `3873`).
    
    This includes also settings where only part of its value is made of an import path, such as `DOWNLOADER_MIDDLEWARES` or `DOWNLOAD_HANDLERS`.

  - \[Downloader middlewares \<topics-downloader-middleware\>\](\#downloader-middlewares-\<topics-downloader-middleware\>) can now override <span class="title-ref">response.request \<scrapy.http.Response.request\></span>.
    
    If a \[downloader middleware \<topics-downloader-middleware\>\](\#downloader-middleware-\<topics-downloader-middleware\>) returns a <span class="title-ref">\~scrapy.http.Response</span> object from <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_response</span> or <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_exception</span> with a custom <span class="title-ref">\~scrapy.http.Request</span> object assigned to \`response.request \<scrapy.http.Response.request\>\`:
    
      - The response is handled by the callback of that custom <span class="title-ref">\~scrapy.http.Request</span> object, instead of being handled by the callback of the original <span class="title-ref">\~scrapy.http.Request</span> object
      - That custom <span class="title-ref">\~scrapy.http.Request</span> object is now sent as the `request` argument to the `response_received` signal, instead of the original <span class="title-ref">\~scrapy.http.Request</span> object
    
    (`4529`, `4632`)

\* When using the \[FTP feed storage backend \<topics-feed-storage-ftp\>\](\#ftp-feed-storage-backend-\<topics-feed-storage-ftp\>):

>   - It is now possible to set the new `overwrite` \[feed option \<feed-options\>\](\#feed-option

  - \--------\<feed-options\>) to `False` to append to an existing file instead of  
    overwriting it
    
      - The FTP password can now be omitted if it is not necessary
    
    (`547`, `716`, `4512`)

  - \* The `__init__` method of <span class="title-ref">\~scrapy.exporters.CsvItemExporter</span> now  
    supports an `errors` parameter to indicate how to handle encoding errors (`4755`)

  - \* When \[using asyncio \<using-asyncio\>\](\#using-asyncio-\<using-asyncio\>), it is now possible to  
    \[set a custom asyncio loop \<using-custom-loops\>\](\#set-a-custom-asyncio-loop-\<using-custom-loops\>) (`4306`, `4414`)

  - \* Serialized requests (see \[topics-jobs\](\#topics-jobs)) now support callbacks that are  
    spider methods that delegate on other callable (`4756`)

  - \* When a response is larger than `DOWNLOAD_MAXSIZE`, the logged  
    message is now a warning, instead of an error (`3874`, `3886`, `4752`)

### Bug fixes

  - The `genspider` command no longer overwrites existing files unless the `--force` option is used (`4561`, `4616`, `4623`)
  - Cookies with an empty value are no longer considered invalid cookies (`4772`)
  - The `runspider` command now supports files with the `.pyw` file extension (`4643`, `4646`)
  - The <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> middleware now simply ignores unsupported proxy values (`3331`, `4778`)
  - Checks for generator callbacks with a `return` statement no longer warn about `return` statements in nested functions (`4720`, `4721`)
  - The system file mode creation mask no longer affects the permissions of files generated using the `startproject` command (`4722`)
  - <span class="title-ref">scrapy.utils.iterators.xmliter</span> now supports namespaced node names (`861`, `4746`)
  - <span class="title-ref">\~scrapy.Request</span> objects can now have `about:` URLs, which can work when using a headless browser (`4835`)

### Documentation

  - The `FEED_URI_PARAMS` setting is now documented (`4671`, `4724`)
  - Improved the documentation of \[link extractors \<topics-link-extractors\>\](\#link-extractors-\<topics-link-extractors\>) with an usage example from a spider callback and reference documentation for the <span class="title-ref">\~scrapy.link.Link</span> class (`4751`, `4775`)
  - Clarified the impact of `CONCURRENT_REQUESTS` when using the <span class="title-ref">\~scrapy.extensions.closespider.CloseSpider</span> extension (`4836`)
  - Removed references to Python 2’s `unicode` type (`4547`, `4703`)
  - We now have an \[official deprecation policy \<deprecation-policy\>\](\#official-deprecation-policy-\<deprecation-policy\>) (`4705`)
  - Our \[documentation policies \<documentation-policies\>\](\#documentation-policies-\<documentation-policies\>) now cover usage of Sphinx’s :rst`versionadded` and :rst`versionchanged` directives, and we have removed usages referencing Scrapy 1.4.0 and earlier versions (`3971`, `4310`)
  - Other documentation cleanups (`4090`, `4782`, `4800`, `4801`, `4809`, `4816`, `4825`)

### Quality assurance

  - Extended typing hints (`4243`, `4691`)
  - Added tests for the `check` command (`4663`)
  - Fixed test failures on Debian (`4726`, `4727`, `4735`)
  - Improved Windows test coverage (`4723`)
  - Switched to \[formatted string literals \<f-strings\>\](\#formatted-string-literals-\<f-strings\>) where possible (`4307`, `4324`, `4672`)
  - Modernized <span class="title-ref">super</span> usage (`4707`)
  - Other code and test cleanups (`1790`, `3288`, `4165`, `4564`, `4651`, `4714`, `4738`, `4745`, `4747`, `4761`, `4765`, `4804`, `4817`, `4820`, `4822`, `4839`)

## Scrapy 2.3.0 (2020-08-04)

Highlights:

  - \* \[Feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) now support \[Google Cloud  
    Storage \<topics-feed-storage-gcs\>\](\#google-cloud

\----storage-\<topics-feed-storage-gcs\>) as a storage backend

  - \* The new `FEED_EXPORT_BATCH_ITEM_COUNT` setting allows to deliver  
    output items in batches of up to the specified number of items.
    
    It also serves as a workaround for \[delayed file delivery \<delayed-file-delivery\>\](\#delayed-file-delivery

  - \----\<delayed-file-delivery\>), which causes Scrapy to only start item delivery  
    after the crawl has finished when using certain storage backends (\[S3 \<topics-feed-storage-s3\>\](\#s3-\<topics-feed-storage-s3\>), \[FTP \<topics-feed-storage-ftp\>\](\#ftp-\<topics-feed-storage-ftp\>), and now \[GCS \<topics-feed-storage-gcs\>\](\#gcs-\<topics-feed-storage-gcs\>)).

  - \* The base implementation of \[item loaders \<topics-loaders\>\](\#item-loaders-\<topics-loaders\>) has been  
    moved into a separate library, \[itemloaders \<itemloaders:index\>\](itemloaders \<itemloaders:index\>.md), allowing usage from outside Scrapy and a separate release schedule

### Deprecation removals

  - Removed the following classes and their parent modules from `scrapy.linkextractors`:
    
      - `htmlparser.HtmlParserLinkExtractor`
      - `regex.RegexLinkExtractor`
      - `sgml.BaseSgmlLinkExtractor`
      - `sgml.SgmlLinkExtractor`
    
    Use <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> instead (`4356`, `4679`)

### Deprecations

  - The `scrapy.utils.python.retry_on_eintr` function is now deprecated (`4683`)

### New features

  - \* \[Feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) support \[Google Cloud  
    Storage \<topics-feed-storage-gcs\>\](\#google-cloud

\----storage-\<topics-feed-storage-gcs\>) (`685`, `3608`)

  - New `FEED_EXPORT_BATCH_ITEM_COUNT` setting for batch deliveries (`4250`, `4434`)
  - The `parse` command now allows specifying an output file (`4317`, `4377`)
  - <span class="title-ref">Request.from\_curl \<scrapy.http.Request.from\_curl\></span> and <span class="title-ref">\~scrapy.utils.curl.curl\_to\_request\_kwargs</span> now also support `--data-raw` (`4612`)
  - A `parse` callback may now be used in built-in spider subclasses, such as <span class="title-ref">\~scrapy.spiders.CrawlSpider</span> (`712`, `732`, `781`, `4254` )

### Bug fixes

  - \* Fixed the \[CSV exporting \<topics-feed-format-csv\>\](\#csv-exporting-\<topics-feed-format-csv\>) of  
    \[dataclass items \<dataclass-items\>\](\#dataclass-items-\<dataclass-items\>) and \[attr.s items \<attrs-items\>\](\#attr.s-items

\----\<attrs-items\>) (`4667`, `4668`)

  - <span class="title-ref">Request.from\_curl \<scrapy.http.Request.from\_curl\></span> and <span class="title-ref">\~scrapy.utils.curl.curl\_to\_request\_kwargs</span> now set the request method to `POST` when a request body is specified and no request method is specified (`4612`)
  - The processing of ANSI escape sequences in enabled in Windows 10.0.14393 and later, where it is required for colored output (`4393`, `4403`)

### Documentation

  - Updated the [OpenSSL cipher list format](https://docs.openssl.org/master/man1/openssl-ciphers/#cipher-list-format) link in the documentation about the `DOWNLOADER_CLIENT_TLS_CIPHERS` setting (`4653`)
  - Simplified the code example in \[topics-loaders-dataclass\](\#topics-loaders-dataclass) (`4652`)

### Quality assurance

  - The base implementation of \[item loaders \<topics-loaders\>\](\#item-loaders-\<topics-loaders\>) has been moved into \[itemloaders \<itemloaders:index\>\](itemloaders \<itemloaders:index\>.md) (`4005`, `4516`)
  - Fixed a silenced error in some scheduler tests (`4644`, `4645`)
  - Renewed the localhost certificate used for SSL tests (`4650`)
  - Removed cookie-handling code specific to Python 2 (`4682`)
  - Stopped using Python 2 unicode literal syntax (`4704`)
  - Stopped using a backlash for line continuation (`4673`)
  - Removed unneeded entries from the MyPy exception list (`4690`)
  - Automated tests now pass on Windows as part of our continuous integration system (`4458`)
  - Automated tests now pass on the latest PyPy version for supported Python versions in our continuous integration system (`4504`)

## Scrapy 2.2.1 (2020-07-17)

  - The `startproject` command no longer makes unintended changes to the permissions of files in the destination folder, such as removing execution permissions (`4662`, `4666`)

## Scrapy 2.2.0 (2020-06-24)

Highlights:

  - Python 3.5.2+ is required now

<!-- end list -->

  - \* \[dataclass objects \<dataclass-items\>\](\#dataclass-objects-\<dataclass-items\>) and  
    \[attrs objects \<attrs-items\>\](\#attrs-objects-\<attrs-items\>) are now valid \[item types \<item-types\>\](\#item-types

\--\<item-types\>) \* New <span class="title-ref">TextResponse.json \<scrapy.http.TextResponse.json\></span> method \* New `bytes_received` signal that allows canceling response download \* <span class="title-ref">\~scrapy.downloadermiddlewares.cookies.CookiesMiddleware</span> fixes

### Backward-incompatible changes

  - Support for Python 3.5.0 and 3.5.1 has been dropped; Scrapy now refuses to run with a Python version lower than 3.5.2, which introduced <span class="title-ref">typing.Type</span> (`4615`)

### Deprecations

  - <span class="title-ref">TextResponse.body\_as\_unicode \<scrapy.http.TextResponse.body\_as\_unicode\></span> is now deprecated, use <span class="title-ref">TextResponse.text \<scrapy.http.TextResponse.text\></span> instead (`4546`, `4555`, `4579`)
  - <span class="title-ref">scrapy.item.BaseItem</span> is now deprecated, use <span class="title-ref">scrapy.item.Item</span> instead (`4534`)

### New features

  - \* \[dataclass objects \<dataclass-items\>\](\#dataclass-objects-\<dataclass-items\>) and  
    \[attrs objects \<attrs-items\>\](\#attrs-objects-\<attrs-items\>) are now valid \[item types \<item-types\>\](\#item-types

  - \----\<item-types\>), and a new [itemadapter](https://github.com/scrapy/itemadapter) library makes it easy to  
    write code that \[supports any item type \<supporting-item-types\>\](\#supports-any-item-type-\<supporting-item-types\>) (`2749`, `2807`, `3761`, `3881`, `4642`)

  - \* A new <span class="title-ref">TextResponse.json \<scrapy.http.TextResponse.json\></span> method  
    allows to deserialize JSON responses (`2444`, `4460`, `4574`)

  - \* A new `bytes_received` signal allows monitoring response download  
    progress and \[stopping downloads \<topics-stop-response-download\>\](\#stopping-downloads-\<topics-stop-response-download\>) (`4205`, `4559`)

  - \* The dictionaries in the result list of a \[media pipeline  
    \<topics-media-pipeline\>\](\#media-pipeline

  - \----\<topics-media-pipeline\>) now include a new key, `status`, which indicates  
    if the file was downloaded or, if the file was not downloaded, why it was not downloaded; see <span class="title-ref">FilesPipeline.get\_media\_requests \<scrapy.pipelines.files.FilesPipeline.get\_media\_requests\></span> for more information (`2893`, `4486`)

  - \* When using \[Google Cloud Storage \<media-pipeline-gcs\>\](\#google-cloud-storage-\<media-pipeline-gcs\>) for  
    a \[media pipeline \<topics-media-pipeline\>\](\#media-pipeline-\<topics-media-pipeline\>), a warning is now logged if the configured credentials do not grant the required permissions (`4346`, `4508`)

  - \* \[Link extractors \<topics-link-extractors\>\](\#link-extractors-\<topics-link-extractors\>) are now serializable,  
    as long as you do not use \[lambdas \<lambda\>\](\#lambdas-\<lambda\>) for parameters; for example, you can now pass link extractors in <span class="title-ref">Request.cb\_kwargs \<scrapy.http.Request.cb\_kwargs\></span> or <span class="title-ref">Request.meta \<scrapy.http.Request.meta\></span> when \[persisting scheduled requests \<topics-jobs\>\](\#persisting

\----scheduled-requests-\<topics-jobs\>) (`4554`)

  - Upgraded the \[pickle protocol \<pickle-protocols\>\](\#pickle-protocol-\<pickle-protocols\>) that Scrapy uses from protocol 2 to protocol 4, improving serialization capabilities and performance (`4135`, `4541`)
  - <span class="title-ref">scrapy.utils.misc.create\_instance</span> now raises a <span class="title-ref">TypeError</span> exception if the resulting instance is `None` (`4528`, `4532`)

### Bug fixes

  - <span class="title-ref">\~scrapy.downloadermiddlewares.cookies.CookiesMiddleware</span> no longer discards cookies defined in <span class="title-ref">Request.headers \<scrapy.http.Request.headers\></span> (`1992`, `2400`)
  - <span class="title-ref">\~scrapy.downloadermiddlewares.cookies.CookiesMiddleware</span> no longer re-encodes cookies defined as <span class="title-ref">bytes</span> in the `cookies` parameter of the `__init__` method of <span class="title-ref">\~scrapy.http.Request</span> (`2400`, `3575`)
  - When `FEEDS` defines multiple URIs, `FEED_STORE_EMPTY` is `False` and the crawl yields no items, Scrapy no longer stops feed exports after the first URI (`4621`, `4626`)
  - <span class="title-ref">\~scrapy.spiders.Spider</span> callbacks defined using \[coroutine syntax \<topics/coroutines\>\](coroutine syntax \<topics/coroutines\>.md) no longer need to return an iterable, and may instead return a <span class="title-ref">\~scrapy.http.Request</span> object, an \[item \<topics-items\>\](\#item-\<topics-items\>), or `None` (`4609`)
  - The `startproject` command now ensures that the generated project folders and files have the right permissions (`4604`)
  - Fix a <span class="title-ref">KeyError</span> exception being sometimes raised from <span class="title-ref">scrapy.utils.datatypes.LocalWeakReferencedCache</span> (`4597`, `4599`)
  - When `FEEDS` defines multiple URIs, log messages about items being stored now contain information from the corresponding feed, instead of always containing information about only one of the feeds (`4619`, `4629`)

### Documentation

  - \* Added a new section about \[accessing cb\_kwargs from errbacks  
    \<errback-cb\_kwargs\>\](\#accessing-cb\_kwargs-from-errbacks

\----\<errback-cb\_kwargs\>) (`4598`, `4634`)

  - Covered [chompjs](https://github.com/Nykakin/chompjs) in \[topics-parsing-javascript\](\#topics-parsing-javascript) (`4556`, `4562`)
  - Removed from \[topics/coroutines\](topics/coroutines.md) the warning about the API being experimental (`4511`, `4513`)
  - Removed references to unsupported versions of \[Twisted \<twisted:index\>\](Twisted \<twisted:index\>.md) (`4533`)

<!-- end list -->

  - \* Updated the description of the \[screenshot pipeline example  
    \<ScreenshotPipeline\>\](\#screenshot-pipeline-example

  - \----\<screenshotpipeline\>), which now uses \[coroutine syntax  
    \<topics/coroutines\>\](coroutine syntax \<topics/coroutines\>.md) instead of returning a <span class="title-ref">\~twisted.internet.defer.Deferred</span> (`4514`, `4593`)

  - \* Removed a misleading import line from the  
    <span class="title-ref">scrapy.utils.log.configure\_logging</span> code example (`4510`, `4587`)

  - \* The display-on-hover behavior of internal documentation references now also  
    covers links to \[commands \<topics-commands\>\](\#commands-\<topics-commands\>), <span class="title-ref">Request.meta \<scrapy.http.Request.meta\></span> keys, \[settings \<topics-settings\>\](\#settings-\<topics-settings\>) and \[signals \<topics-signals\>\](\#signals-\<topics-signals\>) (`4495`, `4563`)

  - \* It is again possible to download the documentation for offline reading  
    (`4578`, `4585`)

  - \* Removed backslashes preceding `*args` and `**kwargs` in some function  
    and method signatures (`4592`, `4596`)

### Quality assurance

  - \* Adjusted the code base further to our \[style guidelines  
    \<coding-style\>\](\#style-guidelines

  - \----\<coding-style\>) (`4237`, `4525`, `4538`,  
    `4539`, `4540`, `4542`, `4543`, `4544`, `4545`, `4557`, `4558`, `4566`, `4568`, `4572`)

  - \* Removed remnants of Python 2 support (`4550`, `4553`,  
    `4568`)

  - \* Improved code sharing between the `crawl` and `runspider`  
    commands (`4548`, `4552`)

  - \* Replaced `chain(*iterable)` with `chain.from_iterable(iterable)`  
    (`4635`)

  - \* You may now run the `asyncio` tests with Tox on any Python version  
    (`4521`)

  - \* Updated test requirements to reflect an incompatibility with pytest 5.4 and  
    5.4.1 (`4588`)

  - \* Improved <span class="title-ref">\~scrapy.spiderloader.SpiderLoader</span> test coverage for  
    scenarios involving duplicate spider names (`4549`, `4560`)

  - \* Configured Travis CI to also run the tests with Python 3.5.2  
    (`4518`, `4615`)

  - \* Added a [Pylint](https://www.pylint.org/) job to Travis CI  
    (`3727`)

<!-- end list -->

  - Added a [Mypy](https://mypy-lang.org/) job to Travis CI (`4637`)
  - Made use of set literals in tests (`4573`)
  - Cleaned up the Travis CI configuration (`4517`, `4519`, `4522`, `4537`)

## Scrapy 2.1.0 (2020-04-24)

Highlights:

  - New `FEEDS` setting to export to multiple feeds
  - New <span class="title-ref">Response.ip\_address \<scrapy.http.Response.ip\_address\></span> attribute

### Backward-incompatible changes

  - <span class="title-ref">AssertionError</span> exceptions triggered by \[assert \<assert\>\](\#assert-\<assert\>) statements have been replaced by new exception types, to support running Python in optimized mode (see `-O`) without changing Scrapy’s behavior in any unexpected ways.
    
    If you catch an <span class="title-ref">AssertionError</span> exception from Scrapy, update your code to catch the corresponding new exception.
    
    (`4440`)

### Deprecation removals

  - The `LOG_UNSERIALIZABLE_REQUESTS` setting is no longer supported, use `SCHEDULER_DEBUG` instead (`4385`)
  - The `REDIRECT_MAX_METAREFRESH_DELAY` setting is no longer supported, use `METAREFRESH_MAXDELAY` instead (`4385`)
  - The <span class="title-ref">\~scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware</span> middleware has been removed, including the entire <span class="title-ref">scrapy.downloadermiddlewares.chunked</span> module; chunked transfers work out of the box (`4431`)
  - The `spiders` property has been removed from <span class="title-ref">\~scrapy.crawler.Crawler</span>, use <span class="title-ref">CrawlerRunner.spider\_loader \<scrapy.crawler.CrawlerRunner.spider\_loader\></span> or instantiate `SPIDER_LOADER_CLASS` with your settings instead (`4398`)
  - The `MultiValueDict`, `MultiValueDictKeyError`, and `SiteNode` classes have been removed from `scrapy.utils.datatypes` (`4400`)

### Deprecations

  - The `FEED_FORMAT` and `FEED_URI` settings have been deprecated in favor of the new `FEEDS` setting (`1336`, `3858`, `4507`)

### New features

  - A new setting, `FEEDS`, allows configuring multiple output feeds with different settings each (`1336`, `3858`, `4507`)
  - The `crawl` and `runspider` commands now support multiple `-o` parameters (`1336`, `3858`, `4507`)
  - The `crawl` and `runspider` commands now support specifying an output format by appending `:<format>` to the output file (`1336`, `3858`, `4507`)
  - The new <span class="title-ref">Response.ip\_address \<scrapy.http.Response.ip\_address\></span> attribute gives access to the IP address that originated a response (`3903`, `3940`)
  - A warning is now issued when a value in <span class="title-ref">\~scrapy.spiders.Spider.allowed\_domains</span> includes a port (`50`, `3198`, `4413`)
  - Zsh completion now excludes used option aliases from the completion list (`4438`)

### Bug fixes

  - \[Request serialization \<request-serialization\>\](\#request-serialization-\<request-serialization\>) no longer breaks for callbacks that are spider attributes which are assigned a function with a different name (`4500`)
  - `None` values in <span class="title-ref">\~scrapy.spiders.Spider.allowed\_domains</span> no longer cause a <span class="title-ref">TypeError</span> exception (`4410`)
  - Zsh completion no longer allows options after arguments (`4438`)
  - zope.interface 5.0.0 and later versions are now supported (`4447`, `4448`)
  - `Spider.make_requests_from_url`, deprecated in Scrapy 1.4.0, now issues a warning when used (`4412`)

### Documentation

  - Improved the documentation about signals that allow their handlers to return a <span class="title-ref">\~twisted.internet.defer.Deferred</span> (`4295`, `4390`)
  - Our PyPI entry now includes links for our documentation, our source code repository and our issue tracker (`4456`)
  - Covered the [curl2scrapy](https://michael-shub.github.io/curl2scrapy/) service in the documentation (`4206`, `4455`)
  - Removed references to the Guppy library, which only works in Python 2 (`4285`, `4343`)
  - Extended use of InterSphinx to link to Python 3 documentation (`4444`, `4445`)
  - Added support for Sphinx 3.0 and later (`4475`, `4480`, `4496`, `4503`)

### Quality assurance

  - Removed warnings about using old, removed settings (`4404`)
  - Removed a warning about importing <span class="title-ref">\~twisted.internet.testing.StringTransport</span> from `twisted.test.proto_helpers` in Twisted 19.7.0 or newer (`4409`)
  - Removed outdated Debian package build files (`4384`)
  - Removed <span class="title-ref">object</span> usage as a base class (`4430`)
  - Removed code that added support for old versions of Twisted that we no longer support (`4472`)
  - Fixed code style issues (`4468`, `4469`, `4471`, `4481`)
  - Removed <span class="title-ref">twisted.internet.defer.returnValue</span> calls (`4443`, `4446`, `4489`)

## Scrapy 2.0.1 (2020-03-18)

  - <span class="title-ref">Response.follow\_all \<scrapy.http.Response.follow\_all\></span> now supports an empty URL iterable as input (`4408`, `4420`)
  - Removed top-level `~twisted.internet.reactor` imports to prevent errors about the wrong Twisted reactor being installed when setting a different Twisted reactor using `TWISTED_REACTOR` (`4401`, `4406`)
  - Fixed tests (`4422`)

## Scrapy 2.0.0 (2020-03-03)

Highlights:

  - Python 2 support has been removed
  - \[Partial \<topics/coroutines\>\](Partial \<topics/coroutines\>.md) \[coroutine syntax \<async\>\](\#coroutine-syntax-\<async\>) support and \[experimental \<topics/asyncio\>\](experimental \<topics/asyncio\>.md) `asyncio` support
  - New <span class="title-ref">Response.follow\_all \<scrapy.http.Response.follow\_all\></span> method
  - \[FTP support \<media-pipeline-ftp\>\](\#ftp-support-\<media-pipeline-ftp\>) for media pipelines
  - New <span class="title-ref">Response.certificate \<scrapy.http.Response.certificate\></span> attribute
  - IPv6 support through `DNS_RESOLVER`

### Backward-incompatible changes

  - Python 2 support has been removed, following [Python 2 end-of-life on January 1, 2020](https://www.python.org/doc/sunset-python-2/) (`4091`, `4114`, `4115`, `4121`, `4138`, `4231`, `4242`, `4304`, `4309`, `4373`)

  - Retry gaveups (see `RETRY_TIMES`) are now logged as errors instead of as debug information (`3171`, `3566`)

  - File extensions that <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> ignores by default now also include `7z`, `7zip`, `apk`, `bz2`, `cdr`, `dmg`, `ico`, `iso`, `tar`, `tar.gz`, `webm`, and `xz` (`1837`, `2067`, `4066`)

  - The `METAREFRESH_IGNORE_TAGS` setting is now an empty list by default, following web browser behavior (`3844`, `4311`)

  - The <span class="title-ref">\~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware</span> now includes spaces after commas in the value of the `Accept-Encoding` header that it sets, following web browser behavior (`4293`)

  - The `__init__` method of custom download handlers (see `DOWNLOAD_HANDLERS`) or subclasses of the following downloader handlers no longer receives a `settings` parameter:
    
      - <span class="title-ref">scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler</span>
      - <span class="title-ref">scrapy.core.downloader.handlers.file.FileDownloadHandler</span>
    
    Use the `from_settings` or `from_crawler` class methods to expose such a parameter to your custom download handlers.
    
    (`4126`)

  - We have refactored the <span class="title-ref">scrapy.core.scheduler.Scheduler</span> class and related queue classes (see `SCHEDULER_PRIORITY_QUEUE`, `SCHEDULER_DISK_QUEUE` and `SCHEDULER_MEMORY_QUEUE`) to make it easier to implement custom scheduler queue classes. See \[2-0-0-scheduler-queue-changes\](\#2-0-0-scheduler-queue-changes) below for details.

  - Overridden settings are now logged in a different format. This is more in line with similar information logged at startup (`4199`)

### Deprecation removals

  - The \[Scrapy shell \<topics-shell\>\](\#scrapy-shell-\<topics-shell\>) no longer provides a <span class="title-ref">sel</span> proxy object, use <span class="title-ref">response.selector \<scrapy.http.Response.selector\></span> instead (`4347`)
  - LevelDB support has been removed (`4112`)
  - The following functions have been removed from `scrapy.utils.python`: `isbinarytext`, `is_writable`, `setattr_default`, `stringify_dict` (`4362`)

### Deprecations

  - Using environment variables prefixed with `SCRAPY_` to override settings is deprecated (`4300`, `4374`, `4375`)
  - <span class="title-ref">scrapy.linkextractors.FilteringLinkExtractor</span> is deprecated, use <span class="title-ref">scrapy.linkextractors.LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> instead (`4045`)
  - The `noconnect` query string argument of proxy URLs is deprecated and should be removed from proxy URLs (`4198`)
  - The <span class="title-ref">next \<scrapy.utils.python.MutableChain.next\></span> method of <span class="title-ref">scrapy.utils.python.MutableChain</span> is deprecated, use the global <span class="title-ref">next</span> function or <span class="title-ref">MutableChain.\_\_next\_\_ \<scrapy.utils.python.MutableChain.\_\_next\_\_\></span> instead (`4153`)

### New features

  - Added \[partial support \<topics/coroutines\>\](partial support \<topics/coroutines\>.md) for Python’s \[coroutine syntax \<async\>\](\#coroutine-syntax-\<async\>) and \[experimental support \<topics/asyncio\>\](experimental support \<topics/asyncio\>.md) for `asyncio` and `asyncio`-powered libraries (`4010`, `4259`, `4269`, `4270`, `4271`, `4316`, `4318`)
  - The new <span class="title-ref">Response.follow\_all \<scrapy.http.Response.follow\_all\></span> method offers the same functionality as <span class="title-ref">Response.follow \<scrapy.http.Response.follow\></span> but supports an iterable of URLs as input and returns an iterable of requests (`2582`, `4057`, `4286`)

<!-- end list -->

  - \* \[Media pipelines \<topics-media-pipeline\>\](\#media-pipelines-\<topics-media-pipeline\>) now support \[FTP  
    storage \<media-pipeline-ftp\>\](\#ftp

\----storage-\<media-pipeline-ftp\>) (`3928`, `3961`)

  - The new <span class="title-ref">Response.certificate \<scrapy.http.Response.certificate\></span> attribute exposes the SSL certificate of the server as a <span class="title-ref">twisted.internet.ssl.Certificate</span> object for HTTPS responses (`2726`, `4054`)
  - A new `DNS_RESOLVER` setting allows enabling IPv6 support (`1031`, `4227`)
  - A new `SCRAPER_SLOT_MAX_ACTIVE_SIZE` setting allows configuring the existing soft limit that pauses request downloads when the total response data being processed is too high (`1410`, `3551`)
  - A new `TWISTED_REACTOR` setting allows customizing the `~twisted.internet.reactor` that Scrapy uses, allowing to \[enable asyncio support \<topics/asyncio\>\](enable asyncio support \<topics/asyncio\>.md) or deal with a \[common macOS issue \<faq-specific-reactor\>\](\#common-macos-issue-\<faq-specific-reactor\>) (`2905`, `4294`)
  - Scheduler disk and memory queues may now use the class methods `from_crawler` or `from_settings` (`3884`)
  - The new <span class="title-ref">Response.cb\_kwargs \<scrapy.http.Response.cb\_kwargs\></span> attribute serves as a shortcut for <span class="title-ref">Response.request.cb\_kwargs \<scrapy.http.Request.cb\_kwargs\></span> (`4331`)
  - <span class="title-ref">Response.follow \<scrapy.http.Response.follow\></span> now supports a `flags` parameter, for consistency with <span class="title-ref">\~scrapy.http.Request</span> (`4277`, `4279`)
  - \[Item loader processors \<topics-loaders-processors\>\](\#item-loader-processors-\<topics-loaders-processors\>) can now be regular functions, they no longer need to be methods (`3899`)
  - <span class="title-ref">\~scrapy.spiders.Rule</span> now accepts an `errback` parameter (`4000`)
  - <span class="title-ref">\~scrapy.http.Request</span> no longer requires a `callback` parameter when an `errback` parameter is specified (`3586`, `4008`)

<!-- end list -->

  - \* <span class="title-ref">\~scrapy.logformatter.LogFormatter</span> now supports some additional  
    methods:
    
      - <span class="title-ref">\~scrapy.logformatter.LogFormatter.download\_error</span> for download errors
      - <span class="title-ref">\~scrapy.logformatter.LogFormatter.item\_error</span> for exceptions raised during item processing by \[item pipelines \<topics-item-pipeline\>\](\#item-pipelines

\--------\<topics-item-pipeline\>)

>   - <span class="title-ref">\~scrapy.logformatter.LogFormatter.spider\_error</span> for exceptions raised from \[spider callbacks \<topics-spiders\>\](\#spider-callbacks-\<topics-spiders\>)
> 
> (`374`, `3986`, `3989`, `4176`, `4188`)

  - The `FEED_URI` setting now supports <span class="title-ref">pathlib.Path</span> values (`3731`, `4074`)
  - A new `request_left_downloader` signal is sent when a request leaves the downloader (`4303`)
  - Scrapy logs a warning when it detects a request callback or errback that uses `yield` but also returns a value, since the returned value would be lost (`3484`, `3869`)
  - <span class="title-ref">\~scrapy.spiders.Spider</span> objects now raise an <span class="title-ref">AttributeError</span> exception if they do not have a <span class="title-ref">\~scrapy.spiders.Spider.start\_urls</span> attribute nor reimplement <span class="title-ref">\~scrapy.spiders.Spider.start\_requests</span>, but have a `start_url` attribute (`4133`, `4170`)
  - <span class="title-ref">\~scrapy.exporters.BaseItemExporter</span> subclasses may now use `super().__init__(**kwargs)` instead of `self._configure(kwargs)` in their `__init__` method, passing `dont_fail=True` to the parent `__init__` method if needed, and accessing `kwargs` at `self._kwargs` after calling their parent `__init__` method (`4193`, `4370`)
  - A new `keep_fragments` parameter of `scrapy.utils.request.request_fingerprint` allows to generate different fingerprints for requests with different fragments in their URL (`4104`)
  - Download handlers (see `DOWNLOAD_HANDLERS`) may now use the `from_settings` and `from_crawler` class methods that other Scrapy components already supported (`4126`)
  - <span class="title-ref">scrapy.utils.python.MutableChain.\_\_iter\_\_</span> now returns `self`, [allowing it to be used as a sequence](https://lgtm.com/rules/4850080/) (`4153`)

### Bug fixes

  - The `crawl` command now also exits with exit code 1 when an exception happens before the crawling starts (`4175`, `4207`)
  - <span class="title-ref">LinkExtractor.extract\_links \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract\_links\></span> no longer re-encodes the query string or URLs from non-UTF-8 responses in UTF-8 (`998`, `1403`, `1949`, `4321`)
  - The first spider middleware (see `SPIDER_MIDDLEWARES`) now also processes exceptions raised from callbacks that are generators (`4260`, `4272`)
  - Redirects to URLs starting with 3 slashes (`///`) are now supported (`4032`, `4042`)
  - <span class="title-ref">\~scrapy.http.Request</span> no longer accepts strings as `url` simply because they have a colon (`2552`, `4094`)
  - The correct encoding is now used for attach names in <span class="title-ref">\~scrapy.mail.MailSender</span> (`4229`, `4239`)
  - <span class="title-ref">\~scrapy.dupefilters.RFPDupeFilter</span>, the default `DUPEFILTER_CLASS`, no longer writes an extra `\r` character on each line in Windows, which made the size of the `requests.seen` file unnecessarily large on that platform (`4283`)
  - Z shell auto-completion now looks for `.html` files, not `.http` files, and covers the `-h` command-line switch (`4122`, `4291`)
  - Adding items to a <span class="title-ref">scrapy.utils.datatypes.LocalCache</span> object without a `limit` defined no longer raises a <span class="title-ref">TypeError</span> exception (`4123`)
  - Fixed a typo in the message of the <span class="title-ref">ValueError</span> exception raised when <span class="title-ref">scrapy.utils.misc.create\_instance</span> gets both `settings` and `crawler` set to `None` (`4128`)

### Documentation

  - API documentation now links to an online, syntax-highlighted view of the corresponding source code (`4148`)
  - Links to unexisting documentation pages now allow access to the sidebar (`4152`, `4169`)
  - Cross-references within our documentation now display a tooltip when hovered (`4173`, `4183`)
  - Improved the documentation about <span class="title-ref">LinkExtractor.extract\_links \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract\_links\></span> and simplified \[topics-link-extractors\](\#topics-link-extractors) (`4045`)
  - Clarified how <span class="title-ref">ItemLoader.item \<scrapy.loader.ItemLoader.item\></span> works (`3574`, `4099`)
  - Clarified that <span class="title-ref">logging.basicConfig</span> should not be used when also using <span class="title-ref">\~scrapy.crawler.CrawlerProcess</span> (`2149`, `2352`, `3146`, `3960`)
  - Clarified the requirements for <span class="title-ref">\~scrapy.http.Request</span> objects \[when using persistence \<request-serialization\>\](\#when-using-persistence-\<request-serialization\>) (`4124`, `4139`)

<!-- end list -->

  - \* Clarified how to install a \[custom image pipeline  
    \<media-pipeline-example\>\](\#custom-image-pipeline

\----\<media-pipeline-example\>) (`4034`, `4252`)

  - \* Fixed the signatures of the `file_path` method in \[media pipeline  
    \<topics-media-pipeline\>\](\#media-pipeline

\----\<topics-media-pipeline\>) examples (`4290`)

  - Covered a backward-incompatible change in Scrapy 1.7.0 affecting custom <span class="title-ref">scrapy.core.scheduler.Scheduler</span> subclasses (`4274`)
  - Improved the `README.rst` and `CODE_OF_CONDUCT.md` files (`4059`)
  - Documentation examples are now checked as part of our test suite and we have fixed some of the issues detected (`4142`, `4146`, `4171`, `4184`, `4190`)
  - Fixed logic issues, broken links and typos (`4247`, `4258`, `4282`, `4288`, `4305`, `4308`, `4323`, `4338`, `4359`, `4361`)
  - Improved consistency when referring to the `__init__` method of an object (`4086`, `4088`)
  - Fixed an inconsistency between code and output in \[intro-overview\](\#intro-overview) (`4213`)
  - Extended `~sphinx.ext.intersphinx` usage (`4147`, `4172`, `4185`, `4194`, `4197`)
  - We now use a recent version of Python to build the documentation (`4140`, `4249`)
  - Cleaned up documentation (`4143`, `4275`)

### Quality assurance

  - Re-enabled proxy `CONNECT` tests (`2545`, `4114`)
  - Added [Bandit](https://bandit.readthedocs.io/en/latest/) security checks to our test suite (`4162`, `4181`)
  - Added [Flake8](https://flake8.pycqa.org/en/latest/) style checks to our test suite and applied many of the corresponding changes (`3944`, `3945`, `4137`, `4157`, `4167`, `4174`, `4186`, `4195`, `4238`, `4246`, `4355`, `4360`, `4365`)
  - Improved test coverage (`4097`, `4218`, `4236`)
  - Started reporting slowest tests, and improved the performance of some of them (`4163`, `4164`)
  - Fixed broken tests and refactored some tests (`4014`, `4095`, `4244`, `4268`, `4372`)
  - Modified the \[tox \<tox:index\>\](tox \<tox:index\>.md) configuration to allow running tests with any Python version, run [Bandit](https://bandit.readthedocs.io/en/latest/) and [Flake8](https://flake8.pycqa.org/en/latest/) tests by default, and enforce a minimum tox version programmatically (`4179`)
  - Cleaned up code (`3937`, `4208`, `4209`, `4210`, `4212`, `4369`, `4376`, `4378`)

### Changes to scheduler queue classes

The following changes may impact any custom queue classes of all types:

  - The `push` method no longer receives a second positional parameter containing `request.priority * -1`. If you need that value, get it from the first positional parameter, `request`, instead, or use the new <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue.priority</span> method in <span class="title-ref">scrapy.core.scheduler.ScrapyPriorityQueue</span> subclasses.

The following changes may impact custom priority queue classes:

  - In the `__init__` method or the `from_crawler` or `from_settings` class methods:
      - The parameter that used to contain a factory function, `qfactory`, is now passed as a keyword parameter named `downstream_queue_cls`.
      - A new keyword parameter has been added: `key`. It is a string that is always an empty string for memory queues and indicates the `JOB_DIR` value for disk queues.
      - The parameter for disk queues that contains data from the previous crawl, `startprios` or `slot_startprios`, is now passed as a keyword parameter named `startprios`.
      - The `serialize` parameter is no longer passed. The disk queue class must take care of request serialization on its own before writing to disk, using the <span class="title-ref">\~scrapy.utils.reqser.request\_to\_dict</span> and <span class="title-ref">\~scrapy.utils.reqser.request\_from\_dict</span> functions from the `scrapy.utils.reqser` module.

The following changes may impact custom disk and memory queue classes:

  - The signature of the `__init__` method is now `__init__(self, crawler, key)`.

The following changes affect specifically the <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue</span> and <span class="title-ref">\~scrapy.core.scheduler.DownloaderAwarePriorityQueue</span> classes from `scrapy.core.scheduler` and may affect subclasses:

  - In the `__init__` method, most of the changes described above apply.
    
    `__init__` may still receive all parameters as positional parameters, however:
    
      - `downstream_queue_cls`, which replaced `qfactory`, must be instantiated differently.
        
        `qfactory` was instantiated with a priority value (integer).
        
        Instances of `downstream_queue_cls` should be created using the new <span class="title-ref">ScrapyPriorityQueue.qfactory \<scrapy.core.scheduler.ScrapyPriorityQueue.qfactory\></span> or <span class="title-ref">DownloaderAwarePriorityQueue.pqfactory \<scrapy.core.scheduler.DownloaderAwarePriorityQueue.pqfactory\></span> methods.
    
      - The new `key` parameter displaced the `startprios` parameter 1 position to the right.

  - The following class attributes have been added:
    
      - <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue.crawler</span>
      - <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue.downstream\_queue\_cls</span> (details above)
      - <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue.key</span> (details above)

  - The `serialize` attribute has been removed (details above)

The following changes affect specifically the <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue</span> class and may affect subclasses:

  - A new <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue.priority</span> method has been added which, given a request, returns `request.priority * -1`.
    
    It is used in <span class="title-ref">\~scrapy.core.scheduler.ScrapyPriorityQueue.push</span> to make up for the removal of its `priority` parameter.

  - The `spider` attribute has been removed. Use <span class="title-ref">crawler.spider \<scrapy.core.scheduler.ScrapyPriorityQueue.crawler\></span> instead.

The following changes affect specifically the <span class="title-ref">\~scrapy.core.scheduler.DownloaderAwarePriorityQueue</span> class and may affect subclasses:

  - A new <span class="title-ref">\~scrapy.core.scheduler.DownloaderAwarePriorityQueue.pqueues</span> attribute offers a mapping of downloader slot names to the corresponding instances of <span class="title-ref">\~scrapy.core.scheduler.DownloaderAwarePriorityQueue.downstream\_queue\_cls</span>.

(`3884`)

## Scrapy 1.8.4 (2024-02-14)

**Security bug fixes:**

  - Due to its [ReDoS vulnerabilities](), `scrapy.utils.iterators.xmliter` is now deprecated in favor of <span class="title-ref">\~scrapy.utils.iterators.xmliter\_lxml</span>, which <span class="title-ref">\~scrapy.spiders.XMLFeedSpider</span> now uses.
    
    To minimize the impact of this change on existing code, <span class="title-ref">\~scrapy.utils.iterators.xmliter\_lxml</span> now supports indicating the node namespace as a prefix in the node name, and big files with highly nested trees when using libxml2 2.7+.
    
    Please, see the [cc65-xxvf-f7r9 security advisory]() for more information.

  - `DOWNLOAD_MAXSIZE` and `DOWNLOAD_WARNSIZE` now also apply to the decompressed response body. Please, see the [7j7m-v7m3-jqm7 security advisory]() for more information.

  - Also in relation with the [7j7m-v7m3-jqm7 security advisory](), use of the `scrapy.downloadermiddlewares.decompression` module is discouraged and will trigger a warning.

  - The `Authorization` header is now dropped on redirects to a different domain. Please, see the [cw9j-q3vf-hrrv security advisory]() for more information.

## Scrapy 1.8.3 (2022-07-25)

**Security bug fix:**

  - When <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> processes a request with `proxy` metadata, and that `proxy` metadata includes proxy credentials, <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> sets the `Proxy-Authorization` header, but only if that header is not already set.
    
    There are third-party proxy-rotation downloader middlewares that set different `proxy` metadata every time they process a request.
    
    Because of request retries and redirects, the same request can be processed by downloader middlewares more than once, including both <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> and any third-party proxy-rotation downloader middleware.
    
    These third-party proxy-rotation downloader middlewares could change the `proxy` metadata of a request to a new value, but fail to remove the `Proxy-Authorization` header from the previous value of the `proxy` metadata, causing the credentials of one proxy to be sent to a different proxy.
    
    To prevent the unintended leaking of proxy credentials, the behavior of <span class="title-ref">\~scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware</span> is now as follows when processing a request:
    
      - If the request being processed defines `proxy` metadata that includes credentials, the `Proxy-Authorization` header is always updated to feature those credentials.
    
      - If the request being processed defines `proxy` metadata without credentials, the `Proxy-Authorization` header is removed *unless* it was originally defined for the same proxy URL.
        
        To remove proxy credentials while keeping the same proxy URL, remove the `Proxy-Authorization` header.
    
      - If the request has no `proxy` metadata, or that metadata is a falsy value (e.g. `None`), the `Proxy-Authorization` header is removed.
        
        It is no longer possible to set a proxy URL through the `proxy` metadata but set the credentials through the `Proxy-Authorization` header. Set proxy credentials through the `proxy` metadata instead.

## Scrapy 1.8.2 (2022-03-01)

**Security bug fixes:**

  - When a <span class="title-ref">\~scrapy.http.Request</span> object with cookies defined gets a redirect response causing a new <span class="title-ref">\~scrapy.http.Request</span> object to be scheduled, the cookies defined in the original <span class="title-ref">\~scrapy.http.Request</span> object are no longer copied into the new <span class="title-ref">\~scrapy.http.Request</span> object.
    
    If you manually set the `Cookie` header on a <span class="title-ref">\~scrapy.http.Request</span> object and the domain name of the redirect URL is not an exact match for the domain of the URL of the original <span class="title-ref">\~scrapy.http.Request</span> object, your `Cookie` header is now dropped from the new <span class="title-ref">\~scrapy.http.Request</span> object.
    
    The old behavior could be exploited by an attacker to gain access to your cookies. Please, see the [cjvr-mfj7-j4j8 security advisory]() for more information.
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    It is still possible to enable the sharing of cookies between different domains with a shared domain suffix (e.g. `example.com` and any subdomain) by defining the shared domain suffix (e.g. `example.com`) as the cookie domain when defining your cookies. See the documentation of the <span class="title-ref">\~scrapy.http.Request</span> class for more information.
    
    </div>

  - When the domain of a cookie, either received in the `Set-Cookie` header of a response or defined in a <span class="title-ref">\~scrapy.http.Request</span> object, is set to a [public suffix](https://publicsuffix.org/), the cookie is now ignored unless the cookie domain is the same as the request domain.
    
    The old behavior could be exploited by an attacker to inject cookies into your requests to some other domains. Please, see the [mfjm-vh54-3f96 security advisory]() for more information.

## Scrapy 1.8.1 (2021-10-05)

  - **Security bug fix:**
    
    If you use <span class="title-ref">\~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware</span> (i.e. the `http_user` and `http_pass` spider attributes) for HTTP authentication, any request exposes your credentials to the request target.
    
    To prevent unintended exposure of authentication credentials to unintended domains, you must now additionally set a new, additional spider attribute, `http_auth_domain`, and point it to the specific domain to which the authentication credentials must be sent.
    
    If the `http_auth_domain` spider attribute is not set, the domain of the first request will be considered the HTTP authentication target, and authentication credentials will only be sent in requests targeting that domain.
    
    If you need to send the same HTTP authentication credentials to multiple domains, you can use <span class="title-ref">w3lib.http.basic\_auth\_header</span> instead to set the value of the `Authorization` header of your requests.
    
    If you *really* want your spider to send the same HTTP authentication credentials to any domain, set the `http_auth_domain` spider attribute to `None`.
    
    Finally, if you are a user of [scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash), know that this version of Scrapy breaks compatibility with scrapy-splash 0.7.2 and earlier. You will need to upgrade scrapy-splash to a greater version for it to continue to work.

## Scrapy 1.8.0 (2019-10-28)

Highlights:

  - Dropped Python 3.4 support and updated minimum requirements; made Python 3.8 support official
  - New <span class="title-ref">Request.from\_curl \<scrapy.http.Request.from\_curl\></span> class method
  - New `ROBOTSTXT_PARSER` and `ROBOTSTXT_USER_AGENT` settings
  - New `DOWNLOADER_CLIENT_TLS_CIPHERS` and `DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING` settings

### Backward-incompatible changes

  - Python 3.4 is no longer supported, and some of the minimum requirements of Scrapy have also changed:
    
      - \[cssselect \<cssselect:index\>\](cssselect \<cssselect:index\>.md) 0.9.1
      - [cryptography](https://cryptography.io/en/latest/) 2.0
      - [lxml](https://lxml.de/) 3.5.0
      - [pyOpenSSL](https://www.pyopenssl.org/en/stable/) 16.2.0
      - [queuelib](https://github.com/scrapy/queuelib) 1.4.2
      - [service\_identity](https://service-identity.readthedocs.io/en/stable/) 16.0.0
      - [six](https://six.readthedocs.io/) 1.10.0
      - [Twisted](https://twisted.org/) 17.9.0 (16.0.0 with Python 2)
      - [zope.interface](https://zopeinterface.readthedocs.io/en/latest/) 4.1.3
    
    (`3892`)

  - `JSONRequest` is now called <span class="title-ref">\~scrapy.http.JsonRequest</span> for consistency with similar classes (`3929`, `3982`)

  - If you are using a custom context factory (`DOWNLOADER_CLIENTCONTEXTFACTORY`), its `__init__` method must accept two new parameters: `tls_verbose_logging` and `tls_ciphers` (`2111`, `3392`, `3442`, `3450`)

  - <span class="title-ref">\~scrapy.loader.ItemLoader</span> now turns the values of its input item into lists:
    
    `` `pycon     >>> item = MyItem()     >>> item["field"] = "value1"     >>> loader = ItemLoader(item=item)     >>> item["field"]     ['value1']  This is needed to allow adding values to existing fields ( ``loader.add\_value('field', 'value2')\`\`).
    
    (`3804`, `3819`, `3897`, `3976`, `3998`, `4036`)

See also \[1.8-deprecation-removals\](\#1.8-deprecation-removals) below.

New features `` ` ~~~~~~~~~~~~  *   A new `Request.from_curl <scrapy.http.Request.from_curl>` class     method allows [creating a request from a cURL command     <requests-from-curl>](#creating-a-request-from-a-curl-command ----<requests-from-curl>) (:issue:`2985`, :issue:`3862`)  *   A new :setting:`ROBOTSTXT_PARSER` setting allows choosing which robots.txt_     parser to use. It includes built-in support for     [RobotFileParser <python-robotfileparser>](#robotfileparser-<python-robotfileparser>),     [Protego <protego-parser>](#protego-<protego-parser>) (default), Reppy, and     [Robotexclusionrulesparser <rerp-parser>](#robotexclusionrulesparser-<rerp-parser>), and allows you to     [implement support for additional parsers     <support-for-new-robots-parser>](#implement-support-for-additional-parsers ----<support-for-new-robots-parser>) (:issue:`754`, :issue:`2669`,     :issue:`3796`, :issue:`3935`, :issue:`3969`, :issue:`4006`)  *   A new :setting:`ROBOTSTXT_USER_AGENT` setting allows defining a separate     user agent string to use for robots.txt_ parsing (:issue:`3931`,     :issue:`3966`)  *   `~scrapy.spiders.Rule` no longer requires a `LinkExtractor     <scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor>` parameter     (:issue:`781`, :issue:`4016`)  *   Use the new :setting:`DOWNLOADER_CLIENT_TLS_CIPHERS` setting to customize     the TLS/SSL ciphers used by the default HTTP/1.1 downloader (:issue:`3392`,     :issue:`3442`)  *   Set the new :setting:`DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING` setting to ``True``to enable debug-level messages about TLS connection parameters     after establishing HTTPS connections (:issue:`2111`, :issue:`3450`)  *   Callbacks that receive keyword arguments     (see `Request.cb_kwargs <scrapy.http.Request.cb_kwargs>`) can now be     tested using the new `@cb_kwargs     <scrapy.contracts.default.CallbackKeywordArgumentsContract>`     [spider contract <topics-contracts>](#spider-contract-<topics-contracts>) (:issue:`3985`, :issue:`3988`)  *   When a `@scrapes <scrapy.contracts.default.ScrapesContract>` spider     contract fails, all missing fields are now reported (:issue:`766`,     :issue:`3939`)  *   [Custom log formats <custom-log-formats>](#custom-log-formats-<custom-log-formats>) can now drop messages by     having the corresponding methods of the configured :setting:`LOG_FORMATTER`     return``None``(:issue:`3984`, :issue:`3987`)  *   A much improved completion definition is now available for Zsh_     (:issue:`4069`)   Bug fixes ~~~~~~~~~  *   `ItemLoader.load_item() <scrapy.loader.ItemLoader.load_item>` no     longer makes later calls to `ItemLoader.get_output_value()     <scrapy.loader.ItemLoader.get_output_value>` or     `ItemLoader.load_item() <scrapy.loader.ItemLoader.load_item>` return     empty data (:issue:`3804`, :issue:`3819`, :issue:`3897`, :issue:`3976`,     :issue:`3998`, :issue:`4036`)  *   Fixed `~scrapy.statscollectors.DummyStatsCollector` raising a     `TypeError` exception (:issue:`4007`, :issue:`4052`)  *   `FilesPipeline.file_path     <scrapy.pipelines.files.FilesPipeline.file_path>` and     `ImagesPipeline.file_path     <scrapy.pipelines.images.ImagesPipeline.file_path>` no longer choose     file extensions that are not `registered with IANA`_ (:issue:`1287`,     :issue:`3953`, :issue:`3954`)  *   When using botocore_ to persist files in S3, all botocore-supported headers     are properly mapped now (:issue:`3904`, :issue:`3905`)  *   FTP passwords in :setting:`FEED_URI` containing percent-escaped characters     are now properly decoded (:issue:`3941`)  *   A memory-handling and error-handling issue in     `scrapy.utils.ssl.get_temp_key_info` has been fixed (:issue:`3920`)   Documentation ~~~~~~~~~~~~~  *   The documentation now covers how to define and configure a [custom log     format <custom-log-formats>](#custom-log ----format-<custom-log-formats>) (:issue:`3616`, :issue:`3660`)  *   API documentation added for `~scrapy.exporters.MarshalItemExporter`     and `~scrapy.exporters.PythonItemExporter` (:issue:`3973`)  *   API documentation added for `~scrapy.item.BaseItem` and     `~scrapy.item.ItemMeta` (:issue:`3999`)  *   Minor documentation fixes (:issue:`2998`, :issue:`3398`, :issue:`3597`,     :issue:`3894`, :issue:`3934`, :issue:`3978`, :issue:`3993`, :issue:`4022`,     :issue:`4028`, :issue:`4033`, :issue:`4046`, :issue:`4050`, :issue:`4055`,     :issue:`4056`, :issue:`4061`, :issue:`4072`, :issue:`4071`, :issue:`4079`,     :issue:`4081`, :issue:`4089`, :issue:`4093`)   .. _1.8-deprecation-removals:  Deprecation removals ~~~~~~~~~~~~~~~~~~~~  *``scrapy.xlib``has been removed (:issue:`4015`)   .. _1.8-deprecations:  Deprecations ~~~~~~~~~~~~  *   The LevelDB_ storage backend     (``scrapy.extensions.httpcache.LeveldbCacheStorage``) of     `~scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware` is     deprecated (:issue:`4085`, :issue:`4092`)  *   Use of the undocumented``SCRAPY\_PICKLED\_SETTINGS\_TO\_OVERRIDE``environment     variable is deprecated (:issue:`3910`)  *``scrapy.item.DictItem``is deprecated, use `~scrapy.item.Item`     instead (:issue:`3999`)   Other changes ~~~~~~~~~~~~~  *   Minimum versions of optional Scrapy requirements that are covered by     continuous integration tests have been updated:      *   botocore_ 1.3.23     *   Pillow_ 3.4.2      Lower versions of these optional requirements may work, but it is not     guaranteed (:issue:`3892`)  *   GitHub templates for bug reports and feature requests (:issue:`3126`,     :issue:`3471`, :issue:`3749`, :issue:`3754`)  *   Continuous integration fixes (:issue:`3923`)  *   Code cleanup (:issue:`3391`, :issue:`3907`, :issue:`3946`, :issue:`3950`,     :issue:`4023`, :issue:`4031`)   .. _release-1.7.4:  Scrapy 1.7.4 (2019-10-21) -------------------------  Revert the fix for :issue:`3804` (:issue:`3819`), which has a few undesired side effects (:issue:`3897`, :issue:`3976`).  As a result, when an item loader is initialized with an item, `ItemLoader.load_item() <scrapy.loader.ItemLoader.load_item>` once again makes later calls to `ItemLoader.get_output_value() <scrapy.loader.ItemLoader.get_output_value>` or `ItemLoader.load_item() <scrapy.loader.ItemLoader.load_item>` return empty data.   .. _release-1.7.3:  Scrapy 1.7.3 (2019-08-01) -------------------------  Enforce lxml 4.3.5 or lower for Python 3.4 (:issue:`3912`, :issue:`3918`).   .. _release-1.7.2:  Scrapy 1.7.2 (2019-07-23) -------------------------  Fix Python 2 support (:issue:`3889`, :issue:`3893`, :issue:`3896`).   .. _release-1.7.1:  Scrapy 1.7.1 (2019-07-18) -------------------------  Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.   .. _release-1.7.0:  Scrapy 1.7.0 (2019-07-18) -------------------------  .. note:: Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI           is the result of an erroneous commit tagging and does not include all           the changes described below.  Highlights:  * Improvements for crawls targeting multiple domains * A cleaner way to pass arguments to callbacks * A new class for JSON requests * Improvements for rule-based spiders * New features for feed exports  Backward-incompatible changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  *``429``is now part of the :setting:`RETRY_HTTP_CODES` setting by default      This change is **backward incompatible**. If you don’t want to retry``429``, you must override :setting:`RETRY_HTTP_CODES` accordingly.  *   `~scrapy.crawler.Crawler`,     `CrawlerRunner.crawl <scrapy.crawler.CrawlerRunner.crawl>` and     `CrawlerRunner.create_crawler <scrapy.crawler.CrawlerRunner.create_crawler>`     no longer accept a `~scrapy.spiders.Spider` subclass instance, they     only accept a `~scrapy.spiders.Spider` subclass now.      `~scrapy.spiders.Spider` subclass instances were never meant to     work, and they were not working as one would expect: instead of using the     passed `~scrapy.spiders.Spider` subclass instance, their     `~scrapy.spiders.Spider.from_crawler` method was called to generate     a new instance.  *   Non-default values for the :setting:`SCHEDULER_PRIORITY_QUEUE` setting     may stop working. Scheduler priority queue classes now need to handle     `~scrapy.http.Request` objects instead of arbitrary Python data     structures.  *   An additional``crawler`parameter has been added to the`\_\_init\_\_``method of the `~scrapy.core.scheduler.Scheduler` class. Custom     scheduler subclasses which don't accept arbitrary parameters in their``\_\_init\_\_``method might break because of this change.      For more information, see :setting:`SCHEDULER`.  See also [1.7-deprecation-removals](#1.7-deprecation-removals) below.   New features ~~~~~~~~~~~~  *   A new scheduler priority queue,``scrapy.pqueues.DownloaderAwarePriorityQueue``, may be     [enabled <broad-crawls-scheduler-priority-queue>](#enabled-<broad-crawls-scheduler-priority-queue>) for a significant     scheduling improvement on crawls targeting multiple web domains, at the     cost of no :setting:`CONCURRENT_REQUESTS_PER_IP` support (:issue:`3520`)  *   A new `Request.cb_kwargs <scrapy.http.Request.cb_kwargs>` attribute     provides a cleaner way to pass keyword arguments to callback methods     (:issue:`1138`, :issue:`3563`)  *   A new `JSONRequest <scrapy.http.JsonRequest>` class offers a more     convenient way to build JSON requests (:issue:`3504`, :issue:`3505`)  *   A``process\_request`` callback passed to the `~scrapy.spiders.Rule` ``\_\_init\_\_``method now receives the `~scrapy.http.Response` object that     originated the request as its second argument (:issue:`3682`)  *   A new``restrict\_text`` parameter for the     `LinkExtractor <scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor>` ``\_\_init\_\_``method allows filtering links by linking text (:issue:`3622`,     :issue:`3635`)  *   A new :setting:`FEED_STORAGE_S3_ACL` setting allows defining a custom ACL     for feeds exported to Amazon S3 (:issue:`3607`)  *   A new :setting:`FEED_STORAGE_FTP_ACTIVE` setting allows using FTP’s active     connection mode for feeds exported to FTP servers (:issue:`3829`)  *   A new :setting:`METAREFRESH_IGNORE_TAGS` setting allows overriding which     HTML tags are ignored when searching a response for HTML meta tags that     trigger a redirect (:issue:`1422`, :issue:`3768`)  *   A new :reqmeta:`redirect_reasons` request meta key exposes the reason     (status code, meta refresh) behind every followed redirect (:issue:`3581`,     :issue:`3687`)  *   The``SCRAPY\_CHECK`variable is now set to the`true``string during runs     of the :command:`check` command, which allows [detecting contract     check runs from code <detecting-contract-check-runs>](#detecting-contract ----check-runs-from-code-<detecting-contract-check-runs>) (:issue:`3704`,     :issue:`3739`)  *   A new `Item.deepcopy() <scrapy.item.Item.deepcopy>` method makes it     easier to [deep-copy items <copying-items>](#deep-copy-items-<copying-items>) (:issue:`1493`,     :issue:`3671`)  *   `~scrapy.extensions.corestats.CoreStats` also logs``elapsed\_time\_seconds``now (:issue:`3638`)  *   Exceptions from `~scrapy.loader.ItemLoader` [input and output     processors <topics-loaders-processors>](#input-and-output ----processors-<topics-loaders-processors>) are now more verbose     (:issue:`3836`, :issue:`3840`)  *   `~scrapy.crawler.Crawler`,     `CrawlerRunner.crawl <scrapy.crawler.CrawlerRunner.crawl>` and     `CrawlerRunner.create_crawler <scrapy.crawler.CrawlerRunner.create_crawler>`     now fail gracefully if they receive a `~scrapy.spiders.Spider`     subclass instance instead of the subclass itself (:issue:`2283`,     :issue:`3610`, :issue:`3872`)   Bug fixes ~~~~~~~~~  *   `~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception`     is now also invoked for generators (:issue:`220`, :issue:`2061`)  *   System exceptions like KeyboardInterrupt_ are no longer caught     (:issue:`3726`)  *   `ItemLoader.load_item() <scrapy.loader.ItemLoader.load_item>` no     longer makes later calls to `ItemLoader.get_output_value()     <scrapy.loader.ItemLoader.get_output_value>` or     `ItemLoader.load_item() <scrapy.loader.ItemLoader.load_item>` return     empty data (:issue:`3804`, :issue:`3819`)  *   The images pipeline (`~scrapy.pipelines.images.ImagesPipeline`) no     longer ignores these Amazon S3 settings: :setting:`AWS_ENDPOINT_URL`,     :setting:`AWS_REGION_NAME`, :setting:`AWS_USE_SSL`, :setting:`AWS_VERIFY`     (:issue:`3625`)  *   Fixed a memory leak in``scrapy.pipelines.media.MediaPipeline``affecting,     for example, non-200 responses and exceptions from custom middlewares     (:issue:`3813`)  *   Requests with private callbacks are now correctly unserialized from disk     (:issue:`3790`)  *   `FormRequest.from_response() <scrapy.http.FormRequest.from_response>`     now handles invalid methods like major web browsers (:issue:`3777`,     :issue:`3794`)   Documentation ~~~~~~~~~~~~~  *   A new topic, [topics-dynamic-content](#topics-dynamic-content), covers recommended approaches     to read dynamically-loaded data (:issue:`3703`)  *   [topics-broad-crawls](#topics-broad-crawls) now features information about memory usage     (:issue:`1264`, :issue:`3866`)  *   The documentation of `~scrapy.spiders.Rule` now covers how to access     the text of a link when using `~scrapy.spiders.CrawlSpider`     (:issue:`3711`, :issue:`3712`)  *   A new section, [httpcache-storage-custom](#httpcache-storage-custom), covers writing a custom     cache storage backend for     `~scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware`     (:issue:`3683`, :issue:`3692`)  *   A new [FAQ <faq>](#faq-<faq>) entry, [faq-split-item](#faq-split-item), explains what to do     when you want to split an item into multiple items from an item pipeline     (:issue:`2240`, :issue:`3672`)  *   Updated the [FAQ entry about crawl order <faq-bfo-dfo>](#faq-entry-about-crawl-order-<faq-bfo-dfo>) to explain why     the first few requests rarely follow the desired order (:issue:`1739`,     :issue:`3621`)  *   The :setting:`LOGSTATS_INTERVAL` setting (:issue:`3730`), the     `FilesPipeline.file_path <scrapy.pipelines.files.FilesPipeline.file_path>`     and     `ImagesPipeline.file_path <scrapy.pipelines.images.ImagesPipeline.file_path>`     methods (:issue:`2253`, :issue:`3609`) and the     `Crawler.stop() <scrapy.crawler.Crawler.stop>` method (:issue:`3842`)     are now documented  *   Some parts of the documentation that were confusing or misleading are now     clearer (:issue:`1347`, :issue:`1789`, :issue:`2289`, :issue:`3069`,     :issue:`3615`, :issue:`3626`, :issue:`3668`, :issue:`3670`, :issue:`3673`,     :issue:`3728`, :issue:`3762`, :issue:`3861`, :issue:`3882`)  *   Minor documentation fixes (:issue:`3648`, :issue:`3649`, :issue:`3662`,     :issue:`3674`, :issue:`3676`, :issue:`3694`, :issue:`3724`, :issue:`3764`,     :issue:`3767`, :issue:`3791`, :issue:`3797`, :issue:`3806`, :issue:`3812`)  .. _1.7-deprecation-removals:  Deprecation removals ~~~~~~~~~~~~~~~~~~~~  The following deprecated APIs have been removed (:issue:`3578`):  *``scrapy.conf``(use `Crawler.settings     <scrapy.crawler.Crawler.settings>`)  *   From``scrapy.core.downloader.handlers`:      *`http.HttpDownloadHandler`(use`http10.HTTP10DownloadHandler`)  *`scrapy.loader.ItemLoader.\_get\_values`(use`\_get\_xpathvalues`)  *`scrapy.loader.XPathItemLoader``(use `~scrapy.loader.ItemLoader`)  *``scrapy.log`(see [topics-logging](#topics-logging))  *   From`scrapy.pipelines`:      *`files.FilesPipeline.file\_key`(use`file\_path`)      *`images.ImagesPipeline.file\_key`(use`file\_path`)      *`images.ImagesPipeline.image\_key`(use`file\_path`)      *`images.ImagesPipeline.thumb\_key`(use`thumb\_path`)  *   From both`scrapy.selector`and`scrapy.selector.lxmlsel`:      *`HtmlXPathSelector``(use `~scrapy.selector.Selector`)      *``XmlXPathSelector``(use `~scrapy.selector.Selector`)      *``XPathSelector``(use `~scrapy.selector.Selector`)      *``XPathSelectorList``(use `~scrapy.selector.Selector`)  *   From``scrapy.selector.csstranslator`:      *`ScrapyGenericTranslator`(use parsel.csstranslator.GenericTranslator_)      *`ScrapyHTMLTranslator`(use parsel.csstranslator.HTMLTranslator_)      *`ScrapyXPathExpr``(use parsel.csstranslator.XPathExpr_)  *   From `~scrapy.selector.Selector`:      *``\_root`(both the`\_\_init\_\_`method argument and the object property, use`root`)      *`extract\_unquoted`(use`getall`)      *`select`(use`xpath``)  *   From `~scrapy.selector.SelectorList`:      *``extract\_unquoted`(use`getall`)      *`select`(use`xpath`)      *`x`(use`xpath`)  *`scrapy.spiders.BaseSpider``(use `~scrapy.spiders.Spider`)  *   From `~scrapy.spiders.Spider` (and subclasses):      *``DOWNLOAD\_DELAY`(use [download_delay         <spider-download_delay-attribute>](#download_delay --------<spider-download_delay-attribute>))      *`set\_crawler``(use `~scrapy.spiders.Spider.from_crawler`)  *``scrapy.spiders.spiders``(use `~scrapy.spiderloader.SpiderLoader`)  *``scrapy.telnet``(use :mod:`scrapy.extensions.telnet`)  *   From``scrapy.utils.python`:      *`str\_to\_unicode`(use`to\_unicode`)      *`unicode\_to\_str`(use`to\_bytes`)  *`scrapy.utils.response.body\_or\_str``The following deprecated settings have also been removed (:issue:`3578`):  *``SPIDER\_MANAGER\_CLASS``(use :setting:`SPIDER_LOADER_CLASS`)   .. _1.7-deprecations:  Deprecations ~~~~~~~~~~~~  *   The``queuelib.PriorityQueue``value for the     :setting:`SCHEDULER_PRIORITY_QUEUE` setting is deprecated. Use``scrapy.pqueues.ScrapyPriorityQueue`instead.  *`process\_request``callbacks passed to `~scrapy.spiders.Rule` that     do not accept two arguments are deprecated.  *   The following modules are deprecated:      *``scrapy.utils.http``(use `w3lib.http`_)      *``scrapy.utils.markup``(use `w3lib.html`_)      *``scrapy.utils.multipart``(use `urllib3`_)  *   The``scrapy.utils.datatypes.MergeDict``class is deprecated for Python 3     code bases. Use `~collections.ChainMap` instead. (:issue:`3878`)  *   The``scrapy.utils.gz.is\_gzipped`function is deprecated. Use`scrapy.utils.gz.gzip\_magic\_number``instead.       Other changes ~~~~~~~~~~~~~  *   It is now possible to run all tests from the same tox_ environment in     parallel; the documentation now covers [this and other ways to run     tests <running-tests>](#this-and-other-ways-to-run ----tests-<running-tests>) (:issue:`3707`)  *   It is now possible to generate an API documentation coverage report     (:issue:`3806`, :issue:`3810`, :issue:`3860`)  *   The [documentation policies <documentation-policies>](#documentation-policies-<documentation-policies>) now require     docstrings_ (:issue:`3701`) that follow `PEP 257`_ (:issue:`3748`)  *   Internal fixes and cleanup (:issue:`3629`, :issue:`3643`, :issue:`3684`,     :issue:`3698`, :issue:`3734`, :issue:`3735`, :issue:`3736`, :issue:`3737`,     :issue:`3809`, :issue:`3821`, :issue:`3825`, :issue:`3827`, :issue:`3833`,     :issue:`3857`, :issue:`3877`)  .. _release-1.6.0:  Scrapy 1.6.0 (2019-01-30) -------------------------  Highlights:  * better Windows support; * Python 3.7 compatibility; * big documentation improvements, including a switch   from``.extract\_first()`+`.extract()`API to`.get()`+`.getall()``API; * feed exports, FilePipeline and MediaPipeline improvements; * better extensibility: :signal:`item_error` and   :signal:`request_reached_downloader` signals;``from\_crawler`support   for feed exporters, feed storages and dupefilters. *`scrapy.contracts`fixes and new features; * telnet console security improvements, first released as a   backport in [release-1.5.2](#release-1.5.2); * clean-up of the deprecated code; * various bug fixes, small new features and usability improvements across   the codebase.  Selector API changes ~~~~~~~~~~~~~~~~~~~~  While these are not changes in Scrapy itself, but rather in the parsel_ library which Scrapy uses for xpath/css selectors, these changes are worth mentioning here. Scrapy now depends on parsel >= 1.5, and Scrapy documentation is updated to follow recent`parsel`API conventions.  Most visible change is that`.get()`and`.getall()`selector methods are now preferred over`.extract\_first()`and`.extract()`. We feel that these new methods result in a more concise and readable code. See [old-extraction-api](#old-extraction-api) for more details.  .. note::     There are currently **no plans** to deprecate`.extract()`and`.extract\_first()`methods.  Another useful new feature is the introduction of`Selector.attrib`and`SelectorList.attrib`properties, which make it easier to get attributes of HTML elements. See [selecting-attributes](#selecting-attributes).  CSS selectors are cached in parsel >= 1.5, which makes them faster when the same CSS path is used many times. This is very common in case of Scrapy spiders: callbacks are usually called several times, on different pages.  If you're using custom`Selector`or`SelectorList``subclasses, a **backward incompatible** change in parsel may affect your code. See `parsel changelog`_ for a detailed description, as well as for the full list of improvements.    Telnet console ~~~~~~~~~~~~~~  **Backward incompatible**: Scrapy's telnet console now requires username and password. See [topics-telnetconsole](#topics-telnetconsole) for more details. This change fixes a **security issue**; see [release-1.5.2](#release-1.5.2) release notes for details.  New extensibility features ~~~~~~~~~~~~~~~~~~~~~~~~~~  *``from\_crawler``support is added to feed exporters and feed storages. This,   among other things, allows to access Scrapy settings from custom feed   storages and exporters (:issue:`1605`, :issue:`3348`). *``from\_crawler``support is added to dupefilters (:issue:`2956`); this allows   to access e.g. settings or a spider from a dupefilter. * :signal:`item_error` is fired when an error happens in a pipeline   (:issue:`3256`); * :signal:`request_reached_downloader` is fired when Downloader gets   a new Request; this signal can be useful e.g. for custom Schedulers   (:issue:`3393`). * new SitemapSpider `~.SitemapSpider.sitemap_filter` method which allows   to select sitemap entries based on their attributes in SitemapSpider   subclasses (:issue:`3512`). * Lazy loading of Downloader Handlers is now optional; this enables better   initialization error handling in custom Downloader Handlers (:issue:`3394`).  New FilePipeline and MediaPipeline features ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  * Expose more options for S3FilesStore: :setting:`AWS_ENDPOINT_URL`,   :setting:`AWS_USE_SSL`, :setting:`AWS_VERIFY`, :setting:`AWS_REGION_NAME`.   For example, this allows to use alternative or self-hosted   AWS-compatible providers (:issue:`2609`, :issue:`3548`). * ACL support for Google Cloud Storage: :setting:`FILES_STORE_GCS_ACL` and   :setting:`IMAGES_STORE_GCS_ACL` (:issue:`3199`).``scrapy.contracts``improvements ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  * Exceptions in contracts code are handled better (:issue:`3377`); *``dont\_filter=True``is used for contract requests, which allows to test   different callbacks with the same URL (:issue:`3381`); *``request\_cls``attribute in Contract subclasses allow to use different   Request classes in contracts, for example FormRequest (:issue:`3383`). * Fixed errback handling in contracts, e.g. for cases where a contract   is executed for URL which returns non-200 response (:issue:`3371`).  Usability improvements ~~~~~~~~~~~~~~~~~~~~~~  * more stats for RobotsTxtMiddleware (:issue:`3100`) * INFO log level is used to show telnet host/port (:issue:`3115`) * a message is added to IgnoreRequest in RobotsTxtMiddleware (:issue:`3113`) * better validation of``url`argument in`Response.follow``(:issue:`3131`) * non-zero exit code is returned from Scrapy commands when error happens   on spider initialization (:issue:`3226`) * Link extraction improvements: "ftp" is added to scheme list (:issue:`3152`);   "flv" is added to common video extensions (:issue:`3165`) * better error message when an exporter is disabled (:issue:`3358`); *``scrapy shell --help`mentions syntax required for local files   (`./file.html``) - :issue:`3496`. * Referer header value is added to RFPDupeFilter log messages (:issue:`3588`)  Bug fixes ~~~~~~~~~  * fixed issue with extra blank lines in .csv exports under Windows   (:issue:`3039`); * proper handling of pickling errors in Python 3 when serializing objects   for disk queues (:issue:`3082`) * flags are now preserved when copying Requests (:issue:`3342`); * FormRequest.from_response clickdata shouldn't ignore elements with``input\[type=image\]``(:issue:`3153`). * FormRequest.from_response should preserve duplicate keys (:issue:`3247`)  Documentation improvements ~~~~~~~~~~~~~~~~~~~~~~~~~~  * Docs are re-written to suggest .get/.getall API instead of   .extract/.extract_first. Also, [topics-selectors](#topics-selectors) docs are updated   and re-structured to match latest parsel docs; they now contain more topics,   such as [selecting-attributes](#selecting-attributes) or [topics-selectors-css-extensions](#topics-selectors-css-extensions)   (:issue:`3390`). * [topics-developer-tools](#topics-developer-tools) is a new tutorial which replaces   old Firefox and Firebug tutorials (:issue:`3400`). * SCRAPY_PROJECT environment variable is documented (:issue:`3518`); * troubleshooting section is added to install instructions (:issue:`3517`); * improved links to beginner resources in the tutorial   (:issue:`3367`, :issue:`3468`); * fixed :setting:`RETRY_HTTP_CODES` default values in docs (:issue:`3335`); * remove unused``DEPTH\_STATS``option from docs (:issue:`3245`); * other cleanups (:issue:`3347`, :issue:`3350`, :issue:`3445`, :issue:`3544`,   :issue:`3605`).  Deprecation removals ~~~~~~~~~~~~~~~~~~~~  Compatibility shims for pre-1.0 Scrapy module names are removed (:issue:`3318`):  *``scrapy.command`*`scrapy.contrib`(with all submodules) *`scrapy.contrib\_exp`(with all submodules) *`scrapy.dupefilter`*`scrapy.linkextractor`*`scrapy.project`*`scrapy.spider`*`scrapy.spidermanager`*`scrapy.squeue`*`scrapy.stats`*`scrapy.statscol`*`scrapy.utils.decorator`See [module-relocations](#module-relocations) for more information, or use suggestions from Scrapy 1.5.x deprecation warnings to update your code.  Other deprecation removals:  * Deprecated scrapy.interfaces.ISpiderManager is removed; please use   scrapy.interfaces.ISpiderLoader. * Deprecated`CrawlerSettings``class is removed (:issue:`3327`). * Deprecated``Settings.overrides`and`Settings.defaults``attributes   are removed (:issue:`3327`, :issue:`3359`).  Other improvements, cleanups ~~~~~~~~~~~~~~~~~~~~~~~~~~~~  * All Scrapy tests now pass on Windows; Scrapy testing suite is executed   in a Windows environment on CI (:issue:`3315`). * Python 3.7 support (:issue:`3326`, :issue:`3150`, :issue:`3547`). * Testing and CI fixes (:issue:`3526`, :issue:`3538`, :issue:`3308`,   :issue:`3311`, :issue:`3309`, :issue:`3305`, :issue:`3210`, :issue:`3299`) *``scrapy.http.cookies.CookieJar.clear``accepts "domain", "path" and "name"   optional arguments (:issue:`3231`). * additional files are included to sdist (:issue:`3495`); * code style fixes (:issue:`3405`, :issue:`3304`); * unneeded .strip() call is removed (:issue:`3519`); * collections.deque is used to store MiddlewareManager methods instead   of a list (:issue:`3476`)  .. _release-1.5.2:  Scrapy 1.5.2 (2019-01-22) -------------------------  * *Security bugfix*: Telnet console extension can be easily exploited by rogue   websites POSTing content to http://localhost:6023, we haven't found a way to   exploit it from Scrapy, but it is very easy to trick a browser to do so and   elevates the risk for local development environment.    *The fix is backward incompatible*, it enables telnet user-password   authentication by default with a random generated password. If you can't   upgrade right away, please consider setting :setting:`TELNETCONSOLE_PORT`   out of its default value.    See [telnet console <topics-telnetconsole>](#telnet-console-<topics-telnetconsole>) documentation for more info  * Backport CI build failure under GCE environment due to boto import error.  .. _release-1.5.1:  Scrapy 1.5.1 (2018-07-12) -------------------------  This is a maintenance release with important bug fixes, but no new features:  *``O(N^2)``gzip decompression issue which affected Python 3 and PyPy   is fixed (:issue:`3281`); * skipping of TLS validation errors is improved (:issue:`3166`); * Ctrl-C handling is fixed in Python 3.5+ (:issue:`3096`); * testing fixes (:issue:`3092`, :issue:`3263`); * documentation improvements (:issue:`3058`, :issue:`3059`, :issue:`3089`,   :issue:`3123`, :issue:`3127`, :issue:`3189`, :issue:`3224`, :issue:`3280`,   :issue:`3279`, :issue:`3201`, :issue:`3260`, :issue:`3284`, :issue:`3298`,   :issue:`3294`).   .. _release-1.5.0:  Scrapy 1.5.0 (2017-12-29) -------------------------  This release brings small new features and improvements across the codebase. Some highlights:  * Google Cloud Storage is supported in FilesPipeline and ImagesPipeline. * Crawling with proxy servers becomes more efficient, as connections   to proxies can be reused now. * Warnings, exception and logging messages are improved to make debugging   easier. *``scrapy parse`command now allows to set custom request meta via`--meta``argument. * Compatibility with Python 3.6, PyPy and PyPy3 is improved;   PyPy and PyPy3 are now supported officially, by running tests on CI. * Better default handling of HTTP 308, 522 and 524 status codes. * Documentation is improved, as usual.  Backward Incompatible Changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  * Scrapy 1.5 drops support for Python 3.3. * Default Scrapy User-Agent now uses https link to scrapy.org (:issue:`2983`).   **This is technically backward-incompatible**; override   :setting:`USER_AGENT` if you relied on old value. * Logging of settings overridden by``custom\_settings`is fixed;   **this is technically backward-incompatible** because the logger   changes from`\[scrapy.utils.log\]`to`\[scrapy.crawler\]``. If you're   parsing Scrapy logs, please update your log parsers (:issue:`1343`). * LinkExtractor now ignores``m4v`extension by default, this is change   in behavior. * 522 and 524 status codes are added to`RETRY\_HTTP\_CODES``(:issue:`2851`)  New features ~~~~~~~~~~~~  - Support``\<link\>`tags in`Response.follow``(:issue:`2785`) - Support for``ptpython``REPL (:issue:`2654`) - Google Cloud Storage support for FilesPipeline and ImagesPipeline   (:issue:`2923`). - New``--meta``option of the "scrapy parse" command allows to pass additional   request.meta (:issue:`2883`) - Populate spider variable when using``shell.inspect\_response``(:issue:`2812`) - Handle HTTP 308 Permanent Redirect (:issue:`2844`) - Add 522 and 524 to``RETRY\_HTTP\_CODES``(:issue:`2851`) - Log versions information at startup (:issue:`2857`) -``scrapy.mail.MailSender``now works in Python 3 (it requires Twisted 17.9.0) - Connections to proxy servers are reused (:issue:`2743`) - Add template for a downloader middleware (:issue:`2755`) - Explicit message for NotImplementedError when parse callback not defined   (:issue:`2831`) - CrawlerProcess got an option to disable installation of root log handler   (:issue:`2921`) - LinkExtractor now ignores``m4v``extension by default - Better log messages for responses over :setting:`DOWNLOAD_WARNSIZE` and   :setting:`DOWNLOAD_MAXSIZE` limits (:issue:`2927`) - Show warning when a URL is put to``Spider.allowed\_domains``instead of   a domain (:issue:`2250`).  Bug fixes ~~~~~~~~~  - Fix logging of settings overridden by``custom\_settings`;   **this is technically backward-incompatible** because the logger   changes from`\[scrapy.utils.log\]`to`\[scrapy.crawler\]``, so please   update your log parsers if needed (:issue:`1343`) - Default Scrapy User-Agent now uses https link to scrapy.org (:issue:`2983`).   **This is technically backward-incompatible**; override   :setting:`USER_AGENT` if you relied on old value. - Fix PyPy and PyPy3 test failures, support them officially   (:issue:`2793`, :issue:`2935`, :issue:`2990`, :issue:`3050`, :issue:`2213`,   :issue:`3048`) - Fix DNS resolver when``DNSCACHE\_ENABLED=False``(:issue:`2811`) - Add``cryptography``for Debian Jessie tox test env (:issue:`2848`) - Add verification to check if Request callback is callable (:issue:`2766`) - Port``extras/qpsclient.py``to Python 3 (:issue:`2849`) - Use getfullargspec under the scenes for Python 3 to stop DeprecationWarning   (:issue:`2862`) - Update deprecated test aliases (:issue:`2876`) - Fix``SitemapSpider``support for alternate links (:issue:`2853`)  Docs ~~~~  - Added missing bullet point for the``AUTOTHROTTLE\_TARGET\_CONCURRENCY``setting. (:issue:`2756`) - Update Contributing docs, document new support channels   (:issue:`2762`, issue:`3038`) - Include references to Scrapy subreddit in the docs - Fix broken links; use``<https://>``for external links   (:issue:`2978`, :issue:`2982`, :issue:`2958`) - Document CloseSpider extension better (:issue:`2759`) - Use``pymongo.collection.Collection.insert\_one()``in MongoDB example   (:issue:`2781`) - Spelling mistake and typos   (:issue:`2828`, :issue:`2837`, :issue:`2884`, :issue:`2924`) - Clarify``CSVFeedSpider.headers``documentation (:issue:`2826`) - Document``DontCloseSpider`exception and clarify`spider\_idle``(:issue:`2791`) - Update "Releases" section in README (:issue:`2764`) - Fix rst syntax in``DOWNLOAD\_FAIL\_ON\_DATALOSS``docs (:issue:`2763`) - Small fix in description of startproject arguments (:issue:`2866`) - Clarify data types in Response.body docs (:issue:`2922`) - Add a note about``request.meta\['depth'\]``to DepthMiddleware docs (:issue:`2374`) - Add a note about``request.meta\['dont\_merge\_cookies'\]``to CookiesMiddleware   docs (:issue:`2999`) - Up-to-date example of project structure (:issue:`2964`, :issue:`2976`) - A better example of ItemExporters usage (:issue:`2989`) - Document``from\_crawler``methods for spider and downloader middlewares   (:issue:`3019`)  .. _release-1.4.0:  Scrapy 1.4.0 (2017-05-18) -------------------------  Scrapy 1.4 does not bring that many breathtaking new features but quite a few handy improvements nonetheless.  Scrapy now supports anonymous FTP sessions with customizable user and password via the new :setting:`FTP_USER` and :setting:`FTP_PASSWORD` settings. And if you're using Twisted version 17.1.0 or above, FTP is now available with Python 3.  There's a new `response.follow <scrapy.http.TextResponse.follow>` method for creating requests; **it is now a recommended way to create Requests in Scrapy spiders**. This method makes it easier to write correct spiders;``response.follow`has several advantages over creating`scrapy.Request`objects directly:  * it handles relative URLs; * it works properly with non-ascii URLs on non-UTF8 pages; * in addition to absolute and relative URLs it supports Selectors;   for`\<a\>`elements it can also extract their href values.  For example, instead of this::      for href in response.css('li.page a::attr(href)').extract():         url = response.urljoin(href)         yield scrapy.Request(url, self.parse, encoding=response.encoding)  One can now write this::      for a in response.css('li.page a'):         yield response.follow(a, self.parse)  Link extractors are also improved. They work similarly to what a regular modern browser would do: leading and trailing whitespace are removed from attributes (think`href=" <http://example.com>"`) when building`Link`objects. This whitespace-stripping also happens for`action`attributes with`FormRequest`.  **Please also note that link extractors do not canonicalize URLs by default anymore.** This was puzzling users every now and then, and it's not what browsers do in fact, so we removed that extra transformation on extracted links.  For those of you wanting more control on the`Referer:`header that Scrapy sends when following links, you can set your own`Referrer Policy`. Prior to Scrapy 1.4, the default`RefererMiddleware``would simply and blindly set it to the URL of the response that generated the HTTP request (which could leak information on your URL seeds). By default, Scrapy now behaves much like your regular browser does. And this policy is fully customizable with W3C standard values (or with something really custom of your own if you wish). See :setting:`REFERRER_POLICY` for details.  To make Scrapy spiders easier to debug, Scrapy logs more stats by default in 1.4: memory usage stats, detailed retry stats, detailed HTTP error code stats. A similar change is that HTTP cache path is also visible in logs now.  Last but not least, Scrapy now has the option to make JSON and XML items more human-readable, with newlines between items and even custom indenting offset, using the new :setting:`FEED_EXPORT_INDENT` setting.  Enjoy! (Or read on for the rest of changes in this release.)  Deprecations and Backward Incompatible Changes ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  - Default to``canonicalize=False``in   `scrapy.linkextractors.LinkExtractor   <scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor>`   (:issue:`2537`, fixes :issue:`1941` and :issue:`1982`):   **warning, this is technically backward-incompatible** - Enable memusage extension by default (:issue:`2539`, fixes :issue:`2187`);   **this is technically backward-incompatible** so please check if you have   any non-default``[MEMUSAGE]()**\*\`\` options set. - \`\`EDITOR\`\` environment variable now takes precedence over \`\`EDITOR\`\` option defined in settings.py (:issue:\`1829\`); Scrapy default settings no longer depend on environment variables.**This is technically a backward incompatible change\*\*. - `Spider.make_requests_from_url` is deprecated (`1728`, fixes `1495`).

### New Features

  - Accept proxy credentials in `proxy` request meta key (`2526`)
  - Support [brotli-compressed](https://www.ietf.org/rfc/rfc7932.txt) content; requires optional [brotlipy](https://github.com/python-hyper/brotlipy/) (`2535`)
  - New \[response.follow \<response-follow-example\>\](\#response.follow-\<response-follow-example\>) shortcut for creating requests (`1940`)
  - Added `flags` argument and attribute to <span class="title-ref">Request \<scrapy.http.Request\></span> objects (`2047`)
  - Support Anonymous FTP (`2342`)
  - Added `retry/count`, `retry/max_reached` and `retry/reason_count/<reason>` stats to <span class="title-ref">RetryMiddleware \<scrapy.downloadermiddlewares.retry.RetryMiddleware\></span> (`2543`)
  - Added `httperror/response_ignored_count` and `httperror/response_ignored_status_count/<status>` stats to <span class="title-ref">HttpErrorMiddleware \<scrapy.spidermiddlewares.httperror.HttpErrorMiddleware\></span> (`2566`)
  - Customizable `Referrer policy <REFERRER_POLICY>` in <span class="title-ref">RefererMiddleware \<scrapy.spidermiddlewares.referer.RefererMiddleware\></span> (`2306`)
  - New `data:` URI download handler (`2334`, fixes `2156`)
  - Log cache directory when HTTP Cache is used (`2611`, fixes `2604`)
  - Warn users when project contains duplicate spider names (fixes `2181`)
  - `scrapy.utils.datatypes.CaselessDict` now accepts `Mapping` instances and not only dicts (`2646`)
  - \[Media downloads \<topics-media-pipeline\>\](\#media-downloads-\<topics-media-pipeline\>), with <span class="title-ref">\~scrapy.pipelines.files.FilesPipeline</span> or <span class="title-ref">\~scrapy.pipelines.images.ImagesPipeline</span>, can now optionally handle HTTP redirects using the new `MEDIA_ALLOW_REDIRECTS` setting (`2616`, fixes `2004`)
  - Accept non-complete responses from websites using a new `DOWNLOAD_FAIL_ON_DATALOSS` setting (`2590`, fixes `2586`)
  - Optional pretty-printing of JSON and XML items via `FEED_EXPORT_INDENT` setting (`2456`, fixes `1327`)
  - Allow dropping fields in `FormRequest.from_response` formdata when `None` value is passed (`667`)
  - Per-request retry times with the new `max_retry_times` meta key (`2642`)
  - `python -m scrapy` as a more explicit alternative to `scrapy` command (`2740`)

### Bug fixes

  - LinkExtractor now strips leading and trailing whitespaces from attributes (`2547`, fixes `1614`)
  - Properly handle whitespaces in action attribute in <span class="title-ref">\~scrapy.http.FormRequest</span> (`2548`)
  - Buffer CONNECT response bytes from proxy until all HTTP headers are received (`2495`, fixes `2491`)
  - FTP downloader now works on Python 3, provided you use Twisted\>=17.1 (`2599`)
  - Use body to choose response type after decompressing content (`2393`, fixes `2145`)
  - Always decompress `Content-Encoding: gzip` at <span class="title-ref">HttpCompressionMiddleware \<scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware\></span> stage (`2391`)
  - Respect custom log level in `Spider.custom_settings` (`2581`, fixes `1612`)
  - 'make htmlview' fix for macOS (`2661`)
  - Remove "commands" from the command list (`2695`)
  - Fix duplicate Content-Length header for POST requests with empty body (`2677`)
  - Properly cancel large downloads, i.e. above `DOWNLOAD_MAXSIZE` (`1616`)
  - ImagesPipeline: fixed processing of transparent PNG images with palette (`2675`)

### Cleanups & Refactoring

  - Tests: remove temp files and folders (`2570`), fixed ProjectUtilsTest on macOS (`2569`), use portable pypy for Linux on Travis CI (`2710`)
  - Separate building request from `_requests_to_follow` in CrawlSpider (`2562`)
  - Remove “Python 3 progress” badge (`2567`)
  - Add a couple more lines to `.gitignore` (`2557`)
  - Remove bumpversion prerelease configuration (`2159`)
  - Add codecov.yml file (`2750`)
  - Set context factory implementation based on Twisted version (`2577`, fixes `2560`)
  - Add omitted `self` arguments in default project middleware template (`2595`)
  - Remove redundant `slot.add_request()` call in ExecutionEngine (`2617`)
  - Catch more specific `os.error` exception in `scrapy.pipelines.files.FSFilesStore` (`2644`)
  - Change "localhost" test server certificate (`2720`)
  - Remove unused `MEMUSAGE_REPORT` setting (`2576`)

### Documentation

  - Binary mode is required for exporters (`2564`, fixes `2553`)
  - Mention issue with <span class="title-ref">FormRequest.from\_response \<scrapy.http.FormRequest.from\_response\></span> due to bug in lxml (`2572`)
  - Use single quotes uniformly in templates (`2596`)
  - Document `ftp_user` and `ftp_password` meta keys (`2587`)
  - Removed section on deprecated `contrib/` (`2636`)
  - Recommend Anaconda when installing Scrapy on Windows (`2477`, fixes `2475`)
  - FAQ: rewrite note on Python 3 support on Windows (`2690`)
  - Rearrange selector sections (`2705`)
  - Remove `__nonzero__` from <span class="title-ref">\~scrapy.selector.SelectorList</span> docs (`2683`)
  - Mention how to disable request filtering in documentation of `DUPEFILTER_CLASS` setting (`2714`)
  - Add sphinx\_rtd\_theme to docs setup readme (`2668`)
  - Open file in text mode in JSON item writer example (`2729`)
  - Clarify `allowed_domains` example (`2670`)

## Scrapy 1.3.3 (2017-03-10)

### Bug fixes

  - Make `SpiderLoader` raise `ImportError` again by default for missing dependencies and wrong `SPIDER_MODULES`. These exceptions were silenced as warnings since 1.3.0. A new setting is introduced to toggle between warning or exception if needed ; see `SPIDER_LOADER_WARN_ONLY` for details.

## Scrapy 1.3.2 (2017-02-13)

### Bug fixes

  - Preserve request class when converting to/from dicts (utils.reqser) (`2510`).
  - Use consistent selectors for author field in tutorial (`2551`).
  - Fix TLS compatibility in Twisted 17+ (`2558`)

## Scrapy 1.3.1 (2017-02-08)

### New features

  - Support `'True'` and `'False'` string values for boolean settings (`2519`); you can now do something like `scrapy crawl myspider -s REDIRECT_ENABLED=False`.
  - Support kwargs with `response.xpath()` to use \[XPath variables \<topics-selectors-xpath-variables\>\](\#xpath-variables-\<topics-selectors-xpath-variables\>) and ad-hoc namespaces declarations ; this requires at least Parsel v1.1 (`2457`).
  - Add support for Python 3.6 (`2485`).
  - Run tests on PyPy (warning: some tests still fail, so PyPy is not supported yet).

### Bug fixes

  - Enforce `DNS_TIMEOUT` setting (`2496`).
  - Fix `view` command ; it was a regression in v1.3.0 (`2503`).
  - Fix tests regarding `*_EXPIRES settings` with Files/Images pipelines (`2460`).
  - Fix name of generated pipeline class when using basic project template (`2466`).
  - Fix compatibility with Twisted 17+ (`2496`, `2528`).
  - Fix `scrapy.Item` inheritance on Python 3.6 (`2511`).
  - Enforce numeric values for components order in `SPIDER_MIDDLEWARES`, `DOWNLOADER_MIDDLEWARES`, `EXTENSIONS` and `SPIDER_CONTRACTS` (`2420`).

### Documentation

  - Reword Code of Conduct section and upgrade to Contributor Covenant v1.4 (`2469`).
  - Clarify that passing spider arguments converts them to spider attributes (`2483`).
  - Document `formid` argument on `FormRequest.from_response()` (`2497`).
  - Add .rst extension to README files (`2507`).
  - Mention LevelDB cache storage backend (`2525`).
  - Use `yield` in sample callback code (`2533`).
  - Add note about HTML entities decoding with `.re()/.re_first()` (`1704`).
  - Typos (`2512`, `2534`, `2531`).

### Cleanups

  - Remove redundant check in `MetaRefreshMiddleware` (`2542`).
  - Faster checks in `LinkExtractor` for allow/deny patterns (`2538`).
  - Remove dead code supporting old Twisted versions (`2544`).

## Scrapy 1.3.0 (2016-12-21)

This release comes rather soon after 1.2.2 for one main reason: it was found out that releases since 0.18 up to 1.2.2 (included) use some backported code from Twisted (`scrapy.xlib.tx.*`), even if newer Twisted modules are available. Scrapy now uses `twisted.web.client` and `twisted.internet.endpoints` directly. (See also cleanups below.)

As it is a major change, we wanted to get the bug fix out quickly while not breaking any projects using the 1.2 series.

### New Features

  - `MailSender` now accepts single strings as values for `to` and `cc` arguments (`2272`)
  - `scrapy fetch url`, `scrapy shell url` and `fetch(url)` inside Scrapy shell now follow HTTP redirections by default (`2290`); See `fetch` and `shell` for details.
  - `HttpErrorMiddleware` now logs errors with `INFO` level instead of `DEBUG`; this is technically **backward incompatible** so please check your log parsers.
  - By default, logger names now use a long-form path, e.g. `[scrapy.extensions.logstats]`, instead of the shorter "top-level" variant of prior releases (e.g. `[scrapy]`); this is **backward incompatible** if you have log parsers expecting the short logger name part. You can switch back to short logger names using `LOG_SHORT_NAMES` set to `True`.

### Dependencies & Cleanups

  - Scrapy now requires Twisted \>= 13.1 which is the case for many Linux distributions already.
  - As a consequence, we got rid of `scrapy.xlib.tx.*` modules, which copied some of Twisted code for users stuck with an "old" Twisted version
  - `ChunkedTransferMiddleware` is deprecated and removed from the default downloader middlewares.

## Scrapy 1.2.3 (2017-03-03)

  - Packaging fix: disallow unsupported Twisted versions in setup.py

## Scrapy 1.2.2 (2016-12-06)

### Bug fixes

  - Fix a cryptic traceback when a pipeline fails on `open_spider()` (`2011`)
  - Fix embedded IPython shell variables (fixing `396` that re-appeared in 1.2.0, fixed in `2418`)
  - A couple of patches when dealing with robots.txt:
      - handle (non-standard) relative sitemap URLs (`2390`)
      - handle non-ASCII URLs and User-Agents in Python 2 (`2373`)

### Documentation

  - Document `"download_latency"` key in `Request`'s `meta` dict (`2033`)
  - Remove page on (deprecated & unsupported) Ubuntu packages from ToC (`2335`)
  - A few fixed typos (`2346`, `2369`, `2369`, `2380`) and clarifications (`2354`, `2325`, `2414`)

### Other changes

  - Advertize [conda-forge](https://anaconda.org/conda-forge/scrapy) as Scrapy's official conda channel (`2387`)
  - More helpful error messages when trying to use `.css()` or `.xpath()` on non-Text Responses (`2264`)
  - `startproject` command now generates a sample `middlewares.py` file (`2335`)
  - Add more dependencies' version info in `scrapy version` verbose output (`2404`)
  - Remove all `*.pyc` files from source distribution (`2386`)

## Scrapy 1.2.1 (2016-10-21)

### Bug fixes

  - Include OpenSSL's more permissive default ciphers when establishing TLS/SSL connections (`2314`).
  - Fix "Location" HTTP header decoding on non-ASCII URL redirects (`2321`).

### Documentation

  - Fix JsonWriterPipeline example (`2302`).
  - Various notes: `2330` on spider names, `2329` on middleware methods processing order, `2327` on getting multi-valued HTTP headers as lists.

### Other changes

  - Removed `www.` from `start_urls` in built-in spider templates (`2299`).

## Scrapy 1.2.0 (2016-10-03)

### New Features

  - New `FEED_EXPORT_ENCODING` setting to customize the encoding used when writing items to a file. This can be used to turn off `\uXXXX` escapes in JSON output. This is also useful for those wanting something else than UTF-8 for XML or CSV output (`2034`).
  - `startproject` command now supports an optional destination directory to override the default one based on the project name (`2005`).
  - New `SCHEDULER_DEBUG` setting to log requests serialization failures (`1610`).
  - JSON encoder now supports serialization of `set` instances (`2058`).
  - Interpret `application/json-amazonui-streaming` as `TextResponse` (`1503`).
  - `scrapy` is imported by default when using shell tools (`shell`, \[inspect\_response \<topics-shell-inspect-response\>\](\#inspect\_response-\<topics-shell-inspect-response\>)) (`2248`).

### Bug fixes

  - DefaultRequestHeaders middleware now runs before UserAgent middleware (`2088`). **Warning: this is technically backward incompatible**, though we consider this a bug fix.
  - HTTP cache extension and plugins that use the `.scrapy` data directory now work outside projects (`1581`). **Warning: this is technically backward incompatible**, though we consider this a bug fix.
  - `Selector` does not allow passing both `response` and `text` anymore (`2153`).
  - Fixed logging of wrong callback name with `scrapy parse` (`2169`).
  - Fix for an odd gzip decompression bug (`1606`).
  - Fix for selected callbacks when using `CrawlSpider` with `scrapy parse <parse>` (`2225`).
  - Fix for invalid JSON and XML files when spider yields no items (`872`).
  - Implement `flush()` for `StreamLogger` avoiding a warning in logs (`2125`).

### Refactoring

  - `canonicalize_url` has been moved to [w3lib.url](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url) (`2168`).

### Tests & Requirements

Scrapy's new requirements baseline is Debian 8 "Jessie". It was previously Ubuntu 12.04 Precise. What this means in practice is that we run continuous integration tests with these (main) packages versions at a minimum: Twisted 14.0, pyOpenSSL 0.14, lxml 3.4.

Scrapy may very well work with older versions of these packages (the code base still has switches for older Twisted versions for example) but it is not guaranteed (because it's not tested anymore).

### Documentation

  - Grammar fixes: `2128`, `1566`.
  - Download stats badge removed from README (`2160`).
  - New Scrapy \[architecture diagram \<topics-architecture\>\](\#architecture-diagram-\<topics-architecture\>) (`2165`).
  - Updated `Response` parameters documentation (`2197`).
  - Reworded misleading `RANDOMIZE_DOWNLOAD_DELAY` description (`2190`).
  - Add StackOverflow as a support channel (`2257`).

## Scrapy 1.1.4 (2017-03-03)

  - Packaging fix: disallow unsupported Twisted versions in setup.py

## Scrapy 1.1.3 (2016-09-22)

### Bug fixes

  - Class attributes for subclasses of `ImagesPipeline` and `FilesPipeline` work as they did before 1.1.1 (`2243`, fixes `2198`)

### Documentation

  - \[Overview \<intro-overview\>\](\#overview-\<intro-overview\>) and \[tutorial \<intro-tutorial\>\](\#tutorial-\<intro-tutorial\>) rewritten to use <http://toscrape.com> websites (`2236`, `2249`, `2252`).

## Scrapy 1.1.2 (2016-08-18)

### Bug fixes

  - Introduce a missing `IMAGES_STORE_S3_ACL` setting to override the default ACL policy in `ImagesPipeline` when uploading images to S3 (note that default ACL policy is "private" -- instead of "public-read" --since Scrapy 1.1.0)
  - `IMAGES_EXPIRES` default value set back to 90 (the regression was introduced in 1.1.1)

## Scrapy 1.1.1 (2016-07-13)

### Bug fixes

  - Add "Host" header in CONNECT requests to HTTPS proxies (`2069`)
  - Use response `body` when choosing response class (`2001`, fixes `2000`)
  - Do not fail on canonicalizing URLs with wrong netlocs (`2038`, fixes `2010`)
  - a few fixes for `HttpCompressionMiddleware` (and `SitemapSpider`):
      - Do not decode HEAD responses (`2008`, fixes `1899`)
      - Handle charset parameter in gzip Content-Type header (`2050`, fixes `2049`)
      - Do not decompress gzip octet-stream responses (`2065`, fixes `2063`)
  - Catch (and ignore with a warning) exception when verifying certificate against IP-address hosts (`2094`, fixes `2092`)
  - Make `FilesPipeline` and `ImagesPipeline` backward compatible again regarding the use of legacy class attributes for customization (`1989`, fixes `1985`)

### New features

  - Enable genspider command outside project folder (`2052`)
  - Retry HTTPS CONNECT `TunnelError` by default (`1974`)

### Documentation

  - `FEED_TEMPDIR` setting at lexicographical position (`9b3c72c`)
  - Use idiomatic `.extract_first()` in overview (`1994`)
  - Update years in copyright notice (`c2c8036`)
  - Add information and example on errbacks (`1995`)
  - Use "url" variable in downloader middleware example (`2015`)
  - Grammar fixes (`2054`, `2120`)
  - New FAQ entry on using BeautifulSoup in spider callbacks (`2048`)
  - Add notes about Scrapy not working on Windows with Python 3 (`2060`)
  - Encourage complete titles in pull requests (`2026`)

### Tests

  - Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1 (`2095`)

## Scrapy 1.1.0 (2016-05-11)

This 1.1 release brings a lot of interesting features and bug fixes:

  - Scrapy 1.1 has beta Python 3 support (requires Twisted \>= 15.5). See \[news\_betapy3\](\#news\_betapy3) for more details and some limitations.
  - Hot new features:
      - Item loaders now support nested loaders (`1467`).
      - `FormRequest.from_response` improvements (`1382`, `1137`).
      - Added setting `AUTOTHROTTLE_TARGET_CONCURRENCY` and improved AutoThrottle docs (`1324`).
      - Added `response.text` to get body as unicode (`1730`).
      - Anonymous S3 connections (`1358`).
      - Deferreds in downloader middlewares (`1473`). This enables better robots.txt handling (`1471`).
      - HTTP caching now follows RFC2616 more closely, added settings `HTTPCACHE_ALWAYS_STORE` and `HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS` (`1151`).
      - Selectors were extracted to the [parsel](https://github.com/scrapy/parsel) library (`1409`). This means you can use Scrapy Selectors without Scrapy and also upgrade the selectors engine without needing to upgrade Scrapy.
      - HTTPS downloader now does TLS protocol negotiation by default, instead of forcing TLS 1.0. You can also set the SSL/TLS method using the new `DOWNLOADER_CLIENT_TLS_METHOD`.
  - These bug fixes may require your attention:
      - Don't retry bad requests (HTTP 400) by default (`1289`). If you need the old behavior, add `400` to `RETRY_HTTP_CODES`.
      - Fix shell files argument handling (`1710`, `1550`). If you try `scrapy shell index.html` it will try to load the URL `http://index.html`, use `scrapy shell ./index.html` to load a local file.
      - Robots.txt compliance is now enabled by default for newly-created projects (`1724`). Scrapy will also wait for robots.txt to be downloaded before proceeding with the crawl (`1735`). If you want to disable this behavior, update `ROBOTSTXT_OBEY` in `settings.py` file after creating a new project.
      - Exporters now work on unicode, instead of bytes by default (`1080`). If you use <span class="title-ref">\~scrapy.exporters.PythonItemExporter</span>, you may want to update your code to disable binary mode which is now deprecated.
      - Accept XML node names containing dots as valid (`1533`).
      - When uploading files or images to S3 (with `FilesPipeline` or `ImagesPipeline`), the default ACL policy is now "private" instead of "public" **Warning: backward incompatible\!**. You can use `FILES_STORE_S3_ACL` to change it.
      - We've reimplemented `canonicalize_url()` for more correct output, especially for URLs with non-ASCII characters (`1947`). This could change link extractors output compared to previous Scrapy versions. This may also invalidate some cache entries you could still have from pre-1.1 runs. **Warning: backward incompatible\!**.

Keep reading for more details on other improvements and bug fixes.

### Beta Python 3 Support

We have been [hard at work to make Scrapy run on Python 3](https://github.com/scrapy/scrapy/wiki/Python-3-Porting). As a result, now you can run spiders on Python 3.3, 3.4 and 3.5 (Twisted \>= 15.5 required). Some features are still missing (and some may never be ported).

Almost all builtin extensions/middlewares are expected to work. However, we are aware of some limitations in Python 3:

  - Scrapy does not work on Windows with Python 3
  - Sending emails is not supported
  - FTP download handler is not supported
  - Telnet console is not supported

### Additional New Features and Enhancements

  - Scrapy now has a [Code of Conduct](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md) (`1681`).
  - Command line tool now has completion for zsh (`934`).
  - Improvements to `scrapy shell`:
      - Support for bpython and configure preferred Python shell via `SCRAPY_PYTHON_SHELL` (`1100`, `1444`).
      - Support URLs without scheme (`1498`) **Warning: backward incompatible\!**
      - Bring back support for relative file path (`1710`, `1550`).
  - Added `MEMUSAGE_CHECK_INTERVAL_SECONDS` setting to change default check interval (`1282`).
  - Download handlers are now lazy-loaded on first request using their scheme (`1390`, `1421`).
  - HTTPS download handlers do not force TLS 1.0 anymore; instead, OpenSSL's `SSLv23_method()/TLS_method()` is used allowing to try negotiating with the remote hosts the highest TLS protocol version it can (`1794`, `1629`).
  - `RedirectMiddleware` now skips the status codes from `handle_httpstatus_list` on spider attribute or in `Request`'s `meta` key (`1334`, `1364`, `1447`).
  - Form submission:
      - now works with `<button>` elements too (`1469`).
      - an empty string is now used for submit buttons without a value (`1472`)
  - Dict-like settings now have per-key priorities (`1135`, `1149` and `1586`).
  - Sending non-ASCII emails (`1662`)
  - `CloseSpider` and `SpiderState` extensions now get disabled if no relevant setting is set (`1723`, `1725`).
  - Added method `ExecutionEngine.close` (`1423`).
  - Added method `CrawlerRunner.create_crawler` (`1528`).
  - Scheduler priority queue can now be customized via `SCHEDULER_PRIORITY_QUEUE` (`1822`).
  - `.pps` links are now ignored by default in link extractors (`1835`).
  - temporary data folder for FTP and S3 feed storages can be customized using a new `FEED_TEMPDIR` setting (`1847`).
  - `FilesPipeline` and `ImagesPipeline` settings are now instance attributes instead of class attributes, enabling spider-specific behaviors (`1891`).
  - `JsonItemExporter` now formats opening and closing square brackets on their own line (first and last lines of output file) (`1950`).
  - If available, `botocore` is used for `S3FeedStorage`, `S3DownloadHandler` and `S3FilesStore` (`1761`, `1883`).
  - Tons of documentation updates and related fixes (`1291`, `1302`, `1335`, `1683`, `1660`, `1642`, `1721`, `1727`, `1879`).
  - Other refactoring, optimizations and cleanup (`1476`, `1481`, `1477`, `1315`, `1290`, `1750`, `1881`).

### Deprecations and Removals

  - Added `to_bytes` and `to_unicode`, deprecated `str_to_unicode` and `unicode_to_str` functions (`778`).
  - `binary_is_text` is introduced, to replace use of `isbinarytext` (but with inverse return value) (`1851`)
  - The `optional_features` set has been removed (`1359`).
  - The `--lsprof` command line option has been removed (`1689`). **Warning: backward incompatible**, but doesn't break user code.
  - The following datatypes were deprecated (`1720`):
      - `scrapy.utils.datatypes.MultiValueDictKeyError`
      - `scrapy.utils.datatypes.MultiValueDict`
      - `scrapy.utils.datatypes.SiteNode`
  - The previously bundled `scrapy.xlib.pydispatch` library was deprecated and replaced by [pydispatcher](https://pypi.org/project/PyDispatcher/).

### Relocations

  - `telnetconsole` was relocated to `extensions/` (`1524`).
      - Note: telnet is not enabled on Python 3 (<https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595>)

### Bugfixes

  - Scrapy does not retry requests that got a `HTTP 400 Bad Request` response anymore (`1289`). **Warning: backward incompatible\!**
  - Support empty password for http\_proxy config (`1274`).
  - Interpret `application/x-json` as `TextResponse` (`1333`).
  - Support link rel attribute with multiple values (`1201`).
  - Fixed `scrapy.http.FormRequest.from_response` when there is a `<base>` tag (`1564`).
  - Fixed `TEMPLATES_DIR` handling (`1575`).
  - Various `FormRequest` fixes (`1595`, `1596`, `1597`).
  - Makes `_monkeypatches` more robust (`1634`).
  - Fixed bug on `XMLItemExporter` with non-string fields in items (`1738`).
  - Fixed startproject command in macOS (`1635`).
  - Fixed <span class="title-ref">\~scrapy.exporters.PythonItemExporter</span> and CSVExporter for non-string item types (`1737`).
  - Various logging related fixes (`1294`, `1419`, `1263`, `1624`, `1654`, `1722`, `1726` and `1303`).
  - Fixed bug in `utils.template.render_templatefile()` (`1212`).
  - sitemaps extraction from `robots.txt` is now case-insensitive (`1902`).
  - HTTPS+CONNECT tunnels could get mixed up when using multiple proxies to same remote host (`1912`).

## Scrapy 1.0.7 (2017-03-03)

  - Packaging fix: disallow unsupported Twisted versions in setup.py

## Scrapy 1.0.6 (2016-05-04)

  - FIX: RetryMiddleware is now robust to non-standard HTTP status codes (`1857`)
  - FIX: Filestorage HTTP cache was checking wrong modified time (`1875`)
  - DOC: Support for Sphinx 1.4+ (`1893`)
  - DOC: Consistency in selectors examples (`1869`)

## Scrapy 1.0.5 (2016-02-04)

  - FIX: \[Backport\] Ignore bogus links in LinkExtractors (fixes `907`, `108195e`)
  - TST: Changed buildbot makefile to use 'pytest' (`1f3d90a`)
  - DOC: Fixed typos in tutorial and media-pipeline (`808a9ea` and `803bd87`)
  - DOC: Add AjaxCrawlMiddleware to DOWNLOADER\_MIDDLEWARES\_BASE in settings docs (`aa94121`)

## Scrapy 1.0.4 (2015-12-30)

  - Ignoring xlib/tx folder, depending on Twisted version. (`7dfa979`)
  - Run on new travis-ci infra (`6e42f0b`)
  - Spelling fixes (`823a1cc`)
  - escape nodename in xmliter regex (`da3c155`)
  - test xml nodename with dots (`4418fc3`)
  - TST don't use broken Pillow version in tests (`a55078c`)
  - disable log on version command. closes \#1426 (`86fc330`)
  - disable log on startproject command (`db4c9fe`)
  - Add PyPI download stats badge (`df2b944`)
  - don't run tests twice on Travis if a PR is made from a scrapy/scrapy branch (`a83ab41`)
  - Add Python 3 porting status badge to the README (`73ac80d`)
  - fixed RFPDupeFilter persistence (`97d080e`)
  - TST a test to show that dupefilter persistence is not working (`97f2fb3`)
  - explicit close file on <file://> scheme handler (`d9b4850`)
  - Disable dupefilter in shell (`c0d0734`)
  - DOC: Add captions to toctrees which appear in sidebar (`aa239ad`)
  - DOC Removed pywin32 from install instructions as it's already declared as dependency. (`10eb400`)
  - Added installation notes about using Conda for Windows and other OSes. (`1c3600a`)
  - Fixed minor grammar issues. (`7f4ddd5`)
  - fixed a typo in the documentation. (`b71f677`)
  - Version 1 now exists (`5456c0e`)
  - fix another invalid xpath error (`0a1366e`)
  - fix ValueError: Invalid XPath: //div/\[id="not-exists"\]/text() on selectors.rst (`ca8d60f`)
  - Typos corrections (`7067117`)
  - fix typos in downloader-middleware.rst and exceptions.rst, middlware -\> middleware (`32f115c`)
  - Add note to Ubuntu install section about Debian compatibility (`23fda69`)
  - Replace alternative macOS install workaround with virtualenv (`98b63ee`)
  - Reference Homebrew's homepage for installation instructions (`1925db1`)
  - Add oldest supported tox version to contributing docs (`5d10d6d`)
  - Note in install docs about pip being already included in python\>=2.7.9 (`85c980e`)
  - Add non-python dependencies to Ubuntu install section in the docs (`fbd010d`)
  - Add macOS installation section to docs (`d8f4cba`)
  - DOC(ENH): specify path to rtd theme explicitly (`de73b1a`)
  - minor: scrapy.Spider docs grammar (`1ddcc7b`)
  - Make common practices sample code match the comments (`1b85bcf`)
  - nextcall repetitive calls (heartbeats). (`55f7104`)
  - Backport fix compatibility with Twisted 15.4.0 (`b262411`)
  - pin pytest to 2.7.3 (`a6535c2`)
  - Merge pull request \#1512 from mgedmin/patch-1 (`8876111`)
  - Merge pull request \#1513 from mgedmin/patch-2 (`5d4daf8`)
  - Typo (`f8d0682`)
  - Fix list formatting (`5f83a93`)
  - fix Scrapy squeue tests after recent changes to queuelib (`3365c01`)
  - Merge pull request \#1475 from rweindl/patch-1 (`2d688cd`)
  - Update tutorial.rst (`fbc1f25`)
  - Merge pull request \#1449 from rhoekman/patch-1 (`7d6538c`)
  - Small grammatical change (`8752294`)
  - Add openssl version to version command (`13c45ac`)

## Scrapy 1.0.3 (2015-08-11)

  - add service\_identity to Scrapy install\_requires (`cbc2501`)
  - Workaround for travis\#296 (`66af9cd`)

## Scrapy 1.0.2 (2015-08-06)

  - Twisted 15.3.0 does not raises PicklingError serializing lambda functions (`b04dd7d`)
  - Minor method name fix (`6f85c7f`)
  - minor: scrapy.Spider grammar and clarity (`9c9d2e0`)
  - Put a blurb about support channels in CONTRIBUTING (`c63882b`)
  - Fixed typos (`a9ae7b0`)
  - Fix doc reference. (`7c8a4fe`)

## Scrapy 1.0.1 (2015-07-01)

  - Unquote request path before passing to FTPClient, it already escape paths (`cc00ad2`)
  - include tests/ to source distribution in MANIFEST.in (`eca227e`)
  - DOC Fix SelectJmes documentation (`b8567bc`)
  - DOC Bring Ubuntu and Archlinux outside of Windows subsection (`392233f`)
  - DOC remove version suffix from Ubuntu package (`5303c66`)
  - DOC Update release date for 1.0 (`c89fa29`)

## Scrapy 1.0.0 (2015-06-19)

You will find a lot of new features and bugfixes in this major release. Make sure to check our updated \[overview \<intro-overview\>\](\#overview-\<intro-overview\>) to get a glance of some of the changes, along with our brushed \[tutorial \<intro-tutorial\>\](\#tutorial-\<intro-tutorial\>).

### Support for returning dictionaries in spiders

Declaring and returning Scrapy Items is no longer necessary to collect the scraped data from your spider, you can now return explicit dictionaries instead.

*Classic version*

    class MyItem(scrapy.Item):
        url = scrapy.Field()
    
    class MySpider(scrapy.Spider):
        def parse(self, response):
            return MyItem(url=response.url)

*New version*

    class MySpider(scrapy.Spider):
        def parse(self, response):
            return {'url': response.url}

### Per-spider settings (GSoC 2014)

Last Google Summer of Code project accomplished an important redesign of the mechanism used for populating settings, introducing explicit priorities to override any given setting. As an extension of that goal, we included a new level of priority for settings that act exclusively for a single spider, allowing them to redefine project settings.

Start using it by defining a <span class="title-ref">\~scrapy.spiders.Spider.custom\_settings</span> class variable in your spider:

    class MySpider(scrapy.Spider):
        custom_settings = {
            "DOWNLOAD_DELAY": 5.0,
            "RETRY_ENABLED": False,
        }

Read more about settings population: \[topics-settings\](\#topics-settings)

### Python Logging

Scrapy 1.0 has moved away from Twisted logging to support Python built in’s as default logging system. We’re maintaining backward compatibility for most of the old custom interface to call logging functions, but you’ll get warnings to switch to the Python logging API entirely.

*Old version*

    from scrapy import log
    log.msg('MESSAGE', log.INFO)

*New version*

    import logging
    logging.info('MESSAGE')

Logging with spiders remains the same, but on top of the <span class="title-ref">\~scrapy.spiders.Spider.log</span> method you’ll have access to a custom <span class="title-ref">\~scrapy.spiders.Spider.logger</span> created for the spider to issue log events:

    class MySpider(scrapy.Spider):
        def parse(self, response):
            self.logger.info('Response received')

Read more in the logging documentation: \[topics-logging\](\#topics-logging)

### Crawler API refactoring (GSoC 2014)

Another milestone for last Google Summer of Code was a refactoring of the internal API, seeking a simpler and easier usage. Check new core interface in: \[topics-api\](\#topics-api)

A common situation where you will face these changes is while running Scrapy from scripts. Here’s a quick example of how to run a Spider manually with the new API:

    from scrapy.crawler import CrawlerProcess
    
    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'
    })
    process.crawl(MySpider)
    process.start()

Bear in mind this feature is still under development and its API may change until it reaches a stable status.

See more examples for scripts running Scrapy: \[topics-practices\](\#topics-practices)

### Module Relocations

There’s been a large rearrangement of modules trying to improve the general structure of Scrapy. Main changes were separating various subpackages into new projects and dissolving both `scrapy.contrib` and `scrapy.contrib_exp` into top level packages. Backward compatibility was kept among internal relocations, while importing deprecated modules expect warnings indicating their new place.

#### Full list of relocations

Outsourced packages

<div class="note">

<div class="title">

Note

</div>

These extensions went through some minor changes, e.g. some setting names were changed. Please check the documentation in each new repository to get familiar with the new usage.

</div>

| Old location              | New location                                                                                                                       |
| ------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| scrapy.commands.deploy    | [scrapyd-client \<https://github.com /scrapy/scrapyd-client\>]() (See other alternatives here: \[topics-deploy\](\#topics-deploy)) |
| scrapy.contrib.djangoitem | [scrapy-djangoitem \<https://github. com/scrapy-plugins/scrapy-djangoite m\>]()                                                    |
| scrapy.webservice         | [scrapy-jsonrpc \<https://github.com /scrapy-plugins/scrapy-jsonrpc\>]()                                                           |

`scrapy.contrib_exp` and `scrapy.contrib` dissolutions

<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Old location</th>
<th>New location</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>scrapy.contrib_exp.downloadermiddleware.decompression</td>
<td>scrapy.downloadermiddlewares.decompression</td>
</tr>
<tr class="even">
<td>scrapy.contrib_exp.iterators</td>
<td>scrapy.utils.iterators</td>
</tr>
<tr class="odd">
<td>scrapy.contrib.downloadermiddleware</td>
<td>scrapy.downloadermiddlewares</td>
</tr>
<tr class="even">
<td>scrapy.contrib.exporter</td>
<td>scrapy.exporters</td>
</tr>
<tr class="odd">
<td>scrapy.contrib.linkextractors</td>
<td>scrapy.linkextractors</td>
</tr>
<tr class="even">
<td>scrapy.contrib.loader</td>
<td>scrapy.loader</td>
</tr>
<tr class="odd">
<td>scrapy.contrib.loader.processor</td>
<td>scrapy.loader.processors</td>
</tr>
<tr class="even">
<td>scrapy.contrib.pipeline</td>
<td>scrapy.pipelines</td>
</tr>
<tr class="odd">
<td>scrapy.contrib.spidermiddleware</td>
<td>scrapy.spidermiddlewares</td>
</tr>
<tr class="even">
<td>scrapy.contrib.spiders</td>
<td>scrapy.spiders</td>
</tr>
<tr class="odd">
<td><ul>
<li>scrapy.contrib.closespider</li>
<li>scrapy.contrib.corestats</li>
<li>scrapy.contrib.debug</li>
<li>scrapy.contrib.feedexport</li>
<li>scrapy.contrib.httpcache</li>
<li>scrapy.contrib.logstats</li>
<li>scrapy.contrib.memdebug</li>
<li>scrapy.contrib.memusage</li>
<li>scrapy.contrib.spiderstate</li>
<li>scrapy.contrib.statsmailer</li>
<li>scrapy.contrib.throttle</li>
</ul></td>
<td>scrapy.extensions.*</td>
</tr>
</tbody>
</table>

Plural renames and Modules unification

| Old location           | New location            |
| ---------------------- | ----------------------- |
| scrapy.command         | scrapy.commands         |
| scrapy.dupefilter      | scrapy.dupefilters      |
| scrapy.linkextractor   | scrapy.linkextractors   |
| scrapy.spider          | scrapy.spiders          |
| scrapy.squeue          | scrapy.squeues          |
| scrapy.statscol        | scrapy.statscollectors  |
| scrapy.utils.decorator | scrapy.utils.decorators |

Class renames

| Old location                       | New location                     |
| ---------------------------------- | -------------------------------- |
| scrapy.spidermanager.SpiderManager | scrapy.spiderloader.SpiderLoader |

Settings renames

| Old location           | New location          |
| ---------------------- | --------------------- |
| SPIDER\_MANAGER\_CLASS | SPIDER\_LOADER\_CLASS |

### Changelog

New Features and Enhancements

  - Python logging (`1060`, `1235`, `1236`, `1240`, `1259`, `1278`, `1286`)
  - FEED\_EXPORT\_FIELDS option (`1159`, `1224`)
  - Dns cache size and timeout options (`1132`)
  - support namespace prefix in xmliter\_lxml (`963`)
  - Reactor threadpool max size setting (`1123`)
  - Allow spiders to return dicts. (`1081`)
  - Add Response.urljoin() helper (`1086`)
  - look in \~/.config/scrapy.cfg for user config (`1098`)
  - handle TLS SNI (`1101`)
  - Selectorlist extract first (`624`, `1145`)
  - Added JmesSelect (`1016`)
  - add gzip compression to filesystem http cache backend (`1020`)
  - CSS support in link extractors (`983`)
  - httpcache dont\_cache meta \#19 \#689 (`821`)
  - add signal to be sent when request is dropped by the scheduler (`961`)
  - avoid download large response (`946`)
  - Allow to specify the quotechar in CSVFeedSpider (`882`)
  - Add referer to "Spider error processing" log message (`795`)
  - process robots.txt once (`896`)
  - GSoC Per-spider settings (`854`)
  - Add project name validation (`817`)
  - GSoC API cleanup (`816`, `1128`, `1147`, `1148`, `1156`, `1185`, `1187`, `1258`, `1268`, `1276`, `1285`, `1284`)
  - Be more responsive with IO operations (`1074` and `1075`)
  - Do leveldb compaction for httpcache on closing (`1297`)

Deprecations and Removals

  - Deprecate htmlparser link extractor (`1205`)
  - remove deprecated code from FeedExporter (`1155`)
  - a leftover for.15 compatibility (`925`)
  - drop support for CONCURRENT\_REQUESTS\_PER\_SPIDER (`895`)
  - Drop old engine code (`911`)
  - Deprecate SgmlLinkExtractor (`777`)

Relocations

  - Move exporters/\_\_init\_\_.py to exporters.py (`1242`)
  - Move base classes to their packages (`1218`, `1233`)
  - Module relocation (`1181`, `1210`)
  - rename SpiderManager to SpiderLoader (`1166`)
  - Remove djangoitem (`1177`)
  - remove scrapy deploy command (`1102`)
  - dissolve contrib\_exp (`1134`)
  - Deleted bin folder from root, fixes \#913 (`914`)
  - Remove jsonrpc based webservice (`859`)
  - Move Test cases under project root dir (`827`, `841`)
  - Fix backward incompatibility for relocated paths in settings (`1267`)

Documentation

  - CrawlerProcess documentation (`1190`)
  - Favoring web scraping over screen scraping in the descriptions (`1188`)
  - Some improvements for Scrapy tutorial (`1180`)
  - Documenting Files Pipeline together with Images Pipeline (`1150`)
  - deployment docs tweaks (`1164`)
  - Added deployment section covering scrapyd-deploy and shub (`1124`)
  - Adding more settings to project template (`1073`)
  - some improvements to overview page (`1106`)
  - Updated link in docs/topics/architecture.rst (`647`)
  - DOC reorder topics (`1022`)
  - updating list of Request.meta special keys (`1071`)
  - DOC document download\_timeout (`898`)
  - DOC simplify extension docs (`893`)
  - Leaks docs (`894`)
  - DOC document from\_crawler method for item pipelines (`904`)
  - Spider\_error doesn't support deferreds (`1292`)
  - Corrections & Sphinx related fixes (`1220`, `1219`, `1196`, `1172`, `1171`, `1169`, `1160`, `1154`, `1127`, `1112`, `1105`, `1041`, `1082`, `1033`, `944`, `866`, `864`, `796`, `1260`, `1271`, `1293`, `1298`)

Bugfixes

  - Item multi inheritance fix (`353`, `1228`)
  - ItemLoader.load\_item: iterate over copy of fields (`722`)
  - Fix Unhandled error in Deferred (RobotsTxtMiddleware) (`1131`, `1197`)
  - Force to read DOWNLOAD\_TIMEOUT as int (`954`)
  - scrapy.utils.misc.load\_object should print full traceback (`902`)
  - Fix bug for ".local" host name (`878`)
  - Fix for Enabled extensions, middlewares, pipelines info not printed anymore (`879`)
  - fix dont\_merge\_cookies bad behaviour when set to false on meta (`846`)

Python 3 In Progress Support

  - disable scrapy.telnet if twisted.conch is not available (`1161`)
  - fix Python 3 syntax errors in ajaxcrawl.py (`1162`)
  - more python3 compatibility changes for urllib (`1121`)
  - assertItemsEqual was renamed to assertCountEqual in Python 3. (`1070`)
  - Import unittest.mock if available. (`1066`)
  - updated deprecated cgi.parse\_qsl to use six's parse\_qsl (`909`)
  - Prevent Python 3 port regressions (`830`)
  - PY3: use MutableMapping for python 3 (`810`)
  - PY3: use six.BytesIO and six.moves.cStringIO (`803`)
  - PY3: fix xmlrpclib and email imports (`801`)
  - PY3: use six for robotparser and urlparse (`800`)
  - PY3: use six.iterkeys, six.iteritems, and tempfile (`799`)
  - PY3: fix has\_key and use six.moves.configparser (`798`)
  - PY3: use six.moves.cPickle (`797`)
  - PY3 make it possible to run some tests in Python3 (`776`)

Tests

  - remove unnecessary lines from py3-ignores (`1243`)
  - Fix remaining warnings from pytest while collecting tests (`1206`)
  - Add docs build to travis (`1234`)
  - TST don't collect tests from deprecated modules. (`1165`)
  - install service\_identity package in tests to prevent warnings (`1168`)
  - Fix deprecated settings API in tests (`1152`)
  - Add test for webclient with POST method and no body given (`1089`)
  - py3-ignores.txt supports comments (`1044`)
  - modernize some of the asserts (`835`)
  - selector.\_\_repr\_\_ test (`779`)

Code refactoring

  - CSVFeedSpider cleanup: use iterate\_spider\_output (`1079`)
  - remove unnecessary check from scrapy.utils.spider.iter\_spider\_output (`1078`)
  - Pydispatch pep8 (`992`)
  - Removed unused 'load=False' parameter from walk\_modules() (`871`)
  - For consistency, use `job_dir` helper in `SpiderState` extension. (`805`)
  - rename "sflo" local variables to less cryptic "log\_observer" (`775`)

## Scrapy 0.24.6 (2015-04-20)

  - encode invalid xpath with unicode\_escape under PY2 (`07cb3e5`)
  - fix IPython shell scope issue and load IPython user config (`2c8e573`)
  - Fix small typo in the docs (`d694019`)
  - Fix small typo (`f92fa83`)
  - Converted sel.xpath() calls to response.xpath() in Extracting the data (`c2c6d15`)

## Scrapy 0.24.5 (2015-02-25)

  - Support new \_getEndpoint Agent signatures on Twisted 15.0.0 (`540b9bc`)
  - DOC a couple more references are fixed (`b4c454b`)
  - DOC fix a reference (`e3c1260`)
  - t.i.b.ThreadedResolver is now a new-style class (`9e13f42`)
  - S3DownloadHandler: fix auth for requests with quoted paths/query params (`cdb9a0b`)
  - fixed the variable types in mailsender documentation (`bb3a848`)
  - Reset items\_scraped instead of item\_count (`edb07a4`)
  - Tentative attention message about what document to read for contributions (`7ee6f7a`)
  - mitmproxy 0.10.1 needs netlib 0.10.1 too (`874fcdd`)
  - pin mitmproxy 0.10.1 as \>0.11 does not work with tests (`c6b21f0`)
  - Test the parse command locally instead of against an external url (`c3a6628`)
  - Patches Twisted issue while closing the connection pool on HTTPDownloadHandler (`d0bf957`)
  - Updates documentation on dynamic item classes. (`eeb589a`)
  - Merge pull request \#943 from Lazar-T/patch-3 (`5fdab02`)
  - typo (`b0ae199`)
  - pywin32 is required by Twisted. closes \#937 (`5cb0cfb`)
  - Update install.rst (`781286b`)
  - Merge pull request \#928 from Lazar-T/patch-1 (`b415d04`)
  - comma instead of fullstop (`627b9ba`)
  - Merge pull request \#885 from jsma/patch-1 (`de909ad`)
  - Update request-response.rst (`3f3263d`)
  - SgmlLinkExtractor - fix for parsing \<area\> tag with Unicode present (`49b40f0`)

## Scrapy 0.24.4 (2014-08-09)

  - pem file is used by mockserver and required by scrapy bench (`5eddc68b63`)
  - scrapy bench needs scrapy.tests\* (`d6cb999`)

## Scrapy 0.24.3 (2014-08-09)

  - no need to waste travis-ci time on py3 for 0.24 (`8e080c1`)
  - Update installation docs (`1d0c096`)
  - There is a trove classifier for Scrapy framework\! (`4c701d7`)
  - update other places where w3lib version is mentioned (`d109c13`)
  - Update w3lib requirement to 1.8.0 (`39d2ce5`)
  - Use w3lib.html.replace\_entities() (remove\_entities() is deprecated) (`180d3ad`)
  - set zip\_safe=False (`a51ee8b`)
  - do not ship tests package (`ee3b371`)
  - scrapy.bat is not needed anymore (`c3861cf`)
  - Modernize setup.py (`362e322`)
  - headers can not handle non-string values (`94a5c65`)
  - fix ftp test cases (`a274a7f`)
  - The sum up of travis-ci builds are taking like 50min to complete (`ae1e2cc`)
  - Update shell.rst typo (`e49c96a`)
  - removes weird indentation in the shell results (`1ca489d`)
  - improved explanations, clarified blog post as source, added link for XPath string functions in the spec (`65c8f05`)
  - renamed UserTimeoutError and ServerTimeouterror \#583 (`037f6ab`)
  - adding some xpath tips to selectors docs (`2d103e0`)
  - fix tests to account for <https://github.com/scrapy/w3lib/pull/23> (`f8d366a`)
  - get\_func\_args maximum recursion fix \#728 (`81344ea`)
  - Updated input/output processor example according to \#560. (`f7c4ea8`)
  - Fixed Python syntax in tutorial. (`db59ed9`)
  - Add test case for tunneling proxy (`f090260`)
  - Bugfix for leaking Proxy-Authorization header to remote host when using tunneling (`d8793af`)
  - Extract links from XHTML documents with MIME-Type "application/xml" (`ed1f376`)
  - Merge pull request \#793 from roysc/patch-1 (`91a1106`)
  - Fix typo in commands.rst (`743e1e2`)
  - better testcase for settings.overrides.setdefault (`e22daaf`)
  - Using CRLF as line marker according to http 1.1 definition (`5ec430b`)

## Scrapy 0.24.2 (2014-07-08)

  - Use a mutable mapping to proxy deprecated settings.overrides and settings.defaults attribute (`e5e8133`)
  - there is not support for python3 yet (`3cd6146`)
  - Update python compatible version set to Debian packages (`fa5d76b`)
  - DOC fix formatting in release notes (`c6a9e20`)

## Scrapy 0.24.1 (2014-06-27)

  - Fix deprecated CrawlerSettings and increase backward compatibility with .defaults attribute (`8e3f20a`)

## Scrapy 0.24.0 (2014-06-26)

### Enhancements

  - Improve Scrapy top-level namespace (`494`, `684`)
  - Add selector shortcuts to responses (`554`, `690`)
  - Add new lxml based LinkExtractor to replace unmaintained SgmlLinkExtractor (`559`, `761`, `763`)
  - Cleanup settings API - part of per-spider settings **GSoC project** (`737`)
  - Add UTF8 encoding header to templates (`688`, `762`)
  - Telnet console now binds to 127.0.0.1 by default (`699`)
  - Update Debian/Ubuntu install instructions (`509`, `549`)
  - Disable smart strings in lxml XPath evaluations (`535`)
  - Restore filesystem based cache as default for http cache middleware (`541`, `500`, `571`)
  - Expose current crawler in Scrapy shell (`557`)
  - Improve testsuite comparing CSV and XML exporters (`570`)
  - New `offsite/filtered` and `offsite/domains` stats (`566`)
  - Support process\_links as generator in CrawlSpider (`555`)
  - Verbose logging and new stats counters for DupeFilter (`553`)
  - Add a mimetype parameter to `MailSender.send()` (`602`)
  - Generalize file pipeline log messages (`622`)
  - Replace unencodeable codepoints with html entities in SGMLLinkExtractor (`565`)
  - Converted SEP documents to rst format (`629`, `630`, `638`, `632`, `636`, `640`, `635`, `634`, `639`, `637`, `631`, `633`, `641`, `642`)
  - Tests and docs for clickdata's nr index in FormRequest (`646`, `645`)
  - Allow to disable a downloader handler just like any other component (`650`)
  - Log when a request is discarded after too many redirections (`654`)
  - Log error responses if they are not handled by spider callbacks (`612`, `656`)
  - Add content-type check to http compression mw (`193`, `660`)
  - Run pypy tests using latest pypi from ppa (`674`)
  - Run test suite using pytest instead of trial (`679`)
  - Build docs and check for dead links in tox environment (`687`)
  - Make scrapy.version\_info a tuple of integers (`681`, `692`)
  - Infer exporter's output format from filename extensions (`546`, `659`, `760`)
  - Support case-insensitive domains in `url_is_from_any_domain()` (`693`)
  - Remove pep8 warnings in project and spider templates (`698`)
  - Tests and docs for `request_fingerprint` function (`597`)
  - Update SEP-19 for GSoC project `per-spider settings` (`705`)
  - Set exit code to non-zero when contracts fails (`727`)
  - Add a setting to control what class is instantiated as Downloader component (`738`)
  - Pass response in `item_dropped` signal (`724`)
  - Improve `scrapy check` contracts command (`733`, `752`)
  - Document `spider.closed()` shortcut (`719`)
  - Document `request_scheduled` signal (`746`)
  - Add a note about reporting security issues (`697`)
  - Add LevelDB http cache storage backend (`626`, `500`)
  - Sort spider list output of `scrapy list` command (`742`)
  - Multiple documentation enhancements and fixes (`575`, `587`, `590`, `596`, `610`, `617`, `618`, `627`, `613`, `643`, `654`, `675`, `663`, `711`, `714`)

### Bugfixes

  - Encode unicode URL value when creating Links in RegexLinkExtractor (`561`)
  - Ignore None values in ItemLoader processors (`556`)
  - Fix link text when there is an inner tag in SGMLLinkExtractor and HtmlParserLinkExtractor (`485`, `574`)
  - Fix wrong checks on subclassing of deprecated classes (`581`, `584`)
  - Handle errors caused by inspect.stack() failures (`582`)
  - Fix a reference to unexistent engine attribute (`593`, `594`)
  - Fix dynamic itemclass example usage of type() (`603`)
  - Use lucasdemarchi/codespell to fix typos (`628`)
  - Fix default value of attrs argument in SgmlLinkExtractor to be tuple (`661`)
  - Fix XXE flaw in sitemap reader (`676`)
  - Fix engine to support filtered start requests (`707`)
  - Fix offsite middleware case on urls with no hostnames (`745`)
  - Testsuite doesn't require PIL anymore (`585`)

## Scrapy 0.22.2 (released 2014-02-14)

  - fix a reference to unexistent engine.slots. closes \#593 (`13c099a`)
  - downloaderMW doc typo (spiderMW doc copy remnant) (`8ae11bf`)
  - Correct typos (`1346037`)

## Scrapy 0.22.1 (released 2014-02-08)

  - localhost666 can resolve under certain circumstances (`2ec2279`)
  - test inspect.stack failure (`cc3eda3`)
  - Handle cases when inspect.stack() fails (`8cb44f9`)
  - Fix wrong checks on subclassing of deprecated classes. closes \#581 (`46d98d6`)
  - Docs: 4-space indent for final spider example (`13846de`)
  - Fix HtmlParserLinkExtractor and tests after \#485 merge (`368a946`)
  - BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (`b566388`)
  - BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (`c1cb418`)
  - BaseSgmlLinkExtractor: Fixed unknown\_endtag() so that it only set current\_link=None when the end tag match the opening tag (`7e4d627`)
  - Fix tests for Travis-CI build (`76c7e20`)
  - replace unencodeable codepoints with html entities. fixes \#562 and \#285 (`5f87b17`)
  - RegexLinkExtractor: encode URL unicode value when creating Links (`d0ee545`)
  - Updated the tutorial crawl output with latest output. (`8da65de`)
  - Updated shell docs with the crawler reference and fixed the actual shell output. (`875b9ab`)
  - PEP8 minor edits. (`f89efaf`)
  - Expose current crawler in the Scrapy shell. (`5349cec`)
  - Unused re import and PEP8 minor edits. (`387f414`)
  - Ignore None's values when using the ItemLoader. (`0632546`)
  - DOC Fixed HTTPCACHE\_STORAGE typo in the default value which is now Filesystem instead Dbm. (`cde9a8c`)
  - show Ubuntu setup instructions as literal code (`fb5c9c5`)
  - Update Ubuntu installation instructions (`70fb105`)
  - Merge pull request \#550 from stray-leone/patch-1 (`6f70b6a`)
  - modify the version of Scrapy Ubuntu package (`725900d`)
  - fix 0.22.0 release date (`af0219a`)
  - fix typos in news.rst and remove (not released yet) header (`b7f58f4`)

## Scrapy 0.22.0 (released 2014-01-17)

### Enhancements

  - \[**Backward incompatible**\] Switched HTTPCacheMiddleware backend to filesystem (`541`) To restore old backend set `HTTPCACHE_STORAGE` to `scrapy.contrib.httpcache.DbmCacheStorage`
  - Proxy https:// urls using CONNECT method (`392`, `397`)
  - Add a middleware to crawl ajax crawlable pages as defined by google (`343`)
  - Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (`510`, `519`)
  - Selectors register EXSLT namespaces by default (`472`)
  - Unify item loaders similar to selectors renaming (`461`)
  - Make `RFPDupeFilter` class easily subclassable (`533`)
  - Improve test coverage and forthcoming Python 3 support (`525`)
  - Promote startup info on settings and middleware to INFO level (`520`)
  - Support partials in `get_func_args` util (`506`, issue:<span class="title-ref">504</span>)
  - Allow running individual tests via tox (`503`)
  - Update extensions ignored by link extractors (`498`)
  - Add middleware methods to get files/images/thumbs paths (`490`)
  - Improve offsite middleware tests (`478`)
  - Add a way to skip default Referer header set by RefererMiddleware (`475`)
  - Do not send `x-gzip` in default `Accept-Encoding` header (`469`)
  - Support defining http error handling using settings (`466`)
  - Use modern python idioms wherever you find legacies (`497`)
  - Improve and correct documentation (`527`, `524`, `521`, `517`, `512`, `505`, `502`, `489`, `465`, `460`, `425`, `536`)

### Fixes

  - Update Selector class imports in CrawlSpider template (`484`)
  - Fix unexistent reference to `engine.slots` (`464`)
  - Do not try to call `body_as_unicode()` on a non-TextResponse instance (`462`)
  - Warn when subclassing XPathItemLoader, previously it only warned on instantiation. (`523`)
  - Warn when subclassing XPathSelector, previously it only warned on instantiation. (`537`)
  - Multiple fixes to memory stats (`531`, `530`, `529`)
  - Fix overriding url in `FormRequest.from_response()` (`507`)
  - Fix tests runner under pip 1.5 (`513`)
  - Fix logging error when spider name is unicode (`479`)

## Scrapy 0.20.2 (released 2013-12-09)

  - Update CrawlSpider Template with Selector changes (`6d1457d`)
  - fix method name in tutorial. closes GH-480 (`b4fc359`

## Scrapy 0.20.1 (released 2013-11-28)

  - include\_package\_data is required to build wheels from published sources (`5ba1ad5`)
  - process\_parallel was leaking the failures on its internal deferreds. closes \#458 (`419a780`)

## Scrapy 0.20.0 (released 2013-11-08)

### Enhancements

  - New Selector's API including CSS selectors (`395` and `426`),
  - Request/Response url/body attributes are now immutable (modifying them had been deprecated for a long time)
  - `ITEM_PIPELINES` is now defined as a dict (instead of a list)
  - Sitemap spider can fetch alternate URLs (`360`)
  - `Selector.remove_namespaces()` now remove namespaces from element's attributes. (`416`)
  - Paved the road for Python 3.3+ (`435`, `436`, `431`, `452`)
  - New item exporter using native python types with nesting support (`366`)
  - Tune HTTP1.1 pool size so it matches concurrency defined by settings (`b43b5f575`)
  - scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (`327`)
  - New FilesPipeline with functionality factored out from ImagesPipeline (`370`, `409`)
  - Recommend Pillow instead of PIL for image handling (`317`)
  - Added Debian packages for Ubuntu Quantal and Raring (`86230c0`)
  - Mock server (used for tests) can listen for HTTPS requests (`410`)
  - Remove multi spider support from multiple core components (`422`, `421`, `420`, `419`, `423`, `418`)
  - Travis-CI now tests Scrapy changes against development versions of `w3lib` and `queuelib` python packages.
  - Add pypy 2.1 to continuous integration tests (`ecfa7431`)
  - Pylinted, pep8 and removed old-style exceptions from source (`430`, `432`)
  - Use importlib for parametric imports (`445`)
  - Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (`372`)
  - Bugfix crawling shutdown on SIGINT (`450`)
  - Do not submit `reset` type inputs in FormRequest.from\_response (`b326b87`)
  - Do not silence download errors when request errback raises an exception (`684cfc0`)

### Bugfixes

  - Fix tests under Django 1.6 (`b6bed44c`)
  - Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler
  - Fix inconsistencies among Twisted releases (`406`)
  - Fix Scrapy shell bugs (`418`, `407`)
  - Fix invalid variable name in setup.py (`429`)
  - Fix tutorial references (`387`)
  - Improve request-response docs (`391`)
  - Improve best practices docs (`399`, `400`, `401`, `402`)
  - Improve django integration docs (`404`)
  - Document `bindaddress` request meta (`37c24e01d7`)
  - Improve `Request` class documentation (`226`)

### Other

  - Dropped Python 2.6 support (`448`)
  - Add \[cssselect \<cssselect:index\>\](cssselect \<cssselect:index\>.md) python package as install dependency
  - Drop libxml2 and multi selector's backend support, [lxml](https://lxml.de/) is required from now on.
  - Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.
  - Running test suite now requires `mock` python library (`390`)

### Thanks

Thanks to everyone who contribute to this release\!

List of contributors sorted by number of commits:

    69 Daniel Graña <dangra@...>
    37 Pablo Hoffman <pablo@...>
    13 Mikhail Korobov <kmike84@...>
     9 Alex Cepoi <alex.cepoi@...>
     9 alexanderlukanin13 <alexander.lukanin.13@...>
     8 Rolando Espinoza La fuente <darkrho@...>
     8 Lukasz Biedrycki <lukasz.biedrycki@...>
     6 Nicolas Ramirez <nramirez.uy@...>
     3 Paul Tremberth <paul.tremberth@...>
     2 Martin Olveyra <molveyra@...>
     2 Stefan <misc@...>
     2 Rolando Espinoza <darkrho@...>
     2 Loren Davie <loren@...>
     2 irgmedeiros <irgmedeiros@...>
     1 Stefan Koch <taikano@...>
     1 Stefan <cct@...>
     1 scraperdragon <dragon@...>
     1 Kumara Tharmalingam <ktharmal@...>
     1 Francesco Piccinno <stack.box@...>
     1 Marcos Campal <duendex@...>
     1 Dragon Dave <dragon@...>
     1 Capi Etheriel <barraponto@...>
     1 cacovsky <amarquesferraz@...>
     1 Berend Iwema <berend@...>

## Scrapy 0.18.4 (released 2013-10-10)

  - IPython refuses to update the namespace. fix \#396 (`3d32c4f`)
  - Fix AlreadyCalledError replacing a request in shell command. closes \#407 (`b1d8919`)
  - Fix start\_requests laziness and early hangs (`89faf52`)

## Scrapy 0.18.3 (released 2013-10-03)

  - fix regression on lazy evaluation of start requests (`12693a5`)
  - forms: do not submit reset inputs (`e429f63`)
  - increase unittest timeouts to decrease travis false positive failures (`912202e`)
  - backport master fixes to json exporter (`cfc2d46`)
  - Fix permission and set umask before generating sdist tarball (`06149e0`)

## Scrapy 0.18.2 (released 2013-09-03)

  - Backport `scrapy check` command fixes and backward compatible multi crawler process(`339`)

## Scrapy 0.18.1 (released 2013-08-27)

  - remove extra import added by cherry picked changes (`d20304e`)
  - fix crawling tests under twisted pre 11.0.0 (`1994f38`)
  - py26 can not format zero length fields {} (`abf756f`)
  - test PotentiaDataLoss errors on unbound responses (`b15470d`)
  - Treat responses without content-length or Transfer-Encoding as good responses (`c4bf324`)
  - do no include ResponseFailed if http11 handler is not enabled (`6cbe684`)
  - New HTTP client wraps connection lost in ResponseFailed exception. fix \#373 (`1a20bba`)
  - limit travis-ci build matrix (`3b01bb8`)
  - Merge pull request \#375 from peterarenot/patch-1 (`fa766d7`)
  - Fixed so it refers to the correct folder (`3283809`)
  - added Quantal & Raring to support Ubuntu releases (`1411923`)
  - fix retry middleware which didn't retry certain connection errors after the upgrade to http1 client, closes GH-373 (`bb35ed0`)
  - fix XmlItemExporter in Python 2.7.4 and 2.7.5 (`de3e451`)
  - minor updates to 0.18 release notes (`c45e5f1`)
  - fix contributors list format (`0b60031`)

## Scrapy 0.18.0 (released 2013-08-09)

  - Lot of improvements to testsuite run using Tox, including a way to test on pypi
  - Handle GET parameters for AJAX crawlable urls (`3fe2a32`)
  - Use lxml recover option to parse sitemaps (`347`)
  - Bugfix cookie merging by hostname and not by netloc (`352`)
  - Support disabling `HttpCompressionMiddleware` using a flag setting (`359`)
  - Support xml namespaces using `iternodes` parser in `XMLFeedSpider` (`12`)
  - Support `dont_cache` request meta flag (`19`)
  - Bugfix `scrapy.utils.gz.gunzip` broken by changes in python 2.7.4 (`4dc76e`)
  - Bugfix url encoding on `SgmlLinkExtractor` (`24`)
  - Bugfix `TakeFirst` processor shouldn't discard zero (0) value (`59`)
  - Support nested items in xml exporter (`66`)
  - Improve cookies handling performance (`77`)
  - Log dupe filtered requests once (`105`)
  - Split redirection middleware into status and meta based middlewares (`78`)
  - Use HTTP1.1 as default downloader handler (`109` and `318`)
  - Support xpath form selection on `FormRequest.from_response` (`185`)
  - Bugfix unicode decoding error on `SgmlLinkExtractor` (`199`)
  - Bugfix signal dispatching on pypi interpreter (`205`)
  - Improve request delay and concurrency handling (`206`)
  - Add RFC2616 cache policy to `HttpCacheMiddleware` (`212`)
  - Allow customization of messages logged by engine (`214`)
  - Multiples improvements to `DjangoItem` (`217`, `218`, `221`)
  - Extend Scrapy commands using setuptools entry points (`260`)
  - Allow spider `allowed_domains` value to be set/tuple (`261`)
  - Support `settings.getdict` (`269`)
  - Simplify internal `scrapy.core.scraper` slot handling (`271`)
  - Added `Item.copy` (`290`)
  - Collect idle downloader slots (`297`)
  - Add `ftp://` scheme downloader handler (`329`)
  - Added downloader benchmark webserver and spider tools \[benchmarking\](\#benchmarking)
  - Moved persistent (on disk) queues to a separate project ([queuelib](https://github.com/scrapy/queuelib)) which Scrapy now depends on
  - Add Scrapy commands using external libraries (`260`)
  - Added `--pdb` option to `scrapy` command line tool
  - Added <span class="title-ref">XPathSelector.remove\_namespaces \<scrapy.selector.Selector.remove\_namespaces\></span> which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in \[topics-selectors\](\#topics-selectors).
  - Several improvements to spider contracts
  - New default middleware named MetaRefreshMiddleware that handles meta-refresh html tag redirections,
  - MetaRefreshMiddleware and RedirectMiddleware have different priorities to address \#62
  - added from\_crawler method to spiders
  - added system tests with mock server
  - more improvements to macOS compatibility (thanks Alex Cepoi)
  - several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)
  - support custom download slots
  - added --spider option to "shell" command.
  - log overridden settings when Scrapy starts

Thanks to everyone who contribute to this release. Here is a list of contributors sorted by number of commits:

    130 Pablo Hoffman <pablo@...>
     97 Daniel Graña <dangra@...>
     20 Nicolás Ramírez <nramirez.uy@...>
     13 Mikhail Korobov <kmike84@...>
     12 Pedro Faustino <pedrobandim@...>
     11 Steven Almeroth <sroth77@...>
      5 Rolando Espinoza La fuente <darkrho@...>
      4 Michal Danilak <mimino.coder@...>
      4 Alex Cepoi <alex.cepoi@...>
      4 Alexandr N Zamaraev (aka tonal) <tonal@...>
      3 paul <paul.tremberth@...>
      3 Martin Olveyra <molveyra@...>
      3 Jordi Llonch <llonchj@...>
      3 arijitchakraborty <myself.arijit@...>
      2 Shane Evans <shane.evans@...>
      2 joehillen <joehillen@...>
      2 Hart <HartSimha@...>
      2 Dan <ellisd23@...>
      1 Zuhao Wan <wanzuhao@...>
      1 whodatninja <blake@...>
      1 vkrest <v.krestiannykov@...>
      1 tpeng <pengtaoo@...>
      1 Tom Mortimer-Jones <tom@...>
      1 Rocio Aramberri <roschegel@...>
      1 Pedro <pedro@...>
      1 notsobad <wangxiaohugg@...>
      1 Natan L <kuyanatan.nlao@...>
      1 Mark Grey <mark.grey@...>
      1 Luan <luanpab@...>
      1 Libor Nenadál <libor.nenadal@...>
      1 Juan M Uys <opyate@...>
      1 Jonas Brunsgaard <jonas.brunsgaard@...>
      1 Ilya Baryshev <baryshev@...>
      1 Hasnain Lakhani <m.hasnain.lakhani@...>
      1 Emanuel Schorsch <emschorsch@...>
      1 Chris Tilden <chris.tilden@...>
      1 Capi Etheriel <barraponto@...>
      1 cacovsky <amarquesferraz@...>
      1 Berend Iwema <berend@...>

## Scrapy 0.16.5 (released 2013-05-30)

  - obey request method when Scrapy deploy is redirected to a new endpoint (`8c4fcee`)
  - fix inaccurate downloader middleware documentation. refs \#280 (`40667cb`)
  - doc: remove links to diveintopython.org, which is no longer available. closes \#246 (`bd58bfa`)
  - Find form nodes in invalid html5 documents (`e3d6945`)
  - Fix typo labeling attrs type bool instead of list (`a274276`)

## Scrapy 0.16.4 (released 2013-01-23)

  - fixes spelling errors in documentation (`6d2b3aa`)
  - add doc about disabling an extension. refs \#132 (`c90de33`)
  - Fixed error message formatting. log.err() doesn't support cool formatting and when error occurred, the message was: "ERROR: Error processing %(item)s" (`c16150c`)
  - lint and improve images pipeline error logging (`56b45fc`)
  - fixed doc typos (`243be84`)
  - add documentation topics: Broad Crawls & Common Practices (`1fbb715`)
  - fix bug in Scrapy parse command when spider is not specified explicitly. closes \#209 (`c72e682`)
  - Update docs/topics/commands.rst (`28eac7a`)

## Scrapy 0.16.3 (released 2012-12-07)

  - Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (`487b9b5`)
  - add error details when image pipeline fails (`8232569`)
  - improve macOS compatibility (`8dcf8aa`)
  - setup.py: use README.rst to populate long\_description (`7b5310d`)
  - doc: removed obsolete references to ClientForm (`80f9bb6`)
  - correct docs for default storage backend (`2aa491b`)
  - doc: removed broken proxyhub link from FAQ (`bdf61c4`)
  - Fixed docs typo in SpiderOpenCloseLogging example (`7184094`)

## Scrapy 0.16.2 (released 2012-11-09)

  - Scrapy contracts: python2.6 compat (`a4a9199`)
  - Scrapy contracts verbose option (`ec41673`)
  - proper unittest-like output for Scrapy contracts (`86635e4`)
  - added open\_in\_browser to debugging doc (`c9b690d`)
  - removed reference to global Scrapy stats from settings doc (`dd55067`)
  - Fix SpiderState bug in Windows platforms (`58998f4`)

## Scrapy 0.16.1 (released 2012-10-26)

  - fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (`8c780fd`)
  - better backward compatibility for scrapy.conf.settings (`3403089`)
  - extended documentation on how to access crawler stats from extensions (`c4da0b5`)
  - removed .hgtags (no longer needed now that Scrapy uses git) (`d52c188`)
  - fix dashes under rst headers (`fa4f7f9`)
  - set release date for 0.16.0 in news (`e292246`)

## Scrapy 0.16.0 (released 2012-10-18)

Scrapy changes:

  - added \[topics-contracts\](\#topics-contracts), a mechanism for testing spiders in a formal/reproducible way
  - added options `-o` and `-t` to the `runspider` command
  - documented \[topics/autothrottle\](topics/autothrottle.md) and added to extensions installed by default. You still need to enable it with `AUTOTHROTTLE_ENABLED`
  - major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (`stats_spider_opened`, etc). Stats are much simpler now, backward compatibility is kept on the Stats Collector API and signals.
  - added <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_start\_requests</span> method to spider middlewares
  - dropped Signals singleton. Signals should now be accessed through the Crawler.signals attribute. See the signals documentation for more info.
  - dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.
  - documented \[topics-api\](\#topics-api)
  - `lxml` is now the default selectors backend instead of `libxml2`
  - ported FormRequest.from\_response() to use [lxml](https://lxml.de/) instead of [ClientForm](https://pypi.org/project/ClientForm/)
  - removed modules: `scrapy.xlib.BeautifulSoup` and `scrapy.xlib.ClientForm`
  - SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (`10ed28b`)
  - StackTraceDump extension: also dump trackref live references (`fe2ce93`)
  - nested items now fully supported in JSON and JSONLines exporters
  - added `cookiejar` Request meta key to support multiple cookie sessions per spider
  - decoupled encoding detection code to [w3lib.encoding](https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py), and ported Scrapy code to use that module
  - dropped support for Python 2.5. See <https://www.zyte.com/blog/scrapy-0-15-dropping-support-for-python-2-5/>
  - dropped support for Twisted 2.5
  - added `REFERER_ENABLED` setting, to control referer middleware
  - changed default user agent to: `Scrapy/VERSION (+http://scrapy.org)`
  - removed (undocumented) `HTMLImageLinkExtractor` class from `scrapy.contrib.linkextractors.image`
  - removed per-spider settings (to be replaced by instantiating multiple crawler objects)
  - `USER_AGENT` spider attribute will no longer work, use `user_agent` attribute instead
  - `DOWNLOAD_TIMEOUT` spider attribute will no longer work, use `download_timeout` attribute instead
  - removed `ENCODING_ALIASES` setting, as encoding auto-detection has been moved to the [w3lib](https://github.com/scrapy/w3lib) library
  - promoted \[topics-djangoitem\](\#topics-djangoitem) to main contrib
  - LogFormatter method now return dicts(instead of strings) to support lazy formatting (`164`, `dcef7b0`)
  - downloader handlers (`DOWNLOAD_HANDLERS` setting) now receive settings as the first argument of the `__init__` method
  - replaced memory usage accounting with (more portable) [resource](https://docs.python.org/2/library/resource.html) module, removed `scrapy.utils.memory` module
  - removed signal: `scrapy.mail.mail_sent`
  - removed `TRACK_REFS` setting, now \[trackrefs \<topics-leaks-trackrefs\>\](\#trackrefs-\<topics-leaks-trackrefs\>) is always enabled
  - DBM is now the default storage backend for HTTP cache middleware
  - number of log messages (per level) are now tracked through Scrapy stats (stat name: `log_count/LEVEL`)
  - number received responses are now tracked through Scrapy stats (stat name: `response_received_count`)
  - removed `scrapy.log.started` attribute

## Scrapy 0.14.4

  - added precise to supported Ubuntu distros (`b7e46df`)
  - fixed bug in json-rpc webservice reported in <https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion>. also removed no longer supported 'run' command from extras/scrapy-ws.py (`340fbdb`)
  - meta tag attributes for content-type http equiv can be in any order. \#123 (`0cb68af`)
  - replace "import Image" by more standard "from PIL import Image". closes \#88 (`4d17048`)
  - return trial status as bin/runtests.sh exit value. \#118 (`b7b2e7f`)

## Scrapy 0.14.3

  - forgot to include pydispatch license. \#118 (`fd85f9c`)
  - include egg files used by testsuite in source distribution. \#118 (`c897793`)
  - update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs \#107 (`2548dcc`)
  - added note to docs/topics/firebug.rst about google directory being shut down (`668e352`)
  - don't discard slot when empty, just save in another dict in order to recycle if needed again. (`8e9f607`)
  - do not fail handling unicode xpaths in libxml2 backed selectors (`b830e95`)
  - fixed minor mistake in Request objects documentation (`bf3c9ee`)
  - fixed minor defect in link extractors documentation (`ba14f38`)
  - removed some obsolete remaining code related to sqlite support in Scrapy (`0665175`)

## Scrapy 0.14.2

  - move buffer pointing to start of file before computing checksum. refs \#92 (`6a5bef2`)
  - Compute image checksum before persisting images. closes \#92 (`9817df1`)
  - remove leaking references in cached failures (`673a120`)
  - fixed bug in MemoryUsage extension: get\_engine\_status() takes exactly 1 argument (0 given) (`11133e9`)
  - fixed struct.error on http compression middleware. closes \#87 (`1423140`)
  - ajax crawling wasn't expanding for unicode urls (`0de3fb4`)
  - Catch start\_requests iterator errors. refs \#83 (`454a21d`)
  - Speed-up libxml2 XPathSelector (`2fbd662`)
  - updated versioning doc according to recent changes (`0a070f5`)
  - scrapyd: fixed documentation link (`2b4e4c3`)
  - extras/makedeb.py: no longer obtaining version from git (`caffe0e`)

## Scrapy 0.14.1

  - extras/makedeb.py: no longer obtaining version from git (`caffe0e`)
  - bumped version to 0.14.1 (`6cb9e1c`)
  - fixed reference to tutorial directory (`4b86bd6`)
  - doc: removed duplicated callback argument from Request.replace() (`1aeccdd`)
  - fixed formatting of scrapyd doc (`8bf19e6`)
  - Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (`14a8e6e`)
  - added comment about why we disable ssl on boto images upload (`5223575`)
  - SSL handshaking hangs when doing too many parallel connections to S3 (`63d583d`)
  - change tutorial to follow changes on dmoz site (`bcb3198`)
  - Avoid \_disconnectedDeferred AttributeError exception in Twisted\>=11.1.0 (`98f3f87`)
  - allow spider to set autothrottle max concurrency (`175a4b5`)

## Scrapy 0.14

### New features and settings

  - Support for AJAX crawlable urls

  - New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (`2737`)

  - added `-o` option to `scrapy crawl`, a shortcut for dumping scraped items into a file (or standard output using `-`)

  - Added support for passing custom settings to Scrapyd `schedule.json` api (`2779`, `2783`)

  - New `ChunkedTransferMiddleware` (enabled by default) to support [chunked transfer encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding) (`2769`)

  - Add boto 2.0 support for S3 downloader handler (`2763`)

  - Added [marshal](https://docs.python.org/2/library/marshal.html) to formats supported by feed exports (`2744`)

  - In request errbacks, offending requests are now received in `failure.request` attribute (`2738`)

  -   - Big downloader refactoring to support per domain/ip concurrency limits (`2732`)
        
          -   - `CONCURRENT_REQUESTS_PER_SPIDER` setting has been deprecated and replaced by:
                
                  - `CONCURRENT_REQUESTS`, `CONCURRENT_REQUESTS_PER_DOMAIN`, `CONCURRENT_REQUESTS_PER_IP`
        
          - check the documentation for more details

  - Added builtin caching DNS resolver (`2728`)

  - Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: \[scaws\](<https://github.com/scrapinghub/scaws>) (`2706`, `2714`)

  - Moved spider queues to scrapyd: `scrapy.spiderqueue` -\> `scrapyd.spiderqueue` (`2708`)

  - Moved sqlite utils to scrapyd: `scrapy.utils.sqlite` -\> `scrapyd.sqlite` (`2781`)

  - Real support for returning iterators on `start_requests()` method. The iterator is now consumed during the crawl when the spider is getting idle (`2704`)

  - Added `REDIRECT_ENABLED` setting to quickly enable/disable the redirect middleware (`2697`)

  - Added `RETRY_ENABLED` setting to quickly enable/disable the retry middleware (`2694`)

  - Added `CloseSpider` exception to manually close spiders (`2691`)

  - Improved encoding detection by adding support for HTML5 meta charset declaration (`2690`)

  - Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (`2688`)

  - Added `SitemapSpider` (see documentation in Spiders page) (`2658`)

  - Added `LogStats` extension for periodically logging basic stats (like crawled pages and scraped items) (`2657`)

  - Make handling of gzipped responses more robust (\#319, `2643`). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an `IOError`.

  - Simplified \!MemoryDebugger extension to use stats for dumping memory debugging info (`2639`)

  - Added new command to edit spiders: `scrapy edit` (`2636`) and `-e` flag to `genspider` command that uses it (`2653`)

  - Changed default representation of items to pretty-printed dicts. (`2631`). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.

  - Added `spider_error` signal (`2628`)

  - Added `COOKIES_ENABLED` setting (`2625`)

  - Stats are now dumped to Scrapy log (default value of `STATS_DUMP` setting has been changed to `True`). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.

  - Added support for dynamically adjusting download delay and maximum concurrent requests (`2599`)

  - Added new DBM HTTP cache storage backend (`2576`)

  - Added `listjobs.json` API to Scrapyd (`2571`)

  - `CsvItemExporter`: added `join_multivalued` parameter (`2578`)

  - Added namespace support to `xmliter_lxml` (`2552`)

  - Improved cookies middleware by making `COOKIES_DEBUG` nicer and documenting it (`2579`)

  - Several improvements to Scrapyd and Link extractors

### Code rearranged and removed

  -   - Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (`2630`)
        
          - original item\_scraped signal was removed
          - original item\_passed signal was renamed to item\_scraped
          - old log lines `Scraped Item...` were removed
          - old log lines `Passed Item...` were renamed to `Scraped Item...` lines and downgraded to `DEBUG` level

  -   - Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:
        
          - [w3lib](https://github.com/scrapy/w3lib) (several functions from `scrapy.utils.{http,markup,multipart,response,url}`, done in `2584`)
          - [scrapely](https://github.com/scrapy/scrapely) (was `scrapy.contrib.ibl`, done in `2586`)

  - Removed unused function: `scrapy.utils.request.request_info()` (`2577`)

  - Removed googledir project from `examples/googledir`. There's now a new example project called `dirbot` available on GitHub: <https://github.com/scrapy/dirbot>

  - Removed support for default field values in Scrapy items (`2616`)

  - Removed experimental crawlspider v2 (`2632`)

  - Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe filtering class as before (`DUPEFILTER_CLASS` setting) (`2640`)

  - Removed support for passing urls to `scrapy crawl` command (use `scrapy parse` instead) (`2704`)

  - Removed deprecated Execution Queue (`2704`)

  - Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (`2780`)

  - removed `CONCURRENT_SPIDERS` setting (use scrapyd maxproc instead) (`2789`)

  - Renamed attributes of core components: downloader.sites -\> downloader.slots, scraper.sites -\> scraper.slots (`2717`, `2718`)

  - Renamed setting `CLOSESPIDER_ITEMPASSED` to `CLOSESPIDER_ITEMCOUNT` (`2655`). Backward compatibility kept.

## Scrapy 0.12

The numbers like \#NNN reference tickets in the old issue tracker (Trac) which is no longer available.

### New features and improvements

  - Passed item is now sent in the `item` argument of the `item_passed
    <item_scraped>` (\#273)
  - Added verbose option to `scrapy version` command, useful for bug reports (\#298)
  - HTTP cache now stored by default in the project data dir (\#279)
  - Added project data storage directory (\#276, \#277)
  - Documented file structure of Scrapy projects (see command-line tool doc)
  - New lxml backend for XPath selectors (\#147)
  - Per-spider settings (\#245)
  - Support exit codes to signal errors in Scrapy commands (\#248)
  - Added `-c` argument to `scrapy shell` command
  - Made `libxml2` optional (\#260)
  - New `deploy` command (\#261)
  - Added `CLOSESPIDER_PAGECOUNT` setting (\#253)
  - Added `CLOSESPIDER_ERRORCOUNT` setting (\#254)

### Scrapyd changes

  - Scrapyd now uses one process per spider
  - It stores one log file per spider run, and rotate them keeping the latest 5 logs per spider (by default)
  - A minimal web ui was added, available at <http://localhost:6800> by default
  - There is now a `scrapy server` command to start a Scrapyd server of the current project

### Changes to settings

  - added `HTTPCACHE_ENABLED` setting (False by default) to enable HTTP cache middleware
  - changed `HTTPCACHE_EXPIRATION_SECS` semantics: now zero means "never expire".

### Deprecated/obsoleted functionality

  - Deprecated `runserver` command in favor of `server` command which starts a Scrapyd server. See also: Scrapyd changes
  - Deprecated `queue` command in favor of using Scrapyd `schedule.json` API. See also: Scrapyd changes
  - Removed the \!LxmlItemLoader (experimental contrib which never graduated to main contrib)

## Scrapy 0.10

The numbers like \#NNN reference tickets in the old issue tracker (Trac) which is no longer available.

### New features and improvements

  - New Scrapy service called `scrapyd` for deploying Scrapy crawlers in production (\#218) (documentation available)
  - Simplified Images pipeline usage which doesn't require subclassing your own images pipeline now (\#217)
  - Scrapy shell now shows the Scrapy log by default (\#206)
  - Refactored execution queue in a common base code and pluggable backends called "spider queues" (\#220)
  - New persistent spider queue (based on SQLite) (\#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.
  - Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)
  - Feed exporters with pluggable backends (\#197) (documentation available)
  - Deferred signals (\#193)
  - Added two new methods to item pipeline open\_spider(), close\_spider() with deferred support (\#195)
  - Support for overriding default request headers per spider (\#181)
  - Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (\#186)
  - Split Debian package into two packages - the library and the service (\#187)
  - Scrapy log refactoring (\#188)
  - New extension for keeping persistent spider contexts among different runs (\#203)
  - Added `dont_redirect` request.meta key for avoiding redirects (\#233)
  - Added `dont_retry` request.meta key for avoiding retries (\#234)

### Command-line tool changes

  - New `scrapy` command which replaces the old `scrapy-ctl.py` (\#199)
      - there is only one global `scrapy` command now, instead of one `scrapy-ctl.py` per project
      - Added `scrapy.bat` script for running more conveniently from Windows
  - Added bash completion to command-line tool (\#210)
  - Renamed command `start` to `runserver` (\#209)

### API changes

  - `url` and `body` attributes of Request objects are now read-only (\#230)

  - `Request.copy()` and `Request.replace()` now also copies their `callback` and `errback` attributes (\#231)

  - Removed `UrlFilterMiddleware` from `scrapy.contrib` (already disabled by default)

  - Offsite middleware doesn't filter out any request coming from a spider that doesn't have a allowed\_domains attribute (\#225)

  - Removed Spider Manager `load()` method. Now spiders are loaded in the `__init__` method itself.

  -   - Changes to Scrapy Manager (now called "Crawler"):
        
          - `scrapy.core.manager.ScrapyManager` class renamed to `scrapy.crawler.Crawler`
          - `scrapy.core.manager.scrapymanager` singleton moved to `scrapy.project.crawler`

  - Moved module: `scrapy.contrib.spidermanager` to `scrapy.spidermanager`

  - Spider Manager singleton moved from `scrapy.spider.spiders` to the ``spiders` attribute of``scrapy.project.crawler\`\` singleton.

  -   - moved Stats Collector classes: (\#204)
        
          - `scrapy.stats.collector.StatsCollector` to `scrapy.statscol.StatsCollector`
          - `scrapy.stats.collector.SimpledbStatsCollector` to `scrapy.contrib.statscol.SimpledbStatsCollector`

  - default per-command settings are now specified in the `default_settings` attribute of command object class (\#201)

  -   - changed arguments of Item pipeline `process_item()` method from `(spider, item)` to `(item, spider)`
        
          - backward compatibility kept (with deprecation warning)

  -   - moved `scrapy.core.signals` module to `scrapy.signals`
        
          - backward compatibility kept (with deprecation warning)

  -   - moved `scrapy.core.exceptions` module to `scrapy.exceptions`
        
          - backward compatibility kept (with deprecation warning)

  - added `handles_request()` class method to `BaseSpider`

  - dropped `scrapy.log.exc()` function (use `scrapy.log.err()` instead)

  - dropped `component` argument of `scrapy.log.msg()` function

  - dropped `scrapy.log.log_level` attribute

  - Added `from_settings()` class methods to Spider Manager, and Item Pipeline Manager

### Changes to settings

  - Added `HTTPCACHE_IGNORE_SCHEMES` setting to ignore certain schemes on \!HttpCacheMiddleware (\#225)
  - Added `SPIDER_QUEUE_CLASS` setting which defines the spider queue to use (\#220)
  - Added `KEEP_ALIVE` setting (\#220)
  - Removed `SERVICE_QUEUE` setting (\#220)
  - Removed `COMMANDS_SETTINGS_MODULE` setting (\#201)
  - Renamed `REQUEST_HANDLERS` to `DOWNLOAD_HANDLERS` and make download handlers classes (instead of functions)

## Scrapy 0.9

The numbers like \#NNN reference tickets in the old issue tracker (Trac) which is no longer available.

### New features and improvements

  - Added SMTP-AUTH support to scrapy.mail
  - New settings added: `MAIL_USER`, `MAIL_PASS` (`2065` | \#149)
  - Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (`2039`)
  - Added web service for controlling Scrapy process (this also deprecates the web console. (`2053` | \#167)
  - Support for running Scrapy as a service, for production systems (`1988`, `2054`, `2055`, `2056`, `2057` | \#168)
  - Added wrapper induction library (documentation only available in source code for now). (`2011`)
  - Simplified and improved response encoding support (`1961`, `1969`)
  - Added `LOG_ENCODING` setting (`1956`, documentation available)
  - Added `RANDOMIZE_DOWNLOAD_DELAY` setting (enabled by default) (`1923`, doc available)
  - `MailSender` is no longer IO-blocking (`1955` | \#146)
  - Linkextractors and new Crawlspider now handle relative base tag urls (`1960` | \#148)
  - Several improvements to Item Loaders and processors (`2022`, `2023`, `2024`, `2025`, `2026`, `2027`, `2028`, `2029`, `2030`)
  - Added support for adding variables to telnet console (`2047` | \#165)
  - Support for requests without callbacks (`2050` | \#166)

### API changes

  - Change `Spider.domain_name` to `Spider.name` (SEP-012, `1975`)
  - `Response.encoding` is now the detected encoding (`1961`)
  - `HttpErrorMiddleware` now returns None or raises an exception (`2006` | \#157)
  - `scrapy.command` modules relocation (`2035`, `2036`, `2037`)
  - Added `ExecutionQueue` for feeding spiders to scrape (`2034`)
  - Removed `ExecutionEngine` singleton (`2039`)
  - Ported `S3ImagesStore` (images pipeline) to use boto and threads (`2033`)
  - Moved module: `scrapy.management.telnet` to `scrapy.telnet` (`2047`)

### Changes to default settings

  - Changed default `SCHEDULER_ORDER` to `DFO` (`1939`)

## Scrapy 0.8

The numbers like \#NNN reference tickets in the old issue tracker (Trac) which is no longer available.

### New features

  - Added DEFAULT\_RESPONSE\_ENCODING setting (`1809`)
  - Added `dont_click` argument to `FormRequest.from_response()` method (`1813`, `1816`)
  - Added `clickdata` argument to `FormRequest.from_response()` method (`1802`, `1803`)
  - Added support for HTTP proxies (`HttpProxyMiddleware`) (`1781`, `1785`)
  - Offsite spider middleware now logs messages when filtering out requests (`1841`)

### Backward-incompatible changes

  - Changed `scrapy.utils.response.get_meta_refresh()` signature (`1804`)

  - Removed deprecated `scrapy.item.ScrapedItem` class - use `scrapy.item.Item instead` (`1838`)

  - Removed deprecated `scrapy.xpath` module - use `scrapy.selector` instead. (`1836`)

  - Removed deprecated `core.signals.domain_open` signal - use `core.signals.domain_opened` instead (`1822`)

  -   - `log.msg()` now receives a `spider` argument (`1822`)
        
          - Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the `spider` argument and pass spider references. If you really want to pass a string, use the `component` argument instead.

  - Changed core signals `domain_opened`, `domain_closed`, `domain_idle`

  -   - Changed Item pipeline to use spiders instead of domains
        
          - The `domain` argument of `process_item()` item pipeline method was changed to `spider`, the new signature is: `process_item(spider, item)` (`1827` | \#105)
          - To quickly port your code (to work with Scrapy 0.8) just use `spider.domain_name` where you previously used `domain`.

  -   - Changed Stats API to use spiders instead of domains (`1849` | \#113)
        
          - `StatsCollector` was changed to receive spider references (instead of domains) in its methods (`set_value`, `inc_value`, etc).
          - added `StatsCollector.iter_spider_stats()` method
          - removed `StatsCollector.list_domains()` method
          - Also, Stats signals were renamed and now pass around spider references (instead of domains). Here's a summary of the changes:
          - To quickly port your code (to work with Scrapy 0.8) just use `spider.domain_name` where you previously used `domain`. `spider_stats` contains exactly the same data as `domain_stats`.

  -   - `CloseDomain` extension moved to `scrapy.contrib.closespider.CloseSpider` (`1833`)
        
          -   - Its settings were also renamed:
                
                  - `CLOSEDOMAIN_TIMEOUT` to `CLOSESPIDER_TIMEOUT`
                  - `CLOSEDOMAIN_ITEMCOUNT` to `CLOSESPIDER_ITEMCOUNT`

  - Removed deprecated `SCRAPYSETTINGS_MODULE` environment variable - use `SCRAPY_SETTINGS_MODULE` instead (`1840`)

  - Renamed setting: `REQUESTS_PER_DOMAIN` to `CONCURRENT_REQUESTS_PER_SPIDER` (`1830`, `1844`)

  - Renamed setting: `CONCURRENT_DOMAINS` to `CONCURRENT_SPIDERS` (`1830`)

  - Refactored HTTP Cache middleware

  - HTTP Cache middleware has been heavily refactored, retaining the same functionality except for the domain sectorization which was removed. (`1843` )

  - Renamed exception: `DontCloseDomain` to `DontCloseSpider` (`1859` | \#120)

  - Renamed extension: `DelayedCloseDomain` to `SpiderCloseDelay` (`1861` | \#121)

  - Removed obsolete `scrapy.utils.markup.remove_escape_chars` function - use `scrapy.utils.markup.replace_escape_chars` instead (`1865`)

## Scrapy 0.7

First release of Scrapy.

---

addons.md

---

# Add-ons

Scrapy's add-on system is a framework which unifies managing and configuring components that extend Scrapy's core functionality, such as middlewares, extensions, or pipelines. It provides users with a plug-and-play experience in Scrapy extension management, and grants extensive configuration control to developers.

## Activating and configuring add-ons

During <span class="title-ref">\~scrapy.crawler.Crawler</span> initialization, the list of enabled add-ons is read from your `ADDONS` setting.

The `ADDONS` setting is a dict in which every key is an add-on class or its import path and the value is its priority.

This is an example where two add-ons are enabled in a project's `settings.py`:

    ADDONS = {
        'path.to.someaddon': 0,
        SomeAddonClass: 1,
    }

## Writing your own add-ons

Add-ons are Python classes that include the following method:

<div class="method">

update\_settings(settings)

This method is called during the initialization of the <span class="title-ref">\~scrapy.crawler.Crawler</span>. Here, you should perform dependency checks (e.g. for external Python libraries) and update the <span class="title-ref">\~scrapy.settings.Settings</span> object as wished, e.g. enable components for this add-on or set required configuration of other extensions.

  - param settings  
    The settings object storing Scrapy/component configuration

  - type settings  
    <span class="title-ref">\~scrapy.settings.Settings</span>

</div>

They can also have the following method:

<div class="classmethod" data-noindex="">

from\_crawler(cls, crawler)

If present, this class method is called to create an add-on instance from a <span class="title-ref">\~scrapy.crawler.Crawler</span>. It must return a new instance of the add-on. The crawler object provides access to all Scrapy core components like settings and signals; it is a way for the add-on to access them and hook its functionality into Scrapy.

  - param crawler  
    The crawler that uses this add-on

  - type crawler  
    <span class="title-ref">\~scrapy.crawler.Crawler</span>

</div>

The settings set by the add-on should use the `addon` priority (see \[populating-settings\](\#populating-settings) and <span class="title-ref">scrapy.settings.BaseSettings.set</span>):

    class MyAddon:
        def update_settings(self, settings):
            settings.set("DNSCACHE_ENABLED", True, "addon")

This allows users to override these settings in the project or spider configuration. This is not possible with settings that are mutable objects, such as the dict that is a value of `ITEM_PIPELINES`. In these cases you can provide an add-on-specific setting that governs whether the add-on will modify `ITEM_PIPELINES`:

    class MyAddon:
        def update_settings(self, settings):
            if settings.getbool("MYADDON_ENABLE_PIPELINE"):
                settings["ITEM_PIPELINES"]["path.to.mypipeline"] = 200

If the `update_settings` method raises <span class="title-ref">scrapy.exceptions.NotConfigured</span>, the add-on will be skipped. This makes it easy to enable an add-on only when some conditions are met.

### Fallbacks

Some components provided by add-ons need to fall back to "default" implementations, e.g. a custom download handler needs to send the request that it doesn't handle via the default download handler, or a stats collector that includes some additional processing but otherwise uses the default stats collector. And it's possible that a project needs to use several custom components of the same type, e.g. two custom download handlers that support different kinds of custom requests and still need to use the default download handler for other requests. To make such use cases easier to configure, we recommend that such custom components should be written in the following way:

1.  The custom component (e.g. `MyDownloadHandler`) shouldn't inherit from the default Scrapy one (e.g. `scrapy.core.downloader.handlers.http.HTTPDownloadHandler`), but instead be able to load the class of the fallback component from a special setting (e.g. `MY_FALLBACK_DOWNLOAD_HANDLER`), create an instance of it and use it.
2.  The add-ons that include these components should read the current value of the default setting (e.g. `DOWNLOAD_HANDLERS`) in their `update_settings()` methods, save that value into the fallback setting (`MY_FALLBACK_DOWNLOAD_HANDLER` mentioned earlier) and set the default setting to the component provided by the add-on (e.g. `MyDownloadHandler`). If the fallback setting is already set by the user, they shouldn't change it.
3.  This way, if there are several add-ons that want to modify the same setting, all of them will fallback to the component from the previous one and then to the Scrapy default. The order of that depends on the priority order in the `ADDONS` setting.

## Add-on examples

Set some basic configuration:

`` `python     class MyAddon:         def update_settings(self, settings):             settings["ITEM_PIPELINES"]["path.to.mypipeline"] = 200             settings.set("DNSCACHE_ENABLED", True, "addon")  Check dependencies:  .. code-block:: python      class MyAddon:         def update_settings(self, settings):             try:                 import boto             except ImportError:                 raise NotConfigured("MyAddon requires the boto library")             ...  Access the crawler instance:  .. code-block:: python      class MyAddon:         def __init__(self, crawler) -> None:             super().__init__()             self.crawler = crawler          @classmethod         def from_crawler(cls, crawler):             return cls(crawler)          def update_settings(self, settings): ...  Use a fallback component:  .. code-block:: python      from scrapy.core.downloader.handlers.http import HTTPDownloadHandler       FALLBACK_SETTING = "MY_FALLBACK_DOWNLOAD_HANDLER"       class MyHandler:         lazy = False          def __init__(self, settings, crawler):             dhcls = load_object(settings.get(FALLBACK_SETTING))             self._fallback_handler = create_instance(                 dhcls,                 settings=None,                 crawler=crawler,             )          def download_request(self, request, spider):             if request.meta.get("my_params"):                 # handle the request                 ...             else:                 return self._fallback_handler.download_request(request, spider)       class MyAddon:         def update_settings(self, settings):             if not settings.get(FALLBACK_SETTING):                 settings.set(                     FALLBACK_SETTING,                     settings.getwithbase("DOWNLOAD_HANDLERS")["https"],                     "addon",                 )             settings["DOWNLOAD_HANDLERS"]["https"] = MyHandler ``\`

---

api.md

---

# Core API

This section documents the Scrapy core API, and it's intended for developers of extensions and middlewares.

## Crawler API

The main entry point to Scrapy API is the <span class="title-ref">\~scrapy.crawler.Crawler</span> object, passed to extensions through the `from_crawler` class method. This object provides access to all Scrapy core components, and it's the only way for extensions to access them and hook their functionality into Scrapy.

<div class="module" data-synopsis="The Scrapy crawler">

scrapy.crawler

</div>

The Extension Manager is responsible for loading and keeping track of installed extensions and it's configured through the `EXTENSIONS` setting which contains a dictionary of all available extensions and their order similar to how you \[configure the downloader middlewares \<topics-downloader-middleware-setting\>\](\#configure-the-downloader-middlewares \<topics-downloader-middleware-setting\>).

<div class="Crawler(spidercls, settings)">

The Crawler object must be instantiated with a <span class="title-ref">scrapy.Spider</span> subclass and a <span class="title-ref">scrapy.settings.Settings</span> object.

<div class="attribute">

request\_fingerprinter

The request fingerprint builder of this crawler.

This is used from extensions and middlewares to build short, unique identifiers for requests. See \[request-fingerprints\](\#request-fingerprints).

</div>

<div class="attribute">

settings

The settings manager of this crawler.

This is used by extensions & middlewares to access the Scrapy settings of this crawler.

For an introduction on Scrapy settings see \[topics-settings\](\#topics-settings).

For the API see <span class="title-ref">\~scrapy.settings.Settings</span> class.

</div>

<div class="attribute">

signals

The signals manager of this crawler.

This is used by extensions & middlewares to hook themselves into Scrapy functionality.

For an introduction on signals see \[topics-signals\](\#topics-signals).

For the API see <span class="title-ref">\~scrapy.signalmanager.SignalManager</span> class.

</div>

<div class="attribute">

stats

The stats collector of this crawler.

This is used from extensions & middlewares to record stats of their behaviour, or access stats collected by other extensions.

For an introduction on stats collection see \[topics-stats\](\#topics-stats).

For the API see <span class="title-ref">\~scrapy.statscollectors.StatsCollector</span> class.

</div>

<div class="attribute">

extensions

The extension manager that keeps track of enabled extensions.

Most extensions won't need to access this attribute.

For an introduction on extensions and a list of available extensions on Scrapy see \[topics-extensions\](\#topics-extensions).

</div>

<div class="attribute">

engine

The execution engine, which coordinates the core crawling logic between the scheduler, downloader and spiders.

Some extension may want to access the Scrapy engine, to inspect or modify the downloader and scheduler behaviour, although this is an advanced use and this API is not yet stable.

</div>

<div class="attribute">

spider

Spider currently being crawled. This is an instance of the spider class provided while constructing the crawler, and it is created after the arguments given in the <span class="title-ref">crawl</span> method.

</div>

<div class="method">

crawl(*args,*\*kwargs)

Starts the crawler by instantiating its spider class with the given `args` and `kwargs` arguments, while setting the execution engine in motion. Should be called only once.

Returns a deferred that is fired when the crawl is finished.

</div>

<div class="automethod">

stop

</div>

</div>

<div class="autoclass" data-members="">

CrawlerRunner

</div>

<div class="autoclass" data-show-inheritance="" data-members="" data-inherited-members="">

CrawlerProcess

</div>

## Settings API

<div class="module" data-synopsis="Settings manager">

scrapy.settings

</div>

<div class="attribute">

SETTINGS\_PRIORITIES

Dictionary that sets the key name and priority level of the default settings priorities used in Scrapy.

Each item defines a settings entry point, giving it a code name for identification and an integer priority. Greater priorities take more precedence over lesser ones when setting and retrieving values in the <span class="title-ref">\~scrapy.settings.Settings</span> class.

  - \`\`\`python
    
      - SETTINGS\_PRIORITIES = {  
        "default": 0, "command": 10, "addon": 15, "project": 20, "spider": 30, "cmdline": 40,
    
    }

For a detailed explanation on each settings sources, see: \[topics-settings\](\#topics-settings).

</div>

<div class="autofunction">

get\_settings\_priority

</div>

<div class="autoclass" data-show-inheritance="" data-members="">

Settings

</div>

<div class="autoclass" data-members="">

BaseSettings

</div>

<div id="topics-api-spiderloader">

SpiderLoader API `` ` ================  .. module:: scrapy.spiderloader    :synopsis: The spider loader  .. class:: SpiderLoader      This class is in charge of retrieving and handling the spider classes     defined across the project.      Custom spider loaders can be employed by specifying their path in the     :setting:`SPIDER_LOADER_CLASS` project setting. They must fully implement     the `scrapy.interfaces.ISpiderLoader` interface to guarantee an     errorless execution.      .. method:: from_settings(settings)         This class method is used by Scrapy to create an instance of the class.        It's called with the current project settings, and it loads the spiders        found recursively in the modules of the :setting:`SPIDER_MODULES`        setting.         :param settings: project settings        :type settings: `~scrapy.settings.Settings` instance      .. method:: load(spider_name)         Get the Spider class with the given name. It'll look into the previously        loaded spiders for a spider class with name ``spider\_name``and will raise        a KeyError if not found.         :param spider_name: spider class name        :type spider_name: str      .. method:: list()         Get the names of the available spiders in the project.      .. method:: find_by_request(request)         List the spiders' names that can handle the given request. Will try to        match the request's url against the domains of the spiders.         :param request: queried request        :type request: `~scrapy.Request` instance  .. _topics-api-signals:  Signals API ===========  .. automodule:: scrapy.signalmanager     :synopsis: The signal manager     :members:     :undoc-members:  .. _topics-api-stats:  Stats Collector API ===================  There are several Stats Collectors available under the :mod:`scrapy.statscollectors` module and they all implement the Stats Collector API defined by the `~scrapy.statscollectors.StatsCollector` class (which they all inherit from).  .. module:: scrapy.statscollectors    :synopsis: Stats Collectors  .. class:: StatsCollector      .. method:: get_value(key, default=None)          Return the value for the given stats key or default if it doesn't exist.      .. method:: get_stats()          Get all stats from the currently running spider as a dict.      .. method:: set_value(key, value)          Set the given value for the given stats key.      .. method:: set_stats(stats)          Override the current stats with the dict passed in``stats\`\` argument.

</div>

> 
> 
> <div class="method">
> 
> inc\_value(key, count=1, start=0)
> 
> Increment the value of the given stats key, by the given count, assuming the start value given (when it's not set).
> 
> </div>
> 
> <div class="method">
> 
> max\_value(key, value)
> 
> Set the given value for the given key only if current value for the same key is lower than value. If there is no current value for the given key, the value is always set.
> 
> </div>
> 
> <div class="method">
> 
> min\_value(key, value)
> 
> Set the given value for the given key only if current value for the same key is greater than value. If there is no current value for the given key, the value is always set.
> 
> </div>
> 
> <div class="method">
> 
> clear\_stats()
> 
> Clear all stats.
> 
> </div>
> 
> The following methods are not part of the stats collection api but instead used when implementing custom stats collectors:
> 
> <div class="method">
> 
> open\_spider(spider)
> 
> Open the given spider for stats collection.
> 
> </div>
> 
> <div class="method">
> 
> close\_spider(spider)
> 
> Close the given spider. After this is called, no more specific stats can be accessed or collected.
> 
> </div>

---

architecture.md

---

# Architecture overview

This document describes the architecture of Scrapy and how its components interact.

## Overview

The following diagram shows an overview of the Scrapy architecture with its components and an outline of the data flow that takes place inside the system (shown by the red arrows). A brief description of the components is included below with links for more detailed information about them. The data flow is also described below.

## Data flow

![Scrapy architecture](_images/scrapy_architecture_02.png)

The data flow in Scrapy is controlled by the execution engine, and goes like this:

1.  The \[Engine \<component-engine\>\](\#engine-\<component-engine\>) gets the initial Requests to crawl from the \[Spider \<component-spiders\>\](\#spider-\<component-spiders\>).
2.  The \[Engine \<component-engine\>\](\#engine-\<component-engine\>) schedules the Requests in the \[Scheduler \<component-scheduler\>\](\#scheduler-\<component-scheduler\>) and asks for the next Requests to crawl.
3.  The \[Scheduler \<component-scheduler\>\](\#scheduler-\<component-scheduler\>) returns the next Requests to the \[Engine \<component-engine\>\](\#engine-\<component-engine\>).
4.  The \[Engine \<component-engine\>\](\#engine-\<component-engine\>) sends the Requests to the \[Downloader \<component-downloader\>\](\#downloader-\<component-downloader\>), passing through the \[Downloader Middlewares \<component-downloader-middleware\>\](\#downloader-middlewares-\<component-downloader-middleware\>) (see <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_request</span>).
5.  Once the page finishes downloading the \[Downloader \<component-downloader\>\](\#downloader-\<component-downloader\>) generates a Response (with that page) and sends it to the Engine, passing through the \[Downloader Middlewares \<component-downloader-middleware\>\](\#downloader-middlewares-\<component-downloader-middleware\>) (see <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_response</span>).
6.  The \[Engine \<component-engine\>\](\#engine-\<component-engine\>) receives the Response from the \[Downloader \<component-downloader\>\](\#downloader-\<component-downloader\>) and sends it to the \[Spider \<component-spiders\>\](\#spider-\<component-spiders\>) for processing, passing through the \[Spider Middleware \<component-spider-middleware\>\](\#spider-middleware-\<component-spider-middleware\>) (see <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_spider\_input</span>).
7.  The \[Spider \<component-spiders\>\](\#spider-\<component-spiders\>) processes the Response and returns scraped items and new Requests (to follow) to the \[Engine \<component-engine\>\](\#engine-\<component-engine\>), passing through the \[Spider Middleware \<component-spider-middleware\>\](\#spider-middleware-\<component-spider-middleware\>) (see <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_spider\_output</span>).
8.  The \[Engine \<component-engine\>\](\#engine-\<component-engine\>) sends processed items to \[Item Pipelines \<component-pipelines\>\](\#item-pipelines-\<component-pipelines\>), then send processed Requests to the \[Scheduler \<component-scheduler\>\](\#scheduler-\<component-scheduler\>) and asks for possible next Requests to crawl.
9.  The process repeats (from step 3) until there are no more requests from the \[Scheduler \<component-scheduler\>\](\#scheduler-\<component-scheduler\>).

## Components

### Scrapy Engine

The engine is responsible for controlling the data flow between all components of the system, and triggering events when certain actions occur. See the \[Data Flow \<data-flow\>\](\#data-flow-\<data-flow\>) section above for more details.

### Scheduler

The \[scheduler \<topics-scheduler\>\](\#scheduler-\<topics-scheduler\>) receives requests from the engine and enqueues them for feeding them later (also to the engine) when the engine requests them.

### Downloader

The Downloader is responsible for fetching web pages and feeding them to the engine which, in turn, feeds them to the spiders.

### Spiders

Spiders are custom classes written by Scrapy users to parse responses and extract \[items \<topics-items\>\](\#items-\<topics-items\>) from them or additional requests to follow. For more information see \[topics-spiders\](\#topics-spiders).

### Item Pipeline

The Item Pipeline is responsible for processing the items once they have been extracted (or scraped) by the spiders. Typical tasks include cleansing, validation and persistence (like storing the item in a database). For more information see \[topics-item-pipeline\](\#topics-item-pipeline).

### Downloader middlewares

Downloader middlewares are specific hooks that sit between the Engine and the Downloader and process requests when they pass from the Engine to the Downloader, and responses that pass from Downloader to the Engine.

Use a Downloader middleware if you need to do one of the following:

  - process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website);
  - change received response before passing it to a spider;
  - send a new Request instead of passing received response to a spider;
  - pass response to a spider without fetching a web page;
  - silently drop some requests.

For more information see \[topics-downloader-middleware\](\#topics-downloader-middleware).

### Spider middlewares

Spider middlewares are specific hooks that sit between the Engine and the Spiders and are able to process spider input (responses) and output (items and requests).

Use a Spider middleware if you need to

  - post-process output of spider callbacks - change/add/remove requests or items;
  - post-process start\_requests;
  - handle spider exceptions;
  - call errback instead of callback for some of the requests based on response content.

For more information see \[topics-spider-middleware\](\#topics-spider-middleware).

## Event-driven networking

Scrapy is written with [Twisted](https://twisted.org/), a popular event-driven networking framework for Python. Thus, it's implemented using a non-blocking (aka asynchronous) code for concurrency.

For more information about asynchronous programming and Twisted see these links:

  - \[twisted:core/howto/defer-intro\](twisted:core/howto/defer-intro.md)
  - [Twisted Introduction - Krondo](https://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/)

---

asyncio.md

---

# asyncio

<div class="versionadded">

2.0

</div>

Scrapy has partial support for `asyncio`. After you \[install the asyncio reactor \<install-asyncio\>\](\#install-the asyncio-reactor-\<install-asyncio\>), you may use `asyncio` and `asyncio`-powered libraries in any \[coroutine \<coroutines\>\](coroutine \<coroutines\>.md).

## Installing the asyncio reactor

To enable `asyncio` support, set the `TWISTED_REACTOR` setting to `'twisted.internet.asyncioreactor.AsyncioSelectorReactor'`.

If you are using <span class="title-ref">\~scrapy.crawler.CrawlerRunner</span>, you also need to install the <span class="title-ref">\~twisted.internet.asyncioreactor.AsyncioSelectorReactor</span> reactor manually. You can do that using \`\~scrapy.utils.reactor.install\_reactor\`:

    install_reactor('twisted.internet.asyncioreactor.AsyncioSelectorReactor')

## Handling a pre-installed reactor

`twisted.internet.reactor` and some other Twisted imports install the default Twisted reactor as a side effect. Once a Twisted reactor is installed, it is not possible to switch to a different reactor at run time.

If you \[configure the asyncio Twisted reactor \<install-asyncio\>\](\#configure-the-asyncio-twisted-reactor-\<install-asyncio\>) and, at run time, Scrapy complains that a different reactor is already installed, chances are you have some such imports in your code.

You can usually fix the issue by moving those offending module-level Twisted imports to the method or function definitions where they are used. For example, if you have something like:

`` `python     from twisted.internet import reactor       def my_function():         reactor.callLater(...)  Switch to something like:  .. code-block:: python      def my_function():         from twisted.internet import reactor          reactor.callLater(...)  Alternatively, you can try to [manually install the asyncio reactor ](#manually-install-the-asyncio-reactor ) `` \<install-asyncio\><span class="title-ref">, with </span>\~scrapy.utils.reactor.install\_reactor\`, before those imports happen.

## Awaiting on Deferreds

When the asyncio reactor isn't installed, you can await on Deferreds in the coroutines directly. When it is installed, this is not possible anymore, due to specifics of the Scrapy coroutine integration (the coroutines are wrapped into <span class="title-ref">asyncio.Future</span> objects, not into <span class="title-ref">\~twisted.internet.defer.Deferred</span> directly), and you need to wrap them into Futures. Scrapy provides two helpers for this:

<div class="autofunction">

scrapy.utils.defer.deferred\_to\_future

</div>

<div class="autofunction">

scrapy.utils.defer.maybe\_deferred\_to\_future

</div>

<div class="tip">

<div class="title">

Tip

</div>

If you need to use these functions in code that aims to be compatible with lower versions of Scrapy that do not provide these functions, down to Scrapy 2.0 (earlier versions do not support `asyncio`), you can copy the implementation of these functions into your own code.

</div>

## Enforcing asyncio as a requirement

If you are writing a \[component \<topics-components\>\](\#component-\<topics-components\>) that requires asyncio to work, use <span class="title-ref">scrapy.utils.reactor.is\_asyncio\_reactor\_installed</span> to \[enforce it as a requirement \<enforce-component-requirements\>\](\#enforce-it-as-a-requirement-\<enforce-component-requirements\>). For example:

`` `python     from scrapy.utils.reactor import is_asyncio_reactor_installed       class MyComponent:         def __init__(self):             if not is_asyncio_reactor_installed():                 raise ValueError(                     f"{MyComponent.__qualname__} requires the asyncio Twisted "                     f"reactor. Make sure you have it configured in the "                     f"TWISTED_REACTOR setting. See the asyncio documentation "                     f"of Scrapy for more information."                 )   .. _asyncio-windows:  Windows-specific notes ``\` ======================

The Windows implementation of `asyncio` can use two event loop implementations, <span class="title-ref">\~asyncio.ProactorEventLoop</span> (default) and <span class="title-ref">\~asyncio.SelectorEventLoop</span>. However, only <span class="title-ref">\~asyncio.SelectorEventLoop</span> works with Twisted.

Scrapy changes the event loop class to <span class="title-ref">\~asyncio.SelectorEventLoop</span> automatically when you change the `TWISTED_REACTOR` setting or call <span class="title-ref">\~scrapy.utils.reactor.install\_reactor</span>.

<div class="note">

<div class="title">

Note

</div>

Other libraries you use may require <span class="title-ref">\~asyncio.ProactorEventLoop</span>, e.g. because it supports subprocesses (this is the case with [playwright](https://github.com/microsoft/playwright-python)), so you cannot use them together with Scrapy on Windows (but you should be able to use them on WSL or native Linux).

</div>

## Using custom asyncio loops

You can also use custom asyncio event loops with the asyncio reactor. Set the `ASYNCIO_EVENT_LOOP` setting to the import path of the desired event loop class to use it instead of the default asyncio event loop.

---

autothrottle.md

---

# AutoThrottle extension

This is an extension for automatically throttling crawling speed based on load of both the Scrapy server and the website you are crawling.

## Design goals

1.  be nicer to sites instead of using default download delay of zero
2.  automatically adjust Scrapy to the optimum crawling speed, so the user doesn't have to tune the download delays to find the optimum one. The user only needs to specify the maximum concurrent requests it allows, and the extension does the rest.

## How it works

Scrapy allows defining the concurrency and delay of different download slots, e.g. through the `DOWNLOAD_SLOTS` setting. By default requests are assigned to slots based on their URL domain, although it is possible to customize the download slot of any request.

The AutoThrottle extension adjusts the delay of each download slot dynamically, to make your spider send `AUTOTHROTTLE_TARGET_CONCURRENCY` concurrent requests on average to each remote website.

It uses download latency to compute the delays. The main idea is the following: if a server needs `latency` seconds to respond, a client should send a request each `latency/N` seconds to have `N` requests processed in parallel.

Instead of adjusting the delays one can just set a small fixed download delay and impose hard limits on concurrency using `CONCURRENT_REQUESTS_PER_DOMAIN` or `CONCURRENT_REQUESTS_PER_IP` options. It will provide a similar effect, but there are some important differences:

  - because the download delay is small there will be occasional bursts of requests;
  - often non-200 (error) responses can be returned faster than regular responses, so with a small download delay and a hard concurrency limit crawler will be sending requests to server faster when server starts to return errors. But this is an opposite of what crawler should do - in case of errors it makes more sense to slow down: these errors may be caused by the high request rate.

AutoThrottle doesn't have these issues.

## Throttling algorithm

AutoThrottle algorithm adjusts download delays based on the following rules:

1.  spiders always start with a download delay of `AUTOTHROTTLE_START_DELAY`;
2.  when a response is received, the target download delay is calculated as `latency / N` where `latency` is a latency of the response, and `N` is `AUTOTHROTTLE_TARGET_CONCURRENCY`.
3.  download delay for next requests is set to the average of previous download delay and the target download delay;
4.  latencies of non-200 responses are not allowed to decrease the delay;
5.  download delay can't become less than `DOWNLOAD_DELAY` or greater than `AUTOTHROTTLE_MAX_DELAY`

<div class="note">

<div class="title">

Note

</div>

The AutoThrottle extension honours the standard Scrapy settings for concurrency and delay. This means that it will respect `CONCURRENT_REQUESTS_PER_DOMAIN` and `CONCURRENT_REQUESTS_PER_IP` options and never set a download delay lower than `DOWNLOAD_DELAY`.

</div>

<div id="download-latency">

In Scrapy, the download latency is measured as the time elapsed between establishing the TCP connection and receiving the HTTP headers.

</div>

Note that these latencies are very hard to measure accurately in a cooperative multitasking environment because Scrapy may be busy processing a spider callback, for example, and unable to attend downloads. However, these latencies should still give a reasonable estimate of how busy Scrapy (and ultimately, the server) is, and this extension builds on that premise.

<div class="reqmeta">

autothrottle\_dont\_adjust\_delay

</div>

## Prevent specific requests from triggering slot delay adjustments

AutoThrottle adjusts the delay of download slots based on the latencies of responses that belong to that download slot. The only exceptions are non-200 responses, which are only taken into account to increase that delay, but ignored if they would decrease that delay.

You can also set the `autothrottle_dont_adjust_delay` request metadata key to `True` in any request to prevent its response latency from impacting the delay of its download slot:

`` `python     from scrapy import Request      Request("https://example.com", meta={"autothrottle_dont_adjust_delay": True})  Note, however, that AutoThrottle still determines the starting delay of every ``<span class="title-ref"> download slot by setting the </span><span class="title-ref">download\_delay</span><span class="title-ref"> attribute on the running spider. If you want AutoThrottle not to impact a download slot at all, in addition to setting this meta key in all requests that use that download slot, you might want to set a custom value for the </span><span class="title-ref">delay</span><span class="title-ref"> attribute of that download slot, e.g. using :setting:\`DOWNLOAD\_SLOTS</span>.

## Settings

The settings used to control the AutoThrottle extension are:

  - `AUTOTHROTTLE_ENABLED`
  - `AUTOTHROTTLE_START_DELAY`
  - `AUTOTHROTTLE_MAX_DELAY`
  - `AUTOTHROTTLE_TARGET_CONCURRENCY`
  - `AUTOTHROTTLE_DEBUG`
  - `CONCURRENT_REQUESTS_PER_DOMAIN`
  - `CONCURRENT_REQUESTS_PER_IP`
  - `DOWNLOAD_DELAY`

For more information see \[autothrottle-algorithm\](\#autothrottle-algorithm).

<div class="setting">

AUTOTHROTTLE\_ENABLED

</div>

### AUTOTHROTTLE\_ENABLED

Default: `False`

Enables the AutoThrottle extension.

<div class="setting">

AUTOTHROTTLE\_START\_DELAY

</div>

### AUTOTHROTTLE\_START\_DELAY

Default: `5.0`

The initial download delay (in seconds).

<div class="setting">

AUTOTHROTTLE\_MAX\_DELAY

</div>

### AUTOTHROTTLE\_MAX\_DELAY

Default: `60.0`

The maximum download delay (in seconds) to be set in case of high latencies.

<div class="setting">

AUTOTHROTTLE\_TARGET\_CONCURRENCY

</div>

### AUTOTHROTTLE\_TARGET\_CONCURRENCY

Default: `1.0`

Average number of requests Scrapy should be sending in parallel to remote websites. It must be higher than `0.0`.

By default, AutoThrottle adjusts the delay to send a single concurrent request to each of the remote websites. Set this option to a higher value (e.g. `2.0`) to increase the throughput and the load on remote servers. A lower `AUTOTHROTTLE_TARGET_CONCURRENCY` value (e.g. `0.5`) makes the crawler more conservative and polite.

Note that `CONCURRENT_REQUESTS_PER_DOMAIN` and `CONCURRENT_REQUESTS_PER_IP` options are still respected when AutoThrottle extension is enabled. This means that if `AUTOTHROTTLE_TARGET_CONCURRENCY` is set to a value higher than `CONCURRENT_REQUESTS_PER_DOMAIN` or `CONCURRENT_REQUESTS_PER_IP`, the crawler won't reach this number of concurrent requests.

At every given time point Scrapy can be sending more or less concurrent requests than `AUTOTHROTTLE_TARGET_CONCURRENCY`; it is a suggested value the crawler tries to approach, not a hard limit.

<div class="setting">

AUTOTHROTTLE\_DEBUG

</div>

### AUTOTHROTTLE\_DEBUG

Default: `False`

Enable AutoThrottle debug mode which will display stats on every response received, so you can see how the throttling parameters are being adjusted in real time.

---

benchmarking.md

---

# Benchmarking

Scrapy comes with a simple benchmarking suite that spawns a local HTTP server and crawls it at the maximum possible speed. The goal of this benchmarking is to get an idea of how Scrapy performs in your hardware, in order to have a common baseline for comparisons. It uses a simple spider that does nothing and just follows links.

To run it use:

    scrapy bench

You should see an output like this:

    2016-12-16 21:18:48 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: quotesbot)
    2016-12-16 21:18:48 [scrapy.utils.log] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 10, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['quotesbot.spiders'], 'LOGSTATS_INTERVAL': 1, 'BOT_NAME': 'quotesbot', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'quotesbot.spiders'}
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled extensions:
    ['scrapy.extensions.closespider.CloseSpider',
     'scrapy.extensions.logstats.LogStats',
     'scrapy.extensions.telnet.TelnetConsole',
     'scrapy.extensions.corestats.CoreStats']
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
    ['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
     'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
     'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
     'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
     'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
     'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
     'scrapy.downloadermiddlewares.retry.RetryMiddleware',
     'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
     'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
     'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
     'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
     'scrapy.downloadermiddlewares.stats.DownloaderStats']
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled spider middlewares:
    ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
     'scrapy.spidermiddlewares.referer.RefererMiddleware',
     'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
     'scrapy.spidermiddlewares.depth.DepthMiddleware']
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled item pipelines:
    []
    2016-12-16 21:18:49 [scrapy.core.engine] INFO: Spider opened
    2016-12-16 21:18:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:50 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 4200 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:51 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:52 [scrapy.extensions.logstats] INFO: Crawled 198 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:54 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:55 [scrapy.extensions.logstats] INFO: Crawled 358 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:56 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:57 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:58 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:59 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)
    2016-12-16 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 518 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:19:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
    {'downloader/request_bytes': 229995,
     'downloader/request_count': 534,
     'downloader/request_method_count/GET': 534,
     'downloader/response_bytes': 1565504,
     'downloader/response_count': 534,
     'downloader/response_status_count/200': 534,
     'finish_reason': 'closespider_timeout',
     'finish_time': datetime.datetime(2016, 12, 16, 16, 19, 0, 647725),
     'log_count/INFO': 17,
     'request_depth_max': 19,
     'response_received_count': 534,
     'scheduler/dequeued': 533,
     'scheduler/dequeued/memory': 533,
     'scheduler/enqueued': 10661,
     'scheduler/enqueued/memory': 10661,
     'start_time': datetime.datetime(2016, 12, 16, 16, 18, 49, 799869)}
    2016-12-16 21:19:00 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)

That tells you that Scrapy is able to crawl about 3000 pages per minute in the hardware where you run it. Note that this is a very simple spider intended to follow links, any custom spider you write will probably do more stuff which results in slower crawl rates. How slower depends on how much your spider does and how well it's written.

Use [scrapy-bench](https://github.com/scrapy/scrapy-bench) for more complex benchmarking.

---

broad-crawls.md

---

# Broad Crawls

Scrapy defaults are optimized for crawling specific sites. These sites are often handled by a single Scrapy spider, although this is not necessary or required (for example, there are generic spiders that handle any given site thrown at them).

In addition to this "focused crawl", there is another common type of crawling which covers a large (potentially unlimited) number of domains, and is only limited by time or other arbitrary constraint, rather than stopping when the domain was crawled to completion or when there are no more requests to perform. These are called "broad crawls" and is the typical crawlers employed by search engines.

These are some common properties often found in broad crawls:

  - they crawl many domains (often, unbounded) instead of a specific set of sites
  - they don't necessarily crawl domains to completion, because it would be impractical (or impossible) to do so, and instead limit the crawl by time or number of pages crawled
  - they are simpler in logic (as opposed to very complex spiders with many extraction rules) because data is often post-processed in a separate stage
  - they crawl many domains concurrently, which allows them to achieve faster crawl speeds by not being limited by any particular site constraint (each site is crawled slowly to respect politeness, but many sites are crawled in parallel)

As said above, Scrapy default settings are optimized for focused crawls, not broad crawls. However, due to its asynchronous architecture, Scrapy is very well suited for performing fast broad crawls. This page summarizes some things you need to keep in mind when using Scrapy for doing broad crawls, along with concrete suggestions of Scrapy settings to tune in order to achieve an efficient broad crawl.

## Use the right `SCHEDULER_PRIORITY_QUEUE`

Scrapy’s default scheduler priority queue is `'scrapy.pqueues.ScrapyPriorityQueue'`. It works best during single-domain crawl. It does not work well with crawling many different domains in parallel

To apply the recommended priority queue use:

`` `python     SCHEDULER_PRIORITY_QUEUE = "scrapy.pqueues.DownloaderAwarePriorityQueue"  .. _broad-crawls-concurrency:  Increase concurrency ``\` ====================

Concurrency is the number of requests that are processed in parallel. There is a global limit (`CONCURRENT_REQUESTS`) and an additional limit that can be set either per domain (`CONCURRENT_REQUESTS_PER_DOMAIN`) or per IP (`CONCURRENT_REQUESTS_PER_IP`).

<div class="note">

<div class="title">

Note

</div>

The scheduler priority queue \[recommended for broad crawls \<broad-crawls-scheduler-priority-queue\>\](\#recommended-for-broad-crawls

</div>

  - \----------\<broad-crawls-scheduler-priority-queue\>) does not support  
    `CONCURRENT_REQUESTS_PER_IP`.

The default global concurrency limit in Scrapy is not suitable for crawling many different domains in parallel, so you will want to increase it. How much to increase it will depend on how much CPU and memory your crawler will have available.

A good starting point is `100`:

`` `python     CONCURRENT_REQUESTS = 100  But the best way to find out is by doing some trials and identifying at what ``\` concurrency your Scrapy process gets CPU bounded. For optimum performance, you should pick a concurrency where CPU usage is at 80-90%.

Increasing concurrency also increases memory usage. If memory usage is a concern, you might need to lower your global concurrency limit accordingly.

## Increase Twisted IO thread pool maximum size

Currently Scrapy does DNS resolution in a blocking way with usage of thread pool. With higher concurrency levels the crawling could be slow or even fail hitting DNS resolver timeouts. Possible solution to increase the number of threads handling DNS queries. The DNS queue will be processed faster speeding up establishing of connection and crawling overall.

To increase maximum thread pool size use:

`` `python     REACTOR_THREADPOOL_MAXSIZE = 20  Setup your own DNS ``\` ==================

If you have multiple crawling processes and single central DNS, it can act like DoS attack on the DNS server resulting to slow down of entire network or even blocking your machines. To avoid this setup your own DNS server with local cache and upstream to some large DNS like OpenDNS or Verizon.

## Reduce log level

When doing broad crawls you are often only interested in the crawl rates you get and any errors found. These stats are reported by Scrapy when using the `INFO` log level. In order to save CPU (and log storage requirements) you should not use `DEBUG` log level when performing large broad crawls in production. Using `DEBUG` level when developing your (broad) crawler may be fine though.

To set the log level use:

`` `python     LOG_LEVEL = "INFO"  Disable cookies ``\` ===============

Disable cookies unless you *really* need. Cookies are often not needed when doing broad crawls (search engine crawlers ignore them), and they improve performance by saving some CPU cycles and reducing the memory footprint of your Scrapy crawler.

To disable cookies use:

`` `python     COOKIES_ENABLED = False  Disable retries ``\` ===============

Retrying failed HTTP requests can slow down the crawls substantially, specially when sites causes are very slow (or fail) to respond, thus causing a timeout error which gets retried many times, unnecessarily, preventing crawler capacity to be reused for other domains.

To disable retries use:

`` `python     RETRY_ENABLED = False  Reduce download timeout ``\` =======================

Unless you are crawling from a very slow connection (which shouldn't be the case for broad crawls) reduce the download timeout so that stuck requests are discarded quickly and free up capacity to process the next ones.

To reduce the download timeout use:

`` `python     DOWNLOAD_TIMEOUT = 15  Disable redirects ``\` =================

Consider disabling redirects, unless you are interested in following them. When doing broad crawls it's common to save redirects and resolve them when revisiting the site at a later crawl. This also help to keep the number of request constant per crawl batch, otherwise redirect loops may cause the crawler to dedicate too many resources on any specific domain.

To disable redirects use:

`` `python     REDIRECT_ENABLED = False  Enable crawling of "Ajax Crawlable Pages" ``\` =========================================

Some pages (up to 1%, based on empirical data from year 2013) declare themselves as ajax crawlable. This means they provide plain HTML version of content that is usually available only via AJAX. Pages can indicate it in two ways:

1)  by using `#!` in URL - this is the default way;
2)  by using a special meta tag - this way is used on "main", "index" website pages.

Scrapy handles (1) automatically; to handle (2) enable \[AjaxCrawlMiddleware \<ajaxcrawl-middleware\>\](\#ajaxcrawlmiddleware-\<ajaxcrawl-middleware\>):

`` `python     AJAXCRAWL_ENABLED = True  When doing broad crawls it's common to crawl a lot of "index" web pages; ``\` AjaxCrawlMiddleware helps to crawl them correctly. It is turned OFF by default because it has some performance overhead, and enabling it for focused crawls doesn't make much sense.

## Crawl in BFO order

\[Scrapy crawls in DFO order by default \<faq-bfo-dfo\>\](\#scrapy-crawls-in-dfo-order-by-default-\<faq-bfo-dfo\>).

In broad crawls, however, page crawling tends to be faster than page processing. As a result, unprocessed early requests stay in memory until the final depth is reached, which can significantly increase memory usage.

\[Crawl in BFO order \<faq-bfo-dfo\>\](\#crawl-in-bfo-order-\<faq-bfo-dfo\>) instead to save memory.

## Be mindful of memory leaks

If your broad crawl shows a high memory usage, in addition to \[crawling in BFO order \<broad-crawls-bfo\>\](\#crawling-in bfo-order-\<broad-crawls-bfo\>) and \[lowering concurrency \<broad-crawls-concurrency\>\](\#lowering-concurrency \<broad-crawls-concurrency\>) you should \[debug your memory leaks \<topics-leaks\>\](\#debug-your-memory-leaks \<topics-leaks\>).

## Install a specific Twisted reactor

If the crawl is exceeding the system's capabilities, you might want to try installing a specific Twisted reactor, via the `TWISTED_REACTOR` setting.

---

commands.md

---

# Command line tool

Scrapy is controlled through the `scrapy` command-line tool, to be referred to here as the "Scrapy tool" to differentiate it from the sub-commands, which we just call "commands" or "Scrapy commands".

The Scrapy tool provides several commands, for multiple purposes, and each one accepts a different set of arguments and options.

(The `scrapy deploy` command has been removed in 1.0 in favor of the standalone `scrapyd-deploy`. See [Deploying your project](https://scrapyd.readthedocs.io/en/latest/deploy.html).)

## Configuration settings

Scrapy will look for configuration parameters in ini-style `scrapy.cfg` files in standard locations:

1.  `/etc/scrapy.cfg` or `c:\scrapy\scrapy.cfg` (system-wide),
2.  `~/.config/scrapy.cfg` (`$XDG_CONFIG_HOME`) and `~/.scrapy.cfg` (`$HOME`) for global (user-wide) settings, and
3.  `scrapy.cfg` inside a Scrapy project's root (see next section).

Settings from these files are merged in the listed order of preference: user-defined values have higher priority than system-wide defaults and project-wide settings will override all others, when defined.

Scrapy also understands, and can be configured through, a number of environment variables. Currently these are:

  - `SCRAPY_SETTINGS_MODULE` (see \[topics-settings-module-envvar\](\#topics-settings-module-envvar))
  - `SCRAPY_PROJECT` (see \[topics-project-envvar\](\#topics-project-envvar))
  - `SCRAPY_PYTHON_SHELL` (see \[topics-shell\](\#topics-shell))

## Default structure of Scrapy projects

Before delving into the command-line tool and its sub-commands, let's first understand the directory structure of a Scrapy project.

Though it can be modified, all Scrapy projects have the same file structure by default, similar to this:

``` none
scrapy.cfg
myproject/
    __init__.py
    items.py
    middlewares.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
```

The directory where the `scrapy.cfg` file resides is known as the *project root directory*. That file contains the name of the python module that defines the project settings. Here is an example:

`` `ini     [settings]     default = myproject.settings  .. _topics-project-envvar:  Sharing the root directory between projects ``\` ===========================================

A project root directory, the one that contains the `scrapy.cfg`, may be shared by multiple Scrapy projects, each with its own settings module.

In that case, you must define one or more aliases for those settings modules under `[settings]` in your `scrapy.cfg` file:

`` `ini     [settings]     default = myproject1.settings     project1 = myproject1.settings     project2 = myproject2.settings  By default, the ``scrapy`command-line tool will use the`default`settings.`<span class="title-ref"> Use the </span><span class="title-ref">SCRAPY\_PROJECT</span><span class="title-ref"> environment variable to specify a different project for </span><span class="title-ref">scrapy</span>\` to use:

``` none
$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot
```

## Using the `scrapy` tool

You can start by running the Scrapy tool with no arguments and it will print some usage help and the available commands:

``` none
Scrapy X.Y - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
```

The first line will print the currently active project if you're inside a Scrapy project. In this example it was run from outside a project. If run from inside a project it would have printed something like this:

``` none
Scrapy X.Y - project: myproject

Usage:
  scrapy <command> [options] [args]

[...]
```

### Creating projects

The first thing you typically do with the `scrapy` tool is create your Scrapy project:

``` none
scrapy startproject myproject [project_dir]
```

That will create a Scrapy project under the `project_dir` directory. If `project_dir` wasn't specified, `project_dir` will be the same as `myproject`.

Next, you go inside the new project directory:

``` none
cd project_dir
```

And you're ready to use the `scrapy` command to manage and control your project from there.

### Controlling projects

You use the `scrapy` tool from inside your projects to control and manage them.

For example, to create a new spider:

``` none
scrapy genspider mydomain mydomain.com
```

Some Scrapy commands (like `crawl`) must be run from inside a Scrapy project. See the \[commands reference \<topics-commands-ref\>\](\#commands-reference-\<topics-commands-ref\>) below for more information on which commands must be run from inside projects, and which not.

Also keep in mind that some commands may have slightly different behaviours when running them from inside projects. For example, the fetch command will use spider-overridden behaviours (such as the `user_agent` attribute to override the user-agent) if the url being fetched is associated with some specific spider. This is intentional, as the `fetch` command is meant to be used to check how spiders are downloading pages.

## Available tool commands

This section contains a list of the available built-in commands with a description and some usage examples. Remember, you can always get more info about each command by running:

``` none
scrapy <command> -h
```

And you can see all available commands with:

``` none
scrapy -h
```

There are two kinds of commands, those that only work from inside a Scrapy project (Project-specific commands) and those that also work without an active Scrapy project (Global commands), though they may behave slightly differently when run from inside a project (as they would use the project overridden settings).

Global commands:

  - `startproject`
  - `genspider`
  - `settings`
  - `runspider`
  - `shell`
  - `fetch`
  - `view`
  - `version`

Project-only commands:

  - `crawl`
  - `check`
  - `list`
  - `edit`
  - `parse`
  - `bench`

<div class="command">

startproject

</div>

### startproject

  - Syntax: `scrapy startproject <project_name> [project_dir]`
  - Requires project: *no*

Creates a new Scrapy project named `project_name`, under the `project_dir` directory. If `project_dir` wasn't specified, `project_dir` will be the same as `project_name`.

Usage example:

``` none
$ scrapy startproject myproject
```

<div class="command">

genspider

</div>

### genspider

  - Syntax: `scrapy genspider [-t template] <name> <domain or URL>`
  - Requires project: *no*

<div class="versionadded">

2.6.0 The ability to pass a URL instead of a domain.

</div>

Creates a new spider in the current folder or in the current project's `spiders` folder, if called from inside a project. The `<name>` parameter is set as the spider's `name`, while `<domain or URL>` is used to generate the `allowed_domains` and `start_urls` spider's attributes.

Usage example:

``` none
$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider 'example' using template 'basic'

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider 'scrapyorg' using template 'crawl'
```

This is just a convenient shortcut command for creating spiders based on pre-defined templates, but certainly not the only way to create spiders. You can just create the spider source code files yourself, instead of using this command.

<div class="command">

crawl

</div>

### crawl

  - Syntax: `scrapy crawl <spider>`
  - Requires project: *yes*

Start crawling using a spider.

Supported options:

  - `-h, --help`: show a help message and exit
  - `-a NAME=VALUE`: set a spider argument (may be repeated)
  - `--output FILE` or `-o FILE`: append scraped items to the end of FILE (use - for stdout). To define the output format, set a colon at the end of the output URI (i.e. `-o FILE:FORMAT`)
  - `--overwrite-output FILE` or `-O FILE`: dump scraped items into FILE, overwriting any existing file. To define the output format, set a colon at the end of the output URI (i.e. `-O FILE:FORMAT`)

Usage examples:

``` none
$ scrapy crawl myspider
[ ... myspider starts crawling ... ]

$ scrapy crawl -o myfile:csv myspider
[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]

$ scrapy crawl -O myfile:json myspider
[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]
```

<div class="command">

check

</div>

### check

  - Syntax: `scrapy check [-l] <spider>`
  - Requires project: *yes*

Run contract checks.

Usage examples:

``` none
$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> 'RetailPricex' field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4
```

<div class="command">

list

</div>

### list

  - Syntax: `scrapy list`
  - Requires project: *yes*

List all available spiders in the current project. The output is one spider per line.

Usage example:

``` none
$ scrapy list
spider1
spider2
```

<div class="command">

edit

</div>

### edit

  - Syntax: `scrapy edit <spider>`
  - Requires project: *yes*

Edit the given spider using the editor defined in the `EDITOR` environment variable or (if unset) the `EDITOR` setting.

This command is provided only as a convenient shortcut for the most common case, the developer is of course free to choose any tool or IDE to write and debug spiders.

Usage example:

``` none
$ scrapy edit spider1
```

<div class="command">

fetch

</div>

### fetch

  - Syntax: `scrapy fetch <url>`
  - Requires project: *no*

Downloads the given URL using the Scrapy downloader and writes the contents to standard output.

The interesting thing about this command is that it fetches the page the way the spider would download it. For example, if the spider has a `USER_AGENT` attribute which overrides the User Agent, it will use that one.

So this command can be used to "see" how your spider would fetch a certain page.

If used outside a project, no particular per-spider behaviour would be applied and it will just use the default Scrapy downloader settings.

Supported options:

  - `--spider=SPIDER`: bypass spider autodetection and force use of specific spider
  - `--headers`: print the response's HTTP headers instead of the response's body
  - `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)

Usage examples:

``` none
$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['"573c1-254-48c9c87349680"'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}
```

<div class="command">

view

</div>

### view

  - Syntax: `scrapy view <url>`
  - Requires project: *no*

Opens the given URL in a browser, as your Scrapy spider would "see" it. Sometimes spiders see pages differently from regular users, so this can be used to check what the spider "sees" and confirm it's what you expect.

Supported options:

  - `--spider=SPIDER`: bypass spider autodetection and force use of specific spider
  - `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them)

Usage example:

``` none
$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
```

<div class="command">

shell

</div>

### shell

  - Syntax: `scrapy shell [url]`
  - Requires project: *no*

Starts the Scrapy shell for the given URL (if given) or empty if no URL is given. Also supports UNIX-style local file paths, either relative with `./` or `../` prefixes or absolute file paths. See \[topics-shell\](\#topics-shell) for more info.

Supported options:

  - `--spider=SPIDER`: bypass spider autodetection and force use of specific spider
  - `-c code`: evaluate the code in the shell, print the result and exit
  - `--no-redirect`: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL you may pass as argument on the command line; once you are inside the shell, `fetch(url)` will still follow HTTP redirects by default.

Usage example:

``` none
$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
(200, 'http://www.example.com/')

# shell follows HTTP redirects by default
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(200, 'http://example.com/')

# you can disable this with --no-redirect
# (only for the URL passed as command line argument)
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')
```

<div class="command">

parse

</div>

### parse

  - Syntax: `scrapy parse <url> [options]`
  - Requires project: *yes*

Fetches the given URL and parses it with the spider that handles it, using the method passed with the `--callback` option, or `parse` if not given.

Supported options:

  - `--spider=SPIDER`: bypass spider autodetection and force use of specific spider

  - `--a NAME=VALUE`: set spider argument (may be repeated)

  - `--callback` or `-c`: spider method to use as callback for parsing the response

  - `--meta` or `-m`: additional request meta that will be passed to the callback request. This must be a valid json string. Example: --meta='{"foo" : "bar"}'

  - `--cbkwargs`: additional keyword arguments that will be passed to the callback. This must be a valid json string. Example: --cbkwargs='{"foo" : "bar"}'

  - `--pipelines`: process items through pipelines

  - `--rules` or `-r`: use <span class="title-ref">\~scrapy.spiders.CrawlSpider</span> rules to discover the callback (i.e. spider method) to use for parsing the response

  - `--noitems`: don't show scraped items

  - `--nolinks`: don't show extracted links

  - `--nocolour`: avoid using pygments to colorize the output

  - `--depth` or `-d`: depth level for which the requests should be followed recursively (default: 1)

  - `--verbose` or `-v`: display information for each depth level

  - `--output` or `-o`: dump scraped items to a file
    
    <div class="versionadded">
    
    2.3
    
    </div>

Usage example:

``` none
$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  ------------------------------------------------------------
[{'name': 'Example item',
 'category': 'Furniture',
 'length': '12 cm'}]

# Requests  -----------------------------------------------------------------
[]
```

<div class="command">

settings

</div>

### settings

  - Syntax: `scrapy settings [options]`
  - Requires project: *no*

Get the value of a Scrapy setting.

If used inside a project it'll show the project setting value, otherwise it'll show the default Scrapy value for that setting.

Example usage:

``` none
$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
```

<div class="command">

runspider

</div>

### runspider

  - Syntax: `scrapy runspider <spider_file.py>`
  - Requires project: *no*

Run a spider self-contained in a Python file, without having to create a project.

Example usage:

``` none
$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
```

<div class="command">

version

</div>

### version

  - Syntax: `scrapy version [-v]`
  - Requires project: *no*

Prints the Scrapy version. If used with `-v` it also prints Python, Twisted and Platform info, which is useful for bug reports.

<div class="command">

bench

</div>

### bench

  - Syntax: `scrapy bench`
  - Requires project: *no*

Run a quick benchmark test. \[benchmarking\](\#benchmarking).

## Custom project commands

You can also add your custom project commands by using the `COMMANDS_MODULE` setting. See the Scrapy commands in [scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands) for examples on how to implement your commands.

<div class="setting">

COMMANDS\_MODULE

</div>

### COMMANDS\_MODULE

Default: `''` (empty string)

A module to use for looking up custom Scrapy commands. This is used to add custom commands for your Scrapy project.

Example:

`` `python     COMMANDS_MODULE = "mybot.commands"    Register commands via setup.py entry points ``\` -------------------------------------------

You can also add Scrapy commands from an external library by adding a `scrapy.commands` section in the entry points of the library `setup.py` file.

The following example adds `my_command` command:

`` `python from setuptools import setup, find_packages  setup(     name="scrapy-mymodule",     entry_points={         "scrapy.commands": [             "my_command=my_scrapy_module.commands:MyCommand",         ],     }, ) ``\`

---

components.md

---

# Components

A Scrapy component is any class whose objects are created using <span class="title-ref">scrapy.utils.misc.create\_instance</span>.

That includes the classes that you may assign to the following settings:

  - `DNS_RESOLVER`
  - `DOWNLOAD_HANDLERS`
  - `DOWNLOADER_CLIENTCONTEXTFACTORY`
  - `DOWNLOADER_MIDDLEWARES`
  - `DUPEFILTER_CLASS`
  - `EXTENSIONS`
  - `FEED_EXPORTERS`
  - `FEED_STORAGES`
  - `ITEM_PIPELINES`
  - `SCHEDULER`
  - `SCHEDULER_DISK_QUEUE`
  - `SCHEDULER_MEMORY_QUEUE`
  - `SCHEDULER_PRIORITY_QUEUE`
  - `SPIDER_MIDDLEWARES`

Third-party Scrapy components may also let you define additional Scrapy components, usually configurable through \[settings \<topics-settings\>\](\#settings-\<topics-settings\>), to modify their behavior.

## Enforcing component requirements

Sometimes, your components may only be intended to work under certain conditions. For example, they may require a minimum version of Scrapy to work as intended, or they may require certain settings to have specific values.

In addition to describing those conditions in the documentation of your component, it is a good practice to raise an exception from the `__init__` method of your component if those conditions are not met at run time.

In the case of \[downloader middlewares \<topics-downloader-middleware\>\](\#downloader-middlewares-\<topics-downloader-middleware\>), \[extensions \<topics-extensions\>\](\#extensions-\<topics-extensions\>), \[item pipelines \<topics-item-pipeline\>\](\#item-pipelines \<topics-item-pipeline\>), and \[spider middlewares \<topics-spider-middleware\>\](\#spider-middlewares \<topics-spider-middleware\>), you should raise <span class="title-ref">scrapy.exceptions.NotConfigured</span>, passing a description of the issue as a parameter to the exception so that it is printed in the logs, for the user to see. For other components, feel free to raise whatever other exception feels right to you; for example, <span class="title-ref">RuntimeError</span> would make sense for a Scrapy version mismatch, while <span class="title-ref">ValueError</span> may be better if the issue is the value of a setting.

If your requirement is a minimum Scrapy version, you may use <span class="title-ref">scrapy.\_\_version\_\_</span> to enforce your requirement. For example:

`` `python from packaging.version import parse as parse_version  import scrapy   class MyComponent:     def __init__(self):         if parse_version(scrapy.__version__) < parse_version("2.7"):             raise RuntimeError(                 f"{MyComponent.__qualname__} requires Scrapy 2.7 or "                 f"later, which allow defining the process_spider_output "                 f"method of spider middlewares as an asynchronous "                 f"generator."             ) ``\`

---

contracts.md

---

# Spiders Contracts

Testing spiders can get particularly annoying and while nothing prevents you from writing unit tests the task gets cumbersome quickly. Scrapy offers an integrated way of testing your spiders by the means of contracts.

This allows you to test each callback of your spider by hardcoding a sample url and check various constraints for how the callback processes the response. Each contract is prefixed with an `@` and included in the docstring. See the following example:

`` `python     def parse(self, response):         """         This function parses a sample response. Some contracts are mingled         with this docstring.          @url http://www.example.com/s?field-keywords=selfish+gene         @returns items 1 16         @returns requests 0 0         @scrapes Title Author Year Price         """  You can use the following contracts:  .. module:: scrapy.contracts.default  .. class:: UrlContract      This contract ( ``@url`) sets the sample URL used when checking other     contract conditions for this spider. This contract is mandatory. All     callbacks lacking this contract are ignored when running the checks::      @url url  .. class:: CallbackKeywordArgumentsContract      This contract (`@cb\_kwargs``) sets the `cb_kwargs <scrapy.Request.cb_kwargs>`     attribute for the sample request. It must be a valid JSON dictionary.     ::      @cb_kwargs {"arg1": "value1", "arg2": "value2", ...}  .. class:: MetadataContract      This contract (``@meta``) sets the `meta <scrapy.Request.meta>`     attribute for the sample request. It must be a valid JSON dictionary.     ::      @meta {"arg1": "value1", "arg2": "value2", ...}  .. class:: ReturnsContract      This contract (``@returns`) sets lower and upper bounds for the items and     requests returned by the spider. The upper bound is optional::      @returns item(s)|request(s) [min [max]]  .. class:: ScrapesContract      This contract (`@scrapes``) checks that all the items returned by the     callback have the specified fields::      @scrapes field_1 field_2 ...  Use the :command:`check` command to run the contract checks.  Custom Contracts``\` ================

If you find you need more power than the built-in Scrapy contracts you can create and load your own contracts in the project by using the `SPIDER_CONTRACTS` setting:

`` `python     SPIDER_CONTRACTS = {         "myproject.contracts.ResponseCheck": 10,         "myproject.contracts.ItemValidate": 10,     }  Each contract must inherit from `~scrapy.contracts.Contract` and can ``\` override three methods:

<div class="module">

scrapy.contracts

</div>

<div class="Contract(method, *args)">

  - param method  
    callback function to which the contract is associated

  - type method  
    collections.abc.Callable

  - param args  
    list of arguments passed into the docstring (whitespace separated)

  - type args  
    list

<div class="method">

Contract.adjust\_request\_args(args)

This receives a `dict` as an argument containing default arguments for request object. <span class="title-ref">\~scrapy.Request</span> is used by default, but this can be changed with the `request_cls` attribute. If multiple contracts in chain have this attribute defined, the last one is used.

Must return the same or a modified version of it.

</div>

<div class="method">

Contract.pre\_process(response)

This allows hooking in various checks on the response received from the sample request, before it's being passed to the callback.

</div>

<div class="method">

Contract.post\_process(output)

This allows processing the output of the callback. Iterators are converted to lists before being passed to this hook.

</div>

</div>

Raise <span class="title-ref">\~scrapy.exceptions.ContractFail</span> from <span class="title-ref">\~scrapy.contracts.Contract.pre\_process</span> or <span class="title-ref">\~scrapy.contracts.Contract.post\_process</span> if expectations are not met:

<div class="autoclass">

scrapy.exceptions.ContractFail

</div>

Here is a demo contract which checks the presence of a custom header in the response received:

`` `python     from scrapy.contracts import Contract     from scrapy.exceptions import ContractFail       class HasHeaderContract(Contract):         """         Demo contract which checks the presence of a custom header         @has_header X-CustomHeader         """          name = "has_header"          def pre_process(self, response):             for header in self.args:                 if header not in response.headers:                     raise ContractFail("X-CustomHeader not present")  .. _detecting-contract-check-runs:  Detecting check runs ``\` ====================

When `scrapy check` is running, the `SCRAPY_CHECK` environment variable is set to the `true` string. You can use <span class="title-ref">os.environ</span> to perform any change to your spiders or your settings when `scrapy check` is used:

`` `python import os import scrapy   class ExampleSpider(scrapy.Spider):     name = "example"      def __init__(self):         if os.environ.get("SCRAPY_CHECK"):             pass  # Do some scraper adjustments when a check is running ``\`

---

coroutines.md

---

# Coroutines

<div class="versionadded">

2.0

</div>

Scrapy has \[partial support \<coroutine-support\>\](\#partial-support-\<coroutine-support\>) for the \[coroutine syntax \<async\>\](\#coroutine-syntax-\<async\>).

## Supported callables

The following callables may be defined as coroutines using `async def`, and hence use coroutine syntax (e.g. `await`, `async for`, `async with`):

\- <span class="title-ref">\~scrapy.Request</span> callbacks.

> If you are using any custom or third-party \[spider middleware \<topics-spider-middleware\>\](\#spider-middleware

\----\<topics-spider-middleware\>), see \[sync-async-spider-middleware\](\#sync-async-spider-middleware).

> 
> 
> <div class="versionchanged">
> 
> 2.7 Output of async callbacks is now processed asynchronously instead of collecting all of it first.
> 
> </div>

  - The <span class="title-ref">process\_item</span> method of \[item pipelines \<topics-item-pipeline\>\](\#item-pipelines-\<topics-item-pipeline\>).

  - The <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_request</span>, <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_response</span>, and <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_exception</span> methods of \[downloader middlewares \<topics-downloader-middleware-custom\>\](\#downloader-middlewares-\<topics-downloader-middleware-custom\>).

  - \[Signal handlers that support deferreds \<signal-deferred\>\](\#signal-handlers-that-support-deferreds-\<signal-deferred\>).

  - The <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_spider\_output</span> method of \[spider middlewares \<topics-spider-middleware\>\](\#spider-middlewares-\<topics-spider-middleware\>).
    
    It must be defined as an `asynchronous generator`. The input `result` parameter is an `asynchronous iterable`.
    
    See also \[sync-async-spider-middleware\](\#sync-async-spider-middleware) and \[universal-spider-middleware\](\#universal-spider-middleware).
    
    <div class="versionadded">
    
    2.7
    
    </div>

## General usage

There are several use cases for coroutines in Scrapy.

Code that would return Deferreds when written for previous Scrapy versions, such as downloader middlewares and signal handlers, can be rewritten to be shorter and cleaner:

`` `python     from itemadapter import ItemAdapter       class DbPipeline:         def _update_item(self, data, item):             adapter = ItemAdapter(item)             adapter["field"] = data             return item          def process_item(self, item, spider):             adapter = ItemAdapter(item)             dfd = db.get_some_data(adapter["id"])             dfd.addCallback(self._update_item, item)             return dfd  becomes:  .. code-block:: python      from itemadapter import ItemAdapter       class DbPipeline:         async def process_item(self, item, spider):             adapter = ItemAdapter(item)             adapter["field"] = await db.get_some_data(adapter["id"])             return item  Coroutines may be used to call asynchronous code. This includes other ``<span class="title-ref"> coroutines, functions that return Deferreds and functions that return :term:\`awaitable objects \<awaitable\></span> such as <span class="title-ref">\~asyncio.Future</span>. This means you can use many useful Python libraries providing such code:

`` `python     class MySpiderDeferred(Spider):         # ...         async def parse(self, response):             additional_response = await treq.get("https://additional.url")             additional_data = await treq.content(additional_response)             # ... use response and additional_data to yield items and requests       class MySpiderAsyncio(Spider):         # ...         async def parse(self, response):             async with aiohttp.ClientSession() as session:                 async with session.get("https://additional.url") as additional_response:                     additional_data = await additional_response.text()             # ... use response and additional_data to yield items and requests  .. note:: Many libraries that use coroutines, such as `aio-libs`_, require the           :mod:`asyncio` loop and to use them you need to           [enable asyncio support in Scrapy<asyncio>](enable asyncio support in Scrapy<asyncio>.md).  .. note:: If you want to ``await`on Deferreds while using the asyncio reactor,           you need to [wrap them<asyncio-await-dfd>](#wrap-them<asyncio-await-dfd>).  Common use cases for asynchronous code include:  * requesting data from websites, databases and other services (in callbacks,   pipelines and middlewares);`<span class="title-ref"> \* storing data in databases (in pipelines and middlewares); \* delaying the spider initialization until some external event (in the :signal:\`spider\_opened</span> handler); \* calling asynchronous Scrapy methods like <span class="title-ref">ExecutionEngine.download</span> (see \[the screenshot pipeline example\<ScreenshotPipeline\>\](\#the-screenshot-pipeline-example\<screenshotpipeline\>)).

## Inline requests

The spider below shows how to send a request and await its response all from within a spider callback:

`` `python     from scrapy import Spider, Request     from scrapy.utils.defer import maybe_deferred_to_future       class SingleRequestSpider(Spider):         name = "single"         start_urls = ["https://example.org/product"]          async def parse(self, response, **kwargs):             additional_request = Request("https://example.org/price")             deferred = self.crawler.engine.download(additional_request)             additional_response = await maybe_deferred_to_future(deferred)             yield {                 "h1": response.css("h1").get(),                 "price": additional_response.css("#price").get(),             }  You can also send multiple requests in parallel:  .. code-block:: python      from scrapy import Spider, Request     from scrapy.utils.defer import maybe_deferred_to_future     from twisted.internet.defer import DeferredList       class MultipleRequestsSpider(Spider):         name = "multiple"         start_urls = ["https://example.com/product"]          async def parse(self, response, **kwargs):             additional_requests = [                 Request("https://example.com/price"),                 Request("https://example.com/color"),             ]             deferreds = []             for r in additional_requests:                 deferred = self.crawler.engine.download(r)                 deferreds.append(deferred)             responses = await maybe_deferred_to_future(DeferredList(deferreds))             yield {                 "h1": response.css("h1::text").get(),                 "price": responses[0][1].css(".price::text").get(),                 "price2": responses[1][1].css(".color::text").get(),             }   .. _sync-async-spider-middleware:  Mixing synchronous and asynchronous spider middlewares ``\` ======================================================

<div class="versionadded">

2.7

</div>

The output of a <span class="title-ref">\~scrapy.Request</span> callback is passed as the `result` parameter to the <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_spider\_output</span> method of the first \[spider middleware \<topics-spider-middleware\>\](\#spider-middleware-\<topics-spider-middleware\>) from the \[list of active spider middlewares \<topics-spider-middleware-setting\>\](\#list-of-active-spider-middlewares-\<topics-spider-middleware-setting\>). Then the output of that `process_spider_output` method is passed to the `process_spider_output` method of the next spider middleware, and so on for every active spider middleware.

Scrapy supports mixing \[coroutine methods \<async\>\](\#coroutine-methods-\<async\>) and synchronous methods in this chain of calls.

However, if any of the `process_spider_output` methods is defined as a synchronous method, and the previous `Request` callback or `process_spider_output` method is a coroutine, there are some drawbacks to the asynchronous-to-synchronous conversion that Scrapy does so that the synchronous `process_spider_output` method gets a synchronous iterable as its `result` parameter:

  - The whole output of the previous `Request` callback or `process_spider_output` method is awaited at this point.

  - If an exception raises while awaiting the output of the previous `Request` callback or `process_spider_output` method, none of that output will be processed.
    
    This contrasts with the regular behavior, where all items yielded before an exception raises are processed.

Asynchronous-to-synchronous conversions are supported for backward compatibility, but they are deprecated and will stop working in a future version of Scrapy.

To avoid asynchronous-to-synchronous conversions, when defining `Request` callbacks as coroutine methods or when using spider middlewares whose `process_spider_output` method is an `asynchronous generator`, all active spider middlewares must either have their `process_spider_output` method defined as an asynchronous generator or \[define a process\_spider\_output\_async method \<universal-spider-middleware\>\](\#define-a process\_spider\_output\_async-method-\<universal-spider-middleware\>).

<div class="note">

<div class="title">

Note

</div>

When using third-party spider middlewares that only define a synchronous `process_spider_output` method, consider \[making them universal \<universal-spider-middleware\>\](\#making-them-universal-\<universal-spider-middleware\>) through \[subclassing \<tut-inheritance\>\](\#subclassing-\<tut-inheritance\>).

</div>

## Universal spider middlewares

<div class="versionadded">

2.7

</div>

To allow writing a spider middleware that supports asynchronous execution of its `process_spider_output` method in Scrapy 2.7 and later (avoiding \[asynchronous-to-synchronous conversions \<sync-async-spider-middleware\>\](\#asynchronous-to-synchronous-conversions-\<sync-async-spider-middleware\>)) while maintaining support for older Scrapy versions, you may define `process_spider_output` as a synchronous method and define an `asynchronous generator` version of that method with an alternative name: `process_spider_output_async`.

For example:

`` `python     class UniversalSpiderMiddleware:         def process_spider_output(self, response, result, spider):             for r in result:                 # ... do something with r                 yield r          async def process_spider_output_async(self, response, result, spider):             async for r in result:                 # ... do something with r                 yield r  .. note:: This is an interim measure to allow, for a time, to write code that           works in Scrapy 2.7 and later without requiring           asynchronous-to-synchronous conversions, and works in earlier Scrapy           versions as well.            In some future version of Scrapy, however, this feature will be           deprecated and, eventually, in a later version of Scrapy, this           feature will be removed, and all spider middlewares will be expected           to define their ``process\_spider\_output`method as an asynchronous           generator.`\`

---

debug.md

---

# Debugging Spiders

This document explains the most common techniques for debugging spiders. Consider the following Scrapy spider below:

`` `python     import scrapy     from myproject.items import MyItem       class MySpider(scrapy.Spider):         name = "myspider"         start_urls = (             "http://example.com/page1",             "http://example.com/page2",         )          def parse(self, response):             # <processing code not shown>             # collect `item_urls`             for item_url in item_urls:                 yield scrapy.Request(item_url, self.parse_item)          def parse_item(self, response):             # <processing code not shown>             item = MyItem()             # populate `item` fields             # and extract item_details_url             yield scrapy.Request(                 item_details_url, self.parse_details, cb_kwargs={"item": item}             )          def parse_details(self, response, item):             # populate more `item` fields             return item  Basically this is a simple spider which parses two pages of items (the ``<span class="title-ref"> start\_urls). Items also have a details page with additional information, so we use the </span><span class="title-ref">cb\_kwargs</span><span class="title-ref"> functionality of </span>\~scrapy.Request\` to pass a partially populated item.

## Parse Command

The most basic way of checking the output of your spider is to use the `parse` command. It allows to check the behaviour of different parts of the spider at the method level. It has the advantage of being flexible and simple to use, but does not allow debugging code inside a method.

In order to see the item scraped from a specific url:

``` none
$ scrapy parse --spider=myspider -c parse_item -d 2 <item_url>
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 2 <<<
# Scraped Items  ------------------------------------------------------------
[{'url': <item_url>}]

# Requests  -----------------------------------------------------------------
[]
```

Using the `--verbose` or `-v` option we can see the status at each depth level:

``` none
$ scrapy parse --spider=myspider -c parse_item -d 2 -v <item_url>
[ ... scrapy log lines crawling example.com spider ... ]

>>> DEPTH LEVEL: 1 <<<
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[<GET item_details_url>]

>>> DEPTH LEVEL: 2 <<<
# Scraped Items  ------------------------------------------------------------
[{'url': <item_url>}]

# Requests  -----------------------------------------------------------------
[]
```

Checking items scraped from a single start\_url, can also be easily achieved using:

``` none
$ scrapy parse --spider=myspider -d 3 'http://example.com/page1'
```

## Scrapy Shell

While the `parse` command is very useful for checking behaviour of a spider, it is of little help to check what happens inside a callback, besides showing the response received and the output. How to debug the situation when `parse_details` sometimes receives no item?

Fortunately, the `shell` is your bread and butter in this case (see \[topics-shell-inspect-response\](\#topics-shell-inspect-response)):

`` `python     from scrapy.shell import inspect_response       def parse_details(self, response, item=None):         if item:             # populate more `item` fields             return item         else:             inspect_response(response, self)  See also: [topics-shell-inspect-response](#topics-shell-inspect-response).   Open in browser ``\` ===============

Sometimes you just want to see how a certain response looks in a browser, you can use the <span class="title-ref">\~scrapy.utils.response.open\_in\_browser</span> function for that:

<div class="autofunction">

scrapy.utils.response.open\_in\_browser

</div>

## Logging

Logging is another useful option for getting information about your spider run. Although not as convenient, it comes with the advantage that the logs will be available in all future runs should they be necessary again:

`` `python     def parse_details(self, response, item=None):         if item:             # populate more `item` fields             return item         else:             self.logger.warning("No item received for %s", response.url)  For more information, check the [topics-logging](#topics-logging) section.  .. _debug-vscode:  Visual Studio Code ``\` ==================

To debug spiders with Visual Studio Code you can use the following `launch.json`:

``` json
{
    "version": "0.1.0",
    "configurations": [
        {
            "name": "Python: Launch Scrapy Spider",
            "type": "python",
            "request": "launch",
            "module": "scrapy",
            "args": [
                "runspider",
                "${file}"
            ],
            "console": "integratedTerminal"
        }
    ]
}
```

Also, make sure you enable "User Uncaught Exceptions", to catch exceptions in your Scrapy spider.

---

deploy.md

---

# Deploying Spiders

This section describes the different options you have for deploying your Scrapy spiders to run them on a regular basis. Running Scrapy spiders in your local machine is very convenient for the (early) development stage, but not so much when you need to execute long-running spiders or move spiders to run in production continuously. This is where the solutions for deploying Scrapy spiders come in.

Popular choices for deploying Scrapy spiders are:

  - \[Scrapyd \<deploy-scrapyd\>\](\#scrapyd-\<deploy-scrapyd\>) (open source)
  - \[Zyte Scrapy Cloud \<deploy-scrapy-cloud\>\](\#zyte-scrapy-cloud-\<deploy-scrapy-cloud\>) (cloud-based)

## Deploying to a Scrapyd Server

[Scrapyd](https://github.com/scrapy/scrapyd) is an open source application to run Scrapy spiders. It provides a server with HTTP API, capable of running and monitoring Scrapy spiders.

To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool provided by the [scrapyd-client](https://github.com/scrapy/scrapyd-client) package. Please refer to the [scrapyd-deploy documentation](https://scrapyd.readthedocs.io/en/latest/deploy.html) for more information.

Scrapyd is maintained by some of the Scrapy developers.

## Deploying to Zyte Scrapy Cloud

[Zyte Scrapy Cloud](https://www.zyte.com/scrapy-cloud/) is a hosted, cloud-based service by [Zyte](https://www.zyte.com/), the company behind Scrapy.

Zyte Scrapy Cloud removes the need to setup and monitor servers and provides a nice UI to manage spiders and review scraped items, logs and stats.

To deploy spiders to Zyte Scrapy Cloud you can use the [shub](https://shub.readthedocs.io/en/latest/) command line tool. Please refer to the [Zyte Scrapy Cloud documentation](https://docs.zyte.com/scrapy-cloud.html) for more information.

Zyte Scrapy Cloud is compatible with Scrapyd and one can switch between them as needed - the configuration is read from the `scrapy.cfg` file just like `scrapyd-deploy`.

---

developer-tools.md

---

# Using your browser's Developer Tools for scraping

Here is a general guide on how to use your browser's Developer Tools to ease the scraping process. Today almost all browsers come with built in [Developer Tools](https://en.wikipedia.org/wiki/Web_development_tools) and although we will use Firefox in this guide, the concepts are applicable to any other browser.

In this guide we'll introduce the basic tools to use from a browser's Developer Tools by scraping [quotes.toscrape.com](https://quotes.toscrape.com).

## Caveats with inspecting the live browser DOM

Since Developer Tools operate on a live browser DOM, what you'll actually see when inspecting the page source is not the original HTML, but a modified one after applying some browser clean up and executing JavaScript code. Firefox, in particular, is known for adding `<tbody>` elements to tables. Scrapy, on the other hand, does not modify the original page HTML, so you won't be able to extract any data if you use `<tbody>` in your XPath expressions.

Therefore, you should keep in mind the following things:

  - Disable JavaScript while inspecting the DOM looking for XPaths to be used in Scrapy (in the Developer Tools settings click <span class="title-ref">Disable JavaScript</span>)
  - Never use full XPath paths, use relative and clever ones based on attributes (such as `id`, `class`, `width`, etc) or any identifying features like `contains(@href, 'image')`.
  - Never include `<tbody>` elements in your XPath expressions unless you really know what you're doing

## Inspecting a website

By far the most handy feature of the Developer Tools is the <span class="title-ref">Inspector</span> feature, which allows you to inspect the underlying HTML code of any webpage. To demonstrate the Inspector, let's look at the [quotes.toscrape.com](https://quotes.toscrape.com)-site.

On the site we have a total of ten quotes from various authors with specific tags, as well as the Top Ten Tags. Let's say we want to extract all the quotes on this page, without any meta-information about authors, tags, etc.

Instead of viewing the whole source code for the page, we can simply right click on a quote and select `Inspect Element (Q)`, which opens up the <span class="title-ref">Inspector</span>. In it you should see something like this:

![Firefox's Inspector-tool](_images/inspector_01.png)

The interesting part for us is this:

`` `html     <div class="quote" itemscope="" itemtype="http://schema.org/CreativeWork">       <span class="text" itemprop="text">(...)</span>       <span>(...)</span>       <div class="tags">(...)</div>     </div>  If you hover over the first ``div`directly above the`span`tag highlighted`\` in the screenshot, you'll see that the corresponding section of the webpage gets highlighted as well. So now we have a section, but we can't find our quote text anywhere.

The advantage of the <span class="title-ref">Inspector</span> is that it automatically expands and collapses sections and tags of a webpage, which greatly improves readability. You can expand and collapse a tag by clicking on the arrow in front of it or by double clicking directly on the tag. If we expand the `span` tag with the `class= "text"` we will see the quote-text we clicked on. The <span class="title-ref">Inspector</span> lets you copy XPaths to selected elements. Let's try it out.

First open the Scrapy shell at <https://quotes.toscrape.com/> in a terminal:

`` `none     $ scrapy shell "https://quotes.toscrape.com/"  Then, back to your web browser, right-click on the ``span`tag, select`<span class="title-ref"> </span><span class="title-ref">Copy \> XPath</span>\` and paste it in the Scrapy shell like so:

`` `pycon   >>> response.xpath("/html/body/div/div[2]/div[1]/div[1]/span[1]/text()").getall()   ['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”']  Adding ``text()`at the end we are able to extract the first quote with this`<span class="title-ref"> basic selector. But this XPath is not really that clever. All it does is go down a desired path in the source code starting from </span><span class="title-ref">html</span>\`. So let's see if we can refine our XPath a bit:

If we check the <span class="title-ref">Inspector</span> again we'll see that directly beneath our expanded `div` tag we have nine identical `div` tags, each with the same attributes as our first. If we expand any of them, we'll see the same structure as with our first quote: Two `span` tags and one `div` tag. We can expand each `span` tag with the `class="text"` inside our `div` tags and see each quote:

`` `html     <div class="quote" itemscope="" itemtype="http://schema.org/CreativeWork">       <span class="text" itemprop="text">         “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”       </span>       <span>(...)</span>       <div class="tags">(...)</div>     </div>   With this knowledge we can refine our XPath: Instead of a path to follow, ``<span class="title-ref"> we'll simply select all </span><span class="title-ref">span</span><span class="title-ref"> tags with the </span><span class="title-ref">class="text"</span><span class="title-ref"> by using the \`has-class-extension</span>\_:

`` `pycon     >>> response.xpath('//span[has-class("text")]/text()').getall()     ['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',     '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',     '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',     ...]  And with one simple, cleverer XPath we are able to extract all quotes from ``<span class="title-ref"> the page. We could have constructed a loop over our first XPath to increase the number of the last </span><span class="title-ref">div</span><span class="title-ref">, but this would have been unnecessarily complex and by simply constructing an XPath with </span><span class="title-ref">has-class("text")</span>\` we were able to extract all quotes in one line.

The <span class="title-ref">Inspector</span> has a lot of other helpful features, such as searching in the source code or directly scrolling to an element you selected. Let's demonstrate a use case:

Say you want to find the `Next` button on the page. Type `Next` into the search bar on the top right of the <span class="title-ref">Inspector</span>. You should get two results. The first is a `li` tag with the `class="next"`, the second the text of an `a` tag. Right click on the `a` tag and select `Scroll into View`. If you hover over the tag, you'll see the button highlighted. From here we could easily create a \[Link Extractor \<topics-link-extractors\>\](\#link-extractor-\<topics-link-extractors\>) to follow the pagination. On a simple site such as this, there may not be the need to find an element visually but the `Scroll into View` function can be quite useful on complex sites.

Note that the search bar can also be used to search for and test CSS selectors. For example, you could search for `span.text` to find all quote texts. Instead of a full text search, this searches for exactly the `span` tag with the `class="text"` in the page.

## The Network-tool

While scraping you may come across dynamic webpages where some parts of the page are loaded dynamically through multiple requests. While this can be quite tricky, the <span class="title-ref">Network</span>-tool in the Developer Tools greatly facilitates this task. To demonstrate the Network-tool, let's take a look at the page [quotes.toscrape.com/scroll](https://quotes.toscrape.com/scroll).

The page is quite similar to the basic [quotes.toscrape.com](https://quotes.toscrape.com)-page, but instead of the above-mentioned `Next` button, the page automatically loads new quotes when you scroll to the bottom. We could go ahead and try out different XPaths directly, but instead we'll check another quite useful command from the Scrapy shell:

`` `none   $ scrapy shell "quotes.toscrape.com/scroll"   (...)   >>> view(response)  A browser window should open with the webpage but with one ``<span class="title-ref"> crucial difference: Instead of the quotes we just see a greenish bar with the word </span><span class="title-ref">Loading...</span>\`.

![Response from quotes.toscrape.com/scroll](_images/network_01.png)

The `view(response)` command let's us view the response our shell or later our spider receives from the server. Here we see that some basic template is loaded which includes the title, the login-button and the footer, but the quotes are missing. This tells us that the quotes are being loaded from a different request than `quotes.toscrape/scroll`.

If you click on the `Network` tab, you will probably only see two entries. The first thing we do is enable persistent logs by clicking on `Persist Logs`. If this option is disabled, the log is automatically cleared each time you navigate to a different page. Enabling this option is a good default, since it gives us control on when to clear the logs.

If we reload the page now, you'll see the log get populated with six new requests.

![Network tab with persistent logs and requests](_images/network_02.png)

Here we see every request that has been made when reloading the page and can inspect each request and its response. So let's find out where our quotes are coming from:

First click on the request with the name `scroll`. On the right you can now inspect the request. In `Headers` you'll find details about the request headers, such as the URL, the method, the IP-address, and so on. We'll ignore the other tabs and click directly on `Response`.

What you should see in the `Preview` pane is the rendered HTML-code, that is exactly what we saw when we called `view(response)` in the shell. Accordingly the `type` of the request in the log is `html`. The other requests have types like `css` or `js`, but what interests us is the one request called `quotes?page=1` with the type `json`.

If we click on this request, we see that the request URL is `https://quotes.toscrape.com/api/quotes?page=1` and the response is a JSON-object that contains our quotes. We can also right-click on the request and open `Open in new tab` to get a better overview.

![JSON-object returned from the quotes.toscrape API](_images/network_03.png)

With this response we can now easily parse the JSON-object and also request each page to get every quote on the site:

`` `python     import scrapy     import json       class QuoteSpider(scrapy.Spider):         name = "quote"         allowed_domains = ["quotes.toscrape.com"]         page = 1         start_urls = ["https://quotes.toscrape.com/api/quotes?page=1"]          def parse(self, response):             data = json.loads(response.text)             for quote in data["quotes"]:                 yield {"quote": quote["text"]}             if data["has_next"]:                 self.page += 1                 url = f"https://quotes.toscrape.com/api/quotes?page={self.page}"                 yield scrapy.Request(url=url, callback=self.parse)  This spider starts at the first page of the quotes-API. With each ``<span class="title-ref"> response, we parse the </span><span class="title-ref">response.text</span><span class="title-ref"> and assign it to </span><span class="title-ref">data</span><span class="title-ref">. This lets us operate on the JSON-object like on a Python dictionary. We iterate through the </span><span class="title-ref">quotes</span><span class="title-ref"> and print out the </span><span class="title-ref">quote\["text"\]</span><span class="title-ref">. If the handy </span><span class="title-ref">has\_next</span><span class="title-ref"> element is </span><span class="title-ref">true</span><span class="title-ref"> (try loading \`quotes.toscrape.com/api/quotes?page=10</span>\_ in your browser or a page-number greater than 10), we increment the `page` attribute and `yield` a new request, inserting the incremented page-number into our `url`.

<div id="requests-from-curl">

In more complex websites, it could be difficult to easily reproduce the requests, as we could need to add `headers` or `cookies` to make it work. In those cases you can export the requests in [cURL](https://curl.se/) format, by right-clicking on each of them in the network tool and using the <span class="title-ref">\~scrapy.Request.from\_curl()</span> method to generate an equivalent request:

</div>

`` `python     from scrapy import Request      request = Request.from_curl(         "curl 'https://quotes.toscrape.com/api/quotes?page=1' -H 'User-Agent: Mozil"         "la/5.0 (X11; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0' -H 'Acce"         "pt: */*' -H 'Accept-Language: ca,en-US;q=0.7,en;q=0.3' --compressed -H 'X"         "-Requested-With: XMLHttpRequest' -H 'Proxy-Authorization: Basic QFRLLTAzM"         "zEwZTAxLTk5MWUtNDFiNC1iZWRmLTJjNGI4M2ZiNDBmNDpAVEstMDMzMTBlMDEtOTkxZS00MW"         "I0LWJlZGYtMmM0YjgzZmI0MGY0' -H 'Connection: keep-alive' -H 'Referer: http"         "://quotes.toscrape.com/scroll' -H 'Cache-Control: max-age=0'"     )  Alternatively, if you want to know the arguments needed to recreate that ``<span class="title-ref"> request you can use the </span>\~scrapy.utils.curl.curl\_to\_request\_kwargs\` function to get a dictionary with the equivalent arguments:

<div class="autofunction">

scrapy.utils.curl.curl\_to\_request\_kwargs

</div>

Note that to translate a cURL command into a Scrapy request, you may use [curl2scrapy](https://michael-shub.github.io/curl2scrapy/).

As you can see, with a few inspections in the <span class="title-ref">Network</span>-tool we were able to easily replicate the dynamic requests of the scrolling functionality of the page. Crawling dynamic pages can be quite daunting and pages can be very complex, but it (mostly) boils down to identifying the correct request and replicating it in your spider.

---

djangoitem.md

---

- orphan

# DjangoItem

DjangoItem has been moved into a separate project.

It is hosted at:

> <https://github.com/scrapy-plugins/scrapy-djangoitem>

---

downloader-middleware.md

---

# Downloader Middleware

The downloader middleware is a framework of hooks into Scrapy's request/response processing. It's a light, low-level system for globally altering Scrapy's requests and responses.

## Activating a downloader middleware

To activate a downloader middleware component, add it to the `DOWNLOADER_MIDDLEWARES` setting, which is a dict whose keys are the middleware class paths and their values are the middleware orders.

Here's an example:

`` `python     DOWNLOADER_MIDDLEWARES = {         "myproject.middlewares.CustomDownloaderMiddleware": 543,     }  The :setting:`DOWNLOADER_MIDDLEWARES` setting is merged with the ``<span class="title-ref"> :setting:\`DOWNLOADER\_MIDDLEWARES\_BASE</span> setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled middlewares: the first middleware is the one closer to the engine and the last is the one closer to the downloader. In other words, the <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_request</span> method of each middleware will be invoked in increasing middleware order (100, 200, 300, ...) and the <span class="title-ref">\~scrapy.downloadermiddlewares.DownloaderMiddleware.process\_response</span> method of each middleware will be invoked in decreasing order.

To decide which order to assign to your middleware see the `DOWNLOADER_MIDDLEWARES_BASE` setting and pick a value according to where you want to insert the middleware. The order does matter because each middleware performs a different action and your middleware could depend on some previous (or subsequent) middleware being applied.

If you want to disable a built-in middleware (the ones defined in `DOWNLOADER_MIDDLEWARES_BASE` and enabled by default) you must define it in your project's `DOWNLOADER_MIDDLEWARES` setting and assign `None` as its value. For example, if you want to disable the user-agent middleware:

`` `python     DOWNLOADER_MIDDLEWARES = {         "myproject.middlewares.CustomDownloaderMiddleware": 543,         "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware": None,     }  Finally, keep in mind that some middlewares may need to be enabled through a ``\` particular setting. See each middleware documentation for more info.

## Writing your own downloader middleware

Each downloader middleware is a Python class that defines one or more of the methods defined below.

The main entry point is the `from_crawler` class method, which receives a <span class="title-ref">\~scrapy.crawler.Crawler</span> instance. The <span class="title-ref">\~scrapy.crawler.Crawler</span> object gives you access, for example, to the \[settings \<topics-settings\>\](\#settings-\<topics-settings\>).

<div class="module">

scrapy.downloadermiddlewares

</div>

<div class="DownloaderMiddleware">

<div class="note">

<div class="title">

Note

</div>

Any of the downloader middleware methods may also return a deferred.

</div>

<div class="method">

process\_request(request, spider)

This method is called for each request that goes through the download middleware.

<span class="title-ref">process\_request</span> should either: return `None`, return a <span class="title-ref">\~scrapy.Response</span> object, return a <span class="title-ref">\~scrapy.http.Request</span> object, or raise <span class="title-ref">\~scrapy.exceptions.IgnoreRequest</span>.

If it returns `None`, Scrapy will continue processing this request, executing all other middlewares until, finally, the appropriate downloader handler is called the request performed (and its response downloaded).

If it returns a <span class="title-ref">\~scrapy.http.Response</span> object, Scrapy won't bother calling *any* other <span class="title-ref">process\_request</span> or <span class="title-ref">process\_exception</span> methods, or the appropriate download function; it'll return that response. The <span class="title-ref">process\_response</span> methods of installed middleware is always called on every response.

If it returns a <span class="title-ref">\~scrapy.Request</span> object, Scrapy will stop calling <span class="title-ref">process\_request</span> methods and reschedule the returned request. Once the newly returned request is performed, the appropriate middleware chain will be called on the downloaded response.

If it raises an <span class="title-ref">\~scrapy.exceptions.IgnoreRequest</span> exception, the <span class="title-ref">process\_exception</span> methods of installed downloader middleware will be called. If none of them handle the exception, the errback function of the request (`Request.errback`) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions).

  - param request  
    the request being processed

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider for which this request is intended

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="method">

process\_response(request, response, spider)

<span class="title-ref">process\_response</span> should either: return a <span class="title-ref">\~scrapy.http.Response</span> object, return a <span class="title-ref">\~scrapy.Request</span> object or raise a <span class="title-ref">\~scrapy.exceptions.IgnoreRequest</span> exception.

If it returns a <span class="title-ref">\~scrapy.http.Response</span> (it could be the same given response, or a brand-new one), that response will continue to be processed with the <span class="title-ref">process\_response</span> of the next middleware in the chain.

If it returns a <span class="title-ref">\~scrapy.Request</span> object, the middleware chain is halted and the returned request is rescheduled to be downloaded in the future. This is the same behavior as if a request is returned from <span class="title-ref">process\_request</span>.

If it raises an <span class="title-ref">\~scrapy.exceptions.IgnoreRequest</span> exception, the errback function of the request (`Request.errback`) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions).

  - param request  
    the request that originated the response

  - type request  
    is a <span class="title-ref">\~scrapy.Request</span> object

  - param response  
    the response being processed

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> object

  - param spider  
    the spider for which this response is intended

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="method">

process\_exception(request, exception, spider)

Scrapy calls <span class="title-ref">process\_exception</span> when a download handler or a <span class="title-ref">process\_request</span> (from a downloader middleware) raises an exception (including an <span class="title-ref">\~scrapy.exceptions.IgnoreRequest</span> exception)

<span class="title-ref">process\_exception</span> should return: either `None`, a <span class="title-ref">\~scrapy.http.Response</span> object, or a <span class="title-ref">\~scrapy.Request</span> object.

If it returns `None`, Scrapy will continue processing this exception, executing any other <span class="title-ref">process\_exception</span> methods of installed middleware, until no middleware is left and the default exception handling kicks in.

If it returns a <span class="title-ref">\~scrapy.http.Response</span> object, the <span class="title-ref">process\_response</span> method chain of installed middleware is started, and Scrapy won't bother calling any other <span class="title-ref">process\_exception</span> methods of middleware.

If it returns a <span class="title-ref">\~scrapy.Request</span> object, the returned request is rescheduled to be downloaded in the future. This stops the execution of <span class="title-ref">process\_exception</span> methods of the middleware the same as returning a response would.

  - param request  
    the request that generated the exception

  - type request  
    is a <span class="title-ref">\~scrapy.Request</span> object

  - param exception  
    the raised exception

  - type exception  
    an `Exception` object

  - param spider  
    the spider for which this request is intended

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="method">

from\_crawler(cls, crawler)

If present, this classmethod is called to create a middleware instance from a <span class="title-ref">\~scrapy.crawler.Crawler</span>. It must return a new instance of the middleware. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for middleware to access them and hook its functionality into Scrapy.

  - param crawler  
    crawler that uses this middleware

  - type crawler  
    <span class="title-ref">\~scrapy.crawler.Crawler</span> object

</div>

</div>

## Built-in downloader middleware reference

This page describes all downloader middleware components that come with Scrapy. For information on how to use them and how to write your own downloader middleware, see the \[downloader middleware usage guide \<topics-downloader-middleware\>\](\#downloader-middleware-usage-guide \<topics-downloader-middleware\>).

For a list of the components enabled by default (and their orders) see the `DOWNLOADER_MIDDLEWARES_BASE` setting.

### CookiesMiddleware

<div class="module" data-synopsis="Cookies Downloader Middleware">

scrapy.downloadermiddlewares.cookies

</div>

<div class="CookiesMiddleware">

This middleware enables working with sites that require cookies, such as those that use sessions. It keeps track of cookies sent by web servers, and sends them back on subsequent requests (from that spider), just like web browsers do.

<div class="caution">

<div class="title">

Caution

</div>

When non-UTF8 encoded byte sequences are passed to a <span class="title-ref">\~scrapy.Request</span>, the `CookiesMiddleware` will log a warning. Refer to \[topics-logging-advanced-customization\](\#topics-logging-advanced-customization) to customize the logging behaviour.

</div>

<div class="caution">

<div class="title">

Caution

</div>

Cookies set via the `Cookie` header are not considered by the \[cookies-mw\](\#cookies-mw). If you need to set cookies for a request, use the <span class="title-ref">Request.cookies \<scrapy.Request\></span> parameter. This is a known current limitation that is being worked on.

</div>

</div>

The following settings can be used to configure the cookie middleware:

  - `COOKIES_ENABLED`
  - `COOKIES_DEBUG`

<div class="reqmeta">

cookiejar

</div>

#### Multiple cookie sessions per spider

There is support for keeping multiple cookie sessions per spider by using the `cookiejar` Request meta key. By default it uses a single cookie jar (session), but you can pass an identifier to use different ones.

For example:

`` `python     for i, url in enumerate(urls):         yield scrapy.Request(url, meta={"cookiejar": i}, callback=self.parse_page)  Keep in mind that the :reqmeta:`cookiejar` meta key is not "sticky". You need to keep ``\` passing it along on subsequent requests. For example:

`` `python     def parse_page(self, response):         # do some processing         return scrapy.Request(             "http://www.example.com/otherpage",             meta={"cookiejar": response.meta["cookiejar"]},             callback=self.parse_other_page,         )  .. setting:: COOKIES_ENABLED  COOKIES_ENABLED ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Default: `True`

Whether to enable the cookies middleware. If disabled, no cookies will be sent to web servers.

Notice that despite the value of `COOKIES_ENABLED` setting if `Request.``meta['dont_merge_cookies'] <dont_merge_cookies>` evaluates to `True` the request cookies will **not** be sent to the web server and received cookies in <span class="title-ref">\~scrapy.http.Response</span> will **not** be merged with the existing cookies.

For more detailed information see the `cookies` parameter in <span class="title-ref">\~scrapy.Request</span>.

<div class="setting">

COOKIES\_DEBUG

</div>

#### COOKIES\_DEBUG

Default: `False`

If enabled, Scrapy will log all cookies sent in requests (i.e. `Cookie` header) and all cookies received in responses (i.e. `Set-Cookie` header).

Here's an example of a log with `COOKIES_DEBUG` enabled:

    2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened
    2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>
            Cookie: clientlanguage_nl=en_EN
    2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>
            Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
            Set-Cookie: ip_isocode=US
            Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
    2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)
    [...]

### DefaultHeadersMiddleware

<div class="module" data-synopsis="Default Headers Downloader Middleware">

scrapy.downloadermiddlewares.defaultheaders

</div>

<div class="DefaultHeadersMiddleware">

This middleware sets all default requests headers specified in the `DEFAULT_REQUEST_HEADERS` setting.

</div>

### DownloadTimeoutMiddleware

<div class="module" data-synopsis="Download timeout middleware">

scrapy.downloadermiddlewares.downloadtimeout

</div>

<div class="DownloadTimeoutMiddleware">

This middleware sets the download timeout for requests specified in the `DOWNLOAD_TIMEOUT` setting or <span class="title-ref">download\_timeout</span> spider attribute.

</div>

\> **Note** \> You can also set download timeout per-request using `download_timeout` Request.meta key; this is supported even when DownloadTimeoutMiddleware is disabled.

### HttpAuthMiddleware

<div class="module" data-synopsis="HTTP Auth downloader middleware">

scrapy.downloadermiddlewares.httpauth

</div>

<div class="HttpAuthMiddleware">

This middleware authenticates all requests generated from certain spiders using [Basic access authentication](https://en.wikipedia.org/wiki/Basic_access_authentication) (aka. HTTP auth).

To enable HTTP authentication for a spider, set the `http_user` and `http_pass` spider attributes to the authentication data and the `http_auth_domain` spider attribute to the domain which requires this authentication (its subdomains will be also handled in the same way). You can set `http_auth_domain` to `None` to enable the authentication for all requests but you risk leaking your authentication credentials to unrelated domains.

<div class="warning">

<div class="title">

Warning

</div>

In previous Scrapy versions HttpAuthMiddleware sent the authentication data with all requests, which is a security problem if the spider makes requests to several different domains. Currently if the `http_auth_domain` attribute is not set, the middleware will use the domain of the first request, which will work for some spiders but not for others. In the future the middleware will produce an error instead.

</div>

Example:

  - \`\`\`python  
    from scrapy.spiders import CrawlSpider
    
      - class SomeIntranetSiteSpider(CrawlSpider):  
        http\_user = "someuser" http\_pass = "somepass" http\_auth\_domain = "intranet.example.com" name = "intranet.example.com"
        
        \# .. rest of the spider code omitted ...

</div>

HttpCacheMiddleware `` ` -------------------  .. module:: scrapy.downloadermiddlewares.httpcache    :synopsis: HTTP Cache downloader middleware  .. class:: HttpCacheMiddleware      This middleware provides low-level cache to all HTTP requests and responses.     It has to be combined with a cache storage backend as well as a cache policy.      Scrapy ships with the following HTTP cache storage backends:          * [httpcache-storage-fs](#httpcache-storage-fs)         * [httpcache-storage-dbm](#httpcache-storage-dbm)      You can change the HTTP cache storage backend with the :setting:`HTTPCACHE_STORAGE`     setting. Or you can also [implement your own storage backend. <httpcache-storage-custom>](#implement-your-own-storage-backend.-<httpcache-storage-custom>)      Scrapy ships with two HTTP cache policies:          * [httpcache-policy-rfc2616](#httpcache-policy-rfc2616)         * [httpcache-policy-dummy](#httpcache-policy-dummy)      You can change the HTTP cache policy with the :setting:`HTTPCACHE_POLICY`     setting. Or you can also implement your own policy.      .. reqmeta:: dont_cache      You can also avoid caching a response on every policy using :reqmeta:`dont_cache` meta key equals ``True`.  .. module:: scrapy.extensions.httpcache    :noindex:  .. _httpcache-policy-dummy:  Dummy policy (default) ~~~~~~~~~~~~~~~~~~~~~~  .. class:: DummyPolicy      This policy has no awareness of any HTTP Cache-Control directives.     Every request and its corresponding response are cached.  When the same     request is seen again, the response is returned without transferring     anything from the Internet.      The Dummy policy is useful for testing spiders faster (without having     to wait for downloads every time) and for trying your spider offline,     when an Internet connection is not available. The goal is to be able to     "replay" a spider run *exactly as it ran before*.   .. _httpcache-policy-rfc2616:  RFC2616 policy ~~~~~~~~~~~~~~  .. class:: RFC2616Policy      This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP     Cache-Control awareness, aimed at production and used in continuous     runs to avoid downloading unmodified data (to save bandwidth and speed up     crawls).      What is implemented:      * Do not attempt to store responses/requests with`no-store`cache-control directive set     * Do not serve responses from cache if`no-cache`cache-control directive is set even for fresh responses     * Compute freshness lifetime from`max-age`cache-control directive     * Compute freshness lifetime from`Expires`response header     * Compute freshness lifetime from`Last-Modified`response header (heuristic used by Firefox)     * Compute current age from`Age`response header     * Compute current age from`Date`header     * Revalidate stale responses based on`Last-Modified`response header     * Revalidate stale responses based on`ETag`response header     * Set`Date`header for any received response missing it     * Support`max-stale`cache-control directive in requests      This allows spiders to be configured with the full RFC2616 cache policy,     but avoid revalidation on a request-by-request basis, while remaining     conformant with the HTTP spec.      Example:      Add`Cache-Control: max-stale=600`to Request headers to accept responses that     have exceeded their expiration time by no more than 600 seconds.      See also: RFC2616, 14.9.3      What is missing:      *`Pragma: no-cache`support https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1     *`Vary`header support https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6     * Invalidation after updates or deletes https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10     * ... probably others ..   .. _httpcache-storage-fs:  Filesystem storage backend (default) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. class:: FilesystemCacheStorage      File system storage backend is available for the HTTP cache middleware.      Each request/response pair is stored in a different directory containing     the following files:      *`request\_body`- the plain request body      *`request\_headers`- the request headers (in raw HTTP format)      *`response\_body`- the plain response body      *`response\_headers`- the request headers (in raw HTTP format)      *`meta`- some metadata of this cache resource in Python`repr()`format (grep-friendly format)      *`pickled\_meta`- the same metadata in`meta`but pickled for more         efficient deserialization      The directory name is made from the request fingerprint (see`scrapy.utils.request.fingerprint``), and one level of subdirectories is     used to avoid creating too many files into the same directory (which is     inefficient in many file systems). An example directory could be::          /path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7  .. _httpcache-storage-dbm:  DBM storage backend ~~~~~~~~~~~~~~~~~~~  .. class:: DbmCacheStorage      A DBM_ storage backend is also available for the HTTP cache middleware.      By default, it uses the :mod:`dbm`, but you can change it with the     :setting:`HTTPCACHE_DBM_MODULE` setting.  .. _httpcache-storage-custom:  Writing your own storage backend ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You can implement a cache storage backend by creating a Python class that defines the methods described below.  .. module:: scrapy.extensions.httpcache  .. class:: CacheStorage      .. method:: open_spider(spider)        This method gets called after a spider has been opened for crawling. It handles       the :signal:`open_spider <spider_opened>` signal.        :param spider: the spider which has been opened       :type spider: `~scrapy.Spider` object      .. method:: close_spider(spider)        This method gets called after a spider has been closed. It handles       the :signal:`close_spider <spider_closed>` signal.        :param spider: the spider which has been closed       :type spider: `~scrapy.Spider` object      .. method:: retrieve_response(spider, request)        Return response if present in cache, or``None``otherwise.        :param spider: the spider which generated the request       :type spider: `~scrapy.Spider` object        :param request: the request to find cached response for       :type request: `~scrapy.Request` object      .. method:: store_response(spider, request, response)        Store the given response in the cache.        :param spider: the spider for which the response is intended       :type spider: `~scrapy.Spider` object        :param request: the corresponding request the spider generated       :type request: `~scrapy.Request` object        :param response: the response to store in the cache       :type response: `~scrapy.http.Response` object  In order to use your storage backend, set:  * :setting:`HTTPCACHE_STORAGE` to the Python import path of your custom storage class.   HTTPCache middleware settings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The `HttpCacheMiddleware` can be configured through the following settings:  .. setting:: HTTPCACHE_ENABLED  HTTPCACHE_ENABLED ^^^^^^^^^^^^^^^^^  Default:``False`Whether the HTTP cache will be enabled.  .. setting:: HTTPCACHE_EXPIRATION_SECS  HTTPCACHE_EXPIRATION_SECS ^^^^^^^^^^^^^^^^^^^^^^^^^  Default:`0`Expiration time for cached requests, in seconds.  Cached requests older than this time will be re-downloaded. If zero, cached requests will never expire.  .. setting:: HTTPCACHE_DIR  HTTPCACHE_DIR ^^^^^^^^^^^^^  Default:`'httpcache'`The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP cache will be disabled. If a relative path is given, is taken relative to the project data dir. For more info see: [topics-project-structure](#topics-project-structure).  .. setting:: HTTPCACHE_IGNORE_HTTP_CODES  HTTPCACHE_IGNORE_HTTP_CODES ^^^^^^^^^^^^^^^^^^^^^^^^^^^  Default:`\[\]`Don't cache response with these HTTP codes.  .. setting:: HTTPCACHE_IGNORE_MISSING  HTTPCACHE_IGNORE_MISSING ^^^^^^^^^^^^^^^^^^^^^^^^  Default:`False`If enabled, requests not found in the cache will be ignored instead of downloaded.  .. setting:: HTTPCACHE_IGNORE_SCHEMES  HTTPCACHE_IGNORE_SCHEMES ^^^^^^^^^^^^^^^^^^^^^^^^  Default:`\['file'\]`Don't cache responses with these URI schemes.  .. setting:: HTTPCACHE_STORAGE  HTTPCACHE_STORAGE ^^^^^^^^^^^^^^^^^  Default:`'scrapy.extensions.httpcache.FilesystemCacheStorage'`The class which implements the cache storage backend.  .. setting:: HTTPCACHE_DBM_MODULE  HTTPCACHE_DBM_MODULE ^^^^^^^^^^^^^^^^^^^^  Default:`'dbm'`The database module to use in the [DBM storage backend <httpcache-storage-dbm>](#dbm-storage-backend <httpcache-storage-dbm>). This setting is specific to the DBM backend.  .. setting:: HTTPCACHE_POLICY  HTTPCACHE_POLICY ^^^^^^^^^^^^^^^^  Default:`'scrapy.extensions.httpcache.DummyPolicy'`The class which implements the cache policy.  .. setting:: HTTPCACHE_GZIP  HTTPCACHE_GZIP ^^^^^^^^^^^^^^  Default:`False`If enabled, will compress all cached data with gzip. This setting is specific to the Filesystem backend.  .. setting:: HTTPCACHE_ALWAYS_STORE  HTTPCACHE_ALWAYS_STORE ^^^^^^^^^^^^^^^^^^^^^^  Default:`False`If enabled, will cache pages unconditionally.  A spider may wish to have all responses available in the cache, for future use with`Cache-Control: max-stale`, for instance. The DummyPolicy caches all responses but never revalidates them, and sometimes a more nuanced policy is desirable.  This setting still respects`Cache-Control: no-store`directives in responses. If you don't want that, filter`no-store`out of the Cache-Control headers in responses you feed to the cache middleware.  .. setting:: HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS  HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Default:`\[\]``List of Cache-Control directives in responses to be ignored.  Sites often set "no-store", "no-cache", "must-revalidate", etc., but get upset at the traffic a spider can generate if it actually respects those directives. This allows to selectively ignore Cache-Control directives that are known to be unimportant for the sites being crawled.  We assume that the spider will not issue Cache-Control directives in requests unless it actually needs them, so directives in requests are not filtered.  HttpCompressionMiddleware -------------------------  .. module:: scrapy.downloadermiddlewares.httpcompression    :synopsis: Http Compression Middleware  .. class:: HttpCompressionMiddleware     This middleware allows compressed (gzip, deflate) traffic to be    sent/received from web sites.     This middleware also supports decoding `brotli-compressed`_ as well as    `zstd-compressed`_ responses, provided that `brotli`_ or `zstandard`_ is    installed, respectively.        HttpCompressionMiddleware Settings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. setting:: COMPRESSION_ENABLED  COMPRESSION_ENABLED ^^^^^^^^^^^^^^^^^^^  Default:``True`Whether the Compression middleware will be enabled.   HttpProxyMiddleware -------------------  .. module:: scrapy.downloadermiddlewares.httpproxy    :synopsis: Http Proxy Middleware  .. reqmeta:: proxy  .. class:: HttpProxyMiddleware     This middleware sets the HTTP proxy to use for requests, by setting the`proxy``meta value for `~scrapy.Request` objects.     Like the Python standard library module :mod:`urllib.request`, it obeys    the following environment variables:     *``http\_proxy`*`https\_proxy`*`no\_proxy`You can also set the meta key`proxy`per-request, to a value like`<http://some_proxy_server:port>`or`<http://username:password@some_proxy_server:port>`.    Keep in mind this value will take precedence over`http\_proxy`/`https\_proxy`environment variables, and it will also ignore`no\_proxy``environment variable.  OffsiteMiddleware -----------------  .. module:: scrapy.downloadermiddlewares.offsite    :synopsis: Offsite Middleware  .. class:: OffsiteMiddleware     .. versionadded:: 2.11.2     Filters out Requests for URLs outside the domains covered by the spider.     This middleware filters out every request whose host names aren't in the    spider's `~scrapy.Spider.allowed_domains` attribute.    All subdomains of any domain in the list are also allowed.    E.g. the rule``www.example.org`will also allow`bob.www.example.org`but not`www2.example.com`nor`example.com`.     When your spider returns a request for a domain not belonging to those    covered by the spider, this middleware will log a debug message similar to    this one::        DEBUG: Filtered offsite request to 'offsite.example': <GET http://offsite.example/some/page.html>     To avoid filling the log with too much noise, it will only print one of    these messages for each new domain filtered. So, for example, if another    request for`offsite.example`is filtered, no log message will be    printed. But if a request for`other.example``is filtered, a message    will be printed (but only for the first request filtered).     If the spider doesn't define an    `~scrapy.Spider.allowed_domains` attribute, or the    attribute is empty, the offsite middleware will allow all requests.     If the request has the `~scrapy.Request.dont_filter` attribute    set, the offsite middleware will allow the request even if its domain is not    listed in allowed domains.  RedirectMiddleware ------------------  .. module:: scrapy.downloadermiddlewares.redirect    :synopsis: Redirection Middleware  .. class:: RedirectMiddleware     This middleware handles redirection of requests based on response status.  .. reqmeta:: redirect_urls  The urls which the request goes through (while being redirected) can be found in the``redirect\_urls`` `Request.meta <scrapy.Request.meta>` key.  .. reqmeta:: redirect_reasons  The reason behind each redirect in :reqmeta:`redirect_urls` can be found in the ``redirect\_reasons`` `Request.meta <scrapy.Request.meta>` key. For example: ``\[301, 302, 307, 'meta refresh'\]``.  The format of a reason depends on the middleware that handled the corresponding redirect. For example, `RedirectMiddleware` indicates the triggering response status code as an integer, while `MetaRefreshMiddleware` always uses the``'meta refresh'``string as reason.  The `RedirectMiddleware` can be configured through the following settings (see the settings documentation for more info):  * :setting:`REDIRECT_ENABLED` * :setting:`REDIRECT_MAX_TIMES`  .. reqmeta:: dont_redirect  If `Request.meta <scrapy.Request.meta>` has``dont\_redirect`key set to True, the request will be ignored by this middleware.  If you want to handle some redirect status codes in your spider, you can specify these in the`handle\_httpstatus\_list`spider attribute.  For example, if you want the redirect middleware to ignore 301 and 302 responses (and pass them through to your spider) you can do this:`\`python class MySpider(CrawlSpider): handle\_httpstatus\_list = \[301, 302\]

The `handle_httpstatus_list` key of <span class="title-ref">Request.meta </span>``<scrapy.Request.meta>` can also be used to specify which response codes to allow on a per-request basis. You can also set the meta key``handle\_httpstatus\_all`to`True`if you want to allow any response code for a request.   RedirectMiddleware settings ~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. setting:: REDIRECT_ENABLED  REDIRECT_ENABLED ^^^^^^^^^^^^^^^^  Default:`True`Whether the Redirect middleware will be enabled.  .. setting:: REDIRECT_MAX_TIMES  REDIRECT_MAX_TIMES ^^^^^^^^^^^^^^^^^^  Default:`20``The maximum number of redirections that will be followed for a single request. If maximum redirections are exceeded, the request is aborted and ignored.  MetaRefreshMiddleware ---------------------  .. class:: MetaRefreshMiddleware     This middleware handles redirection of requests based on meta-refresh html tag.  The `MetaRefreshMiddleware` can be configured through the following settings (see the settings documentation for more info):  * :setting:`METAREFRESH_ENABLED` * :setting:`METAREFRESH_IGNORE_TAGS` * :setting:`METAREFRESH_MAXDELAY`  This middleware obey :setting:`REDIRECT_MAX_TIMES` setting, :reqmeta:`dont_redirect`, :reqmeta:`redirect_urls` and :reqmeta:`redirect_reasons` request meta keys as described for `RedirectMiddleware`   MetaRefreshMiddleware settings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. setting:: METAREFRESH_ENABLED  METAREFRESH_ENABLED ^^^^^^^^^^^^^^^^^^^  Default:``True`Whether the Meta Refresh middleware will be enabled.  .. setting:: METAREFRESH_IGNORE_TAGS  METAREFRESH_IGNORE_TAGS ^^^^^^^^^^^^^^^^^^^^^^^  Default:`\[\]``Meta tags within these tags are ignored.  .. versionchanged:: 2.0    The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from``\["script", "noscript"\]`to`\[\]``.  .. versionchanged:: 2.11.2    The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from``\[\]`to`\["noscript"\]``.  .. versionchanged:: VERSION    The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from``\[\]`to`\['noscript'\]`.  .. setting:: METAREFRESH_MAXDELAY  METAREFRESH_MAXDELAY ^^^^^^^^^^^^^^^^^^^^  Default:`100``The maximum meta-refresh delay (in seconds) to follow the redirection. Some sites use meta-refresh for redirecting to a session expired page, so we restrict automatic redirection to the maximum delay.  RetryMiddleware ---------------  .. module:: scrapy.downloadermiddlewares.retry    :synopsis: Retry Middleware  .. class:: RetryMiddleware     A middleware to retry failed requests that are potentially caused by    temporary problems such as a connection timeout or HTTP 500 error.  Failed pages are collected on the scraping process and rescheduled at the end, once the spider has finished crawling all regular (non failed) pages.  The `RetryMiddleware` can be configured through the following settings (see the settings documentation for more info):  * :setting:`RETRY_ENABLED` * :setting:`RETRY_TIMES` * :setting:`RETRY_HTTP_CODES` * :setting:`RETRY_EXCEPTIONS`  .. reqmeta:: dont_retry  If `Request.meta <scrapy.Request.meta>` has``dont\_retry``key set to True, the request will be ignored by this middleware.  To retry requests from a spider callback, you can use the `get_retry_request` function:  .. autofunction:: get_retry_request  RetryMiddleware Settings ~~~~~~~~~~~~~~~~~~~~~~~~  .. setting:: RETRY_ENABLED  RETRY_ENABLED ^^^^^^^^^^^^^  Default:``True`Whether the Retry middleware will be enabled.  .. setting:: RETRY_TIMES  RETRY_TIMES ^^^^^^^^^^^  Default:`2``Maximum number of times to retry, in addition to the first download.  Maximum number of retries can also be specified per-request using :reqmeta:`max_retry_times` attribute of `Request.meta <scrapy.Request.meta>`. When initialized, the :reqmeta:`max_retry_times` meta key takes higher precedence over the :setting:`RETRY_TIMES` setting.  .. setting:: RETRY_HTTP_CODES  RETRY_HTTP_CODES ^^^^^^^^^^^^^^^^  Default:``\[500, 502, 503, 504, 522, 524, 408, 429\]``Which HTTP response codes to retry. Other errors (DNS lookup issues, connections lost, etc) are always retried.  In some cases you may want to add 400 to :setting:`RETRY_HTTP_CODES` because it is a common code used to indicate server overload. It is not included by default because HTTP specs say so.  .. setting:: RETRY_EXCEPTIONS  RETRY_EXCEPTIONS ^^^^^^^^^^^^^^^^  Default::      [         'twisted.internet.defer.TimeoutError',         'twisted.internet.error.TimeoutError',         'twisted.internet.error.DNSLookupError',         'twisted.internet.error.ConnectionRefusedError',         'twisted.internet.error.ConnectionDone',         'twisted.internet.error.ConnectError',         'twisted.internet.error.ConnectionLost',         'twisted.internet.error.TCPTimedOutError',         'twisted.web.client.ResponseFailed',         IOError,         'scrapy.core.downloader.handlers.http11.TunnelError',     ]  List of exceptions to retry.  Each list entry may be an exception type or its import path as a string.  An exception will not be caught when the exception type is not in :setting:`RETRY_EXCEPTIONS` or when the maximum number of retries for a request has been exceeded (see :setting:`RETRY_TIMES`). To learn about uncaught exception propagation, see `~scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception`.  .. setting:: RETRY_PRIORITY_ADJUST  RETRY_PRIORITY_ADJUST ^^^^^^^^^^^^^^^^^^^^^  Default:``-1``Adjust retry request priority relative to original request:  - a positive priority adjust means higher priority. - **a negative priority adjust (default) means lower priority.**   .. _topics-dlmw-robots:  RobotsTxtMiddleware -------------------  .. module:: scrapy.downloadermiddlewares.robotstxt    :synopsis: robots.txt middleware  .. class:: RobotsTxtMiddleware      This middleware filters out requests forbidden by the robots.txt exclusion     standard.      To make sure Scrapy respects robots.txt make sure the middleware is enabled     and the :setting:`ROBOTSTXT_OBEY` setting is enabled.      The :setting:`ROBOTSTXT_USER_AGENT` setting can be used to specify the     user agent string to use for matching in the robots.txt_ file. If it     is``None``, the User-Agent header you are sending with the request or the     :setting:`USER_AGENT` setting (in that order) will be used for determining     the user agent to use in the robots.txt_ file.      This middleware has to be combined with a robots.txt_ parser.      Scrapy ships with support for the following robots.txt_ parsers:      * [Protego <protego-parser>](#protego-<protego-parser>) (default)     * [RobotFileParser <python-robotfileparser>](#robotfileparser-<python-robotfileparser>)     * [Robotexclusionrulesparser <rerp-parser>](#robotexclusionrulesparser-<rerp-parser>)      You can change the robots.txt_ parser with the :setting:`ROBOTSTXT_PARSER`     setting. Or you can also [implement support for a new parser <support-for-new-robots-parser>](#implement-support-for-a-new-parser-<support-for-new-robots-parser>).  .. reqmeta:: dont_obey_robotstxt  If `Request.meta <scrapy.Request.meta>` has``dont\_obey\_robotstxt``key set to True the request will be ignored by this middleware even if :setting:`ROBOTSTXT_OBEY` is enabled.  Parsers vary in several aspects:  * Language of implementation  * Supported specification  * Support for wildcard matching  * Usage of `length based rule <https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt#order-of-precedence-for-rules>`_:   in particular for``Allow`and`Disallow``directives, where the most   specific rule based on the length of the path trumps the less specific   (shorter) rule  Performance comparison of different parsers is available at `the following link <https://github.com/scrapy/scrapy/issues/3969>`_.  .. _protego-parser:  Protego parser ~~~~~~~~~~~~~~  Based on `Protego <https://github.com/scrapy/protego>`_:  * implemented in Python  * is compliant with `Google's Robots.txt Specification   <https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt>`_  * supports wildcard matching  * uses the length based rule  Scrapy uses this parser by default.  .. _python-robotfileparser:  RobotFileParser ~~~~~~~~~~~~~~~  Based on `~urllib.robotparser.RobotFileParser`:  * is Python's built-in robots.txt_ parser  * is compliant with `Martijn Koster's 1996 draft specification   <https://www.robotstxt.org/norobots-rfc.txt>`_  * lacks support for wildcard matching  * doesn't use the length based rule  It is faster than Protego and backward-compatible with versions of Scrapy before 1.8.0.  In order to use this parser, set:  * :setting:`ROBOTSTXT_PARSER` to``scrapy.robotstxt.PythonRobotParser``.. _rerp-parser:  Robotexclusionrulesparser ~~~~~~~~~~~~~~~~~~~~~~~~~  Based on `Robotexclusionrulesparser <https://pypi.org/project/robotexclusionrulesparser/>`_:  * implemented in Python  * is compliant with `Martijn Koster's 1996 draft specification   <https://www.robotstxt.org/norobots-rfc.txt>`_  * supports wildcard matching  * doesn't use the length based rule  In order to use this parser:  * Install``Robotexclusionrulesparser`by running`pip install robotexclusionrulesparser``* Set :setting:`ROBOTSTXT_PARSER` setting to``scrapy.robotstxt.RerpRobotParser``.. _support-for-new-robots-parser:  Implementing support for a new parser ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You can implement support for a new robots.txt_ parser by subclassing the abstract base class `~scrapy.robotstxt.RobotParser` and implementing the methods described below.  .. module:: scrapy.robotstxt    :synopsis: robots.txt parser interface and implementations  .. autoclass:: RobotParser    :members:    DownloaderStats ---------------  .. module:: scrapy.downloadermiddlewares.stats    :synopsis: Downloader Stats Middleware  .. class:: DownloaderStats     Middleware that stores stats of all requests, responses and exceptions that    pass through it.     To use this middleware you must enable the :setting:`DOWNLOADER_STATS`    setting.  UserAgentMiddleware -------------------  .. module:: scrapy.downloadermiddlewares.useragent    :synopsis: User Agent Middleware  .. class:: UserAgentMiddleware     Middleware that allows spiders to override the default user agent.     In order for a spider to override the default user agent, its``user\_agent`attribute must be set.  .. _ajaxcrawl-middleware:  AjaxCrawlMiddleware -------------------  .. module:: scrapy.downloadermiddlewares.ajaxcrawl  .. class:: AjaxCrawlMiddleware     Middleware that finds 'AJAX crawlable' page variants based    on meta-fragment html tag.     > **Note** >         Scrapy finds 'AJAX crawlable' pages for URLs like`'<http://example.com/!#foo=bar>'`even without this middleware.        AjaxCrawlMiddleware is necessary when URL doesn't contain`'\!\#'`.        This is often a case for 'index' or 'main' website pages.  AjaxCrawlMiddleware Settings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. setting:: AJAXCRAWL_ENABLED  AJAXCRAWL_ENABLED ^^^^^^^^^^^^^^^^^  Default:`False`Whether the AjaxCrawlMiddleware will be enabled. You may want to enable it for [broad crawls <topics-broad-crawls>](#broad-crawls-<topics-broad-crawls>).  HttpProxyMiddleware settings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. setting:: HTTPPROXY_ENABLED .. setting:: HTTPPROXY_AUTH_ENCODING  HTTPPROXY_ENABLED ^^^^^^^^^^^^^^^^^  Default:`True``Whether or not to enable the `HttpProxyMiddleware`.  HTTPPROXY_AUTH_ENCODING ^^^^^^^^^^^^^^^^^^^^^^^  Default:``"latin-1"\`\`

The default encoding for proxy authentication on <span class="title-ref">HttpProxyMiddleware</span>.

---

dynamic-content.md

---

# Selecting dynamically-loaded content

Some webpages show the desired data when you load them in a web browser. However, when you download them using Scrapy, you cannot reach the desired data using \[selectors \<topics-selectors\>\](\#selectors-\<topics-selectors\>).

When this happens, the recommended approach is to \[find the data source \<topics-finding-data-source\>\](\#find-the-data-source-\<topics-finding-data-source\>) and extract the data from it.

If you fail to do that, and you can nonetheless access the desired data through the \[DOM \<topics-livedom\>\](\#dom-\<topics-livedom\>) from your web browser, see \[topics-javascript-rendering\](\#topics-javascript-rendering).

## Finding the data source

To extract the desired data, you must first find its source location.

If the data is in a non-text-based format, such as an image or a PDF document, use the \[network tool \<topics-network-tool\>\](\#network-tool-\<topics-network-tool\>) of your web browser to find the corresponding request, and \[reproduce it \<topics-reproducing-requests\>\](\#reproduce-it \<topics-reproducing-requests\>).

If your web browser lets you select the desired data as text, the data may be defined in embedded JavaScript code, or loaded from an external resource in a text-based format.

In that case, you can use a tool like [wgrep](https://github.com/stav/wgrep) to find the URL of that resource.

If the data turns out to come from the original URL itself, you must \[inspect the source code of the webpage \<topics-inspecting-source\>\](\#inspect-the-source-code-of-the-webpage-\<topics-inspecting-source\>) to determine where the data is located.

If the data comes from a different URL, you will need to \[reproduce the corresponding request \<topics-reproducing-requests\>\](\#reproduce-the corresponding-request-\<topics-reproducing-requests\>).

## Inspecting the source code of a webpage

Sometimes you need to inspect the source code of a webpage (not the \[DOM \<topics-livedom\>\](\#dom-\<topics-livedom\>)) to determine where some desired data is located.

Use Scrapy’s `fetch` command to download the webpage contents as seen by Scrapy:

    scrapy fetch --nolog https://example.com > response.html

If the desired data is in embedded JavaScript code within a `<script/>` element, see \[topics-parsing-javascript\](\#topics-parsing-javascript).

If you cannot find the desired data, first make sure it’s not just Scrapy: download the webpage with an HTTP client like [curl](https://curl.se/) or [wget](https://www.gnu.org/software/wget/) and see if the information can be found in the response they get.

If they get a response with the desired data, modify your Scrapy <span class="title-ref">\~scrapy.Request</span> to match that of the other HTTP client. For example, try using the same user-agent string (`USER_AGENT`) or the same <span class="title-ref">\~scrapy.Request.headers</span>.

If they also get a response without the desired data, you’ll need to take steps to make your request more similar to that of the web browser. See \[topics-reproducing-requests\](\#topics-reproducing-requests).

## Reproducing requests

Sometimes we need to reproduce a request the way our web browser performs it.

Use the \[network tool \<topics-network-tool\>\](\#network-tool-\<topics-network-tool\>) of your web browser to see how your web browser performs the desired request, and try to reproduce that request with Scrapy.

It might be enough to yield a <span class="title-ref">\~scrapy.Request</span> with the same HTTP method and URL. However, you may also need to reproduce the body, headers and form parameters (see <span class="title-ref">\~scrapy.FormRequest</span>) of that request.

As all major browsers allow to export the requests in [curl](https://curl.se/) format, Scrapy incorporates the method <span class="title-ref">\~scrapy.Request.from\_curl()</span> to generate an equivalent <span class="title-ref">\~scrapy.Request</span> from a cURL command. To get more information visit \[request from curl \<requests-from-curl\>\](\#request-from-curl-\<requests-from-curl\>) inside the network tool section.

Once you get the expected response, you can \[extract the desired data from it \<topics-handling-response-formats\>\](\#extract-the-desired-data-from it-\<topics-handling-response-formats\>).

You can reproduce any request with Scrapy. However, some times reproducing all necessary requests may not seem efficient in developer time. If that is your case, and crawling speed is not a major concern for you, you can alternatively consider \[JavaScript pre-rendering \<topics-javascript-rendering\>\](\#javascript-pre-rendering-\<topics-javascript-rendering\>).

If you get the expected response <span class="title-ref">sometimes</span>, but not always, the issue is probably not your request, but the target server. The target server might be buggy, overloaded, or \[banning \<bans\>\](\#banning-\<bans\>) some of your requests.

Note that to translate a cURL command into a Scrapy request, you may use [curl2scrapy](https://michael-shub.github.io/curl2scrapy/).

## Handling different response formats

Once you have a response with the desired data, how you extract the desired data from it depends on the type of response:

  - \- If the response is HTML, XML or JSON, use \[selectors  
    \<topics-selectors\>\](\#selectors

\----\<topics-selectors\>) as usual.

  - If the response is JSON, use <span class="title-ref">response.json()</span> to load the desired data:
    
      - \`\`\`python  
        data = response.json()
    
    If the desired data is inside HTML or XML code embedded within JSON data, you can load that HTML or XML code into a <span class="title-ref">\~scrapy.Selector</span> and then \[use it \<topics-selectors\>\](\#use-it-\<topics-selectors\>) as usual:
    
    ``` python
    selector = Selector(data["html"])
    ```

  - If the response is JavaScript, or HTML with a `<script/>` element containing the desired data, see \[topics-parsing-javascript\](\#topics-parsing-javascript).

  - If the response is CSS, use a \[regular expression \<library/re\>\](regular expression \<library/re\>.md) to extract the desired data from <span class="title-ref">response.text \<scrapy.http.TextResponse.text\></span>.

<div id="topics-parsing-images">

  - If the response is an image or another format based on images (e.g. PDF), read the response as bytes from <span class="title-ref">response.body \<scrapy.http.TextResponse.body\></span> and use an OCR solution to extract the desired data as text.
    
    For example, you can use [pytesseract](https://github.com/madmaze/pytesseract). To read a table from a PDF, [tabula-py](https://github.com/chezou/tabula-py) may be a better choice.

  - If the response is SVG, or HTML with embedded SVG containing the desired data, you may be able to extract the desired data using \[selectors \<topics-selectors\>\](\#selectors-\<topics-selectors\>), since SVG is based on XML.
    
    Otherwise, you might need to convert the SVG code into a raster image, and \[handle that raster image \<topics-parsing-images\>\](\#handle-that-raster-image-\<topics-parsing-images\>).

</div>

<div id="topics-parsing-javascript">

Parsing JavaScript code `` ` =======================  If the desired data is hardcoded in JavaScript, you first need to get the JavaScript code:  -   If the JavaScript code is in a JavaScript file, simply read     `response.text <scrapy.http.TextResponse.text>`.  -   If the JavaScript code is within a ``\<script/\>`element of an HTML page,     use [selectors <topics-selectors>](#selectors-<topics-selectors>) to extract the text within that`\<script/\>``element.  Once you have a string with the JavaScript code, you can extract the desired data from it:  -   You might be able to use a [regular expression <library/re>](regular expression <library/re>.md) to     extract the desired data in JSON format, which you can then parse with     `json.loads`.      For example, if the JavaScript code contains a separate line like``var data = {"field": "value"};`you can extract that data as follows:`\`pycon \>\>\> pattern = r"bvars+datas*=s*({.*?})s*;s\*n" \>\>\> json\_data = response.css("script::text").re\_first(pattern) \>\>\> json.loads(json\_data) {'field': 'value'}

</div>

  - [chompjs](https://github.com/Nykakin/chompjs) provides an API to parse JavaScript objects into a <span class="title-ref">dict</span>.
    
    For example, if the JavaScript code contains `var data = {field: "value", secondField: "second value"};` you can extract that data as follows:
    
    ``` pycon
    >>> import chompjs
    >>> javascript = response.css("script::text").get()
    >>> data = chompjs.parse_js_object(javascript)
    >>> data
    {'field': 'value', 'secondField': 'second value'}
    ```

  - Otherwise, use [js2xml](https://github.com/scrapinghub/js2xml) to convert the JavaScript code into an XML document that you can parse using \[selectors \<topics-selectors\>\](\#selectors-\<topics-selectors\>).
    
    For example, if the JavaScript code contains `var data = {field: "value"};` you can extract that data as follows:
    
    ``` pycon
    >>> import js2xml
    >>> import lxml.etree
    >>> from parsel import Selector
    >>> javascript = response.css("script::text").get()
    >>> xml = lxml.etree.tostring(js2xml.parse(javascript), encoding="unicode")
    >>> selector = Selector(text=xml)
    >>> selector.css('var[name="data"]').get()
    '<var name="data"><object><property name="field"><string>value</string></property></object></var>'
    ```

<div id="topics-javascript-rendering">

Pre-rendering JavaScript `` ` ========================  On webpages that fetch data from additional requests, reproducing those requests that contain the desired data is the preferred approach. The effort is often worth the result: structured, complete data with minimum parsing time and network transfer.  However, sometimes it can be really hard to reproduce certain requests. Or you may need something that no request can give you, such as a screenshot of a webpage as seen in a web browser.  In these cases use the Splash_ JavaScript-rendering service, along with `scrapy-splash`_ for seamless integration.  Splash returns as HTML the [DOM <topics-livedom>](#dom-<topics-livedom>) of a webpage, so that you can parse it with [selectors <topics-selectors>](#selectors-<topics-selectors>). It provides great flexibility through configuration_ or scripting_.  If you need something beyond what Splash offers, such as interacting with the DOM on-the-fly from Python code instead of using a previously-written script, or handling multiple web browser windows, you might need to [use a headless browser <topics-headless-browsing>](#use-a-headless-browser-<topics-headless-browsing>) instead.     .. _topics-headless-browsing:  Using a headless browser ========================  A `headless browser`_ is a special web browser that provides an API for automation. By installing the [asyncio reactor <install-asyncio>](#asyncio-reactor-<install-asyncio>), it is possible to integrate ``asyncio``-based libraries which handle headless browsers.  One such library is `playwright-python`_ (an official Python port of `playwright`_). The following is a simple snippet to illustrate its usage within a Scrapy spider:``\`python import scrapy from playwright.async\_api import async\_playwright

</div>

>   - class PlaywrightSpider(scrapy.Spider):  
>     name = "playwright" start\_urls = \["data:,"\] \# avoid using the default Scrapy downloader
>     
>       - async def parse(self, response):
>         
>           - async with async\_playwright() as pw:  
>             browser = await pw.chromium.launch() page = await browser.new\_page() await page.goto("<https://example.org>") title = await page.title() return {"title": title}

However, using [playwright-python](https://github.com/microsoft/playwright-python) directly as in the above example \`\`<span class="title-ref"> circumvents most of the Scrapy components (middlewares, dupefilter, etc). We recommend using \`scrapy-playwright</span>\_ for a better integration.

---

email.md

---

# Sending e-mail

<div class="module" data-synopsis="Email sending facility">

scrapy.mail

</div>

Although Python makes sending e-mails relatively easy via the `smtplib` library, Scrapy provides its own facility for sending e-mails which is very easy to use and it's implemented using \[Twisted non-blocking IO \<twisted:core/howto/defer-intro\>\](Twisted non-blocking IO \<twisted:core/howto/defer-intro\>.md), to avoid interfering with the non-blocking IO of the crawler. It also provides a simple API for sending attachments and it's very easy to configure, with a few \[settings \<topics-email-settings\>\](\#settings \<topics-email-settings\>).

## Quick example

There are two ways to instantiate the mail sender. You can instantiate it using the standard `__init__` method:

`` `python     from scrapy.mail import MailSender      mailer = MailSender()  Or you can instantiate it passing a `scrapy.Crawler` instance, which ``\` will respect the \[settings \<topics-email-settings\>\](\#settings-\<topics-email-settings\>):

`` `python     mailer = MailSender.from_crawler(crawler)  And here is how to use it to send an e-mail (without attachments):  .. code-block:: python      mailer.send(         to=["someone@example.com"],         subject="Some subject",         body="Some body",         cc=["another@example.com"],     ) ``\` .. skip: end

## MailSender class reference

MailSender is the preferred class to use for sending emails from Scrapy, as it uses \[Twisted non-blocking IO \<twisted:core/howto/defer-intro\>\](Twisted non-blocking IO \<twisted:core/howto/defer-intro\>.md), like the rest of the framework.

<div class="MailSender(smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, smtpport=None)">

  - param smtphost  
    the SMTP host to use for sending the emails. If omitted, the `MAIL_HOST` setting will be used.

  - type smtphost  
    str

  - param mailfrom  
    the address used to send emails (in the `From:` header). If omitted, the `MAIL_FROM` setting will be used.

  - type mailfrom  
    str

  - param smtpuser  
    the SMTP user. If omitted, the `MAIL_USER` setting will be used. If not given, no SMTP authentication will be performed.

  - type smtphost  
    str or bytes

  - param smtppass  
    the SMTP pass for authentication.

  - type smtppass  
    str or bytes

  - param smtpport  
    the SMTP port to connect to

  - type smtpport  
    int

  - param smtptls  
    enforce using SMTP STARTTLS

  - type smtptls  
    bool

  - param smtpssl  
    enforce using a secure SSL connection

  - type smtpssl  
    bool

<div class="classmethod">

from\_crawler(crawler)

Instantiate using a <span class="title-ref">scrapy.Crawler</span> instance, which will respect \[these Scrapy settings \<topics-email-settings\>\](\#these-scrapy-settings-\<topics-email-settings\>).

  - param crawler  
    the crawler

  - type settings  
    <span class="title-ref">scrapy.Crawler</span> object

</div>

<div class="method">

send(to, subject, body, cc=None, attachs=(), mimetype='text/plain', charset=None)

Send email to the given recipients.

  - param to  
    the e-mail recipients as a string or as a list of strings

  - type to  
    str or list

  - param subject  
    the subject of the e-mail

  - type subject  
    str

  - param cc  
    the e-mails to CC as a string or as a list of strings

  - type cc  
    str or list

  - param body  
    the e-mail body

  - type body  
    str

  - param attachs  
    an iterable of tuples `(attach_name, mimetype, file_object)` where `attach_name` is a string with the name that will appear on the e-mail's attachment, `mimetype` is the mimetype of the attachment and `file_object` is a readable file object with the contents of the attachment

  - type attachs  
    collections.abc.Iterable

  - param mimetype  
    the MIME type of the e-mail

  - type mimetype  
    str

  - param charset  
    the character encoding to use for the e-mail contents

  - type charset  
    str

</div>

</div>

## Mail settings

These settings define the default `__init__` method values of the <span class="title-ref">MailSender</span> class, and can be used to configure e-mail notifications in your project without writing any code (for those extensions and code that uses <span class="title-ref">MailSender</span>).

<div class="setting">

MAIL\_FROM

</div>

### MAIL\_FROM

Default: `'scrapy@localhost'`

Sender email to use (`From:` header) for sending emails.

<div class="setting">

MAIL\_HOST

</div>

### MAIL\_HOST

Default: `'localhost'`

SMTP host to use for sending emails.

<div class="setting">

MAIL\_PORT

</div>

### MAIL\_PORT

Default: `25`

SMTP port to use for sending emails.

<div class="setting">

MAIL\_USER

</div>

### MAIL\_USER

Default: `None`

User to use for SMTP authentication. If disabled no SMTP authentication will be performed.

<div class="setting">

MAIL\_PASS

</div>

### MAIL\_PASS

Default: `None`

Password to use for SMTP authentication, along with `MAIL_USER`.

<div class="setting">

MAIL\_TLS

</div>

### MAIL\_TLS

Default: `False`

Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS.

<div class="setting">

MAIL\_SSL

</div>

### MAIL\_SSL

Default: `False`

Enforce connecting using an SSL encrypted connection

---

exceptions.md

---

# Exceptions

<div class="module" data-synopsis="Scrapy exceptions">

scrapy.exceptions

</div>

## Built-in Exceptions reference

Here's a list of all exceptions included in Scrapy and their usage.

### CloseSpider

<div class="exception">

CloseSpider(reason='cancelled')

This exception can be raised from a spider callback to request the spider to be closed/stopped. Supported arguments:

  - param reason  
    the reason for closing

  - type reason  
    str

</div>

For example:

`` `python     def parse_page(self, response):         if "Bandwidth exceeded" in response.body:             raise CloseSpider("bandwidth_exceeded")  DontCloseSpider ``\` ---------------

<div class="exception">

DontCloseSpider

</div>

This exception can be raised in a `spider_idle` signal handler to prevent the spider from being closed.

### DropItem

<div class="exception">

DropItem

</div>

The exception that must be raised by item pipeline stages to stop processing an Item. For more information see \[topics-item-pipeline\](\#topics-item-pipeline).

### IgnoreRequest

<div class="exception">

IgnoreRequest

</div>

This exception can be raised by the Scheduler or any downloader middleware to indicate that the request should be ignored.

### NotConfigured

<div class="exception">

NotConfigured

</div>

This exception can be raised by some components to indicate that they will remain disabled. Those components include:

  - Extensions
  - Item pipelines
  - Downloader middlewares
  - Spider middlewares

The exception must be raised in the component's `__init__` method.

### NotSupported

<div class="exception">

NotSupported

</div>

This exception is raised to indicate an unsupported feature.

### StopDownload

<div class="versionadded">

2.2

</div>

<div class="exception">

StopDownload(fail=True)

</div>

Raised from a <span class="title-ref">\~scrapy.signals.bytes\_received</span> or <span class="title-ref">\~scrapy.signals.headers\_received</span> signal handler to indicate that no further bytes should be downloaded for a response.

The `fail` boolean parameter controls which method will handle the resulting response:

  - If `fail=True` (default), the request errback is called. The response object is available as the `response` attribute of the `StopDownload` exception, which is in turn stored as the `value` attribute of the received <span class="title-ref">\~twisted.python.failure.Failure</span> object. This means that in an errback defined as `def errback(self, failure)`, the response can be accessed though `failure.value.response`.
  - If `fail=False`, the request callback is called instead.

In both cases, the response could have its body truncated: the body contains all bytes received up until the exception is raised, including the bytes received in the signal handler that raises the exception. Also, the response object is marked with `"download_stopped"` in its <span class="title-ref">Response.flags</span> attribute.

<div class="note">

<div class="title">

Note

</div>

`fail` is a keyword-only parameter, i.e. raising `StopDownload(False)` or `StopDownload(True)` will raise a <span class="title-ref">TypeError</span>.

</div>

See the documentation for the <span class="title-ref">\~scrapy.signals.bytes\_received</span> and <span class="title-ref">\~scrapy.signals.headers\_received</span> signals and the \[topics-stop-response-download\](\#topics-stop-response-download) topic for additional information and examples.

---

exporters.md

---

# Item Exporters

<div class="module" data-synopsis="Item Exporters">

scrapy.exporters

</div>

Once you have scraped your items, you often want to persist or export those items, to use the data in some other application. That is, after all, the whole purpose of the scraping process.

For this purpose Scrapy provides a collection of Item Exporters for different output formats, such as XML, CSV or JSON.

## Using Item Exporters

If you are in a hurry, and just want to use an Item Exporter to output scraped data see the \[topics-feed-exports\](\#topics-feed-exports). Otherwise, if you want to know how Item Exporters work or need more custom functionality (not covered by the default exports), continue reading below.

In order to use an Item Exporter, you must instantiate it with its required args. Each Item Exporter requires different arguments, so check each exporter documentation to be sure, in \[topics-exporters-reference\](\#topics-exporters-reference). After you have instantiated your exporter, you have to:

1\. call the method <span class="title-ref">\~BaseItemExporter.start\_exporting</span> in order to signal the beginning of the exporting process

2\. call the <span class="title-ref">\~BaseItemExporter.export\_item</span> method for each item you want to export

3\. and finally call the <span class="title-ref">\~BaseItemExporter.finish\_exporting</span> to signal the end of the exporting process

Here you can see an \[Item Pipeline \<item-pipeline\>\](Item Pipeline \<item-pipeline\>.md) which uses multiple Item Exporters to group scraped items to different files according to the value of one of their fields:

`` `python     from itemadapter import ItemAdapter     from scrapy.exporters import XmlItemExporter       class PerYearXmlExportPipeline:         """Distribute items across multiple XML files according to their 'year' field"""          def open_spider(self, spider):             self.year_to_exporter = {}          def close_spider(self, spider):             for exporter, xml_file in self.year_to_exporter.values():                 exporter.finish_exporting()                 xml_file.close()          def _exporter_for_item(self, item):             adapter = ItemAdapter(item)             year = adapter["year"]             if year not in self.year_to_exporter:                 xml_file = open(f"{year}.xml", "wb")                 exporter = XmlItemExporter(xml_file)                 exporter.start_exporting()                 self.year_to_exporter[year] = (exporter, xml_file)             return self.year_to_exporter[year][0]          def process_item(self, item, spider):             exporter = self._exporter_for_item(item)             exporter.export_item(item)             return item   .. _topics-exporters-field-serialization:  Serialization of item fields ``\` ============================

By default, the field values are passed unmodified to the underlying serialization library, and the decision of how to serialize them is delegated to each particular serialization library.

However, you can customize how each field value is serialized *before it is passed to the serialization library*.

There are two ways to customize how a field will be serialized, which are described next.

### 1\. Declaring a serializer in the field

If you use <span class="title-ref">\~scrapy.Item</span> you can declare a serializer in the \[field metadata \<topics-items-fields\>\](\#field-metadata-\<topics-items-fields\>). The serializer must be a callable which receives a value and returns its serialized form.

Example:

`` `python     import scrapy       def serialize_price(value):         return f"$ {str(value)}"       class Product(scrapy.Item):         name = scrapy.Field()         price = scrapy.Field(serializer=serialize_price)   2. Overriding the serialize_field() method ``\` ------------------------------------------

You can also override the <span class="title-ref">\~BaseItemExporter.serialize\_field()</span> method to customize how your field value will be exported.

Make sure you call the base class <span class="title-ref">\~BaseItemExporter.serialize\_field()</span> method after your custom code.

Example:

`` `python       from scrapy.exporters import XmlItemExporter         class ProductXmlExporter(XmlItemExporter):           def serialize_field(self, field, name, value):               if name == "price":                   return f"$ {str(value)}"               return super().serialize_field(field, name, value)  .. _topics-exporters-reference:  Built-in Item Exporters reference ``\` =================================

Here is a list of the Item Exporters bundled with Scrapy. Some of them contain output examples, which assume you're exporting these two items:

`` `python     Item(name="Color TV", price="1200")     Item(name="DVD player", price="200")  BaseItemExporter ``\` ----------------

<div class="BaseItemExporter(fields_to_export=None, export_empty_fields=False, encoding=&#39;utf-8&#39;, indent=0, dont_fail=False)">

This is the (abstract) base class for all Item Exporters. It provides support for common features used by all (concrete) Item Exporters, such as defining what fields to export, whether to export empty fields, or which encoding to use.

These features can be configured through the `__init__` method arguments which populate their respective instance attributes: <span class="title-ref">fields\_to\_export</span>, <span class="title-ref">export\_empty\_fields</span>, <span class="title-ref">encoding</span>, <span class="title-ref">indent</span>.

<div class="versionadded">

2.0 The *dont\_fail* parameter.

</div>

<div class="method">

export\_item(item)

Exports the given item. This method must be implemented in subclasses.

</div>

<div class="method">

serialize\_field(field, name, value)

Return the serialized value for the given field. You can override this method (in your custom Item Exporters) if you want to control how a particular field or value will be serialized/exported.

By default, this method looks for a serializer \[declared in the item field \<topics-exporters-serializers\>\](\#declared-in-the-item

</div>

</div>

  - \------field-\<topics-exporters-serializers\>) and returns the result of applying  
    that serializer to the value. If no serializer is found, it returns the value unchanged.
    
      - param field  
        the field being serialized. If the source \[item object \<item-types\>\](\#item-object

  - \----------\<item-types\>) does not define field metadata, *field* is an empty  
    <span class="title-ref">dict</span>. :type field: <span class="title-ref">\~scrapy.Field</span> object or a <span class="title-ref">dict</span> instance
    
      - param name  
        the name of the field being serialized
    
      - type name  
        str
    
      - param value  
        the value being serialized
    
    <div class="method">
    
    start\_exporting()
    
    </div>
    
    Signal the beginning of the exporting process. Some exporters may use this to generate some required header (for example, the <span class="title-ref">XmlItemExporter</span>). You must call this method before exporting any items.
    
    <div class="method">
    
    finish\_exporting()
    
    </div>
    
    Signal the end of the exporting process. Some exporters may use this to generate some required footer (for example, the <span class="title-ref">XmlItemExporter</span>). You must always call this method after you have no more items to export.
    
    <div class="attribute">
    
    fields\_to\_export
    
    </div>
    
    Fields to export, their order and their output names.
    
    Possible values are:
    
      - `None` (all fields, default)
    
      - A list of fields:
        
            ['field1', 'field2']
    
      - A dict where keys are fields and values are output names:
        
            {'field1': 'Field 1', 'field2': 'Field 2'}
    
    <div class="attribute">
    
    export\_empty\_fields
    
    </div>
    
    Whether to include empty/unpopulated item fields in the exported data. Defaults to `False`. Some exporters (like <span class="title-ref">CsvItemExporter</span>) ignore this attribute and always export all empty fields.
    
    This option is ignored for dict items.
    
    <div class="attribute">
    
    encoding
    
    </div>
    
    The output character encoding.
    
    <div class="attribute">
    
    indent
    
    </div>
    
    Amount of spaces used to indent the output on each level. Defaults to `0`.
    
    \* `indent=None` selects the most compact representation, all items in the same line with no indentation \* `indent<=0` each item on its own line, no indentation \* `indent>0` each item on its own line, indented with the provided numeric value

### PythonItemExporter

<div class="autoclass">

PythonItemExporter

</div>

### XmlItemExporter

<div class="XmlItemExporter(file, item_element=&#39;item&#39;, root_element=&#39;items&#39;, **kwargs)">

Exports items in XML format to the specified file object.

  - param file  
    the file-like object to use for exporting the data. Its `write` method should accept `bytes` (a disk file opened in binary mode, a `io.BytesIO` object, etc)

  - param root\_element  
    The name of root element in the exported XML.

  - type root\_element  
    str

  - param item\_element  
    The name of each item element in the exported XML.

  - type item\_element  
    str

The additional keyword arguments of this `__init__` method are passed to the <span class="title-ref">BaseItemExporter</span> `__init__` method.

A typical output of this exporter would be:

``` none
<?xml version="1.0" encoding="utf-8"?>
<items>
  <item>
    <name>Color TV</name>
    <price>1200</price>
 </item>
  <item>
    <name>DVD player</name>
    <price>200</price>
 </item>
</items>
```

Unless overridden in the <span class="title-ref">serialize\_field</span> method, multi-valued fields are exported by serializing each value inside a `<value>` element. This is for convenience, as multi-valued fields are very common.

For example, the item:

``` none
Item(name=['John', 'Doe'], age='23')
```

Would be serialized as:

``` none
<?xml version="1.0" encoding="utf-8"?>
<items>
  <item>
    <name>
      <value>John</value>
      <value>Doe</value>
    </name>
    <age>23</age>
  </item>
</items>
```

</div>

### CsvItemExporter

<div class="CsvItemExporter(file, include_headers_line=True, join_multivalued=&#39;,&#39;, errors=None, **kwargs)">

Exports items in CSV format to the given file-like object. If the <span class="title-ref">fields\_to\_export</span> attribute is set, it will be used to define the CSV columns, their order and their column names. The <span class="title-ref">export\_empty\_fields</span> attribute has no effect on this exporter.

  - param file  
    the file-like object to use for exporting the data. Its `write` method should accept `bytes` (a disk file opened in binary mode, a `io.BytesIO` object, etc)

  - param include\_headers\_line  
    If enabled, makes the exporter output a header line with the field names taken from <span class="title-ref">BaseItemExporter.fields\_to\_export</span> or the first exported item fields.

  - type include\_headers\_line  
    bool

  - param join\_multivalued  
    The char (or chars) that will be used for joining multi-valued fields, if found.

  - type include\_headers\_line  
    str

  - param errors  
    The optional string that specifies how encoding and decoding errors are to be handled. For more information see <span class="title-ref">io.TextIOWrapper</span>.

  - type errors  
    str

The additional keyword arguments of this `__init__` method are passed to the <span class="title-ref">BaseItemExporter</span> `__init__` method, and the leftover arguments to the <span class="title-ref">csv.writer</span> function, so you can use any <span class="title-ref">csv.writer</span> function argument to customize this exporter.

A typical output of this exporter would be:

``` none
product,price
Color TV,1200
DVD player,200
```

</div>

### PickleItemExporter

<div class="PickleItemExporter(file, protocol=0, **kwargs)">

Exports items in pickle format to the given file-like object.

  - param file  
    the file-like object to use for exporting the data. Its `write` method should accept `bytes` (a disk file opened in binary mode, a `io.BytesIO` object, etc)

  - param protocol  
    The pickle protocol to use.

  - type protocol  
    int

For more information, see `pickle`.

The additional keyword arguments of this `__init__` method are passed to the <span class="title-ref">BaseItemExporter</span> `__init__` method.

Pickle isn't a human readable format, so no output examples are provided.

</div>

### PprintItemExporter

<div class="PprintItemExporter(file, **kwargs)">

Exports items in pretty print format to the specified file object.

  - param file  
    the file-like object to use for exporting the data. Its `write` method should accept `bytes` (a disk file opened in binary mode, a `io.BytesIO` object, etc)

The additional keyword arguments of this `__init__` method are passed to the <span class="title-ref">BaseItemExporter</span> `__init__` method.

A typical output of this exporter would be:

``` none
{'name': 'Color TV', 'price': '1200'}
{'name': 'DVD player', 'price': '200'}
```

Longer lines (when present) are pretty-formatted.

</div>

### JsonItemExporter

<div class="JsonItemExporter(file, **kwargs)">

Exports items in JSON format to the specified file-like object, writing all objects as a list of objects. The additional `__init__` method arguments are passed to the <span class="title-ref">BaseItemExporter</span> `__init__` method, and the leftover arguments to the <span class="title-ref">\~json.JSONEncoder</span> `__init__` method, so you can use any <span class="title-ref">\~json.JSONEncoder</span> `__init__` method argument to customize this exporter.

  - param file  
    the file-like object to use for exporting the data. Its `write` method should accept `bytes` (a disk file opened in binary mode, a `io.BytesIO` object, etc)

A typical output of this exporter would be:

``` none
[{"name": "Color TV", "price": "1200"},
{"name": "DVD player", "price": "200"}]
```

<div id="json-with-large-data">

<div class="warning">

<div class="title">

Warning

</div>

JSON is very simple and flexible serialization format, but it doesn't scale well for large amounts of data since incremental (aka. stream-mode) parsing is not well supported (if at all) among JSON parsers (on any language), and most of them just parse the entire object in memory. If you want the power and simplicity of JSON with a more stream-friendly format, consider using <span class="title-ref">JsonLinesItemExporter</span> instead, or splitting the output in multiple chunks.

</div>

</div>

</div>

### JsonLinesItemExporter

<div class="JsonLinesItemExporter(file, **kwargs)">

Exports items in JSON format to the specified file-like object, writing one JSON-encoded item per line. The additional `__init__` method arguments are passed to the <span class="title-ref">BaseItemExporter</span> `__init__` method, and the leftover arguments to the <span class="title-ref">\~json.JSONEncoder</span> `__init__` method, so you can use any <span class="title-ref">\~json.JSONEncoder</span> `__init__` method argument to customize this exporter.

  - param file  
    the file-like object to use for exporting the data. Its `write` method should accept `bytes` (a disk file opened in binary mode, a `io.BytesIO` object, etc)

A typical output of this exporter would be:

``` none
{"name": "Color TV", "price": "1200"}
{"name": "DVD player", "price": "200"}
```

Unlike the one produced by <span class="title-ref">JsonItemExporter</span>, the format produced by this exporter is well suited for serializing large amounts of data.

</div>

### MarshalItemExporter

<div class="autoclass">

MarshalItemExporter

</div>

---

extensions.md

---

# Extensions

The extensions framework provides a mechanism for inserting your own custom functionality into Scrapy.

Extensions are just regular classes.

## Extension settings

Extensions use the \[Scrapy settings \<topics-settings\>\](\#scrapy-settings-\<topics-settings\>) to manage their settings, just like any other Scrapy code.

It is customary for extensions to prefix their settings with their own name, to avoid collision with existing (and future) extensions. For example, a hypothetical extension to handle [Google Sitemaps](https://en.wikipedia.org/wiki/Sitemaps) would use settings like `GOOGLESITEMAP_ENABLED`, `GOOGLESITEMAP_DEPTH`, and so on.

## Loading & activating extensions

Extensions are loaded and activated at startup by instantiating a single instance of the extension class per spider being run. All the extension initialization code must be performed in the class `__init__` method.

To make an extension available, add it to the `EXTENSIONS` setting in your Scrapy settings. In `EXTENSIONS`, each extension is represented by a string: the full Python path to the extension's class name. For example:

`` `python     EXTENSIONS = {         "scrapy.extensions.corestats.CoreStats": 500,         "scrapy.extensions.telnet.TelnetConsole": 500,     }   As you can see, the :setting:`EXTENSIONS` setting is a dict where the keys are ``<span class="title-ref"> the extension paths, and their values are the orders, which define the extension \*loading\* order. The :setting:\`EXTENSIONS</span> setting is merged with the `EXTENSIONS_BASE` setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled extensions.

As extensions typically do not depend on each other, their loading order is irrelevant in most cases. This is why the `EXTENSIONS_BASE` setting defines all extensions with the same order (`0`). However, this feature can be exploited if you need to add an extension which depends on other extensions already loaded.

## Available, enabled and disabled extensions

Not all available extensions will be enabled. Some of them usually depend on a particular setting. For example, the HTTP Cache extension is available by default but disabled unless the `HTTPCACHE_ENABLED` setting is set.

## Disabling an extension

In order to disable an extension that comes enabled by default (i.e. those included in the `EXTENSIONS_BASE` setting) you must set its order to `None`. For example:

`` `python     EXTENSIONS = {         "scrapy.extensions.corestats.CoreStats": None,     }  Writing your own extension ``\` ==========================

Each extension is a Python class. The main entry point for a Scrapy extension (this also includes middlewares and pipelines) is the `from_crawler` class method which receives a `Crawler` instance. Through the Crawler object you can access settings, signals, stats, and also control the crawling behaviour.

Typically, extensions connect to \[signals \<topics-signals\>\](\#signals-\<topics-signals\>) and perform tasks triggered by them.

Finally, if the `from_crawler` method raises the <span class="title-ref">\~scrapy.exceptions.NotConfigured</span> exception, the extension will be disabled. Otherwise, the extension will be enabled.

### Sample extension

Here we will implement a simple extension to illustrate the concepts described in the previous section. This extension will log a message every time:

  - a spider is opened
  - a spider is closed
  - a specific number of items are scraped

The extension will be enabled through the `MYEXT_ENABLED` setting and the number of items will be specified through the `MYEXT_ITEMCOUNT` setting.

Here is the code of such extension:

`` `python     import logging     from scrapy import signals     from scrapy.exceptions import NotConfigured      logger = logging.getLogger(__name__)       class SpiderOpenCloseLogging:         def __init__(self, item_count):             self.item_count = item_count             self.items_scraped = 0          @classmethod         def from_crawler(cls, crawler):             # first check if the extension should be enabled and raise             # NotConfigured otherwise             if not crawler.settings.getbool("MYEXT_ENABLED"):                 raise NotConfigured              # get the number of items from settings             item_count = crawler.settings.getint("MYEXT_ITEMCOUNT", 1000)              # instantiate the extension object             ext = cls(item_count)              # connect the extension object to signals             crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)             crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)             crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)              # return the extension object             return ext          def spider_opened(self, spider):             logger.info("opened spider %s", spider.name)          def spider_closed(self, spider):             logger.info("closed spider %s", spider.name)          def item_scraped(self, item, spider):             self.items_scraped += 1             if self.items_scraped % self.item_count == 0:                 logger.info("scraped %d items", self.items_scraped)   .. _topics-extensions-ref:  Built-in extensions reference ``\` =============================

### General purpose extensions

#### Log Stats extension

<div class="module" data-synopsis="Basic stats logging">

scrapy.extensions.logstats

</div>

<div class="LogStats">

Log basic stats like crawled pages and scraped items.

</div>

#### Core Stats extension

<div class="module" data-synopsis="Core stats collection">

scrapy.extensions.corestats

</div>

<div class="CoreStats">

Enable the collection of core statistics, provided the stats collection is enabled (see \[topics-stats\](\#topics-stats)).

</div>

#### Telnet console extension

<div class="module" data-synopsis="Telnet console">

scrapy.extensions.telnet

</div>

<div class="TelnetConsole">

Provides a telnet console for getting into a Python interpreter inside the currently running Scrapy process, which can be very useful for debugging.

</div>

The telnet console must be enabled by the `TELNETCONSOLE_ENABLED` setting, and the server will listen in the port specified in `TELNETCONSOLE_PORT`.

#### Memory usage extension

<div class="module" data-synopsis="Memory usage extension">

scrapy.extensions.memusage

</div>

<div class="MemoryUsage">

<div class="note">

<div class="title">

Note

</div>

This extension does not work in Windows.

</div>

</div>

Monitors the memory used by the Scrapy process that runs the spider and:

1.  sends a notification e-mail when it exceeds a certain value
2.  closes the spider when it exceeds a certain value

The notification e-mails can be triggered when a certain warning value is reached (`MEMUSAGE_WARNING_MB`) and when the maximum value is reached (`MEMUSAGE_LIMIT_MB`) which will also cause the spider to be closed and the Scrapy process to be terminated.

This extension is enabled by the `MEMUSAGE_ENABLED` setting and can be configured with the following settings:

  - `MEMUSAGE_LIMIT_MB`
  - `MEMUSAGE_WARNING_MB`
  - `MEMUSAGE_NOTIFY_MAIL`
  - `MEMUSAGE_CHECK_INTERVAL_SECONDS`

#### Memory debugger extension

<div class="module" data-synopsis="Memory debugger extension">

scrapy.extensions.memdebug

</div>

<div class="MemoryDebugger">

An extension for debugging memory usage. It collects information about:

</div>

  - objects uncollected by the Python garbage collector
  - objects left alive that shouldn't. For more info, see \[topics-leaks-trackrefs\](\#topics-leaks-trackrefs)

To enable this extension, turn on the `MEMDEBUG_ENABLED` setting. The info will be stored in the stats.

#### Spider state extension

<div class="module" data-synopsis="Spider state extension">

scrapy.extensions.spiderstate

</div>

<div class="SpiderState">

Manages spider state data by loading it before a crawl and saving it after.

</div>

Give a value to the `JOBDIR` setting to enable this extension. When enabled, this extension manages the <span class="title-ref">\~scrapy.Spider.state</span> attribute of your <span class="title-ref">\~scrapy.Spider</span> instance:

  - When your spider closes (`spider_closed`), the contents of its <span class="title-ref">\~scrapy.Spider.state</span> attribute are serialized into a file named `spider.state` in the `JOBDIR` folder.
  - When your spider opens (`spider_opened`), if a previously-generated `spider.state` file exists in the `JOBDIR` folder, it is loaded into the <span class="title-ref">\~scrapy.Spider.state</span> attribute.

For an example, see \[topics-keeping-persistent-state-between-batches\](\#topics-keeping-persistent-state-between-batches).

#### Close spider extension

<div class="module" data-synopsis="Close spider extension">

scrapy.extensions.closespider

</div>

<div class="CloseSpider">

Closes a spider automatically when some conditions are met, using a specific closing reason for each condition.

</div>

The conditions for closing a spider can be configured through the following settings:

  - `CLOSESPIDER_TIMEOUT`
  - `CLOSESPIDER_TIMEOUT_NO_ITEM`
  - `CLOSESPIDER_ITEMCOUNT`
  - `CLOSESPIDER_PAGECOUNT`
  - `CLOSESPIDER_ERRORCOUNT`

\> **Note** \> When a certain closing condition is met, requests which are currently in the downloader queue (up to `CONCURRENT_REQUESTS` requests) are still processed.

<div class="setting">

CLOSESPIDER\_TIMEOUT

</div>

##### CLOSESPIDER\_TIMEOUT

Default: `0`

An integer which specifies a number of seconds. If the spider remains open for more than that number of second, it will be automatically closed with the reason `closespider_timeout`. If zero (or non set), spiders won't be closed by timeout.

<div class="setting">

CLOSESPIDER\_TIMEOUT\_NO\_ITEM

</div>

##### CLOSESPIDER\_TIMEOUT\_NO\_ITEM

Default: `0`

An integer which specifies a number of seconds. If the spider has not produced any items in the last number of seconds, it will be closed with the reason `closespider_timeout_no_item`. If zero (or non set), spiders won't be closed regardless if it hasn't produced any items.

<div class="setting">

CLOSESPIDER\_ITEMCOUNT

</div>

##### CLOSESPIDER\_ITEMCOUNT

Default: `0`

An integer which specifies a number of items. If the spider scrapes more than that amount and those items are passed by the item pipeline, the spider will be closed with the reason `closespider_itemcount`. If zero (or non set), spiders won't be closed by number of passed items.

<div class="setting">

CLOSESPIDER\_PAGECOUNT

</div>

##### CLOSESPIDER\_PAGECOUNT

Default: `0`

An integer which specifies the maximum number of responses to crawl. If the spider crawls more than that, the spider will be closed with the reason `closespider_pagecount`. If zero (or non set), spiders won't be closed by number of crawled responses.

<div class="setting">

CLOSESPIDER\_PAGECOUNT\_NO\_ITEM

</div>

##### CLOSESPIDER\_PAGECOUNT\_NO\_ITEM

Default: `0`

An integer which specifies the maximum number of consecutive responses to crawl without items scraped. If the spider crawls more consecutive responses than that and no items are scraped in the meantime, the spider will be closed with the reason `closespider_pagecount_no_item`. If zero (or not set), spiders won't be closed by number of crawled responses with no items.

<div class="setting">

CLOSESPIDER\_ERRORCOUNT

</div>

##### CLOSESPIDER\_ERRORCOUNT

Default: `0`

An integer which specifies the maximum number of errors to receive before closing the spider. If the spider generates more than that number of errors, it will be closed with the reason `closespider_errorcount`. If zero (or non set), spiders won't be closed by number of errors.

#### StatsMailer extension

<div class="module" data-synopsis="StatsMailer extension">

scrapy.extensions.statsmailer

</div>

<div class="StatsMailer">

This simple extension can be used to send a notification e-mail every time a domain has finished scraping, including the Scrapy stats collected. The email will be sent to all recipients specified in the `STATSMAILER_RCPTS` setting.

</div>

Emails can be sent using the <span class="title-ref">\~scrapy.mail.MailSender</span> class. To see a full list of parameters, including examples on how to instantiate <span class="title-ref">\~scrapy.mail.MailSender</span> and use mail settings, see \[topics-email\](\#topics-email).

<div class="module" data-synopsis="Extensions for debugging Scrapy">

scrapy.extensions.debug

</div>

<div class="module" data-synopsis="Periodic stats logging">

scrapy.extensions.periodic\_log

</div>

#### Periodic log extension

<div class="PeriodicLog">

This extension periodically logs rich stat data as a JSON object:

    2023-08-04 02:30:57 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 162 pages/min), scraped 925 items (at 161 items/min)
    2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {
        "delta": {
            "downloader/request_bytes": 55582,
            "downloader/request_count": 162,
            "downloader/request_method_count/GET": 162,
            "downloader/response_bytes": 618133,
            "downloader/response_count": 162,
            "downloader/response_status_count/200": 162,
            "item_scraped_count": 161
        },
        "stats": {
            "downloader/request_bytes": 338243,
            "downloader/request_count": 992,
            "downloader/request_method_count/GET": 992,
            "downloader/response_bytes": 3836736,
            "downloader/response_count": 976,
            "downloader/response_status_count/200": 976,
            "item_scraped_count": 925,
            "log_count/INFO": 21,
            "log_count/WARNING": 1,
            "scheduler/dequeued": 992,
            "scheduler/dequeued/memory": 992,
            "scheduler/enqueued": 1050,
            "scheduler/enqueued/memory": 1050
        },
        "time": {
            "elapsed": 360.008903,
            "log_interval": 60.0,
            "log_interval_real": 60.006694,
            "start_time": "2023-08-03 23:24:57",
            "utcnow": "2023-08-03 23:30:57"
        }
    }

</div>

This extension logs the following configurable sections:

  - `"delta"` shows how some numeric stats have changed since the last stats log message.
    
    The `PERIODIC_LOG_DELTA` setting determines the target stats. They must have `int` or `float` values.

  - `"stats"` shows the current value of some stats.
    
    The `PERIODIC_LOG_STATS` setting determines the target stats.

  - `"time"` shows detailed timing data.
    
    The `PERIODIC_LOG_TIMING_ENABLED` setting determines whether or not to show this section.

This extension logs data at the start, then on a fixed time interval configurable through the `LOGSTATS_INTERVAL` setting, and finally right before the crawl ends.

Example extension configuration:

`` `python     custom_settings = {         "LOG_LEVEL": "INFO",         "PERIODIC_LOG_STATS": {             "include": ["downloader/", "scheduler/", "log_count/", "item_scraped_count/"],         },         "PERIODIC_LOG_DELTA": {"include": ["downloader/"]},         "PERIODIC_LOG_TIMING_ENABLED": True,         "EXTENSIONS": {             "scrapy.extensions.periodic_log.PeriodicLog": 0,         },     }  .. setting:: PERIODIC_LOG_DELTA  PERIODIC_LOG_DELTA ``\` """"""""""""""""""

Default: `None`

  - `"PERIODIC_LOG_DELTA": True` - show deltas for all `int` and `float` stat values.
  - `"PERIODIC_LOG_DELTA": {"include": ["downloader/", "scheduler/"]}` - show deltas for stats with names containing any configured substring.
  - `"PERIODIC_LOG_DELTA": {"exclude": ["downloader/"]}` - show deltas for all stats with names not containing any configured substring.

<div class="setting">

PERIODIC\_LOG\_STATS

</div>

##### PERIODIC\_LOG\_STATS

Default: `None`

  - `"PERIODIC_LOG_STATS": True` - show the current value of all stats.
  - `"PERIODIC_LOG_STATS": {"include": ["downloader/", "scheduler/"]}` - show current values for stats with names containing any configured substring.
  - `"PERIODIC_LOG_STATS": {"exclude": ["downloader/"]}` - show current values for all stats with names not containing any configured substring.

<div class="setting">

PERIODIC\_LOG\_TIMING\_ENABLED

</div>

##### PERIODIC\_LOG\_TIMING\_ENABLED

Default: `False`

`True` enables logging of timing data (i.e. the `"time"` section).

### Debugging extensions

#### Stack trace dump extension

<div class="StackTraceDump">

Dumps information about the running process when a [SIGQUIT](https://en.wikipedia.org/wiki/SIGQUIT) or [SIGUSR2](https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2) signal is received. The information dumped is the following:

</div>

1.  engine status (using `scrapy.utils.engine.get_engine_status()`)
2.  live references (see \[topics-leaks-trackrefs\](\#topics-leaks-trackrefs))
3.  stack trace of all threads

After the stack trace and engine status is dumped, the Scrapy process continues running normally.

This extension only works on POSIX-compliant platforms (i.e. not Windows), because the [SIGQUIT](https://en.wikipedia.org/wiki/SIGQUIT) and [SIGUSR2](https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2) signals are not available on Windows.

There are at least two ways to send Scrapy the [SIGQUIT](https://en.wikipedia.org/wiki/SIGQUIT) signal:

1.  By pressing Ctrl-while a Scrapy process is running (Linux only?)

2.  By running this command (assuming `<pid>` is the process id of the Scrapy process):
    
        kill -QUIT <pid>

#### Debugger extension

<div class="Debugger">

Invokes a \[Python debugger \<library/pdb\>\](Python debugger \<library/pdb\>.md) inside a running Scrapy process when a [SIGUSR2](https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2) signal is received. After the debugger is exited, the Scrapy process continues running normally.

</div>

This extension only works on POSIX-compliant platforms (i.e. not Windows).

---

feed-exports.md

---

# Feed exports

One of the most frequently required features when implementing scrapers is being able to store the scraped data properly and, quite often, that means generating an "export file" with the scraped data (commonly called "export feed") to be consumed by other systems.

Scrapy provides this functionality out of the box with the Feed Exports, which allows you to generate feeds with the scraped items, using multiple serialization formats and storage backends.

This page provides detailed documentation for all feed export features. If you are looking for a step-by-step guide, check out [Zyte’s export guides](https://docs.zyte.com/web-scraping/guides/export/index.html#exporting-scraped-data).

## Serialization formats

For serializing the scraped data, the feed exports use the \[Item exporters \<topics-exporters\>\](\#item-exporters \<topics-exporters\>). These formats are supported out of the box:

  - \[topics-feed-format-json\](\#topics-feed-format-json)
  - \[topics-feed-format-jsonlines\](\#topics-feed-format-jsonlines)
  - \[topics-feed-format-csv\](\#topics-feed-format-csv)
  - \[topics-feed-format-xml\](\#topics-feed-format-xml)

But you can also extend the supported format through the `FEED_EXPORTERS` setting.

### JSON

  - Value for the `format` key in the `FEEDS` setting: `json`
  - Exporter used: <span class="title-ref">\~scrapy.exporters.JsonItemExporter</span>
  - See \[this warning \<json-with-large-data\>\](\#this-warning-\<json-with-large-data\>) if you're using JSON with large feeds.

### JSON lines

  - Value for the `format` key in the `FEEDS` setting: `jsonlines`
  - Exporter used: <span class="title-ref">\~scrapy.exporters.JsonLinesItemExporter</span>

### CSV

  - Value for the `format` key in the `FEEDS` setting: `csv`
  - Exporter used: <span class="title-ref">\~scrapy.exporters.CsvItemExporter</span>
  - To specify columns to export, their order and their column names, use `FEED_EXPORT_FIELDS`. Other feed exporters can also use this option, but it is important for CSV because unlike many other export formats CSV uses a fixed header.

### XML

  - Value for the `format` key in the `FEEDS` setting: `xml`
  - Exporter used: <span class="title-ref">\~scrapy.exporters.XmlItemExporter</span>

### Pickle

  - Value for the `format` key in the `FEEDS` setting: `pickle`
  - Exporter used: <span class="title-ref">\~scrapy.exporters.PickleItemExporter</span>

### Marshal

  - Value for the `format` key in the `FEEDS` setting: `marshal`
  - Exporter used: <span class="title-ref">\~scrapy.exporters.MarshalItemExporter</span>

## Storages

When using the feed exports you define where to store the feed using one or multiple [URIs](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier) (through the `FEEDS` setting). The feed exports supports multiple storage backend types which are defined by the URI scheme.

The storages backends supported out of the box are:

  - \[topics-feed-storage-fs\](\#topics-feed-storage-fs)
  - \[topics-feed-storage-ftp\](\#topics-feed-storage-ftp)
  - \[topics-feed-storage-s3\](\#topics-feed-storage-s3) (requires [boto3]())
  - \[topics-feed-storage-gcs\](\#topics-feed-storage-gcs) (requires [google-cloud-storage](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python))
  - \[topics-feed-storage-stdout\](\#topics-feed-storage-stdout)

Some storage backends may be unavailable if the required external libraries are not available. For example, the S3 backend is only available if the [boto3]() library is installed.

## Storage URI parameters

The storage URI can also contain parameters that get replaced when the feed is being created. These parameters are:

  - `%(time)s` - gets replaced by a timestamp when the feed is being created
  - `%(name)s` - gets replaced by the spider name

Any other named parameter gets replaced by the spider attribute of the same name. For example, `%(site_id)s` would get replaced by the `spider.site_id` attribute the moment the feed is being created.

Here are some examples to illustrate:

  - Store in FTP using one directory per spider:
      - `ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json`
  - Store in S3 using one directory per spider:
      - `s3://mybucket/scraping/feeds/%(name)s/%(time)s.json`

<div class="note">

<div class="title">

Note

</div>

\[Spider arguments \<spiderargs\>\](\#spider-arguments-\<spiderargs\>) become spider attributes, hence they can also be used as storage URI parameters.

</div>

## Storage backends

### Local filesystem

The feeds are stored in the local filesystem.

  - URI scheme: `file`
  - Example URI: `file:///tmp/export.csv`
  - Required external libraries: none

Note that for the local filesystem storage (only) you can omit the scheme if you specify an absolute path like `/tmp/export.csv` (Unix systems only). Alternatively you can also use a <span class="title-ref">pathlib.Path</span> object.

### FTP

The feeds are stored in a FTP server.

  - URI scheme: `ftp`
  - Example URI: `ftp://user:pass@ftp.example.com/path/to/export.csv`
  - Required external libraries: none

FTP supports two different connection modes: [active or passive](https://stackoverflow.com/a/1699163). Scrapy uses the passive connection mode by default. To use the active connection mode instead, set the `FEED_STORAGE_FTP_ACTIVE` setting to `True`.

The default value for the `overwrite` key in the `FEEDS` for this storage backend is: `True`.

<div class="caution">

<div class="title">

Caution

</div>

The value `True` in `overwrite` will cause you to lose the previous version of your data.

</div>

This storage backend uses \[delayed file delivery \<delayed-file-delivery\>\](\#delayed-file-delivery-\<delayed-file-delivery\>).

### S3

The feeds are stored on [Amazon S3]().

  - URI scheme: `s3`
  - Example URIs:
      - `s3://mybucket/path/to/export.csv`
      - `s3://aws_key:aws_secret@mybucket/path/to/export.csv`
  - Required external libraries: [boto3]() \>= 1.20.0

The AWS credentials can be passed as user/password in the URI, or they can be passed through the following settings:

  - `AWS_ACCESS_KEY_ID`
  - `AWS_SECRET_ACCESS_KEY`
  - `AWS_SESSION_TOKEN` (only needed for [temporary security credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html))

You can also define a custom ACL, custom endpoint, and region name for exported feeds using these settings:

  - `FEED_STORAGE_S3_ACL`
  - `AWS_ENDPOINT_URL`
  - `AWS_REGION_NAME`

The default value for the `overwrite` key in the `FEEDS` for this storage backend is: `True`.

<div class="caution">

<div class="title">

Caution

</div>

The value `True` in `overwrite` will cause you to lose the previous version of your data.

</div>

This storage backend uses \[delayed file delivery \<delayed-file-delivery\>\](\#delayed-file-delivery-\<delayed-file-delivery\>).

### Google Cloud Storage (GCS)

<div class="versionadded">

2.3

</div>

The feeds are stored on [Google Cloud Storage]().

  - URI scheme: `gs`
  - Example URIs:
      - `gs://mybucket/path/to/export.csv`
  - Required external libraries: [google-cloud-storage](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python).

For more information about authentication, please refer to [Google Cloud documentation](https://cloud.google.com/docs/authentication).

You can set a *Project ID* and *Access Control List (ACL)* through the following settings:

  - `FEED_STORAGE_GCS_ACL`
  - `GCS_PROJECT_ID`

The default value for the `overwrite` key in the `FEEDS` for this storage backend is: `True`.

<div class="caution">

<div class="title">

Caution

</div>

The value `True` in `overwrite` will cause you to lose the previous version of your data.

</div>

This storage backend uses \[delayed file delivery \<delayed-file-delivery\>\](\#delayed-file-delivery-\<delayed-file-delivery\>).

### Standard output

The feeds are written to the standard output of the Scrapy process.

  - URI scheme: `stdout`
  - Example URI: `stdout:`
  - Required external libraries: none

### Delayed file delivery

As indicated above, some of the described storage backends use delayed file delivery.

These storage backends do not upload items to the feed URI as those items are scraped. Instead, Scrapy writes items into a temporary local file, and only once all the file contents have been written (i.e. at the end of the crawl) is that file uploaded to the feed URI.

If you want item delivery to start earlier when using one of these storage backends, use `FEED_EXPORT_BATCH_ITEM_COUNT` to split the output items in multiple files, with the specified maximum item count per file. That way, as soon as a file reaches the maximum item count, that file is delivered to the feed URI, allowing item delivery to start way before the end of the crawl.

## Item filtering

<div class="versionadded">

2.6.0

</div>

You can filter items that you want to allow for a particular feed by using the `item_classes` option in \[feeds options \<feed-options\>\](\#feeds-options-\<feed-options\>). Only items of the specified types will be added to the feed.

The `item_classes` option is implemented by the <span class="title-ref">\~scrapy.extensions.feedexport.ItemFilter</span> class, which is the default value of the `item_filter` \[feed option \<feed-options\>\](\#feed-option-\<feed-options\>).

You can create your own custom filtering class by implementing <span class="title-ref">\~scrapy.extensions.feedexport.ItemFilter</span>'s method `accepts` and taking `feed_options` as an argument.

For instance:

`` `python     class MyCustomFilter:         def __init__(self, feed_options):             self.feed_options = feed_options          def accepts(self, item):             if "field1" in item and item["field1"] == "expected_data":                 return True             return False   You can assign your custom filtering class to the ``item\_filter`[option of a feed <feed-options>](#option-of-a-feed-<feed-options>).`<span class="title-ref"> See :setting:\`FEEDS</span> for examples.

### ItemFilter

<div class="autoclass" data-members="">

scrapy.extensions.feedexport.ItemFilter

</div>

## Post-Processing

<div class="versionadded">

2.6.0

</div>

Scrapy provides an option to activate plugins to post-process feeds before they are exported to feed storages. In addition to using \[builtin plugins \<builtin-plugins\>\](\#builtin-plugins-\<builtin-plugins\>), you can create your own \[plugins \<custom-plugins\>\](\#plugins-\<custom-plugins\>).

These plugins can be activated through the `postprocessing` option of a feed. The option must be passed a list of post-processing plugins in the order you want the feed to be processed. These plugins can be declared either as an import string or with the imported class of the plugin. Parameters to plugins can be passed through the feed options. See \[feed options \<feed-options\>\](\#feed-options-\<feed-options\>) for examples.

### Built-in Plugins

<div class="autoclass">

scrapy.extensions.postprocessing.GzipPlugin

</div>

<div class="autoclass">

scrapy.extensions.postprocessing.LZMAPlugin

</div>

<div class="autoclass">

scrapy.extensions.postprocessing.Bz2Plugin

</div>

### Custom Plugins

Each plugin is a class that must implement the following methods:

<div class="method">

\_\_init\_\_(self, file, feed\_options)

Initialize the plugin.

  - param file  
    file-like object having at least the <span class="title-ref">write</span>, <span class="title-ref">tell</span> and <span class="title-ref">close</span> methods implemented

  - param feed\_options  
    feed-specific \[options \<feed-options\>\](\#options-\<feed-options\>)

  - type feed\_options  
    <span class="title-ref">dict</span>

</div>

<div class="method">

write(self, data)

Process and write <span class="title-ref">data</span> (<span class="title-ref">bytes</span> or <span class="title-ref">memoryview</span>) into the plugin's target file. It must return number of bytes written.

</div>

<div class="method">

close(self)

Clean up the plugin.

For example, you might want to close a file wrapper that you might have used to compress data written into the file received in the `__init__` method.

<div class="warning">

<div class="title">

Warning

</div>

Do not close the file from the `__init__` method.

</div>

</div>

To pass a parameter to your plugin, use \[feed options \<feed-options\>\](\#feed-options-\<feed-options\>). You can then access those parameters from the `__init__` method of your plugin.

## Settings

These are the settings used for configuring the feed exports:

  - `FEEDS` (mandatory)
  - `FEED_EXPORT_ENCODING`
  - `FEED_STORE_EMPTY`
  - `FEED_EXPORT_FIELDS`
  - `FEED_EXPORT_INDENT`
  - `FEED_STORAGES`
  - `FEED_STORAGE_FTP_ACTIVE`
  - `FEED_STORAGE_S3_ACL`
  - `FEED_EXPORTERS`
  - `FEED_EXPORT_BATCH_ITEM_COUNT`

<div class="currentmodule">

scrapy.extensions.feedexport

</div>

<div class="setting">

FEEDS

</div>

### FEEDS

<div class="versionadded">

2.1

</div>

Default: `{}`

A dictionary in which every key is a feed URI (or a <span class="title-ref">pathlib.Path</span> object) and each value is a nested dictionary containing configuration parameters for the specific feed.

This setting is required for enabling the feed export feature.

See \[topics-feed-storage-backends\](\#topics-feed-storage-backends) for supported URI schemes.

For instance:

    {
        'items.json': {
            'format': 'json',
            'encoding': 'utf8',
            'store_empty': False,
            'item_classes': [MyItemClass1, 'myproject.items.MyItemClass2'],
            'fields': None,
            'indent': 4,
            'item_export_kwargs': {
               'export_empty_fields': True,
            },
        },
        '/home/user/documents/items.xml': {
            'format': 'xml',
            'fields': ['name', 'price'],
            'item_filter': MyCustomFilter1,
            'encoding': 'latin1',
            'indent': 8,
        },
        pathlib.Path('items.csv.gz'): {
            'format': 'csv',
            'fields': ['price', 'name'],
            'item_filter': 'myproject.filters.MyCustomFilter2',
            'postprocessing': [MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],
            'gzip_compresslevel': 5,
        },
    }

<div id="feed-options">

The following is a list of the accepted keys and the setting that is used as a fallback value if that key is not provided for a specific feed definition:

</div>

  - `format`: the \[serialization format \<topics-feed-format\>\](\#serialization-format-\<topics-feed-format\>).
    
    This setting is mandatory, there is no fallback value.

  - `batch_item_count`: falls back to `FEED_EXPORT_BATCH_ITEM_COUNT`.
    
    <div class="versionadded">
    
    2.3.0
    
    </div>

  - `encoding`: falls back to `FEED_EXPORT_ENCODING`.

  - `fields`: falls back to `FEED_EXPORT_FIELDS`.

  - `item_classes`: list of \[item classes \<topics-items\>\](\#item-classes-\<topics-items\>) to export.
    
    If undefined or empty, all items are exported.
    
    <div class="versionadded">
    
    2.6.0
    
    </div>

  - `item_filter`: a \[filter class \<item-filter\>\](\#filter-class-\<item-filter\>) to filter items to export.
    
    <span class="title-ref">\~scrapy.extensions.feedexport.ItemFilter</span> is used be default.
    
    <div class="versionadded">
    
    2.6.0
    
    </div>

  - `indent`: falls back to `FEED_EXPORT_INDENT`.

  - `item_export_kwargs`: <span class="title-ref">dict</span> with keyword arguments for the corresponding \[item exporter class \<topics-exporters\>\](\#item-exporter-class-\<topics-exporters\>).
    
    <div class="versionadded">
    
    2.4.0
    
    </div>

<!-- end list -->

  - \- `overwrite`: whether to overwrite the file if it already exists  
    (`True`) or append to its content (`False`).
    
    The default value depends on the \[storage backend \<topics-feed-storage-backends\>\](\#storage-backend

\----\<topics-feed-storage-backends\>):

>   - \[topics-feed-storage-fs\](\#topics-feed-storage-fs): `False`
> 
>   - \[topics-feed-storage-ftp\](\#topics-feed-storage-ftp): `True`
>     
>     <div class="note">
>     
>     <div class="title">
>     
>     Note
>     
>     </div>
>     
>     Some FTP servers may not support appending to files (the `APPE` FTP command).
>     
>     </div>
> 
>   - \[topics-feed-storage-s3\](\#topics-feed-storage-s3): `True` (appending is not supported)
> 
>   - \[topics-feed-storage-gcs\](\#topics-feed-storage-gcs): `True` (appending is not supported)
> 
>   - \[topics-feed-storage-stdout\](\#topics-feed-storage-stdout): `False` (overwriting is not supported)
> 
> <div class="versionadded">
> 
> 2.4.0
> 
> </div>

  - `store_empty`: falls back to `FEED_STORE_EMPTY`.

  - `uri_params`: falls back to `FEED_URI_PARAMS`.

  - `postprocessing`: list of \[plugins \<post-processing\>\](\#plugins-\<post-processing\>) to use for post-processing.
    
    The plugins will be used in the order of the list passed.
    
    <div class="versionadded">
    
    2.6.0
    
    </div>

<div class="setting">

FEED\_EXPORT\_ENCODING

</div>

### FEED\_EXPORT\_ENCODING

Default: `None`

The encoding to be used for the feed.

If unset or set to `None` (default) it uses UTF-8 for everything except JSON output, which uses safe numeric encoding (`\uXXXX` sequences) for historic reasons.

Use `utf-8` if you want UTF-8 for JSON too.

<div class="versionchanged">

2.8 The `startproject` command now sets this setting to `utf-8` in the generated `settings.py` file.

</div>

<div class="setting">

FEED\_EXPORT\_FIELDS

</div>

### FEED\_EXPORT\_FIELDS

Default: `None`

Use the `FEED_EXPORT_FIELDS` setting to define the fields to export, their order and their output names. See <span class="title-ref">BaseItemExporter.fields\_to\_export \<scrapy.exporters.BaseItemExporter.fields\_to\_export\></span> for more information.

<div class="setting">

FEED\_EXPORT\_INDENT

</div>

### FEED\_EXPORT\_INDENT

Default: `0`

Amount of spaces used to indent the output on each level. If `FEED_EXPORT_INDENT` is a non-negative integer, then array elements and object members will be pretty-printed with that indent level. An indent level of `0` (the default), or negative, will put each item on a new line. `None` selects the most compact representation.

Currently implemented only by <span class="title-ref">\~scrapy.exporters.JsonItemExporter</span> and <span class="title-ref">\~scrapy.exporters.XmlItemExporter</span>, i.e. when you are exporting to `.json` or `.xml`.

<div class="setting">

FEED\_STORE\_EMPTY

</div>

### FEED\_STORE\_EMPTY

Default: `True`

Whether to export empty feeds (i.e. feeds with no items). If `False`, and there are no items to export, no new files are created and existing files are not modified, even if the \[overwrite feed option \<feed-options\>\](\#overwrite-feed-option-\<feed-options\>) is enabled.

<div class="setting">

FEED\_STORAGES

</div>

### FEED\_STORAGES

Default: `{}`

A dict containing additional feed storage backends supported by your project. The keys are URI schemes and the values are paths to storage classes.

<div class="setting">

FEED\_STORAGE\_FTP\_ACTIVE

</div>

### FEED\_STORAGE\_FTP\_ACTIVE

Default: `False`

Whether to use the active connection mode when exporting feeds to an FTP server (`True`) or use the passive connection mode instead (`False`, default).

For information about FTP connection modes, see [What is the difference between active and passive FTP?](https://stackoverflow.com/a/1699163).

<div class="setting">

FEED\_STORAGE\_S3\_ACL

</div>

### FEED\_STORAGE\_S3\_ACL

Default: `''` (empty string)

A string containing a custom ACL for feeds exported to Amazon S3 by your project.

For a complete list of available values, access the [Canned ACL]() section on Amazon S3 docs.

<div class="setting">

FEED\_STORAGES\_BASE

</div>

### FEED\_STORAGES\_BASE

Default:

`` `python     {         "": "scrapy.extensions.feedexport.FileFeedStorage",         "file": "scrapy.extensions.feedexport.FileFeedStorage",         "stdout": "scrapy.extensions.feedexport.StdoutFeedStorage",         "s3": "scrapy.extensions.feedexport.S3FeedStorage",         "ftp": "scrapy.extensions.feedexport.FTPFeedStorage",     }  A dict containing the built-in feed storage backends supported by Scrapy. You ``<span class="title-ref"> can disable any of these backends by assigning </span><span class="title-ref">None</span><span class="title-ref"> to their URI scheme in :setting:\`FEED\_STORAGES</span>. E.g., to disable the built-in FTP storage backend (without replacement), place this in your `settings.py`:

`` `python     FEED_STORAGES = {         "ftp": None,     }  .. setting:: FEED_EXPORTERS  FEED_EXPORTERS ``\` --------------

Default: `{}`

A dict containing additional exporters supported by your project. The keys are serialization formats and the values are paths to \[Item exporter \<topics-exporters\>\](\#item-exporter \<topics-exporters\>) classes.

<div class="setting">

FEED\_EXPORTERS\_BASE

</div>

### FEED\_EXPORTERS\_BASE

Default:

`` `python     {         "json": "scrapy.exporters.JsonItemExporter",         "jsonlines": "scrapy.exporters.JsonLinesItemExporter",         "jsonl": "scrapy.exporters.JsonLinesItemExporter",         "jl": "scrapy.exporters.JsonLinesItemExporter",         "csv": "scrapy.exporters.CsvItemExporter",         "xml": "scrapy.exporters.XmlItemExporter",         "marshal": "scrapy.exporters.MarshalItemExporter",         "pickle": "scrapy.exporters.PickleItemExporter",     }  A dict containing the built-in feed exporters supported by Scrapy. You can ``<span class="title-ref"> disable any of these exporters by assigning </span><span class="title-ref">None</span><span class="title-ref"> to their serialization format in :setting:\`FEED\_EXPORTERS</span>. E.g., to disable the built-in CSV exporter (without replacement), place this in your `settings.py`:

`` `python     FEED_EXPORTERS = {         "csv": None,     }   .. setting:: FEED_EXPORT_BATCH_ITEM_COUNT  FEED_EXPORT_BATCH_ITEM_COUNT ``\` ----------------------------

<div class="versionadded">

2.3.0

</div>

Default: `0`

If assigned an integer number higher than `0`, Scrapy generates multiple output files storing up to the specified number of items in each output file.

When generating multiple output files, you must use at least one of the following placeholders in the feed URI to indicate how the different output file names are generated:

  - `%(batch_time)s` - gets replaced by a timestamp when the feed is being created (e.g. `2020-03-28T14-45-08.237134`)

  - `%(batch_id)d` - gets replaced by the 1-based sequence number of the batch.
    
    Use \[printf-style string formatting \<python:old-string-formatting\>\](\#printf-style-string-formatting-\<python:old-string-formatting\>) to alter the number format. For example, to make the batch ID a 5-digit number by introducing leading zeroes as needed, use `%(batch_id)05d` (e.g. `3` becomes `00003`, `123` becomes `00123`).

For instance, if your settings include:

`` `python     FEED_EXPORT_BATCH_ITEM_COUNT = 100  And your :command:`crawl` command line is::      scrapy crawl spidername -o "dirname/%(batch_id)d-filename%(batch_time)s.json"  The command line above can generate a directory tree like::      ->projectname     -->dirname     --->1-filename2020-03-28T14-45-08.237134.json     --->2-filename2020-03-28T14-45-09.148903.json     --->3-filename2020-03-28T14-45-10.046092.json  Where the first and second files contain exactly 100 items. The last one contains ``\` 100 items or fewer.

<div class="setting">

FEED\_URI\_PARAMS

</div>

### FEED\_URI\_PARAMS

Default: `None`

A string with the import path of a function to set the parameters to apply with \[printf-style string formatting \<python:old-string-formatting\>\](\#printf-style-string-formatting-\<python:old-string-formatting\>) to the feed URI.

The function signature should be as follows:

<div class="function">

uri\_params(params, spider)

Return a <span class="title-ref">dict</span> of key-value pairs to apply to the feed URI using \[printf-style string formatting \<python:old-string-formatting\>\](\#printf-style-string-formatting-\<python:old-string-formatting\>).

  - param params  
    default key-value pairs

> Specifically:
> 
>   - `batch_id`: ID of the file batch. See `FEED_EXPORT_BATCH_ITEM_COUNT`.
>     
>     If `FEED_EXPORT_BATCH_ITEM_COUNT` is `0`, `batch_id` is always `1`.
>     
>     <div class="versionadded">
>     
>     2.3.0
>     
>     </div>
> 
>   - `batch_time`: UTC date and time, in ISO format with `:` replaced with `-`.
>     
>     See `FEED_EXPORT_BATCH_ITEM_COUNT`.
>     
>     <div class="versionadded">
>     
>     2.3.0
>     
>     </div>
> 
>   - `time`: `batch_time`, with microseconds set to `0`.

  - type params  
    dict

  - param spider  
    source spider of the feed items

  - type spider  
    scrapy.Spider

<div class="caution">

<div class="title">

Caution

</div>

The function should return a new dictionary, modifying the received `params` in-place is deprecated.

</div>

</div>

For example, to include the <span class="title-ref">name \<scrapy.Spider.name\></span> of the source spider in the feed URI:

1.  Define the following function somewhere in your project:
    
      - \`\`\`python  
        \# myproject/utils.py def uri\_params(params, spider): return {\*\*params, "spider\_name": spider.name}

2.  Point `FEED_URI_PARAMS` to that function in your settings:
    
    ``` python
    # myproject/settings.py
    FEED_URI_PARAMS = "myproject.utils.uri_params"
    ```

3.  Use `%(spider_name)s` in your feed URI:
    
        scrapy crawl <spider_name> -o "%(spider_name)s.jsonl"

\`\`\` .. \_Amazon S3: <https://aws.amazon.com/s3/> .. \_boto3: <https://github.com/boto/boto3> .. \_Canned ACL: <https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl> .. \_Google Cloud Storage: <https://cloud.google.com/storage/>

---

item-pipeline.md

---

# Item Pipeline

After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially.

Each item pipeline component (sometimes referred as just "Item Pipeline") is a Python class that implements a simple method. They receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed.

Typical uses of item pipelines are:

  - cleansing HTML data
  - validating scraped data (checking that the items contain certain fields)
  - checking for duplicates (and dropping them)
  - storing the scraped item in a database

## Writing your own item pipeline

Each item pipeline component is a Python class that must implement the following method:

<div class="method">

process\_item(self, item, spider)

This method is called for every item pipeline component.

<span class="title-ref">item</span> is an \[item object \<item-types\>\](\#item-object-\<item-types\>), see \[supporting-item-types\](\#supporting-item-types).

<span class="title-ref">process\_item</span> must either: return an \[item object \<item-types\>\](\#item-object-\<item-types\>), return a <span class="title-ref">\~twisted.internet.defer.Deferred</span> or raise a <span class="title-ref">\~scrapy.exceptions.DropItem</span> exception.

Dropped items are no longer processed by further pipeline components.

  - param item  
    the scraped item

  - type item  
    \[item object \<item-types\>\](\#item-object-\<item-types\>)

  - param spider  
    the spider which scraped the item

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

Additionally, they may also implement the following methods:

<div class="method">

open\_spider(self, spider)

This method is called when the spider is opened.

  - param spider  
    the spider which was opened

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="method">

close\_spider(self, spider)

This method is called when the spider is closed.

  - param spider  
    the spider which was closed

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="classmethod">

from\_crawler(cls, crawler)

If present, this class method is called to create a pipeline instance from a <span class="title-ref">\~scrapy.crawler.Crawler</span>. It must return a new instance of the pipeline. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for pipeline to access them and hook its functionality into Scrapy.

  - param crawler  
    crawler that uses this pipeline

  - type crawler  
    <span class="title-ref">\~scrapy.crawler.Crawler</span> object

</div>

## Item pipeline example

### Price validation and dropping items with no prices

Let's take a look at the following hypothetical pipeline that adjusts the `price` attribute for those items that do not include VAT (`price_excludes_vat` attribute), and drops those items which don't contain a price:

`` `python     from itemadapter import ItemAdapter     from scrapy.exceptions import DropItem       class PricePipeline:         vat_factor = 1.15          def process_item(self, item, spider):             adapter = ItemAdapter(item)             if adapter.get("price"):                 if adapter.get("price_excludes_vat"):                     adapter["price"] = adapter["price"] * self.vat_factor                 return item             else:                 raise DropItem("Missing price")   Write items to a JSON lines file ``\` --------------------------------

The following pipeline stores all scraped items (from all spiders) into a single `items.jsonl` file, containing one item per line serialized in JSON format:

`` `python    import json     from itemadapter import ItemAdapter      class JsonWriterPipeline:        def open_spider(self, spider):            self.file = open("items.jsonl", "w")         def close_spider(self, spider):            self.file.close()         def process_item(self, item, spider):            line = json.dumps(ItemAdapter(item).asdict()) + "\n"            self.file.write(line)            return item  .. note:: The purpose of JsonWriterPipeline is just to introduce how to write    item pipelines. If you really want to store all scraped items into a JSON    file you should use the [Feed exports <topics-feed-exports>](#feed-exports-<topics-feed-exports>).  Write items to MongoDB ``\` ----------------------

In this example we'll write items to [MongoDB](https://www.mongodb.com/) using [pymongo](). MongoDB address and database name are specified in Scrapy settings; MongoDB collection is named after item class.

The main point of this example is to show how to use <span class="title-ref">from\_crawler</span> method and how to clean up the resources properly.

`` `python     import pymongo     from itemadapter import ItemAdapter       class MongoPipeline:         collection_name = "scrapy_items"          def __init__(self, mongo_uri, mongo_db):             self.mongo_uri = mongo_uri             self.mongo_db = mongo_db          @classmethod         def from_crawler(cls, crawler):             return cls(                 mongo_uri=crawler.settings.get("MONGO_URI"),                 mongo_db=crawler.settings.get("MONGO_DATABASE", "items"),             )          def open_spider(self, spider):             self.client = pymongo.MongoClient(self.mongo_uri)             self.db = self.client[self.mongo_db]          def close_spider(self, spider):             self.client.close()          def process_item(self, item, spider):             self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())             return item ``\` .. \_pymongo: <https://pymongo.readthedocs.io/en/stable/>

### Take screenshot of item

This example demonstrates how to use \[coroutine syntax \<coroutines\>\](coroutine syntax \<coroutines\>.md) in the <span class="title-ref">process\_item</span> method.

This item pipeline makes a request to a locally-running instance of [Splash](https://splash.readthedocs.io/en/stable/) to render a screenshot of the item URL. After the request response is downloaded, the item pipeline saves the screenshot to a file and adds the filename to the item.

`` `python     import hashlib     from pathlib import Path     from urllib.parse import quote      import scrapy     from itemadapter import ItemAdapter     from scrapy.http.request import NO_CALLBACK     from scrapy.utils.defer import maybe_deferred_to_future       class ScreenshotPipeline:         """Pipeline that uses Splash to render screenshot of         every Scrapy item."""          SPLASH_URL = "http://localhost:8050/render.png?url={}"          async def process_item(self, item, spider):             adapter = ItemAdapter(item)             encoded_item_url = quote(adapter["url"])             screenshot_url = self.SPLASH_URL.format(encoded_item_url)             request = scrapy.Request(screenshot_url, callback=NO_CALLBACK)             response = await maybe_deferred_to_future(                 spider.crawler.engine.download(request)             )              if response.status != 200:                 # Error happened, return item.                 return item              # Save screenshot to file, filename will be hash of url.             url = adapter["url"]             url_hash = hashlib.md5(url.encode("utf8")).hexdigest()             filename = f"{url_hash}.png"             Path(filename).write_bytes(response.body)              # Store filename in item.             adapter["screenshot_filename"] = filename             return item    Duplicates filter ``\` -----------------

A filter that looks for duplicate items, and drops those items that were already processed. Let's say that our items have a unique id, but our spider returns multiples items with the same id:

`` `python     from itemadapter import ItemAdapter     from scrapy.exceptions import DropItem       class DuplicatesPipeline:         def __init__(self):             self.ids_seen = set()          def process_item(self, item, spider):             adapter = ItemAdapter(item)             if adapter["id"] in self.ids_seen:                 raise DropItem(f"Item ID already seen: {adapter['id']}")             else:                 self.ids_seen.add(adapter["id"])                 return item   Activating an Item Pipeline component ``\` =====================================

To activate an Item Pipeline component you must add its class to the `ITEM_PIPELINES` setting, like in the following example:

`` `python    ITEM_PIPELINES = {        "myproject.pipelines.PricePipeline": 300,        "myproject.pipelines.JsonWriterPipeline": 800,    }  The integer values you assign to classes in this setting determine the ``\` order in which they run: items go through from lower valued to higher valued classes. It's customary to define these numbers in the 0-1000 range.

---

items.md

---

# Items

<div class="module" data-synopsis="Item and Field classes">

scrapy.item

</div>

The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. \[Spiders \<topics-spiders\>\](\#spiders-\<topics-spiders\>) may return the extracted data as <span class="title-ref">items</span>, Python objects that define key-value pairs.

Scrapy supports \[multiple types of items \<item-types\>\](\#multiple-types-of-items-\<item-types\>). When you create an item, you may use whichever type of item you want. When you write code that receives an item, your code should \[work for any item type \<supporting-item-types\>\](\#work-for-any-item-type \<supporting-item-types\>).

## Item Types

Scrapy supports the following types of items, via the [itemadapter](https://github.com/scrapy/itemadapter) library: \[dictionaries \<dict-items\>\](\#dictionaries-\<dict-items\>), \[Item objects \<item-objects\>\](\#item-objects-\<item-objects\>), \[dataclass objects \<dataclass-items\>\](\#dataclass-objects-\<dataclass-items\>), and \[attrs objects \<attrs-items\>\](\#attrs-objects-\<attrs-items\>).

### Dictionaries

As an item type, <span class="title-ref">dict</span> is convenient and familiar.

### Item objects

<span class="title-ref">Item</span> provides a <span class="title-ref">dict</span>-like API plus additional features that make it the most feature-complete item type:

<div class="autoclass" data-members="copy, deepcopy, fields" data-undoc-members="">

scrapy.Item

</div>

<span class="title-ref">Item</span> objects replicate the standard <span class="title-ref">dict</span> API, including its `__init__` method.

<span class="title-ref">Item</span> allows the defining of field names, so that:

  - <span class="title-ref">KeyError</span> is raised when using undefined field names (i.e. prevents typos going unnoticed)
  - \[Item exporters \<topics-exporters\>\](\#item-exporters-\<topics-exporters\>) can export all fields by default even if the first scraped object does not have values for all of them

<span class="title-ref">Item</span> also allows the defining of field metadata, which can be used to \[customize serialization \<topics-exporters-field-serialization\>\](\#customize-serialization-\<topics-exporters-field-serialization\>).

`trackref` tracks <span class="title-ref">Item</span> objects to help find memory leaks (see \[topics-leaks-trackrefs\](\#topics-leaks-trackrefs)).

Example:

`` `python     from scrapy.item import Item, Field       class CustomItem(Item):         one_field = Field()         another_field = Field()  .. _dataclass-items:  Dataclass objects ``\` -----------------

<div class="versionadded">

2.2

</div>

<span class="title-ref">\~dataclasses.dataclass</span> allows the defining of item classes with field names, so that \[item exporters \<topics-exporters\>\](\#item-exporters-\<topics-exporters\>) can export all fields by default even if the first scraped object does not have values for all of them.

Additionally, `dataclass` items also allow you to:

  - define the type and default value of each defined field.
  - define custom field metadata through <span class="title-ref">dataclasses.field</span>, which can be used to \[customize serialization \<topics-exporters-field-serialization\>\](\#customize-serialization-\<topics-exporters-field-serialization\>).

Example:

`` `python     from dataclasses import dataclass       @dataclass     class CustomItem:         one_field: str         another_field: int  .. note:: Field types are not enforced at run time.  .. _attrs-items:  attr.s objects ``\` --------------

<div class="versionadded">

2.2

</div>

<span class="title-ref">attr.s</span> allows the defining of item classes with field names, so that \[item exporters \<topics-exporters\>\](\#item-exporters-\<topics-exporters\>) can export all fields by default even if the first scraped object does not have values for all of them.

Additionally, `attr.s` items also allow to:

  - define the type and default value of each defined field.
  - define custom field \[metadata \<attrs:metadata\>\](\#metadata-\<attrs:metadata\>), which can be used to \[customize serialization \<topics-exporters-field-serialization\>\](\#customize-serialization-\<topics-exporters-field-serialization\>).

In order to use this type, the \[attrs package \<attrs:index\>\](attrs package \<attrs:index\>.md) needs to be installed.

Example:

`` `python     import attr       @attr.s     class CustomItem:         one_field = attr.ib()         another_field = attr.ib()   Working with Item objects ``\` =========================

### Declaring Item subclasses

Item subclasses are declared using a simple class definition syntax and <span class="title-ref">Field</span> objects. Here is an example:

`` `python     import scrapy       class Product(scrapy.Item):         name = scrapy.Field()         price = scrapy.Field()         stock = scrapy.Field()         tags = scrapy.Field()         last_updated = scrapy.Field(serializer=str)  .. note:: Those familiar with `Django`_ will notice that Scrapy Items are    declared similar to `Django Models`_, except that Scrapy Items are much    simpler as there is no concept of different field types. ``\` .. \_Django Models: <https://docs.djangoproject.com/en/dev/topics/db/models/>

### Declaring fields

<span class="title-ref">Field</span> objects are used to specify metadata for each field. For example, the serializer function for the `last_updated` field illustrated in the example above.

You can specify any kind of metadata for each field. There is no restriction on the values accepted by <span class="title-ref">Field</span> objects. For this same reason, there is no reference list of all available metadata keys. Each key defined in <span class="title-ref">Field</span> objects could be used by a different component, and only those components know about it. You can also define and use any other <span class="title-ref">Field</span> key in your project too, for your own needs. The main goal of <span class="title-ref">Field</span> objects is to provide a way to define all field metadata in one place. Typically, those components whose behaviour depends on each field use certain field keys to configure that behaviour. You must refer to their documentation to see which metadata keys are used by each component.

It's important to note that the <span class="title-ref">Field</span> objects used to declare the item do not stay assigned as class attributes. Instead, they can be accessed through the <span class="title-ref">\~scrapy.Item.fields</span> attribute.

<div class="autoclass">

scrapy.Field

The <span class="title-ref">Field</span> class is just an alias to the built-in <span class="title-ref">dict</span> class and doesn't provide any extra functionality or attributes. In other words, <span class="title-ref">Field</span> objects are plain-old Python dicts. A separate class is used to support the \[item declaration syntax \<topics-items-declaring\>\](\#item-declaration-syntax-\<topics-items-declaring\>) based on class attributes.

</div>

<div class="note">

<div class="title">

Note

</div>

Field metadata can also be declared for `dataclass` and `attrs` items. Please refer to the documentation for [dataclasses.field]() and [attr.ib]() for additional information.

</div>

### Working with Item objects

Here are some examples of common tasks performed with items, using the `Product` item \[declared above \<topics-items-declaring\>\](\#declared-above--\<topics-items-declaring\>). You will notice the API is very similar to the <span class="title-ref">dict</span> API.

#### Creating items

`` `pycon     >>> product = Product(name="Desktop PC", price=1000)     >>> print(product)     Product(name='Desktop PC', price=1000)   Getting field values ``\` ''''''''''''''''''''

`` `pycon     >>> product["name"]     Desktop PC     >>> product.get("name")     Desktop PC      >>> product["price"]     1000      >>> product["last_updated"]     Traceback (most recent call last):         ...     KeyError: 'last_updated'      >>> product.get("last_updated", "not set")     not set      >>> product["lala"]  # getting unknown field     Traceback (most recent call last):         ...     KeyError: 'lala'      >>> product.get("lala", "unknown field")     'unknown field'      >>> "name" in product  # is name field populated?     True      >>> "last_updated" in product  # is last_updated populated?     False      >>> "last_updated" in product.fields  # is last_updated a declared field?     True      >>> "lala" in product.fields  # is lala a declared field?     False   Setting field values ``\` ''''''''''''''''''''

`` `pycon     >>> product["last_updated"] = "today"     >>> product["last_updated"]     today      >>> product["lala"] = "test"  # setting unknown field     Traceback (most recent call last):         ...     KeyError: 'Product does not support field: lala'   Accessing all populated values ``\` ''''''''''''''''''''''''''''''

To access all populated values, just use the typical <span class="title-ref">dict</span> API:

`` `pycon     >>> product.keys()     ['price', 'name']      >>> product.items()     [('price', 1000), ('name', 'Desktop PC')]   .. _copying-items:  Copying items ``\` '''''''''''''

To copy an item, you must first decide whether you want a shallow copy or a deep copy.

If your item contains `mutable` values like lists or dictionaries, a shallow copy will keep references to the same mutable values across all different copies.

For example, if you have an item with a list of tags, and you create a shallow copy of that item, both the original item and the copy have the same list of tags. Adding a tag to the list of one of the items will add the tag to the other item as well.

If that is not the desired behavior, use a deep copy instead.

See `copy` for more information.

To create a shallow copy of an item, you can either call <span class="title-ref">\~scrapy.Item.copy</span> on an existing item (`product2 = product.copy()`) or instantiate your item class from an existing item (`product2 = Product(product)`).

To create a deep copy, call <span class="title-ref">\~scrapy.Item.deepcopy</span> instead (`product2 = product.deepcopy()`).

#### Other common tasks

Creating dicts from items:

`` `pycon     >>> dict(product)  # create a dict from all populated values     {'price': 1000, 'name': 'Desktop PC'}      Creating items from dicts:      >>> Product({"name": "Laptop PC", "price": 1500})     Product(price=1500, name='Laptop PC')      >>> Product({"name": "Laptop PC", "lala": 1500})  # warning: unknown field in dict     Traceback (most recent call last):         ...     KeyError: 'Product does not support field: lala'   Extending Item subclasses ``\` -------------------------

You can extend Items (to add more fields or to change some metadata for some fields) by declaring a subclass of your original Item.

For example:

`` `python     class DiscountedProduct(Product):         discount_percent = scrapy.Field(serializer=str)         discount_expiration_date = scrapy.Field()  You can also extend field metadata by using the previous field metadata and ``\` appending more values, or changing existing values, like this:

`` `python     class SpecificProduct(Product):         name = scrapy.Field(Product.fields["name"], serializer=my_serializer)  That adds (or replaces) the ``serializer`metadata key for the`name`field,`\` keeping all the previously existing metadata values.

## Supporting All Item Types

In code that receives an item, such as methods of \[item pipelines \<topics-item-pipeline\>\](\#item-pipelines \<topics-item-pipeline\>) or \[spider middlewares \<topics-spider-middleware\>\](\#spider-middlewares \<topics-spider-middleware\>), it is a good practice to use the <span class="title-ref">\~itemadapter.ItemAdapter</span> class and the <span class="title-ref">\~itemadapter.is\_item</span> function to write code that works for any supported item type.

## Other classes related to items

<div class="autoclass">

ItemMeta

</div>

---

jobs.md

---

# Jobs: pausing and resuming crawls

Sometimes, for big sites, it's desirable to pause crawls and be able to resume them later.

Scrapy supports this functionality out of the box by providing the following facilities:

  - a scheduler that persists scheduled requests on disk
  - a duplicates filter that persists visited requests on disk
  - an extension that keeps some spider state (key/value pairs) persistent between batches

## Job directory

To enable persistence support you just need to define a *job directory* through the `JOBDIR` setting. This directory will be for storing all required data to keep the state of a single job (i.e. a spider run). It's important to note that this directory must not be shared by different spiders, or even different jobs/runs of the same spider, as it's meant to be used for storing the state of a *single* job.

## How to use it

To start a spider with persistence support enabled, run it like this:

    scrapy crawl somespider -s JOBDIR=crawls/somespider-1

Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending a signal), and resume it later by issuing the same command:

    scrapy crawl somespider -s JOBDIR=crawls/somespider-1

## Keeping persistent state between batches

Sometimes you'll want to keep some persistent spider state between pause/resume batches. You can use the `spider.state` attribute for that, which should be a dict. There's \[a built-in extension \<topics-extensions-ref-spiderstate\>\](\#a-built-in-extension-\<topics-extensions-ref-spiderstate\>) that takes care of serializing, storing and loading that attribute from the job directory, when the spider starts and stops.

Here's an example of a callback that uses the spider state (other spider code is omitted for brevity):

`` `python     def parse_item(self, response):         # parse item here         self.state["items_count"] = self.state.get("items_count", 0) + 1  Persistence gotchas ``\` ===================

There are a few things to keep in mind if you want to be able to use the Scrapy persistence support:

### Cookies expiration

Cookies may expire. So, if you don't resume your spider quickly the requests scheduled may no longer work. This won't be an issue if your spider doesn't rely on cookies.

### Request serialization

For persistence to work, <span class="title-ref">\~scrapy.Request</span> objects must be serializable with `pickle`, except for the `callback` and `errback` values passed to their `__init__` method, which must be methods of the running <span class="title-ref">\~scrapy.Spider</span> class.

If you wish to log the requests that couldn't be serialized, you can set the `SCHEDULER_DEBUG` setting to `True` in the project's settings page. It is `False` by default.

---

leaks.md

---

# Debugging memory leaks

In Scrapy, objects such as requests, responses and items have a finite lifetime: they are created, used for a while, and finally destroyed.

From all those objects, the Request is probably the one with the longest lifetime, as it stays waiting in the Scheduler queue until it's time to process it. For more info see \[topics-architecture\](\#topics-architecture).

As these Scrapy objects have a (rather long) lifetime, there is always the risk of accumulating them in memory without releasing them properly and thus causing what is known as a "memory leak".

To help debugging memory leaks, Scrapy provides a built-in mechanism for tracking objects references called \[trackref \<topics-leaks-trackrefs\>\](\#trackref-\<topics-leaks-trackrefs\>), and you can also use a third-party library called \[muppy \<topics-leaks-muppy\>\](\#muppy \<topics-leaks-muppy\>) for more advanced memory debugging (see below for more info). Both mechanisms must be used from the \[Telnet Console \<topics-telnetconsole\>\](\#telnet-console \<topics-telnetconsole\>).

## Common causes of memory leaks

It happens quite often (sometimes by accident, sometimes on purpose) that the Scrapy developer passes objects referenced in Requests (for example, using the <span class="title-ref">\~scrapy.Request.cb\_kwargs</span> or <span class="title-ref">\~scrapy.Request.meta</span> attributes or the request callback function) and that effectively bounds the lifetime of those referenced objects to the lifetime of the Request. This is, by far, the most common cause of memory leaks in Scrapy projects, and a quite difficult one to debug for newcomers.

In big projects, the spiders are typically written by different people and some of those spiders could be "leaking" and thus affecting the rest of the other (well-written) spiders when they get to run concurrently, which, in turn, affects the whole crawling process.

The leak could also come from a custom middleware, pipeline or extension that you have written, if you are not releasing the (previously allocated) resources properly. For example, allocating resources on `spider_opened` but not releasing them on `spider_closed` may cause problems if you're running \[multiple spiders per process \<run-multiple-spiders\>\](\#multiple-spiders-per-process-\<run-multiple-spiders\>).

### Too Many Requests?

By default Scrapy keeps the request queue in memory; it includes <span class="title-ref">\~scrapy.Request</span> objects and all objects referenced in Request attributes (e.g. in <span class="title-ref">\~scrapy.Request.cb\_kwargs</span> and <span class="title-ref">\~scrapy.Request.meta</span>). While not necessarily a leak, this can take a lot of memory. Enabling \[persistent job queue \<topics-jobs\>\](\#persistent-job-queue-\<topics-jobs\>) could help keeping memory usage in control.

## Debugging memory leaks with `trackref`

`trackref` is a module provided by Scrapy to debug the most common cases of memory leaks. It basically tracks the references to all live Request, Response, Item, Spider and Selector objects.

You can enter the telnet console and inspect how many objects (of the classes mentioned above) are currently alive using the `prefs()` function which is an alias to the <span class="title-ref">\~scrapy.utils.trackref.print\_live\_refs</span> function:

    telnet localhost 6023
    
    ```pycon
        >>> prefs()
        Live References
    
        ExampleSpider                       1   oldest: 15s ago
        HtmlResponse                       10   oldest: 1s ago
        Selector                            2   oldest: 0s ago
        FormRequest                       878   oldest: 7s ago

As you can see, that report also shows the "age" of the oldest object in each `` ` class. If you're running multiple spiders per process chances are you can figure out which spider is leaking by looking at the oldest request or response. You can get the oldest object of each class using the `~scrapy.utils.trackref.get_oldest` function (from the telnet console).  Which objects are tracked? --------------------------  The objects tracked by ``trackrefs``are all from these classes (and all its subclasses):  * `scrapy.Request` * `scrapy.http.Response` * `scrapy.Item` * `scrapy.Selector` * `scrapy.Spider`  A real example --------------  Let's see a concrete example of a hypothetical case of memory leaks. Suppose we have some spider with a line similar to this one::      return Request(f"http://www.somenastyspider.com/product.php?pid={product_id}",                    callback=self.parse, cb_kwargs={'referer': response})  That line is passing a response reference inside a request which effectively ties the response lifetime to the requests' one, and that would definitely cause memory leaks.  Let's see how we can discover the cause (without knowing it a priori, of course) by using the``trackref`tool.  After the crawler is running for a few minutes and we notice its memory usage has grown a lot, we can enter its telnet console and check the live references:`\`pycon \>\>\> prefs() Live References

> SomenastySpider 1 oldest: 15s ago HtmlResponse 3890 oldest: 265s ago Selector 2 oldest: 0s ago Request 3878 oldest: 250s ago

The fact that there are so many live responses (and that they're so old) is `` ` definitely suspicious, as responses should have a relatively short lifetime compared to Requests. The number of responses is similar to the number of requests, so it looks like they are tied in a some way. We can now go and check the code of the spider to discover the nasty line that is generating the leaks (passing response references inside requests).  Sometimes extra information about live objects can be helpful. Let's check the oldest response: ``\`pycon \>\>\> from scrapy.utils.trackref import get\_oldest \>\>\> r = get\_oldest("HtmlResponse") \>\>\> r.url '<http://www.somenastyspider.com/product.php?pid=123>'

If you want to iterate over all objects, instead of getting the oldest one, you `` ` can use the `scrapy.utils.trackref.iter_all` function: ``\`pycon \>\>\> from scrapy.utils.trackref import iter\_all \>\>\> \[r.url for r in iter\_all("HtmlResponse")\] \['<http://www.somenastyspider.com/product.php?pid=123>', '<http://www.somenastyspider.com/product.php?pid=584>', ...\]

Too many spiders? `` ` -----------------  If your project has too many spiders executed in parallel, the output of `prefs()` can be difficult to read. For this reason, that function has a ``ignore`argument which can be used to ignore a particular class (and all its subclasses). For example, this won't show any live references to spiders:`\`pycon \>\>\> from scrapy.spiders import Spider \>\>\> prefs(ignore=Spider)

<div class="module" data-synopsis="Track references of live objects">

scrapy.utils.trackref

</div>

scrapy.utils.trackref module `` ` ----------------------------  Here are the functions available in the :mod:`~scrapy.utils.trackref` module.  .. class:: object_ref      Inherit from this class if you want to track live     instances with the ``trackref`module.  .. function:: print_live_refs(class_name, ignore=NoneType)      Print a report of live references, grouped by class name.      :param ignore: if given, all objects from the specified class (or tuple of         classes) will be ignored.     :type ignore: type or tuple  .. function:: get_oldest(class_name)      Return the oldest object alive with the given class name, or`None``if     none is found. Use `print_live_refs` first to get a list of all     tracked live objects per class name.  .. function:: iter_all(class_name)      Return an iterator over all objects alive with the given class name, or``None``if none is found. Use `print_live_refs` first to get a list     of all tracked live objects per class name.  .. _topics-leaks-muppy:  Debugging memory leaks with muppy =================================``trackref`provides a very convenient mechanism for tracking down memory leaks, but it only keeps track of the objects that are more likely to cause memory leaks. However, there are other cases where the memory leaks could come from other (more or less obscure) objects. If this is your case, and you can't find your leaks using`trackref``, you still have another resource: the muppy library.  You can use muppy from `Pympler`_.    If you use``pip`, you can install muppy with the following command::      pip install Pympler  Here's an example to view all Python objects available in the heap using muppy:`\`pycon \>\>\> from pympler import muppy \>\>\> all\_objects = muppy.get\_objects() \>\>\> len(all\_objects) 28667 \>\>\> from pympler import summary \>\>\> suml = summary.summarize(all\_objects) \>\>\> [summary.print]()(suml) types | \# objects | total size ==================================== | =========== | ============ \<class 'str | 9822 | 1.10 MB \<class 'dict | 1658 | 856.62 KB \<class 'type | 436 | 443.60 KB \<class 'code | 2974 | 419.56 KB \<class '\_io.BufferedWriter | 2 | 256.34 KB \<class 'set | 420 | 159.88 KB \<class '\_io.BufferedReader | 1 | 128.17 KB \<class 'wrapper\_descriptor | 1130 | 88.28 KB \<class 'tuple | 1304 | 86.57 KB \<class 'weakref | 1013 | 79.14 KB \<class 'builtin\_function\_or\_method | 958 | 67.36 KB \<class 'method\_descriptor | 865 | 60.82 KB \<class 'abc.ABCMeta | 62 | 59.96 KB \<class 'list | 446 | 58.52 KB \<class 'int | 1425 | 43.20 KB

For more info about muppy, refer to the [muppy documentation](https://pythonhosted.org/Pympler/muppy.html).

<div id="topics-leaks-without-leaks">

Leaks without leaks \`\`\` ===================

</div>

Sometimes, you may notice that the memory usage of your Scrapy process will only increase, but never decrease. Unfortunately, this could happen even though neither Scrapy nor your project are leaking memory. This is due to a (not so well) known problem of Python, which may not return released memory to the operating system in some cases. For more information on this issue see:

  - [Python Memory Management](https://www.evanjones.ca/python-memory.html)
  - [Python Memory Management Part 2](https://www.evanjones.ca/python-memory-part2.html)
  - [Python Memory Management Part 3](https://www.evanjones.ca/python-memory-part3.html)

The improvements proposed by Evan Jones, which are detailed in [this paper](https://www.evanjones.ca/memoryallocator/), got merged in Python 2.5, but this only reduces the problem, it doesn't fix it completely. To quote the paper:

> *Unfortunately, this patch can only free an arena if there are no more objects allocated in it anymore. This means that fragmentation is a large issue. An application could have many megabytes of free memory, scattered throughout all the arenas, but it will be unable to free any of it. This is a problem experienced by all memory allocators. The only way to solve it is to move to a compacting garbage collector, which is able to move objects in memory. This would require significant changes to the Python interpreter.*

To keep memory consumption reasonable you can split the job into several smaller jobs or enable \[persistent job queue \<topics-jobs\>\](\#persistent-job-queue-\<topics-jobs\>) and stop/start spider from time to time.

---

link-extractors.md

---

# Link Extractors

A link extractor is an object that extracts links from responses.

The `__init__` method of <span class="title-ref">\~scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor</span> takes settings that determine which links may be extracted. <span class="title-ref">LxmlLinkExtractor.extract\_links \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract\_links\></span> returns a list of matching <span class="title-ref">\~scrapy.link.Link</span> objects from a <span class="title-ref">\~scrapy.http.Response</span> object.

Link extractors are used in <span class="title-ref">\~scrapy.spiders.CrawlSpider</span> spiders through a set of <span class="title-ref">\~scrapy.spiders.Rule</span> objects.

You can also use link extractors in regular spiders. For example, you can instantiate <span class="title-ref">LinkExtractor \<scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor\></span> into a class variable in your spider, and use it from your spider callbacks:

`` `python     def parse(self, response):         for link in self.link_extractor.extract_links(response):             yield Request(link.url, callback=self.parse)  .. _topics-link-extractors-ref:  Link extractor reference ``\` ========================

<div class="module" data-synopsis="Link extractors classes">

scrapy.linkextractors

</div>

The link extractor class is <span class="title-ref">scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor</span>. For convenience it can also be imported as `scrapy.linkextractors.LinkExtractor`:

    from scrapy.linkextractors import LinkExtractor

## LxmlLinkExtractor

<div class="module" data-synopsis="lxml&#39;s HTMLParser-based link extractors">

scrapy.linkextractors.lxmlhtml

</div>

<div class="LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=(&#39;a&#39;, &#39;area&#39;), attrs=(&#39;href&#39;,), canonicalize=False, unique=True, process_value=None, strip=True)">

LxmlLinkExtractor is the recommended link extractor with handy filtering options. It is implemented using lxml's robust HTMLParser.

  - param allow  
    a single regular expression (or list of regular expressions) that the (absolute) urls must match in order to be extracted. If not given (or empty), it will match all links.

  - type allow  
    str or list

  - param deny  
    a single regular expression (or list of regular expressions) that the (absolute) urls must match in order to be excluded (i.e. not extracted). It has precedence over the `allow` parameter. If not given (or empty) it won't exclude any links.

  - type deny  
    str or list

  - param allow\_domains  
    a single value or a list of string containing domains which will be considered for extracting the links

  - type allow\_domains  
    str or list

  - param deny\_domains  
    a single value or a list of strings containing domains which won't be considered for extracting the links

  - type deny\_domains  
    str or list

  - param deny\_extensions  
    a single value or list of strings containing extensions that should be ignored when extracting links. If not given, it will default to <span class="title-ref">scrapy.linkextractors.IGNORED\_EXTENSIONS</span>.
    
    <div class="versionchanged">
    
    2.0 <span class="title-ref">\~scrapy.linkextractors.IGNORED\_EXTENSIONS</span> now includes `7z`, `7zip`, `apk`, `bz2`, `cdr`, `dmg`, `ico`, `iso`, `tar`, `tar.gz`, `webm`, and `xz`.
    
    </div>

  - type deny\_extensions  
    list

  - param restrict\_xpaths  
    is an XPath (or list of XPath's) which defines regions inside the response where links should be extracted from. If given, only the text selected by those XPath will be scanned for links.

  - type restrict\_xpaths  
    str or list

  - param restrict\_css  
    a CSS selector (or list of selectors) which defines regions inside the response where links should be extracted from. Has the same behaviour as `restrict_xpaths`.

  - type restrict\_css  
    str or list

  - param restrict\_text  
    a single regular expression (or list of regular expressions) that the link's text must match in order to be extracted. If not given (or empty), it will match all links. If a list of regular expressions is given, the link will be extracted if it matches at least one.

  - type restrict\_text  
    str or list

  - param tags  
    a tag or a list of tags to consider when extracting links. Defaults to `('a', 'area')`.

  - type tags  
    str or list

  - param attrs  
    an attribute or list of attributes which should be considered when looking for links to extract (only for those tags specified in the `tags` parameter). Defaults to `('href',)`

  - type attrs  
    list

  - param canonicalize  
    canonicalize each extracted url (using w3lib.url.canonicalize\_url). Defaults to `False`. Note that canonicalize\_url is meant for duplicate checking; it can change the URL visible at server side, so the response can be different for requests with canonicalized and raw URLs. If you're using LinkExtractor to follow links it is more robust to keep the default `canonicalize=False`.

  - type canonicalize  
    bool

  - param unique  
    whether duplicate filtering should be applied to extracted links.

  - type unique  
    bool

  - param process\_value  
    a function which receives each value extracted from the tag and attributes scanned and can modify the value and return a new one, or return `None` to ignore the link altogether. If not given, `process_value` defaults to `lambda x: x`.
    
    For example, to extract links from this code:
    
    ``` html
    <a href="javascript:goToPage('../other/page.html'); return false">Link text</a>
    ```
    
    You can use the following function in `process_value`:
    
      - \`\`\`python
        
          - def process\_value(value):  
            m = re.search(r"<javascript:goToPage\>('(.\*?)'", value) if m: return m.group(1)

  - type process\_value  
    collections.abc.Callable

  - param strip  
    whether to strip whitespaces from extracted attributes. According to HTML5 standard, leading and trailing whitespaces must be stripped from `href` attributes of `<a>`, `<area>` and many other elements, `src` attribute of `<img>`, `<iframe>` elements, etc., so LinkExtractor strips space chars by default. Set `strip=False` to turn it off (e.g. if you're extracting urls from elements or attributes which allow leading/trailing whitespaces).

  - type strip  
    bool

<div class="automethod">

extract\_links

</div>

</div>

Link \`\`\` ----

<div class="module" data-synopsis="Link from link extractors">

scrapy.link

</div>

<div class="autoclass">

Link

</div>

---

loaders.md

---

# Item Loaders

<div class="module" data-synopsis="Item Loader class">

scrapy.loader

</div>

Item Loaders provide a convenient mechanism for populating scraped \[items \<topics-items\>\](\#items \<topics-items\>). Even though items can be populated directly, Item Loaders provide a much more convenient API for populating them from a scraping process, by automating some common tasks like parsing the raw extracted data before assigning it.

In other words, \[items \<topics-items\>\](\#items-\<topics-items\>) provide the *container* of scraped data, while Item Loaders provide the mechanism for *populating* that container.

Item Loaders are designed to provide a flexible, efficient and easy mechanism for extending and overriding different field parsing rules, either by spider, or by source format (HTML, XML, etc) without becoming a nightmare to maintain.

<div class="note">

<div class="title">

Note

</div>

Item Loaders are an extension of the [itemloaders](https://itemloaders.readthedocs.io/en/latest/) library that make it easier to work with Scrapy by adding support for \[responses \<topics-request-response\>\](\#responses-\<topics-request-response\>).

</div>

## Using Item Loaders to populate items

To use an Item Loader, you must first instantiate it. You can either instantiate it with an \[item object \<topics-items\>\](\#item-object-\<topics-items\>) or without one, in which case an \[item object \<topics-items\>\](\#item-object-\<topics-items\>) is automatically created in the Item Loader `__init__` method using the \[item \<topics-items\>\](\#item-\<topics-items\>) class specified in the <span class="title-ref">ItemLoader.default\_item\_class</span> attribute.

Then, you start collecting values into the Item Loader, typically using \[Selectors \<topics-selectors\>\](\#selectors-\<topics-selectors\>). You can add more than one value to the same item field; the Item Loader will know how to "join" those values later using a proper processing function.

<div class="note">

<div class="title">

Note

</div>

Collected data is internally stored as lists, allowing to add several values to the same field. If an `item` argument is passed when creating a loader, each of the item's values will be stored as-is if it's already an iterable, or wrapped with a list if it's a single value.

</div>

Here is a typical Item Loader usage in a \[Spider \<topics-spiders\>\](\#spider-\<topics-spiders\>), using the \[Product item \<topics-items-declaring\>\](\#product-item-\<topics-items-declaring\>) declared in the \[Items chapter \<topics-items\>\](\#items chapter-\<topics-items\>):

`` `python     from scrapy.loader import ItemLoader     from myproject.items import Product       def parse(self, response):         l = ItemLoader(item=Product(), response=response)         l.add_xpath("name", '//div[@class="product_name"]')         l.add_xpath("name", '//div[@class="product_title"]')         l.add_xpath("price", '//p[@id="price"]')         l.add_css("stock", "p#stock")         l.add_value("last_updated", "today")  # you can also use literal values         return l.load_item()  By quickly looking at that code, we can see the ``name`field is being`\` extracted from two different XPath locations in the page:

1.  `//div[@class="product_name"]`
2.  `//div[@class="product_title"]`

In other words, data is being collected by extracting it from two XPath locations, using the <span class="title-ref">\~ItemLoader.add\_xpath</span> method. This is the data that will be assigned to the `name` field later.

Afterwards, similar calls are used for `price` and `stock` fields (the latter using a CSS selector with the <span class="title-ref">\~ItemLoader.add\_css</span> method), and finally the `last_update` field is populated directly with a literal value (`today`) using a different method: <span class="title-ref">\~ItemLoader.add\_value</span>.

Finally, when all data is collected, the <span class="title-ref">ItemLoader.load\_item</span> method is called which actually returns the item populated with the data previously extracted and collected with the <span class="title-ref">\~ItemLoader.add\_xpath</span>, <span class="title-ref">\~ItemLoader.add\_css</span>, and <span class="title-ref">\~ItemLoader.add\_value</span> calls.

## Working with dataclass items

By default, \[dataclass items \<dataclass-items\>\](\#dataclass-items-\<dataclass-items\>) require all fields to be passed when created. This could be an issue when using dataclass items with item loaders: unless a pre-populated item is passed to the loader, fields will be populated incrementally using the loader's <span class="title-ref">\~ItemLoader.add\_xpath</span>, <span class="title-ref">\~ItemLoader.add\_css</span> and <span class="title-ref">\~ItemLoader.add\_value</span> methods.

One approach to overcome this is to define items using the <span class="title-ref">\~dataclasses.field</span> function, with a `default` argument:

`` `python     from dataclasses import dataclass, field     from typing import Optional       @dataclass     class InventoryItem:         name: Optional[str] = field(default=None)         price: Optional[float] = field(default=None)         stock: Optional[int] = field(default=None)   .. _topics-loaders-processors:  Input and Output processors ``\` ===========================

An Item Loader contains one input processor and one output processor for each (item) field. The input processor processes the extracted data as soon as it's received (through the <span class="title-ref">\~ItemLoader.add\_xpath</span>, <span class="title-ref">\~ItemLoader.add\_css</span> or <span class="title-ref">\~ItemLoader.add\_value</span> methods) and the result of the input processor is collected and kept inside the ItemLoader. After collecting all data, the <span class="title-ref">ItemLoader.load\_item</span> method is called to populate and get the populated \[item object \<topics-items\>\](\#item-object-\<topics-items\>). That's when the output processor is called with the data previously collected (and processed using the input processor). The result of the output processor is the final value that gets assigned to the item.

Let's see an example to illustrate how the input and output processors are called for a particular field (the same applies for any other field):

`` `python     l = ItemLoader(Product(), some_selector)     l.add_xpath("name", xpath1)  # (1)     l.add_xpath("name", xpath2)  # (2)     l.add_css("name", css)  # (3)     l.add_value("name", "test")  # (4)     return l.load_item()  # (5)  So what happens is:  1. Data from ``xpath1`is extracted, and passed through the *input processor* of    the`name`field. The result of the input processor is collected and kept in    the Item Loader (but not yet assigned to the item).  2. Data from`xpath2`is extracted, and passed through the same *input    processor* used in (1). The result of the input processor is appended to the    data collected in (1) (if any).  3. This case is similar to the previous ones, except that the data is extracted    from the`css`CSS selector, and passed through the same *input    processor* used in (1) and (2). The result of the input processor is appended to the    data collected in (1) and (2) (if any).  4. This case is also similar to the previous ones, except that the value to be    collected is assigned directly, instead of being extracted from a XPath    expression or a CSS selector.    However, the value is still passed through the input processors. In this    case, since the value is not iterable it is converted to an iterable of a    single element before passing it to the input processor, because input    processor always receive iterables.  5. The data collected in steps (1), (2), (3) and (4) is passed through    the *output processor* of the`name`field.    The result of the output processor is the value assigned to the`name`field in the item.  It's worth noticing that processors are just callable objects, which are called`\` with the data to be parsed, and return a parsed value. So you can use any function as input or output processor. The only requirement is that they must accept one (and only one) positional argument, which will be an iterable.

<div class="versionchanged">

2.0 Processors no longer need to be methods.

</div>

<div class="note">

<div class="title">

Note

</div>

Both input and output processors must receive an iterable as their first argument. The output of those functions can be anything. The result of input processors will be appended to an internal list (in the Loader) containing the collected values (for that field). The result of the output processors is the value that will be finally assigned to the item.

</div>

The other thing you need to keep in mind is that the values returned by input processors are collected internally (in lists) and then passed to output processors to populate the fields.

Last, but not least, [itemloaders](https://itemloaders.readthedocs.io/en/latest/) comes with some \[commonly used processors \<itemloaders:built-in-processors\>\](\#commonly-used processors-\<itemloaders:built-in-processors\>) built-in for convenience.

## Declaring Item Loaders

Item Loaders are declared using a class definition syntax. Here is an example:

`` `python     from itemloaders.processors import TakeFirst, MapCompose, Join     from scrapy.loader import ItemLoader       class ProductLoader(ItemLoader):         default_output_processor = TakeFirst()          name_in = MapCompose(str.title)         name_out = Join()          price_in = MapCompose(str.strip)          # ...  As you can see, input processors are declared using the ``\_in`suffix while`<span class="title-ref"> output processors are declared using the </span><span class="title-ref">\_out</span><span class="title-ref"> suffix. And you can also declare a default input/output processors using the \`ItemLoader.default\_input\_processor</span> and <span class="title-ref">ItemLoader.default\_output\_processor</span> attributes.

## Declaring Input and Output Processors

As seen in the previous section, input and output processors can be declared in the Item Loader definition, and it's very common to declare input processors this way. However, there is one more place where you can specify the input and output processors to use: in the \[Item Field \<topics-items-fields\>\](\#item-field-\<topics-items-fields\>) metadata. Here is an example:

  - `` `python     import scrapy     from itemloaders.processors import Join, MapCompose, TakeFirst     from w3lib.html import remove_tags       def filter_price(value):         if value.isdigit():             return value       class Product(scrapy.Item):         name = scrapy.Field(             input_processor=MapCompose(remove_tags),             output_processor=Join(),         )         price = scrapy.Field(             input_processor=MapCompose(remove_tags, filter_price),             output_processor=TakeFirst(),         )   .. code-block:: pycon      >>> from scrapy.loader import ItemLoader     >>> il = ItemLoader(item=Product())     >>> il.add_value("name", ["Welcome to my", "<strong>website</strong>"])     >>> il.add_value("price", ["&euro;", "<span>1000</span>"])     >>> il.load_item()     {'name': 'Welcome to my website', 'price': '1000'}  The precedence order, for both input and output processors, is as follows:  1. Item Loader field-specific attributes: ``field\_in`and`field\_out`(most    precedence)`<span class="title-ref"> 2. Field metadata (</span><span class="title-ref">input\_processor</span><span class="title-ref"> and </span><span class="title-ref">output\_processor</span><span class="title-ref"> key) 3. Item Loader defaults: \`ItemLoader.default\_input\_processor</span> and  
    <span class="title-ref">ItemLoader.default\_output\_processor</span> (least precedence)

See also: \[topics-loaders-extending\](\#topics-loaders-extending).

## Item Loader Context

The Item Loader Context is a dict of arbitrary key/values which is shared among all input and output processors in the Item Loader. It can be passed when declaring, instantiating or using Item Loader. They are used to modify the behaviour of the input/output processors.

For example, suppose you have a function `parse_length` which receives a text value and extracts a length from it:

`` `python     def parse_length(text, loader_context):         unit = loader_context.get("unit", "m")         # ... length parsing code goes here ...         return parsed_length  By accepting a ``loader\_context`argument the function is explicitly telling`<span class="title-ref"> the Item Loader that it's able to receive an Item Loader context, so the Item Loader passes the currently active context when calling it, and the processor function (</span><span class="title-ref">parse\_length</span>\` in this case) can thus use them.

There are several ways to modify Item Loader context values:

1.  By modifying the currently active Item Loader context (<span class="title-ref">\~ItemLoader.context</span> attribute):
    
      - \`\`\`python  
        loader = ItemLoader(product) loader.context\["unit"\] = "cm"

2.  On Item Loader instantiation (the keyword arguments of Item Loader `__init__` method are stored in the Item Loader context):
    
    ``` python
    loader = ItemLoader(product, unit="cm")
    ```

3.  On Item Loader declaration, for those input/output processors that support instantiating them with an Item Loader context. <span class="title-ref">\~processor.MapCompose</span> is one of them:
    
    ``` python
    class ProductLoader(ItemLoader):
        length_out = MapCompose(parse_length, unit="cm")
    ```

ItemLoader objects `` ` ==================  .. autoclass:: scrapy.loader.ItemLoader     :members:     :inherited-members:  .. _topics-loaders-nested:  Nested Loaders ==============  When parsing related values from a subsection of a document, it can be useful to create nested loaders.  Imagine you're extracting details from a footer of a page that looks something like:  Example::      <footer>         <a class="social" href="https://facebook.com/whatever">Like Us</a>         <a class="social" href="https://twitter.com/whatever">Follow Us</a>         <a class="email" href="mailto:whatever@example.com">Email Us</a>     </footer>  Without nested loaders, you need to specify the full xpath (or css) for each value that you wish to extract.  Example: ``\`python loader = ItemLoader(item=Item()) \# load stuff not in the footer loader.add\_xpath("social", '//footer/a\[@class = "social"\]/@href') loader.add\_xpath("email", '//footer/a\[@class = "email"\]/@href') loader.load\_item()

Instead, you can create a nested loader with the footer selector and add values `` ` relative to the footer.  The functionality is the same but you avoid repeating the footer selector.  Example: ``\`python loader = ItemLoader(item=Item()) \# load stuff not in the footer footer\_loader = loader.nested\_xpath("//footer") footer\_loader.add\_xpath("social", 'a\[@class = "social"\]/@href') footer\_loader.add\_xpath("email", 'a\[@class = "email"\]/@href') \# no need to call footer\_loader.load\_item() loader.load\_item()

You can nest loaders arbitrarily and they work with either xpath or css selectors. `` ` As a general guideline, use nested loaders when they make your code simpler but do not go overboard with nesting or your parser can become difficult to read.  .. _topics-loaders-extending:  Reusing and extending Item Loaders ==================================  As your project grows bigger and acquires more and more spiders, maintenance becomes a fundamental problem, especially when you have to deal with many different parsing rules for each spider, having a lot of exceptions, but also wanting to reuse the common processors.  Item Loaders are designed to ease the maintenance burden of parsing rules, without losing flexibility and, at the same time, providing a convenient mechanism for extending and overriding them. For this reason Item Loaders support traditional Python class inheritance for dealing with differences of specific spiders (or groups of spiders).  Suppose, for example, that some particular site encloses their product names in three dashes (e.g. ``---Plasma TV---`) and you don't want to end up scraping those dashes in the final product names.  Here's how you can remove those dashes by reusing and extending the default Product Item Loader (`ProductLoader`):`\`python from itemloaders.processors import MapCompose from myproject.ItemLoaders import ProductLoader

>   - def strip\_dashes(x):  
>     return x.strip("-")
> 
>   - class SiteSpecificLoader(ProductLoader):  
>     name\_in = MapCompose(strip\_dashes, ProductLoader.name\_in)

Another case where extending Item Loaders can be very helpful is when you have `` ` multiple source formats, for example XML and HTML. In the XML version you may want to remove ``CDATA`occurrences. Here's an example of how to do it:`\`python from itemloaders.processors import MapCompose from myproject.ItemLoaders import ProductLoader from myproject.utils.xml import remove\_cdata

>   - class XmlProductLoader(ProductLoader):  
>     name\_in = MapCompose(remove\_cdata, ProductLoader.name\_in)

And that's how you typically extend input processors.

As for output processors, it is more common to declare them in the field metadata, \`\`\` as they usually depend only on the field and not on each specific site parsing rule (as input processors do). See also: \[topics-loaders-processors-declaring\](\#topics-loaders-processors-declaring).

There are many other possible ways to extend, inherit and override your Item Loaders, and different Item Loaders hierarchies may fit better for different projects. Scrapy only provides the mechanism; it doesn't impose any specific organization of your Loaders collection - that's up to you and your project's needs.

---

logging.md

---

# Logging

<div class="note">

<div class="title">

Note

</div>

`scrapy.log` has been deprecated alongside its functions in favor of explicit calls to the Python standard logging. Keep reading to learn more about the new logging system.

</div>

Scrapy uses `logging` for event logging. We'll provide some simple examples to get you started, but for more advanced use-cases it's strongly suggested to read thoroughly its documentation.

Logging works out of the box, and can be configured to some extent with the Scrapy settings listed in \[topics-logging-settings\](\#topics-logging-settings).

Scrapy calls <span class="title-ref">scrapy.utils.log.configure\_logging</span> to set some reasonable defaults and handle those settings in \[topics-logging-settings\](\#topics-logging-settings) when running commands, so it's recommended to manually call it if you're running Scrapy from scripts as described in \[run-from-script\](\#run-from-script).

## Log levels

Python's builtin logging defines 5 different levels to indicate the severity of a given log message. Here are the standard ones, listed in decreasing order:

1.  `logging.CRITICAL` - for critical errors (highest severity)
2.  `logging.ERROR` - for regular errors
3.  `logging.WARNING` - for warning messages
4.  `logging.INFO` - for informational messages
5.  `logging.DEBUG` - for debugging messages (lowest severity)

## How to log messages

Here's a quick example of how to log a message using the `logging.WARNING` level:

`` `python     import logging      logging.warning("This is a warning")  There are shortcuts for issuing log messages on any of the standard 5 levels, ``<span class="title-ref"> and there's also a general </span><span class="title-ref">logging.log</span>\` method which takes a given level as argument. If needed, the last example could be rewritten as:

`` `python     import logging      logging.log(logging.WARNING, "This is a warning")  On top of that, you can create different "loggers" to encapsulate messages. (For ``\` example, a common practice is to create different loggers for every module). These loggers can be configured independently, and they allow hierarchical constructions.

The previous examples use the root logger behind the scenes, which is a top level logger where all messages are propagated to (unless otherwise specified). Using `logging` helpers is merely a shortcut for getting the root logger explicitly, so this is also an equivalent of the last snippets:

`` `python     import logging      logger = logging.getLogger()     logger.warning("This is a warning")  You can use a different logger just by getting its name with the ``<span class="title-ref"> </span><span class="title-ref">logging.getLogger</span>\` function:

`` `python     import logging      logger = logging.getLogger("mycustomlogger")     logger.warning("This is a warning")  Finally, you can ensure having a custom logger for any module you're working on ``<span class="title-ref"> by using the </span><span class="title-ref">\_\_name\_\_</span>\` variable, which is populated with current module's path:

`` `python     import logging      logger = logging.getLogger(__name__)     logger.warning("This is a warning")  .. seealso::      Module logging, [HowTo <howto/logging>](HowTo <howto/logging>.md)         Basic Logging Tutorial      Module logging, [Loggers <logger>](#loggers-<logger>)         Further documentation on loggers  .. _topics-logging-from-spiders:  Logging from Spiders ``\` ====================

Scrapy provides a <span class="title-ref">\~scrapy.Spider.logger</span> within each Spider instance, which can be accessed and used like this:

`` `python     import scrapy       class MySpider(scrapy.Spider):         name = "myspider"         start_urls = ["https://scrapy.org"]          def parse(self, response):             self.logger.info("Parse function called on %s", response.url)  That logger is created using the Spider's name, but you can use any custom ``\` Python logger you want. For example:

`` `python     import logging     import scrapy      logger = logging.getLogger("mycustomlogger")       class MySpider(scrapy.Spider):         name = "myspider"         start_urls = ["https://scrapy.org"]          def parse(self, response):             logger.info("Parse function called on %s", response.url)  .. _topics-logging-configuration:  Logging configuration ``\` =====================

Loggers on their own don't manage how messages sent through them are displayed. For this task, different "handlers" can be attached to any logger instance and they will redirect those messages to appropriate destinations, such as the standard output, files, emails, etc.

By default, Scrapy sets and configures a handler for the root logger, based on the settings below.

### Logging settings

These settings can be used to configure the logging:

  - `LOG_FILE`
  - `LOG_FILE_APPEND`
  - `LOG_ENABLED`
  - `LOG_ENCODING`
  - `LOG_LEVEL`
  - `LOG_FORMAT`
  - `LOG_DATEFORMAT`
  - `LOG_STDOUT`
  - `LOG_SHORT_NAMES`

The first couple of settings define a destination for log messages. If `LOG_FILE` is set, messages sent through the root logger will be redirected to a file named `LOG_FILE` with encoding `LOG_ENCODING`. If unset and `LOG_ENABLED` is `True`, log messages will be displayed on the standard error. If `LOG_FILE` is set and `LOG_FILE_APPEND` is `False`, the file will be overwritten (discarding the output from previous runs, if any). Lastly, if `LOG_ENABLED` is `False`, there won't be any visible log output.

`LOG_LEVEL` determines the minimum level of severity to display, those messages with lower severity will be filtered out. It ranges through the possible levels listed in \[topics-logging-levels\](\#topics-logging-levels).

`LOG_FORMAT` and `LOG_DATEFORMAT` specify formatting strings used as layouts for all messages. Those strings can contain any placeholders listed in \[logging's logrecord attributes docs \<logrecord-attributes\>\](\#logging's-logrecord-attributes-docs-\<logrecord-attributes\>) and \[datetime's strftime and strptime directives \<strftime-strptime-behavior\>\](\#datetime's-strftime-and-strptime-directives-\<strftime-strptime-behavior\>) respectively.

If `LOG_SHORT_NAMES` is set, then the logs will not display the Scrapy component that prints the log. It is unset by default, hence logs contain the Scrapy component responsible for that log output.

### Command-line options

There are command-line arguments, available for all commands, that you can use to override some of the Scrapy settings regarding logging.

  -   - `--logfile FILE`  
        Overrides `LOG_FILE`

  -   - `--loglevel/-L LEVEL`  
        Overrides `LOG_LEVEL`

  -   - `--nolog`  
        Sets `LOG_ENABLED` to `False`

<div class="seealso">

  - Module `logging.handlers`  
    Further documentation on available handlers

</div>

### Custom Log Formats

A custom log format can be set for different actions by extending <span class="title-ref">\~scrapy.logformatter.LogFormatter</span> class and making `LOG_FORMATTER` point to your new class.

<div class="autoclass" data-members="">

scrapy.logformatter.LogFormatter

</div>

### Advanced customization

Because Scrapy uses stdlib logging module, you can customize logging using all features of stdlib logging.

For example, let's say you're scraping a website which returns many HTTP 404 and 500 responses, and you want to hide all messages like this:

    2016-12-16 22:00:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring
    response <500 https://quotes.toscrape.com/page/1-34/>: HTTP status code
    is not handled or not allowed

The first thing to note is a logger name - it is in brackets: `[scrapy.spidermiddlewares.httperror]`. If you get just `[scrapy]` then `LOG_SHORT_NAMES` is likely set to True; set it to False and re-run the crawl.

Next, we can see that the message has INFO level. To hide it we should set logging level for `scrapy.spidermiddlewares.httperror` higher than INFO; next level after INFO is WARNING. It could be done e.g. in the spider's `__init__` method:

`` `python     import logging     import scrapy       class MySpider(scrapy.Spider):         # ...         def __init__(self, *args, **kwargs):             logger = logging.getLogger("scrapy.spidermiddlewares.httperror")             logger.setLevel(logging.WARNING)             super().__init__(*args, **kwargs)  If you run this spider again then INFO messages from ``<span class="title-ref"> </span><span class="title-ref">scrapy.spidermiddlewares.httperror</span>\` logger will be gone.

You can also filter log records by <span class="title-ref">\~logging.LogRecord</span> data. For example, you can filter log records by message content using a substring or a regular expression. Create a <span class="title-ref">logging.Filter</span> subclass and equip it with a regular expression pattern to filter out unwanted messages:

`` `python     import logging     import re       class ContentFilter(logging.Filter):         def filter(self, record):             match = re.search(r"\d{3} [Ee]rror, retrying", record.message)             if match:                 return False  A project-level filter may be attached to the root ``\` handler created by Scrapy, this is a wieldy way to filter all loggers in different parts of the project (middlewares, spider, etc.):

`` `python  import logging  import scrapy    class MySpider(scrapy.Spider):      # ...      def __init__(self, *args, **kwargs):          for handler in logging.root.handlers:              handler.addFilter(ContentFilter())  Alternatively, you may choose a specific logger ``\` and hide it without affecting other loggers:

`` `python     import logging     import scrapy       class MySpider(scrapy.Spider):         # ...         def __init__(self, *args, **kwargs):             logger = logging.getLogger("my_logger")             logger.addFilter(ContentFilter())   scrapy.utils.log module ``\` =======================

<div class="module" data-synopsis="Logging utils">

scrapy.utils.log

</div>

<div class="autofunction">

configure\_logging

`configure_logging` is automatically called when using Scrapy commands or <span class="title-ref">\~scrapy.crawler.CrawlerProcess</span>, but needs to be called explicitly when running custom scripts using <span class="title-ref">\~scrapy.crawler.CrawlerRunner</span>. In that case, its usage is not required but it's recommended.

Another option when running custom scripts is to manually configure the logging. To do this you can use <span class="title-ref">logging.basicConfig</span> to set a basic root handler.

Note that <span class="title-ref">\~scrapy.crawler.CrawlerProcess</span> automatically calls `configure_logging`, so it is recommended to only use <span class="title-ref">logging.basicConfig</span> together with <span class="title-ref">\~scrapy.crawler.CrawlerRunner</span>.

This is an example on how to redirect `INFO` or higher messages to a file:

\`\`\`python import logging

  - logging.basicConfig(  
    filename="log.txt", format="%(levelname)s: %(message)s", level=logging.INFO

)

</div>

Refer to \[run-from-script\](\#run-from-script) for more details about using Scrapy this way. \`\`\`

---

media-pipeline.md

---

# Downloading and processing files and images

<div class="currentmodule">

scrapy.pipelines.images

</div>

Scrapy provides reusable \[item pipelines \</topics/item-pipeline\>\](item pipelines \</topics/item-pipeline\>.md) for downloading files attached to a particular item (for example, when you scrape products and also want to download their images locally). These pipelines share a bit of functionality and structure (we refer to them as media pipelines), but typically you'll either use the Files Pipeline or the Images Pipeline.

Both pipelines implement these features:

  - Avoid re-downloading media that was downloaded recently
  - Specifying where to store the media (filesystem directory, FTP server, Amazon S3 bucket, Google Cloud Storage bucket)

The Images Pipeline has a few extra functions for processing images:

  - Convert all downloaded images to a common format (JPG) and mode (RGB)
  - Thumbnail generation
  - Check images width/height to make sure they meet a minimum constraint

The pipelines also keep an internal queue of those media URLs which are currently being scheduled for download, and connect those responses that arrive containing the same media to that queue. This avoids downloading the same media more than once when it's shared by several items.

## Using the Files Pipeline

The typical workflow, when using the <span class="title-ref">FilesPipeline</span> goes like this:

1.  In a Spider, you scrape an item and put the URLs of the desired into a `file_urls` field.
2.  The item is returned from the spider and goes to the item pipeline.
3.  When the item reaches the <span class="title-ref">FilesPipeline</span>, the URLs in the `file_urls` field are scheduled for download using the standard Scrapy scheduler and downloader (which means the scheduler and downloader middlewares are reused), but with a higher priority, processing them before other pages are scraped. The item remains "locked" at that particular pipeline stage until the files have finish downloading (or fail for some reason).
4.  When the files are downloaded, another field (`files`) will be populated with the results. This field will contain a list of dicts with information about the downloaded files, such as the downloaded path, the original scraped url (taken from the `file_urls` field), the file checksum and the file status. The files in the list of the `files` field will retain the same order of the original `file_urls` field. If some file failed downloading, an error will be logged and the file won't be present in the `files` field.

## Using the Images Pipeline

Using the <span class="title-ref">ImagesPipeline</span> is a lot like using the <span class="title-ref">FilesPipeline</span>, except the default field names used are different: you use `image_urls` for the image URLs of an item and it will populate an `images` field for the information about the downloaded images.

The advantage of using the <span class="title-ref">ImagesPipeline</span> for image files is that you can configure some extra functions like generating thumbnails and filtering the images based on their size.

The Images Pipeline requires [Pillow](https://github.com/python-pillow/Pillow) 7.1.0 or greater. It is used for thumbnailing and normalizing images to JPEG/RGB format.

## Enabling your Media Pipeline

<div class="setting">

IMAGES\_STORE

</div>

<div class="setting">

FILES\_STORE

</div>

To enable your media pipeline you must first add it to your project `ITEM_PIPELINES` setting.

For Images Pipeline, use:

`` `python     ITEM_PIPELINES = {"scrapy.pipelines.images.ImagesPipeline": 1}  For Files Pipeline, use:  .. code-block:: python      ITEM_PIPELINES = {"scrapy.pipelines.files.FilesPipeline": 1}  .. note::     You can also use both the Files and Images Pipeline at the same time.   Then, configure the target storage setting to a valid value that will be used ``<span class="title-ref"> for storing the downloaded images. Otherwise the pipeline will remain disabled, even if you include it in the :setting:\`ITEM\_PIPELINES</span> setting.

For the Files Pipeline, set the `FILES_STORE` setting:

`` `python    FILES_STORE = "/path/to/valid/dir"  For the Images Pipeline, set the :setting:`IMAGES_STORE` setting:  .. code-block:: python     IMAGES_STORE = "/path/to/valid/dir"  .. _topics-file-naming:  File Naming ``\` ===========

### Default File Naming

By default, files are stored using an [SHA-1 hash](https://en.wikipedia.org/wiki/SHA_hash_functions) of their URLs for the file names.

For example, the following image URL:

    http://www.example.com/image.jpg

Whose `SHA-1 hash` is:

    3afec3b4765f8f0a07b78f98c07b83f013567a0a

Will be downloaded and stored using your chosen \[storage method \<topics-supported-storage\>\](\#storage-method-\<topics-supported-storage\>) and the following file name:

    3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg

### Custom File Naming

You may wish to use a different calculated file name for saved files. For example, classifying an image by including meta in the file name.

Customize file names by overriding the `file_path` method of your media pipeline.

For example, an image pipeline with image URL:

    http://www.example.com/product/images/large/front/0000000004166

Can be processed into a file name with a condensed hash and the perspective `front`:

    00b08510e4_front.jpg

By overriding `file_path` like this:

`` `python   import hashlib     def file_path(self, request, response=None, info=None, *, item=None):       image_url_hash = hashlib.shake_256(request.url.encode()).hexdigest(5)       image_perspective = request.url.split("/")[-2]       image_filename = f"{image_url_hash}_{image_perspective}.jpg"        return image_filename  .. warning::   If your custom file name scheme relies on meta data that can vary between   scrapes it may lead to unexpected re-downloading of existing media using   new file names.    For example, if your custom file name scheme uses a product title and the   site changes an item's product title between scrapes, Scrapy will re-download   the same media using updated file names.  For more information about the ``file\_path`method, see [topics-media-pipeline-override](#topics-media-pipeline-override).  .. _topics-supported-storage:  Supported Storage`\` =================

### File system storage

File system storage will save files to the following path:

    <IMAGES_STORE>/full/<FILE_NAME>

Where:

  - `<IMAGES_STORE>` is the directory defined in `IMAGES_STORE` setting for the Images Pipeline.
  - `full` is a sub-directory to separate full images from thumbnails (if used). For more info see \[topics-images-thumbnails\](\#topics-images-thumbnails).
  - `<FILE_NAME>` is the file name assigned to the file. For more info see \[topics-file-naming\](\#topics-file-naming).

### FTP server storage

<div class="versionadded">

2.0

</div>

`FILES_STORE` and `IMAGES_STORE` can point to an FTP server. Scrapy will automatically upload the files to the server.

`FILES_STORE` and `IMAGES_STORE` should be written in one of the following forms:

    ftp://username:password@address:port/path
    ftp://address:port/path

If `username` and `password` are not provided, they are taken from the `FTP_USER` and `FTP_PASSWORD` settings respectively.

FTP supports two different connection modes: active or passive. Scrapy uses the passive connection mode by default. To use the active connection mode instead, set the `FEED_STORAGE_FTP_ACTIVE` setting to `True`.

### Amazon S3 storage

<div class="setting">

FILES\_STORE\_S3\_ACL

</div>

<div class="setting">

IMAGES\_STORE\_S3\_ACL

</div>

If [botocore](https://github.com/boto/botocore) \>= 1.4.87 is installed, `FILES_STORE` and `IMAGES_STORE` can represent an Amazon S3 bucket. Scrapy will automatically upload the files to the bucket.

For example, this is a valid `IMAGES_STORE` value:

`` `python     IMAGES_STORE = "s3://bucket/images"  You can modify the Access Control List (ACL) policy used for the stored files, ``<span class="title-ref"> which is defined by the :setting:\`FILES\_STORE\_S3\_ACL</span> and `IMAGES_STORE_S3_ACL` settings. By default, the ACL is set to `private`. To make the files publicly available use the `public-read` policy:

`` `python     IMAGES_STORE_S3_ACL = "public-read"  For more information, see `canned ACLs`_ in the Amazon S3 Developer Guide.  You can also use other S3-like storages. Storages like self-hosted `Minio`_ or ``<span class="title-ref"> \`Zenko CloudServer</span>\_. All you need to do is set endpoint option in you Scrapy settings:

`` `python     AWS_ENDPOINT_URL = "http://minio.example.com:9000"  For self-hosting you also might feel the need not to use SSL and not to verify SSL connection:  .. code-block:: python      AWS_USE_SSL = False  # or True (None by default)     AWS_VERIFY = False  # or True (None by default) ``\` .. \_canned ACLs: <https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#canned-acl> .. \_Minio: <https://github.com/minio/minio> .. \_Zenko CloudServer: <https://www.zenko.io/cloudserver/>

### Google Cloud Storage

<div class="setting">

FILES\_STORE\_GCS\_ACL

</div>

<div class="setting">

IMAGES\_STORE\_GCS\_ACL

</div>

`FILES_STORE` and `IMAGES_STORE` can represent a Google Cloud Storage bucket. Scrapy will automatically upload the files to the bucket. (requires [google-cloud-storage](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python) )

For example, these are valid `IMAGES_STORE` and `GCS_PROJECT_ID` settings:

`` `python     IMAGES_STORE = "gs://bucket/images/"     GCS_PROJECT_ID = "project_id"  For information about authentication, see this `documentation`_.    You can modify the Access Control List (ACL) policy used for the stored files, ``<span class="title-ref"> which is defined by the :setting:\`FILES\_STORE\_GCS\_ACL</span> and `IMAGES_STORE_GCS_ACL` settings. By default, the ACL is set to `''` (empty string) which means that Cloud Storage applies the bucket's default object ACL to the object. To make the files publicly available use the `publicRead` policy:

`` `python     IMAGES_STORE_GCS_ACL = "publicRead"  For more information, see `Predefined ACLs`_ in the Google Cloud Platform Developer Guide.    Usage example ``\` =============

<div class="setting">

FILES\_URLS\_FIELD

</div>

<div class="setting">

FILES\_RESULT\_FIELD

</div>

<div class="setting">

IMAGES\_URLS\_FIELD

</div>

<div class="setting">

IMAGES\_RESULT\_FIELD

</div>

In order to use a media pipeline, first \[enable it \<topics-media-pipeline-enabling\>\](\#enable-it \<topics-media-pipeline-enabling\>).

Then, if a spider returns an \[item object \<topics-items\>\](\#item-object-\<topics-items\>) with the URLs field (`file_urls` or `image_urls`, for the Files or Images Pipeline respectively), the pipeline will put the results under the respective field (`files` or `images`).

When using \[item types \<item-types\>\](\#item-types-\<item-types\>) for which fields are defined beforehand, you must define both the URLs field and the results field. For example, when using the images pipeline, items must define both the `image_urls` and the `images` field. For instance, using the <span class="title-ref">\~scrapy.Item</span> class:

`` `python     import scrapy       class MyItem(scrapy.Item):         # ... other item fields ...         image_urls = scrapy.Field()         images = scrapy.Field()  If you want to use another field name for the URLs key or for the results key, ``\` it is also possible to override it.

For the Files Pipeline, set `FILES_URLS_FIELD` and/or `FILES_RESULT_FIELD` settings:

`` `python     FILES_URLS_FIELD = "field_name_for_your_files_urls"     FILES_RESULT_FIELD = "field_name_for_your_processed_files"  For the Images Pipeline, set :setting:`IMAGES_URLS_FIELD` and/or ``<span class="title-ref"> :setting:\`IMAGES\_RESULT\_FIELD</span> settings:

`` `python     IMAGES_URLS_FIELD = "field_name_for_your_images_urls"     IMAGES_RESULT_FIELD = "field_name_for_your_processed_images"  If you need something more complex and want to override the custom pipeline ``\` behaviour, see \[topics-media-pipeline-override\](\#topics-media-pipeline-override).

If you have multiple image pipelines inheriting from ImagePipeline and you want to have different settings in different pipelines you can set setting keys preceded with uppercase name of your pipeline class. E.g. if your pipeline is called MyPipeline and you want to have custom IMAGES\_URLS\_FIELD you define setting MYPIPELINE\_IMAGES\_URLS\_FIELD and your custom settings will be used.

## Additional features

### File expiration

<div class="setting">

IMAGES\_EXPIRES

</div>

<div class="setting">

FILES\_EXPIRES

</div>

The Image Pipeline avoids downloading files that were downloaded recently. To adjust this retention delay use the `FILES_EXPIRES` setting (or `IMAGES_EXPIRES`, in case of Images Pipeline), which specifies the delay in number of days:

`` `python     # 120 days of delay for files expiration     FILES_EXPIRES = 120      # 30 days of delay for images expiration     IMAGES_EXPIRES = 30  The default value for both settings is 90 days.  If you have pipeline that subclasses FilesPipeline and you'd like to have ``\` different setting for it you can set setting keys preceded by uppercase class name. E.g. given pipeline class called MyPipeline you can set setting key:

> MYPIPELINE\_FILES\_EXPIRES = 180

and pipeline class MyPipeline will have expiration time set to 180.

The last modified time from the file is used to determine the age of the file in days, which is then compared to the set expiration time to determine if the file is expired.

### Thumbnail generation for images

The Images Pipeline can automatically create thumbnails of the downloaded images.

<div class="setting">

IMAGES\_THUMBS

</div>

In order to use this feature, you must set `IMAGES_THUMBS` to a dictionary where the keys are the thumbnail names and the values are their dimensions.

For example:

`` `python    IMAGES_THUMBS = {        "small": (50, 50),        "big": (270, 270),    }  When you use this feature, the Images Pipeline will create thumbnails of the ``\` each specified size with this format:

    <IMAGES_STORE>/thumbs/<size_name>/<image_id>.jpg

Where:

  - `<size_name>` is the one specified in the `IMAGES_THUMBS` dictionary keys (`small`, `big`, etc)
  - `<image_id>` is the [SHA-1 hash](https://en.wikipedia.org/wiki/SHA_hash_functions) of the image url

Example of image files stored using `small` and `big` thumbnail names:

    <IMAGES_STORE>/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
    <IMAGES_STORE>/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
    <IMAGES_STORE>/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg

The first one is the full image, as downloaded from the site.

### Filtering out small images

<div class="setting">

IMAGES\_MIN\_HEIGHT

</div>

<div class="setting">

IMAGES\_MIN\_WIDTH

</div>

When using the Images Pipeline, you can drop images which are too small, by specifying the minimum allowed size in the `IMAGES_MIN_HEIGHT` and `IMAGES_MIN_WIDTH` settings.

For example:

    IMAGES_MIN_HEIGHT = 110
    IMAGES_MIN_WIDTH = 110

<div class="note">

<div class="title">

Note

</div>

The size constraints don't affect thumbnail generation at all.

</div>

It is possible to set just one size constraint or both. When setting both of them, only images that satisfy both minimum sizes will be saved. For the above example, images of sizes (105 x 105) or (105 x 200) or (200 x 105) will all be dropped because at least one dimension is shorter than the constraint.

By default, there are no size constraints, so all images are processed.

### Allowing redirections

<div class="setting">

MEDIA\_ALLOW\_REDIRECTS

</div>

By default media pipelines ignore redirects, i.e. an HTTP redirection to a media file URL request will mean the media download is considered failed.

To handle media redirections, set this setting to `True`:

    MEDIA_ALLOW_REDIRECTS = True

## Extending the Media Pipelines

<div class="module" data-synopsis="Files Pipeline">

scrapy.pipelines.files

</div>

See here the methods that you can override in your custom Files Pipeline:

<div class="FilesPipeline">

<div class="method">

file\_path(self, request, response=None, info=None, \*, item=None)

This method is called once per downloaded item. It returns the download path of the file originating from the specified <span class="title-ref">response \<scrapy.http.Response\></span>.

In addition to `response`, this method receives the original <span class="title-ref">request \<scrapy.Request\></span>, <span class="title-ref">info \<scrapy.pipelines.media.MediaPipeline.SpiderInfo\></span> and <span class="title-ref">item \<scrapy.Item\></span>

You can override this method to customize the download path of each file.

For example, if file URLs end like regular paths (e.g. `https://example.com/a/b/c/foo.png`), you can use the following approach to download all files into the `files` folder with their original filenames (e.g. `files/foo.png`):

`` `python   from pathlib import PurePosixPath   from scrapy.utils.httpobj import urlparse_cached    from scrapy.pipelines.files import FilesPipeline     class MyFilesPipeline(FilesPipeline):       def file_path(self, request, response=None, info=None, *, item=None):           return "files/" + PurePosixPath(urlparse_cached(request).path).name  Similarly, you can use the ``item``to determine the file path based on some item  property.  By default the `file_path` method returns``full/\<request URL hash\>.\<extension\>\`\`.

<div class="versionadded">

2.4 The *item* parameter.

</div>

</div>

<div class="method">

FilesPipeline.get\_media\_requests(item, info)

As seen on the workflow, the pipeline will get the URLs of the images to download from the item. In order to do this, you can override the <span class="title-ref">\~get\_media\_requests</span> method and return a Request for each file URL:

``` python
from itemadapter import ItemAdapter

def get_media_requests(self, item, info):
    adapter = ItemAdapter(item)
    for file_url in adapter["file_urls"]:
        yield scrapy.Request(file_url)
```

Those requests will be processed by the pipeline and, when they have finished downloading, the results will be sent to the <span class="title-ref">\~item\_completed</span> method, as a list of 2-element tuples. Each tuple will contain `(success, file_info_or_error)` where:

  - `success` is a boolean which is `True` if the image was downloaded successfully or `False` if it failed for some reason
  - `file_info_or_error` is a dict containing the following keys (if success is `True`) or a <span class="title-ref">\~twisted.python.failure.Failure</span> if there was a problem.
      - `url` - the url where the file was downloaded from. This is the url of the request returned from the <span class="title-ref">\~get\_media\_requests</span> method.
    
      - `path` - the path (relative to `FILES_STORE`) where the file was stored
    
      - `checksum` - a [MD5 hash](https://en.wikipedia.org/wiki/MD5) of the image contents
    
      - `status` - the file status indication.
        
        <div class="versionadded">
        
        2.2
        
        </div>
        
        It can be one of the following:
        
          - `downloaded` - file was downloaded.
          - `uptodate` - file was not downloaded, as it was downloaded recently, according to the file expiration policy.
          - `cached` - file was already scheduled for download, by another item sharing the same file.

The list of tuples received by <span class="title-ref">\~item\_completed</span> is guaranteed to retain the same order of the requests returned from the <span class="title-ref">\~get\_media\_requests</span> method.

Here's a typical value of the `results` argument:

``` python
[
    (
        True,
        {
            "checksum": "2b00042f7481c7b056c4b410d28f33cf",
            "path": "full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg",
            "url": "http://www.example.com/files/product1.pdf",
            "status": "downloaded",
        },
    ),
    (False, Failure(...)),
]
```

By default the <span class="title-ref">get\_media\_requests</span> method returns `None` which means there are no files to download for the item.

</div>

<div class="method">

FilesPipeline.item\_completed(results, item, info)

The <span class="title-ref">FilesPipeline.item\_completed</span> method called when all file requests for a single item have completed (either finished downloading, or failed for some reason).

The <span class="title-ref">\~item\_completed</span> method must return the output that will be sent to subsequent item pipeline stages, so you must return (or drop) the item, as you would in any pipeline.

Here is an example of the <span class="title-ref">\~item\_completed</span> method where we store the downloaded file paths (passed in results) in the `file_paths` item field, and we drop the item if it doesn't contain any files:

``` python
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

def item_completed(self, results, item, info):
    file_paths = [x["path"] for ok, x in results if ok]
    if not file_paths:
        raise DropItem("Item contains no files")
    adapter = ItemAdapter(item)
    adapter["file_paths"] = file_paths
    return item
```

By default, the <span class="title-ref">item\_completed</span> method returns the item.

</div>

</div>

<div class="module" data-synopsis="Images Pipeline">

scrapy.pipelines.images

</div>

See here the methods that you can override in your custom Images Pipeline:

<div class="ImagesPipeline">

The <span class="title-ref">ImagesPipeline</span> is an extension of the <span class="title-ref">FilesPipeline</span>, customizing the field names and adding custom behavior for images.

<div class="method">

file\_path(self, request, response=None, info=None, \*, item=None)

This method is called once per downloaded item. It returns the download path of the file originating from the specified <span class="title-ref">response \<scrapy.http.Response\></span>.

In addition to `response`, this method receives the original <span class="title-ref">request \<scrapy.Request\></span>, <span class="title-ref">info \<scrapy.pipelines.media.MediaPipeline.SpiderInfo\></span> and <span class="title-ref">item \<scrapy.Item\></span>

You can override this method to customize the download path of each file.

For example, if file URLs end like regular paths (e.g. `https://example.com/a/b/c/foo.png`), you can use the following approach to download all files into the `files` folder with their original filenames (e.g. `files/foo.png`):

``` python
from pathlib import PurePosixPath
from scrapy.utils.httpobj import urlparse_cached

from scrapy.pipelines.images import ImagesPipeline

class MyImagesPipeline(ImagesPipeline):
    def file_path(self, request, response=None, info=None, *, item=None):
        return "files/" + PurePosixPath(urlparse_cached(request).path).name
```

Similarly, you can use the `item` to determine the file path based on some item property.

By default the <span class="title-ref">file\_path</span> method returns `full/<request URL hash>.<extension>`.

<div class="versionadded">

2.4 The *item* parameter.

</div>

</div>

<div class="method">

ImagesPipeline.thumb\_path(self, request, thumb\_id, response=None, info=None, \*, item=None)

This method is called for every item of `IMAGES_THUMBS` per downloaded item. It returns the thumbnail download path of the image originating from the specified <span class="title-ref">response \<scrapy.http.Response\></span>.

In addition to `response`, this method receives the original <span class="title-ref">request \<scrapy.Request\></span>, `thumb_id`, <span class="title-ref">info \<scrapy.pipelines.media.MediaPipeline.SpiderInfo\></span> and <span class="title-ref">item \<scrapy.Item\></span>.

You can override this method to customize the thumbnail download path of each image. You can use the `item` to determine the file path based on some item property.

By default the <span class="title-ref">thumb\_path</span> method returns `thumbs/<size name>/<request URL hash>.<extension>`.

</div>

<div class="method">

ImagesPipeline.get\_media\_requests(item, info)

Works the same way as <span class="title-ref">FilesPipeline.get\_media\_requests</span> method, but using a different field name for image urls.

Must return a Request for each image URL.

</div>

<div class="method">

ImagesPipeline.item\_completed(results, item, info)

The <span class="title-ref">ImagesPipeline.item\_completed</span> method is called when all image requests for a single item have completed (either finished downloading, or failed for some reason).

Works the same way as <span class="title-ref">FilesPipeline.item\_completed</span> method, but using a different field names for storing image downloading results.

By default, the <span class="title-ref">item\_completed</span> method returns the item.

</div>

</div>

<div id="media-pipeline-example">

Custom Images pipeline example `` ` ==============================  Here is a full example of the Images Pipeline whose methods are exemplified above: ``\`python import scrapy from itemadapter import ItemAdapter from scrapy.exceptions import DropItem from scrapy.pipelines.images import ImagesPipeline

</div>

>   - class MyImagesPipeline(ImagesPipeline):
>     
>       - def get\_media\_requests(self, item, info):
>         
>           - for image\_url in item\["image\_urls"\]:  
>             yield scrapy.Request(image\_url)
>     
>       - def item\_completed(self, results, item, info):  
>         image\_paths = \[x\["path"\] for ok, x in results if ok\] if not image\_paths: raise DropItem("Item contains no images") adapter = ItemAdapter(item) adapter\["image\_paths"\] = image\_paths return item

To enable your custom media pipeline component you must add its class import path to the `` ` :setting:`ITEM_PIPELINES` setting, like in the following example: ``\`python ITEM\_PIPELINES = {"myproject.pipelines.MyImagesPipeline": 300}

\`\`\`

---

practices.md

---

# Common Practices

This section documents common practices when using Scrapy. These are things that cover many topics and don't often fall into any other specific section.

## Run Scrapy from a script

You can use the \[API \<topics-api\>\](\#api-\<topics-api\>) to run Scrapy from a script, instead of the typical way of running Scrapy via `scrapy crawl`.

Remember that Scrapy is built on top of the Twisted asynchronous networking library, so you need to run it inside the Twisted reactor.

The first utility you can use to run your spiders is <span class="title-ref">scrapy.crawler.CrawlerProcess</span>. This class will start a Twisted reactor for you, configuring the logging and setting shutdown handlers. This class is the one used by all Scrapy commands.

Here's an example showing how to run a single spider with it.

`` `python     import scrapy     from scrapy.crawler import CrawlerProcess       class MySpider(scrapy.Spider):         # Your spider definition         ...       process = CrawlerProcess(         settings={             "FEEDS": {                 "items.json": {"format": "json"},             },         }     )      process.crawl(MySpider)     process.start()  # the script will block here until the crawling is finished  Define settings within dictionary in CrawlerProcess. Make sure to check `~scrapy.crawler.CrawlerProcess` ``\` documentation to get acquainted with its usage details.

If you are inside a Scrapy project there are some additional helpers you can use to import those components within the project. You can automatically import your spiders passing their name to <span class="title-ref">\~scrapy.crawler.CrawlerProcess</span>, and use `get_project_settings` to get a <span class="title-ref">\~scrapy.settings.Settings</span> instance with your project settings.

What follows is a working example of how to do that, using the [testspiders](https://github.com/scrapinghub/testspiders) project as example.

`` `python     from scrapy.crawler import CrawlerProcess     from scrapy.utils.project import get_project_settings      process = CrawlerProcess(get_project_settings())      # 'followall' is the name of one of the spiders of the project.     process.crawl("followall", domain="scrapy.org")     process.start()  # the script will block here until the crawling is finished  There's another Scrapy utility that provides more control over the crawling ``<span class="title-ref"> process: \`scrapy.crawler.CrawlerRunner</span>. This class is a thin wrapper that encapsulates some simple helpers to run multiple crawlers, but it won't start or interfere with existing reactors in any way.

Using this class the reactor should be explicitly run after scheduling your spiders. It's recommended you use <span class="title-ref">\~scrapy.crawler.CrawlerRunner</span> instead of <span class="title-ref">\~scrapy.crawler.CrawlerProcess</span> if your application is already using Twisted and you want to run Scrapy in the same reactor.

Note that you will also have to shutdown the Twisted reactor yourself after the spider is finished. This can be achieved by adding callbacks to the deferred returned by the <span class="title-ref">CrawlerRunner.crawl \<scrapy.crawler.CrawlerRunner.crawl\></span> method.

Here's an example of its usage, along with a callback to manually stop the reactor after `MySpider` has finished running.

`` `python     import scrapy     from scrapy.crawler import CrawlerRunner     from scrapy.utils.log import configure_logging       class MySpider(scrapy.Spider):         # Your spider definition         ...       configure_logging({"LOG_FORMAT": "%(levelname)s: %(message)s"})     runner = CrawlerRunner()      d = runner.crawl(MySpider)      from twisted.internet import reactor      d.addBoth(lambda _: reactor.stop())     reactor.run()  # the script will block here until the crawling is finished  Same example but using a non-default reactor, it's only necessary call ``<span class="title-ref"> </span><span class="title-ref">install\_reactor</span><span class="title-ref"> if you are using </span><span class="title-ref">CrawlerRunner</span><span class="title-ref"> since </span><span class="title-ref">CrawlerProcess</span>\` already does this automatically.

`` `python     import scrapy     from scrapy.crawler import CrawlerRunner     from scrapy.utils.log import configure_logging       class MySpider(scrapy.Spider):         # Your spider definition         ...       configure_logging({"LOG_FORMAT": "%(levelname)s: %(message)s"})      from scrapy.utils.reactor import install_reactor      install_reactor("twisted.internet.asyncioreactor.AsyncioSelectorReactor")     runner = CrawlerRunner()     d = runner.crawl(MySpider)      from twisted.internet import reactor      d.addBoth(lambda _: reactor.stop())     reactor.run()  # the script will block here until the crawling is finished  .. seealso:: [twisted:core/howto/reactor-basics](twisted:core/howto/reactor-basics.md)  .. _run-multiple-spiders:  Running multiple spiders in the same process ``\` ============================================

By default, Scrapy runs a single spider per process when you run `scrapy crawl`. However, Scrapy supports running multiple spiders per process using the \[internal API \<topics-api\>\](\#internal-api-\<topics-api\>).

Here is an example that runs multiple spiders simultaneously:

`` `python     import scrapy     from scrapy.crawler import CrawlerProcess     from scrapy.utils.project import get_project_settings       class MySpider1(scrapy.Spider):         # Your first spider definition         ...       class MySpider2(scrapy.Spider):         # Your second spider definition         ...       settings = get_project_settings()     process = CrawlerProcess(settings)     process.crawl(MySpider1)     process.crawl(MySpider2)     process.start()  # the script will block here until all crawling jobs are finished  Same example using `~scrapy.crawler.CrawlerRunner`:  .. code-block:: python      import scrapy     from scrapy.crawler import CrawlerRunner     from scrapy.utils.log import configure_logging     from scrapy.utils.project import get_project_settings       class MySpider1(scrapy.Spider):         # Your first spider definition         ...       class MySpider2(scrapy.Spider):         # Your second spider definition         ...       configure_logging()     settings = get_project_settings()     runner = CrawlerRunner(settings)     runner.crawl(MySpider1)     runner.crawl(MySpider2)     d = runner.join()      from twisted.internet import reactor      d.addBoth(lambda _: reactor.stop())      reactor.run()  # the script will block here until all crawling jobs are finished  Same example but running the spiders sequentially by chaining the deferreds:  .. code-block:: python      from twisted.internet import defer     from scrapy.crawler import CrawlerRunner     from scrapy.utils.log import configure_logging     from scrapy.utils.project import get_project_settings       class MySpider1(scrapy.Spider):         # Your first spider definition         ...       class MySpider2(scrapy.Spider):         # Your second spider definition         ...       settings = get_project_settings()     configure_logging(settings)     runner = CrawlerRunner(settings)       @defer.inlineCallbacks     def crawl():         yield runner.crawl(MySpider1)         yield runner.crawl(MySpider2)         reactor.stop()       from twisted.internet import reactor      crawl()     reactor.run()  # the script will block here until the last crawl call is finished  Different spiders can set different values for the same setting, but when they ``\` run in the same process it may be impossible, by design or because of some limitations, to use these different values. What happens in practice is different for different settings:

  - `SPIDER_LOADER_CLASS` and the ones used by its value (`SPIDER_MODULES`, `SPIDER_LOADER_WARN_ONLY` for the default one) cannot be read from the per-spider settings. These are applied when the <span class="title-ref">\~scrapy.crawler.CrawlerRunner</span> or <span class="title-ref">\~scrapy.crawler.CrawlerProcess</span> object is created.
  - For `TWISTED_REACTOR` and `ASYNCIO_EVENT_LOOP` the first available value is used, and if a spider requests a different reactor an exception will be raised. These are applied when the reactor is installed.
  - For `REACTOR_THREADPOOL_MAXSIZE`, `DNS_RESOLVER` and the ones used by the resolver (`DNSCACHE_ENABLED`, `DNSCACHE_SIZE`, `DNS_TIMEOUT` for ones included in Scrapy) the first available value is used. These are applied when the reactor is started.

<div class="seealso">

\[run-from-script\](\#run-from-script).

</div>

## Distributed crawls

Scrapy doesn't provide any built-in facility for running crawls in a distribute (multi-server) manner. However, there are some ways to distribute crawls, which vary depending on how you plan to distribute them.

If you have many spiders, the obvious way to distribute the load is to setup many Scrapyd instances and distribute spider runs among those.

If you instead want to run a single (big) spider through many machines, what you usually do is partition the urls to crawl and send them to each separate spider. Here is a concrete example:

First, you prepare the list of urls to crawl and put them into separate files/urls:

    http://somedomain.com/urls-to-crawl/spider1/part1.list
    http://somedomain.com/urls-to-crawl/spider1/part2.list
    http://somedomain.com/urls-to-crawl/spider1/part3.list

Then you fire a spider run on 3 different Scrapyd servers. The spider would receive a (spider) argument `part` with the number of the partition to crawl:

    curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
    curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
    curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3

## Avoiding getting banned

Some websites implement certain measures to prevent bots from crawling them, with varying degrees of sophistication. Getting around those measures can be difficult and tricky, and may sometimes require special infrastructure. Please consider contacting [commercial support](https://scrapy.org/support/) if in doubt.

Here are some tips to keep in mind when dealing with these kinds of sites:

  - rotate your user agent from a pool of well-known ones from browsers (google around to get a list of them)
  - disable cookies (see `COOKIES_ENABLED`) as some sites may use cookies to spot bot behaviour
  - use download delays (2 or higher). See `DOWNLOAD_DELAY` setting.
  - if possible, use [Common Crawl](https://commoncrawl.org/) to fetch pages, instead of hitting the sites directly
  - use a pool of rotating IPs. For example, the free [Tor project](https://www.torproject.org/) or paid services like [ProxyMesh](https://proxymesh.com/). An open source alternative is [scrapoxy](https://scrapoxy.io/), a super proxy that you can attach your own proxies to.
  - use a ban avoidance service, such as [Zyte API](https://docs.zyte.com/zyte-api/get-started.html), which provides a [Scrapy plugin](https://github.com/scrapy-plugins/scrapy-zyte-api) and additional features, like [AI web scraping](https://www.zyte.com/ai-web-scraping/)

If you are still unable to prevent your bot getting banned, consider contacting [commercial support](https://scrapy.org/support/).

---

request-response.md

---

# Requests and Responses

<div class="module" data-synopsis="Request and Response classes">

scrapy.http

</div>

Scrapy uses <span class="title-ref">Request</span> and <span class="title-ref">Response</span> objects for crawling web sites.

Typically, <span class="title-ref">Request</span> objects are generated in the spiders and pass across the system until they reach the Downloader, which executes the request and returns a <span class="title-ref">Response</span> object which travels back to the spider that issued the request.

Both <span class="title-ref">Request</span> and <span class="title-ref">Response</span> classes have subclasses which add functionality not required in the base classes. These are described below in \[topics-request-response-ref-request-subclasses\](\#topics-request-response-ref-request-subclasses) and \[topics-request-response-ref-response-subclasses\](\#topics-request-response-ref-response-subclasses).

## Request objects

<div class="autoclass">

Request

  - param url  
    the URL of this request

> If the URL is invalid, a <span class="title-ref">ValueError</span> exception is raised.

  - type url  
    str

  - param callback  
    the function that will be called with the response of this request (once it's downloaded) as its first parameter.
    
    In addition to a function, the following values are supported:
    
      - `None` (default), which indicates that the spider's <span class="title-ref">\~scrapy.Spider.parse</span> method must be used.
      - <span class="title-ref">\~scrapy.http.request.NO\_CALLBACK</span>
    
    For more information, see \[topics-request-response-ref-request-callback-arguments\](\#topics-request-response-ref-request-callback-arguments).
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    If exceptions are raised during processing, `errback` is called instead.
    
    </div>

  - type callback  
    collections.abc.Callable

  - param method  
    the HTTP method of this request. Defaults to `'GET'`.

  - type method  
    str

  - param meta  
    the initial values for the <span class="title-ref">Request.meta</span> attribute. If given, the dict passed in this parameter will be shallow copied.

  - type meta  
    dict

  - param body  
    the request body. If a string is passed, then it's encoded as bytes using the `encoding` passed (which defaults to `utf-8`). If `body` is not given, an empty bytes object is stored. Regardless of the type of this argument, the final value stored will be a bytes object (never a string or `None`).

  - type body  
    bytes or str

  - param headers  
    the headers of this request. The dict values can be strings (for single valued headers) or lists (for multi-valued headers). If `None` is passed as value, the HTTP header will not be sent at all.
    
    > 
    > 
    > <div class="caution">
    > 
    > <div class="title">
    > 
    > Caution
    > 
    > </div>
    > 
    > Cookies set via the `Cookie` header are not considered by the \[cookies-mw\](\#cookies-mw). If you need to set cookies for a request, use the <span class="title-ref">Request.cookies \<scrapy.Request\></span> parameter. This is a known current limitation that is being worked on.
    > 
    > </div>

  - type headers  
    dict

  - param cookies  
    the request cookies. These can be sent in two forms.

> 1.  Using a dict:
> 
> <!-- end list -->
> 
>   - `` `python     request_with_cookies = Request(         url="http://www.example.com",         cookies={"currency": "USD", "country": "UY"},     )  2. Using a list of dicts:  .. code-block:: python      request_with_cookies = Request(         url="https://www.example.com",         cookies=[             {                 "name": "currency",                 "value": "USD",                 "domain": "example.com",                 "path": "/currency",                 "secure": True,             },         ],     )  The latter form allows for customizing the ``domain`and`path``attributes of the cookie. This is only useful if the cookies are saved for later requests.  .. reqmeta:: dont_merge_cookies  When some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That's the typical behaviour of any regular web browser.  Note that setting the :reqmeta:`dont_merge_cookies` key to``True``in `request.meta <scrapy.Request.meta>` causes custom cookies to be ignored.  For more info see [cookies-mw](#cookies-mw).  .. caution:: Cookies set via the``Cookie\`<span class="title-ref"> header are not considered by the \[cookies-mw\](\#cookies-mw). If you need to set cookies for a request, use the \`Request.cookies \<scrapy.Request\></span> parameter. This is a known  
>     current limitation that is being worked on.
> 
> <div class="versionadded">
> 
> 2.6.0 Cookie values that are <span class="title-ref">bool</span>, <span class="title-ref">float</span> or <span class="title-ref">int</span> are casted to <span class="title-ref">str</span>.
> 
> </div>

  - type cookies  
    dict or list

  - param encoding  
    the encoding of this request (defaults to `'utf-8'`). This encoding will be used to percent-encode the URL and to convert the body to bytes (if given as a string).

  - type encoding  
    str

  - param priority  
    the priority of this request (defaults to `0`). The priority is used by the scheduler to define the order used to process requests. Requests with a higher priority value will execute earlier. Negative values are allowed in order to indicate relatively low-priority.

  - type priority  
    int

  - param dont\_filter  
    indicates that this request should not be filtered by the scheduler. This is used when you want to perform an identical request multiple times, to ignore the duplicates filter. Use it with care, or you will get into crawling loops. Default to `False`.

  - type dont\_filter  
    bool

  - param errback  
    a function that will be called if any exception was raised while processing the request. This includes pages that failed with 404 HTTP errors and such. It receives a <span class="title-ref">\~twisted.python.failure.Failure</span> as first parameter. For more information, see \[topics-request-response-ref-errbacks\](\#topics-request-response-ref-errbacks) below.
    
    <div class="versionchanged">
    
    2.0 The *callback* parameter is no longer required when the *errback* parameter is specified.
    
    </div>

  - type errback  
    collections.abc.Callable

  - param flags  
    Flags sent to the request, can be used for logging or similar purposes.

  - type flags  
    list

  - param cb\_kwargs  
    A dict with arbitrary data that will be passed as keyword arguments to the Request's callback.

  - type cb\_kwargs  
    dict

<div class="attribute">

Request.url

A string containing the URL of this request. Keep in mind that this attribute contains the escaped URL, so it can differ from the URL passed in the `__init__` method.

This attribute is read-only. To change the URL of a Request use <span class="title-ref">replace</span>.

</div>

<div class="attribute">

Request.method

A string representing the HTTP method in the request. This is guaranteed to be uppercase. Example: `"GET"`, `"POST"`, `"PUT"`, etc

</div>

<div class="attribute">

Request.headers

A dictionary-like object which contains the request headers.

</div>

<div class="attribute">

Request.body

The request body as bytes.

This attribute is read-only. To change the body of a Request use <span class="title-ref">replace</span>.

</div>

<div class="attribute" value="{}">

Request.meta

A dictionary of arbitrary metadata for the request.

You may extend request metadata as you see fit.

Request metadata can also be accessed through the <span class="title-ref">\~scrapy.http.Response.meta</span> attribute of a response.

To pass data from one spider callback to another, consider using <span class="title-ref">cb\_kwargs</span> instead. However, request metadata may be the right choice in certain scenarios, such as to maintain some debugging data across all follow-up requests (e.g. the source URL).

A common use of request metadata is to define request-specific parameters for Scrapy components (extensions, middlewares, etc.). For example, if you set `dont_retry` to `True`, <span class="title-ref">\~scrapy.downloadermiddlewares.retry.RetryMiddleware</span> will never retry that request, even if it fails. See \[topics-request-meta\](\#topics-request-meta).

You may also use request metadata in your custom Scrapy components, for example, to keep request state information relevant to your component. For example, <span class="title-ref">\~scrapy.downloadermiddlewares.retry.RetryMiddleware</span> uses the `retry_times` metadata key to keep track of how many times a request has been retried so far.

Copying all the metadata of a previous request into a new, follow-up request in a spider callback is a bad practice, because request metadata may include metadata set by Scrapy components that is not meant to be copied into other requests. For example, copying the `retry_times` metadata key into follow-up requests can lower the amount of retries allowed for those follow-up requests.

You should only copy all request metadata from one request to another if the new request is meant to replace the old request, as is often the case when returning a request from a \[downloader middleware \<topics-downloader-middleware\>\](\#downloader-middleware

</div>

</div>

\--------\<topics-downloader-middleware\>) method.

> Also mind that the <span class="title-ref">copy</span> and <span class="title-ref">replace</span> request methods \[shallow-copy \<library/copy\>\](shallow-copy \<library/copy\>.md) request metadata.
> 
> <div class="attribute">
> 
> Request.cb\_kwargs
> 
> </div>
> 
> A dictionary that contains arbitrary metadata for this request. Its contents will be passed to the Request's callback as keyword arguments. It is empty for new Requests, which means by default callbacks only get a <span class="title-ref">Response</span> object as argument.
> 
> This dict is \[shallow copied \<library/copy\>\](shallow copied \<library/copy\>.md) when the request is cloned using the `copy()` or `replace()` methods, and can also be accessed, in your spider, from the `response.cb_kwargs` attribute.
> 
> In case of a failure to process the request, this dict can be accessed as `failure.request.cb_kwargs` in the request's errback. For more information, see \[errback-cb\_kwargs\](\#errback-cb\_kwargs).
> 
> <div class="autoattribute">
> 
> Request.attributes
> 
> </div>
> 
> <div class="method">
> 
> Request.copy()
> 
> </div>
> 
> Return a new Request which is a copy of this Request. See also: \[topics-request-response-ref-request-callback-arguments\](\#topics-request-response-ref-request-callback-arguments).
> 
> <div class="method">
> 
> Request.replace(\[url, method, headers, body, cookies, meta, flags, encoding, priority, dont\_filter, callback, errback, cb\_kwargs\])
> 
> </div>
> 
> Return a Request object with the same members, except for those members given new values by whichever keyword arguments are specified. The <span class="title-ref">Request.cb\_kwargs</span> and <span class="title-ref">Request.meta</span> attributes are shallow copied by default (unless new values are given as arguments). See also \[topics-request-response-ref-request-callback-arguments\](\#topics-request-response-ref-request-callback-arguments).
> 
> <div class="automethod">
> 
> from\_curl
> 
> </div>
> 
> <div class="automethod">
> 
> to\_dict
> 
> </div>

Other functions related to requests `` ` -----------------------------------  .. autofunction:: scrapy.http.request.NO_CALLBACK  .. autofunction:: scrapy.utils.request.request_from_dict   .. _topics-request-response-ref-request-callback-arguments:  Passing additional data to callback functions ---------------------------------------------  The callback of a request is a function that will be called when the response of that request is downloaded. The callback function will be called with the downloaded `Response` object as its first argument.  Example: ``\`python def parse\_page1(self, response): return scrapy.Request( "<http://www.example.com/some_page.html>", callback=self.parse\_page2 )

>   - def parse\_page2(self, response):  
>     \# this would log <http://www.example.com/some_page.html> self.logger.info("Visited %s", response.url)

In some cases you may be interested in passing arguments to those callback `` ` functions so you can receive the arguments later, in the second callback. The following example shows how to achieve this by using the `Request.cb_kwargs` attribute: ``\`python def parse(self, response): request = scrapy.Request( "<http://www.example.com/index.html>", callback=self.parse\_page2, cb\_kwargs=dict(main\_url=response.url), ) request.cb\_kwargs\["foo"\] = "bar" \# add more arguments for the callback yield request

>   - def parse\_page2(self, response, main\_url, foo):
>     
>       - yield dict(  
>         main\_url=main\_url, other\_url=response.url, foo=foo,
>     
>     )

<div class="caution">

<div class="title">

Caution

</div>

<span class="title-ref">Request.cb\_kwargs</span> was introduced in version `1.7`. Prior to that, using <span class="title-ref">Request.meta</span> was recommended for passing information around callbacks. After `1.7`, <span class="title-ref">Request.cb\_kwargs</span> became the preferred way for handling user information, leaving <span class="title-ref">Request.meta</span> for communication with components like middlewares and extensions.

</div>

<div id="topics-request-response-ref-errbacks">

Using errbacks to catch exceptions in request processing `` ` --------------------------------------------------------  The errback of a request is a function that will be called when an exception is raise while processing it.  It receives a `~twisted.python.failure.Failure` as first parameter and can be used to track connection establishment timeouts, DNS errors etc.  Here's an example spider logging all errors and catching some specific errors if needed: ``\`python import scrapy

</div>

> from scrapy.spidermiddlewares.httperror import HttpError from twisted.internet.error import DNSLookupError from twisted.internet.error import TimeoutError, TCPTimedOutError
> 
>   - class ErrbackSpider(scrapy.Spider):  
>     name = "errback\_example" start\_urls = \[ "<http://www.httpbin.org/>", \# HTTP 200 expected "<http://www.httpbin.org/status/404>", \# Not found error "<http://www.httpbin.org/status/500>", \# server issue "<http://www.httpbin.org:12345/>", \# non-responding host, timeout expected "<https://example.invalid/>", \# DNS error expected \]
>     
>       - def start\_requests(self):
>         
>           - for u in self.start\_urls:
>             
>               - yield scrapy.Request(  
>                 u, callback=self.parse\_httpbin, errback=self.errback\_httpbin, dont\_filter=True,
>             
>             )
>     
>       - def parse\_httpbin(self, response):  
>         self.logger.info("Got successful response from {}".format(response.url)) \# do something useful here...
>     
>       - def errback\_httpbin(self, failure):  
>         \# log all failures self.logger.error(repr(failure))
>         
>         \# in case you want to do something special for some errors, \# you may need the failure's type:
>         
>           - if failure.check(HttpError):  
>             \# these exceptions come from HttpError spider middleware \# you can get the non-200 response response = failure.value.response self.logger.error("HttpError on %s", response.url)
>         
>           - elif failure.check(DNSLookupError):  
>             \# this is the original request request = failure.request self.logger.error("DNSLookupError on %s", request.url)
>         
>           - elif failure.check(TimeoutError, TCPTimedOutError):  
>             request = failure.request self.logger.error("TimeoutError on %s", request.url)

<div id="errback-cb_kwargs">

Accessing additional data in errback functions `` ` ----------------------------------------------  In case of a failure to process the request, you may be interested in accessing arguments to the callback functions so you can process further based on the arguments in the errback. The following example shows how to achieve this by using ``Failure.request.cb\_kwargs`:`\`python def parse(self, response): request = scrapy.Request( "<http://www.example.com/index.html>", callback=self.parse\_page2, errback=self.errback\_page2, cb\_kwargs=dict(main\_url=response.url), ) yield request

</div>

>   - def parse\_page2(self, response, main\_url):  
>     pass
> 
>   - def errback\_page2(self, failure):
>     
>       - yield dict(  
>         main\_url=failure.request.cb\_kwargs\["main\_url"\],
>     
>     )

<div id="request-fingerprints">

Request fingerprints `` ` --------------------  There are some aspects of scraping, such as filtering out duplicate requests (see :setting:`DUPEFILTER_CLASS`) or caching responses (see :setting:`HTTPCACHE_POLICY`), where you need the ability to generate a short, unique identifier from a `~scrapy.http.Request` object: a request fingerprint.  You often do not need to worry about request fingerprints, the default request fingerprinter works for most projects.  However, there is no universal way to generate a unique identifier from a request, because different situations require comparing requests differently. For example, sometimes you may need to compare URLs case-insensitively, include URL fragments, exclude certain URL query parameters, include some or all headers, etc.  To change how request fingerprints are built for your requests, use the :setting:`REQUEST_FINGERPRINTER_CLASS` setting.  .. setting:: REQUEST_FINGERPRINTER_CLASS  REQUEST_FINGERPRINTER_CLASS ~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 2.7  Default: `scrapy.utils.request.RequestFingerprinter`  A [request fingerprinter class <custom-request-fingerprinter>](#request-fingerprinter-class-<custom-request-fingerprinter>) or its import path.  .. autoclass:: scrapy.utils.request.RequestFingerprinter  .. _custom-request-fingerprinter:  Writing your own request fingerprinter ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  A request fingerprinter is a class that must implement the following method:  .. currentmodule:: None  .. method:: fingerprint(self, request)     Return a `bytes` object that uniquely identifies *request*.     See also [request-fingerprint-restrictions](#request-fingerprint-restrictions).     :param request: request to fingerprint    :type request: scrapy.http.Request  Additionally, it may also implement the following method:  .. classmethod:: from_crawler(cls, crawler)    :noindex:     If present, this class method is called to create a request fingerprinter    instance from a `~scrapy.crawler.Crawler` object. It must return a    new instance of the request fingerprinter.     *crawler* provides access to all Scrapy core components like settings and    signals; it is a way for the request fingerprinter to access them and hook    its functionality into Scrapy.     :param crawler: crawler that uses this request fingerprinter    :type crawler: `~scrapy.crawler.Crawler` object  .. currentmodule:: scrapy.http  The `fingerprint` method of the default request fingerprinter, `scrapy.utils.request.RequestFingerprinter`, uses `scrapy.utils.request.fingerprint` with its default parameters. For some common use cases you can use `scrapy.utils.request.fingerprint` as well in your `fingerprint` method implementation:  .. autofunction:: scrapy.utils.request.fingerprint  For example, to take the value of a request header named ``X-ID`into account:`\`python \# my\_project/settings.py REQUEST\_FINGERPRINTER\_CLASS = "my\_project.utils.RequestFingerprinter"

</div>

> \# my\_project/utils.py from scrapy.utils.request import fingerprint
> 
>   - class RequestFingerprinter:
>     
>       - def fingerprint(self, request):  
>         return fingerprint(request, include\_headers=\["X-ID"\])

You can also write your own fingerprinting logic from scratch.

However, if you do not use <span class="title-ref">scrapy.utils.request.fingerprint</span>, make sure `` ` you use `~weakref.WeakKeyDictionary` to cache request fingerprints:  -   Caching saves CPU by ensuring that fingerprints are calculated only once     per request, and not once per Scrapy component that needs the fingerprint     of a request.  -   Using `~weakref.WeakKeyDictionary` saves memory by ensuring that     request objects do not stay in memory forever just because you have     references to them in your cache dictionary.  For example, to take into account only the URL of a request, without any prior URL canonicalization or taking the request method or body into account: ``\`python from hashlib import sha1 from weakref import WeakKeyDictionary

> from scrapy.utils.python import to\_bytes
> 
>   - class RequestFingerprinter:  
>     cache = WeakKeyDictionary()
>     
>       - def fingerprint(self, request):
>         
>           - if request not in self.cache:  
>             fp = sha1() fp.update(to\_bytes(request.url)) self.cache\[request\] = fp.digest()
>         
>         return self.cache\[request\]

If you need to be able to override the request fingerprinting for arbitrary `` ` requests from your spider callbacks, you may implement a request fingerprinter that reads fingerprints from `request.meta <scrapy.http.Request.meta>` when available, and then falls back to `scrapy.utils.request.fingerprint`. For example: ``\`python from scrapy.utils.request import fingerprint

>   - class RequestFingerprinter:
>     
>       - def fingerprint(self, request):
>         
>           - if "fingerprint" in request.meta:  
>             return request.meta\["fingerprint"\]
>         
>         return fingerprint(request)

If you need to reproduce the same fingerprinting algorithm as Scrapy 2.6 `` ` without using the deprecated ``'2.6'``value of the :setting:`REQUEST_FINGERPRINTER_IMPLEMENTATION` setting, use the following request fingerprinter:``\`python from hashlib import sha1 from weakref import WeakKeyDictionary

> from scrapy.utils.python import to\_bytes from w3lib.url import canonicalize\_url
> 
>   - class RequestFingerprinter:  
>     cache = WeakKeyDictionary()
>     
>       - def fingerprint(self, request):
>         
>           - if request not in self.cache:  
>             fp = sha1() fp.update(to\_bytes(request.method)) fp.update(to\_bytes(canonicalize\_url(request.url))) fp.update(request.body or b"") self.cache\[request\] = fp.digest()
>         
>         return self.cache\[request\]

<div id="request-fingerprint-restrictions">

Request fingerprint restrictions `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Scrapy components that use request fingerprints may impose additional restrictions on the format of the fingerprints that your [request fingerprinter <custom-request-fingerprinter>](#request fingerprinter-<custom-request-fingerprinter>) generates.  The following built-in Scrapy components have such restrictions:  -   `scrapy.extensions.httpcache.FilesystemCacheStorage` (default     value of :setting:`HTTPCACHE_STORAGE`)      Request fingerprints must be at least 1 byte long.      Path and filename length limits of the file system of     :setting:`HTTPCACHE_DIR` also apply. Inside :setting:`HTTPCACHE_DIR`,     the following directory structure is created:      -   `Spider.name <scrapy.spiders.Spider.name>`          -   first byte of a request fingerprint as hexadecimal              -   fingerprint as hexadecimal                  -   filenames up to 16 characters long      For example, if a request fingerprint is made of 20 bytes (default),     :setting:`HTTPCACHE_DIR` is ``'/home/user/project/.scrapy/httpcache'`,     and the name of your spider is`'my\_spider'``your file system must     support a file path like::          /home/user/project/.scrapy/httpcache/my_spider/01/0123456789abcdef0123456789abcdef01234567/response_headers  -   `scrapy.extensions.httpcache.DbmCacheStorage`      The underlying DBM implementation must support keys as long as twice     the number of bytes of a request fingerprint, plus 5. For example,     if a request fingerprint is made of 20 bytes (default),     45-character-long keys must be supported.   .. _topics-request-meta:  Request.meta special keys =========================  The `Request.meta` attribute can contain any arbitrary data, but there are some special keys recognized by Scrapy and its built-in extensions.  Those are:  * :reqmeta:`autothrottle_dont_adjust_delay` * :reqmeta:`bindaddress` * :reqmeta:`cookiejar` * :reqmeta:`dont_cache` * :reqmeta:`dont_merge_cookies` * :reqmeta:`dont_obey_robotstxt` * :reqmeta:`dont_redirect` * :reqmeta:`dont_retry` * :reqmeta:`download_fail_on_dataloss` * :reqmeta:`download_latency` * :reqmeta:`download_maxsize` * :reqmeta:`download_warnsize` * :reqmeta:`download_timeout` *``ftp\_password``(See :setting:`FTP_PASSWORD` for more info) *``ftp\_user``(See :setting:`FTP_USER` for more info) * :reqmeta:`handle_httpstatus_all` * :reqmeta:`handle_httpstatus_list` * :reqmeta:`max_retry_times` * :reqmeta:`proxy` * :reqmeta:`redirect_reasons` * :reqmeta:`redirect_urls` * :reqmeta:`referrer_policy`  .. reqmeta:: bindaddress  bindaddress -----------  The IP of the outgoing IP address to use for the performing the request.  .. reqmeta:: download_timeout  download_timeout ----------------  The amount of time (in secs) that the downloader will wait before timing out. See also: :setting:`DOWNLOAD_TIMEOUT`.  .. reqmeta:: download_latency  download_latency ----------------  The amount of time spent to fetch the response, since the request has been started, i.e. HTTP message sent over the network. This meta key only becomes available when the response has been downloaded. While most other meta keys are used to control Scrapy behavior, this one is supposed to be read-only.  .. reqmeta:: download_fail_on_dataloss  download_fail_on_dataloss -------------------------  Whether or not to fail on broken responses. See: :setting:`DOWNLOAD_FAIL_ON_DATALOSS`.  .. reqmeta:: max_retry_times  max_retry_times ---------------  The meta key is used set retry times per request. When initialized, the :reqmeta:`max_retry_times` meta key takes higher precedence over the :setting:`RETRY_TIMES` setting.   .. _topics-stop-response-download:  Stopping the download of a Response ===================================  Raising a `~scrapy.exceptions.StopDownload` exception from a handler for the `~scrapy.signals.bytes_received` or `~scrapy.signals.headers_received` signals will stop the download of a given response. See the following example:``\`python import scrapy

</div>

>   - class StopSpider(scrapy.Spider):  
>     name = "stop" start\_urls = \["<https://docs.scrapy.org/en/latest/>"\]
>     
>     @classmethod def from\_crawler(cls, crawler): spider = super().from\_crawler(crawler) crawler.signals.connect( spider.on\_bytes\_received, signal=scrapy.signals.bytes\_received ) return spider
>     
>       - def parse(self, response):  
>         \# 'last\_chars' show that the full response was not downloaded yield {"len": len(response.text), "last\_chars": response.text\[-40:\]}
>     
>       - def on\_bytes\_received(self, data, request, spider):  
>         raise scrapy.exceptions.StopDownload(fail=False)

which produces the following output:

    2020-05-19 17:26:12 [scrapy.core.engine] INFO: Spider opened
    2020-05-19 17:26:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
    2020-05-19 17:26:13 [scrapy.core.downloader.handlers.http11] DEBUG: Download stopped for <GET https://docs.scrapy.org/en/latest/> from signal handler StopSpider.on_bytes_received
    2020-05-19 17:26:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/latest/> (referer: None) ['download_stopped']
    2020-05-19 17:26:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://docs.scrapy.org/en/latest/>
    {'len': 279, 'last_chars': 'dth, initial-scale=1.0">\n  \n  <title>Scr'}
    2020-05-19 17:26:13 [scrapy.core.engine] INFO: Closing spider (finished)

By default, resulting responses are handled by their corresponding errbacks. To `` ` call their callback instead, like in this example, pass ``fail=False``to the `~scrapy.exceptions.StopDownload` exception.   .. _topics-request-response-ref-request-subclasses:  Request subclasses ==================  Here is the list of built-in `Request` subclasses. You can also subclass it to implement your own custom functionality.  FormRequest objects -------------------  The FormRequest class extends the base `Request` with functionality for dealing with HTML forms. It uses `lxml.html forms`_  to pre-populate form fields with form data from `Response` objects.    .. class:: scrapy.http.request.form.FormRequest .. class:: scrapy.http.FormRequest .. class:: scrapy.FormRequest(url, [formdata, ...])      The `FormRequest` class adds a new keyword parameter to the``\_\_init\_\_``method. The     remaining arguments are the same as for the `Request` class and are     not documented here.      :param formdata: is a dictionary (or iterable of (key, value) tuples)        containing HTML Form data which will be url-encoded and assigned to the        body of the request.     :type formdata: dict or collections.abc.Iterable      The `FormRequest` objects support the following class method in     addition to the standard `Request` methods:      .. classmethod:: FormRequest.from_response(response, [formname=None, formid=None, formnumber=0, formdata=None, formxpath=None, formcss=None, clickdata=None, dont_click=False, ...])         Returns a new `FormRequest` object with its form field values        pre-populated with those found in the HTML``\<form\>`element contained        in the given response. For an example see        [topics-request-response-ref-request-userlogin](#topics-request-response-ref-request-userlogin).         The policy is to automatically simulate a click, by default, on any form        control that looks clickable, like a`\<input type="submit"\>``.  Even        though this is quite convenient, and often the desired behaviour,        sometimes it can cause problems which could be hard to debug. For        example, when working with forms that are filled and/or submitted using        javascript, the default `from_response` behaviour may not be the        most appropriate. To disable this behaviour you can set the``dont\_click`argument to`True`. Also, if you want to change the        control clicked (instead of disabling it) you can also use the`clickdata``argument.         .. caution:: Using this method with select elements which have leading           or trailing whitespace in the option values will not work due to a           `bug in lxml`_, which should be fixed in lxml 3.8 and above.         :param response: the response containing a HTML form which will be used           to pre-populate the form fields        :type response: `Response` object         :param formname: if given, the form with name attribute set to this value will be used.        :type formname: str         :param formid: if given, the form with id attribute set to this value will be used.        :type formid: str         :param formxpath: if given, the first form that matches the xpath will be used.        :type formxpath: str         :param formcss: if given, the first form that matches the css selector will be used.        :type formcss: str         :param formnumber: the number of form to use, when the response contains           multiple forms. The first one (and also the default) is``0`.        :type formnumber: int         :param formdata: fields to override in the form data. If a field was           already present in the response`\<form\>`element, its value is           overridden by the one passed in this parameter. If a value passed in           this parameter is`None`, the field will not be included in the           request, even if it was present in the response`\<form\>`element.        :type formdata: dict         :param clickdata: attributes to lookup the control clicked. If it's not          given, the form data will be submitted simulating a click on the          first clickable element. In addition to html attributes, the control          can be identified by its zero-based index relative to other          submittable inputs inside the form, via the`nr`` attribute.        :type clickdata: dict         :param dont_click: If True, the form data will be submitted without          clicking in any element.        :type dont_click: bool         The other parameters of this class method are passed directly to the        `FormRequest` ``\_\_init\_\_``method.  Request usage examples ----------------------  Using FormRequest to send data via HTTP POST ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  If you want to simulate a HTML Form POST in your spider and send a couple of key-value fields, you can return a `FormRequest` object (from your spider) like this:  .. skip: next``\`python return \[ FormRequest( url="<http://www.example.com/post/action>", formdata={"name": "John Doe", "age": "27"}, callback=self.after\_post, ) \]

<div id="topics-request-response-ref-request-userlogin">

Using FormRequest.from\_response() to simulate a user login `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  It is usual for web sites to provide pre-populated form fields through ``\<input type="hidden"\>``elements, such as session related data or authentication tokens (for login pages). When scraping, you'll want these fields to be automatically pre-populated and only override a couple of them, such as the user name and password. You can use the `FormRequest.from_response` method for this job. Here's an example spider which uses it:``\`python import scrapy

</div>

>   - def authentication\_failed(response):  
>     \# TODO: Check the contents of the response and return True if it failed \# or False if it succeeded. pass
> 
>   - class LoginSpider(scrapy.Spider):  
>     name = "example.com" start\_urls = \["<http://www.example.com/users/login.php>"\]
>     
>       - def parse(self, response):
>         
>           - return scrapy.FormRequest.from\_response(  
>             response, formdata={"username": "john", "password": "secret"}, callback=self.after\_login,
>         
>         )
>     
>       - def after\_login(self, response):
>         
>           - if authentication\_failed(response):  
>             self.logger.error("Login failed") return
>         
>         \# continue scraping with authenticated session...

JsonRequest `` ` -----------  The JsonRequest class extends the base `Request` class with functionality for dealing with JSON requests.  .. class:: JsonRequest(url, [... data, dumps_kwargs])     The `JsonRequest` class adds two new keyword parameters to the ``\_\_init\_\_``method. The    remaining arguments are the same as for the `Request` class and are    not documented here.     Using the `JsonRequest` will set the``Content-Type`header to`application/json`and`Accept`header to`application/json, text/javascript, */*; q=0.01``:param data: is any JSON serializable object that needs to be JSON encoded and assigned to body.       if `Request.body` argument is provided this parameter will be ignored.       if `Request.body` argument is not provided and data argument is provided `Request.method` will be       set to``'POST'``automatically.    :type data: object     :param dumps_kwargs: Parameters that will be passed to underlying `json.dumps` method which is used to serialize        data into JSON format.    :type dumps_kwargs: dict     .. autoattribute:: JsonRequest.attributes  JsonRequest usage example -------------------------  Sending a JSON POST request with a JSON payload:  .. skip: next``\`python data = { "name1": "value1", "name2": "value2", } yield JsonRequest(url="<http://www.example.com/post/action>", data=data)

Response objects `` ` ================  .. autoclass:: Response      :param url: the URL of this response     :type url: str      :param status: the HTTP status of the response. Defaults to ``200`.     :type status: int      :param headers: the headers of this response. The dict values can be strings        (for single valued headers) or lists (for multi-valued headers).     :type headers: dict      :param body: the response body. To access the decoded text as a string, use`response.text``from an encoding-aware        [Response subclass <topics-request-response-ref-response-subclasses>](#response-subclass-<topics-request-response-ref-response-subclasses>),        such as `TextResponse`.     :type body: bytes      :param flags: is a list containing the initial values for the        `Response.flags` attribute. If given, the list will be shallow        copied.     :type flags: list      :param request: the initial value of the `Response.request` attribute.         This represents the `Request` that generated this response.     :type request: scrapy.Request      :param certificate: an object representing the server's SSL certificate.     :type certificate: twisted.internet.ssl.Certificate      :param ip_address: The IP address of the server from which the Response originated.     :type ip_address: `ipaddress.IPv4Address` or `ipaddress.IPv6Address`      :param protocol: The protocol that was used to download the response.         For instance: "HTTP/1.0", "HTTP/1.1", "h2"     :type protocol: `str`      .. versionadded:: 2.0.0        The``certificate`parameter.      .. versionadded:: 2.1.0        The`ip\_address`parameter.      .. versionadded:: 2.5.0        The`protocol``parameter.      .. attribute:: Response.url          A string containing the URL of the response.          This attribute is read-only. To change the URL of a Response use         `replace`.      .. attribute:: Response.status          An integer representing the HTTP status of the response. Example:``200`,`404``.      .. attribute:: Response.headers          A dictionary-like object which contains the response headers. Values can         be accessed using `get` to return the first header value with the         specified name or `getlist` to return all header values with the         specified name. For example, this call will give you all cookies in the         headers::              response.headers.getlist('Set-Cookie')      .. attribute:: Response.body          The response body as bytes.          If you want the body as a string, use `TextResponse.text` (only         available in `TextResponse` and subclasses).          This attribute is read-only. To change the body of a Response use         `replace`.      .. attribute:: Response.request          The `Request` object that generated this response. This attribute is         assigned in the Scrapy engine, after the response and the request have passed         through all [Downloader Middlewares <topics-downloader-middleware>](#downloader-middlewares-<topics-downloader-middleware>).         In particular, this means that:          - HTTP redirections will create a new request from the request before           redirection. It has the majority of the same metadata and original           request attributes and gets assigned to the redirected response           instead of the propagation of the original request.          - Response.request.url doesn't always equal Response.url          - This attribute is only available in the spider code, and in the           [Spider Middlewares <topics-spider-middleware>](#spider-middlewares-<topics-spider-middleware>), but not in           Downloader Middlewares (although you have the Request available there by           other means) and handlers of the :signal:`response_downloaded` signal.      .. attribute:: Response.meta          A shortcut to the `Request.meta` attribute of the         `Response.request` object (i.e.``self.request.meta``).          Unlike the `Response.request` attribute, the `Response.meta`         attribute is propagated along redirects and retries, so you will get         the original `Request.meta` sent from your spider.          .. seealso:: `Request.meta` attribute      .. attribute:: Response.cb_kwargs          .. versionadded:: 2.0          A shortcut to the `Request.cb_kwargs` attribute of the         `Response.request` object (i.e.``self.request.cb\_kwargs``).          Unlike the `Response.request` attribute, the         `Response.cb_kwargs` attribute is propagated along redirects and         retries, so you will get the original `Request.cb_kwargs` sent         from your spider.          .. seealso:: `Request.cb_kwargs` attribute      .. attribute:: Response.flags          A list that contains flags for this response. Flags are labels used for         tagging Responses. For example:``'cached'`,`'redirected``', etc. And         they're shown on the string representation of the Response (`__str__`         method) which is used by the engine for logging.      .. attribute:: Response.certificate          .. versionadded:: 2.0.0          A `twisted.internet.ssl.Certificate` object representing         the server's SSL certificate.          Only populated for``https`responses,`None`otherwise.      .. attribute:: Response.ip_address          .. versionadded:: 2.1.0          The IP address of the server from which the Response originated.          This attribute is currently only populated by the HTTP 1.1 download         handler, i.e. for`http(s)``responses. For other handlers,         `ip_address` is always``None`.      .. attribute:: Response.protocol          .. versionadded:: 2.5.0          The protocol that was used to download the response.         For instance: "HTTP/1.0", "HTTP/1.1"          This attribute is currently only populated by the HTTP download         handlers, i.e. for`http(s)``responses. For other handlers,         `protocol` is always``None``.      .. autoattribute:: Response.attributes      .. method:: Response.copy()         Returns a new Response which is a copy of this Response.      .. method:: Response.replace([url, status, headers, body, request, flags, cls])         Returns a Response object with the same members, except for those members        given new values by whichever keyword arguments are specified. The        attribute `Response.meta` is copied by default.      .. method:: Response.urljoin(url)          Constructs an absolute url by combining the Response's `url` with         a possible relative url.          This is a wrapper over `~urllib.parse.urljoin`, it's merely an alias for         making this call::              urllib.parse.urljoin(response.url, url)      .. automethod:: Response.follow      .. automethod:: Response.follow_all   .. _topics-request-response-ref-response-subclasses:  Response subclasses ===================  Here is the list of available built-in Response subclasses. You can also subclass the Response class to implement your own functionality.  TextResponse objects --------------------  .. class:: TextResponse(url, [encoding[, ...]])      `TextResponse` objects adds encoding capabilities to the base     `Response` class, which is meant to be used only for binary data,     such as images, sounds or any media file.      `TextResponse` objects support a new``\_\_init\_\_``method argument, in     addition to the base `Response` objects. The remaining functionality     is the same as for the `Response` class and is not documented here.      :param encoding: is a string which contains the encoding to use for this        response. If you create a `TextResponse` object with a string as        body, it will be converted to bytes encoded using this encoding. If        *encoding* is``None``(default), the encoding will be looked up in the        response headers and body instead.     :type encoding: str      `TextResponse` objects support the following attributes in addition     to the standard `Response` ones:      .. attribute:: TextResponse.text         Response body, as a string.         The same as``response.body.decode(response.encoding)`, but the        result is cached after the first call, so you can access`response.text`multiple times without extra overhead.         > **Note** >`str(response.body)`is not a correct way to convert the response             body into a string:`\`pycon \>\>\> str(b"body") "b'body'"

> 
> 
> <div class="attribute">
> 
> TextResponse.encoding
> 
> A string with the encoding of this response. The encoding is resolved by trying the following mechanisms, in order:
> 
> 1.  the encoding passed in the `__init__` method `encoding` argument
> 2.  the encoding declared in the Content-Type HTTP header. If this encoding is not valid (i.e. unknown), it is ignored and the next resolution mechanism is tried.
> 3.  the encoding declared in the response body. The TextResponse class doesn't provide any special functionality for this. However, the <span class="title-ref">HtmlResponse</span> and <span class="title-ref">XmlResponse</span> classes do.
> 4.  the encoding inferred by looking at the response body. This is the more fragile method but also the last one tried.
> 
> </div>
> 
> <div class="attribute">
> 
> TextResponse.selector
> 
> A <span class="title-ref">\~scrapy.Selector</span> instance using the response as target. The selector is lazily instantiated on first access.
> 
> </div>
> 
> <div class="autoattribute">
> 
> TextResponse.attributes
> 
> </div>
> 
> <span class="title-ref">TextResponse</span> objects support the following methods in addition to the standard <span class="title-ref">Response</span> ones:
> 
> <div class="method">
> 
> TextResponse.jmespath(query)
> 
> A shortcut to `TextResponse.selector.jmespath(query)`:
> 
>     response.jmespath('object.[*]')
> 
> </div>
> 
> <div class="method">
> 
> TextResponse.xpath(query)
> 
> A shortcut to `TextResponse.selector.xpath(query)`:
> 
>     response.xpath('//p')
> 
> </div>
> 
> <div class="method">
> 
> TextResponse.css(query)
> 
> A shortcut to `TextResponse.selector.css(query)`:
> 
>     response.css('p')
> 
> </div>
> 
> <div class="automethod">
> 
> TextResponse.follow
> 
> </div>
> 
> <div class="automethod">
> 
> TextResponse.follow\_all
> 
> </div>
> 
> <div class="automethod">
> 
> TextResponse.json()
> 
> Returns a Python object from deserialized JSON document. The result is cached after the first call.
> 
> </div>
> 
> <div class="method">
> 
> TextResponse.urljoin(url)
> 
> Constructs an absolute url by combining the Response's base url with a possible relative url. The base url shall be extracted from the `<base>` tag, or just the Response's <span class="title-ref">url</span> if there is no such tag.
> 
> </div>

HtmlResponse objects \`\`\` --------------------

<div class="HtmlResponse(url[, ...])">

The <span class="title-ref">HtmlResponse</span> class is a subclass of <span class="title-ref">TextResponse</span> which adds encoding auto-discovering support by looking into the HTML [meta http-equiv](https://www.w3schools.com/TAGS/att_meta_http_equiv.asp) attribute. See <span class="title-ref">TextResponse.encoding</span>.

</div>

### XmlResponse objects

<div class="XmlResponse(url[, ...])">

The <span class="title-ref">XmlResponse</span> class is a subclass of <span class="title-ref">TextResponse</span> which adds encoding auto-discovering support by looking into the XML declaration line. See <span class="title-ref">TextResponse.encoding</span>.

</div>

### JsonResponse objects

<div class="JsonResponse(url[, ...])">

The <span class="title-ref">JsonResponse</span> class is a subclass of <span class="title-ref">TextResponse</span> that is used when the response has a [JSON MIME type](https://mimesniff.spec.whatwg.org/#json-mime-type) in its <span class="title-ref">Content-Type</span> header.

</div>

---

scheduler.md

---

# Scheduler

<div class="module">

scrapy.core.scheduler

</div>

The scheduler component receives requests from the \[engine \<component-engine\>\](\#engine-\<component-engine\>) and stores them into persistent and/or non-persistent data structures. It also gets those requests and feeds them back to the engine when it asks for a next request to be downloaded.

## Overriding the default scheduler

You can use your own custom scheduler class by supplying its full Python path in the `SCHEDULER` setting.

## Minimal scheduler interface

<div class="autoclass" data-members="">

BaseScheduler

</div>

## Default Scrapy scheduler

<div class="autoclass" data-members="" data-special-members="__len__">

Scheduler

</div>

---

scrapyd.md

---

- orphan

# Scrapyd

Scrapyd has been moved into a separate project.

Its documentation is now hosted at:

> <https://scrapyd.readthedocs.io/en/latest/>

---

selectors.md

---

# Selectors

When you're scraping web pages, the most common task you need to perform is to extract data from the HTML source. There are several libraries available to achieve this, such as:

  - [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a very popular web scraping library among Python programmers which constructs a Python object based on the structure of the HTML code and also deals with bad markup reasonably well, but it has one drawback: it's slow.
  - [lxml](https://lxml.de/) is an XML parsing library (which also parses HTML) with a pythonic API based on `~xml.etree.ElementTree`. (lxml is not part of the Python standard library.)

Scrapy comes with its own mechanism for extracting data. They're called selectors because they "select" certain parts of the HTML document specified either by [XPath](https://www.w3.org/TR/xpath/all/) or [CSS](https://www.w3.org/TR/selectors) expressions.

[XPath](https://www.w3.org/TR/xpath/all/) is a language for selecting nodes in XML documents, which can also be used with HTML. [CSS](https://www.w3.org/TR/selectors) is a language for applying styles to HTML documents. It defines selectors to associate those styles with specific HTML elements.

<div class="note">

<div class="title">

Note

</div>

Scrapy Selectors is a thin wrapper around [parsel](https://parsel.readthedocs.io/en/latest/) library; the purpose of this wrapper is to provide better integration with Scrapy Response objects.

[parsel](https://parsel.readthedocs.io/en/latest/) is a stand-alone web scraping library which can be used without Scrapy. It uses [lxml](https://lxml.de/) library under the hood, and implements an easy API on top of lxml API. It means Scrapy selectors are very similar in speed and parsing accuracy to lxml.

</div>

## Using selectors

### Constructing selectors

Response objects expose a <span class="title-ref">\~scrapy.Selector</span> instance on `.selector` attribute:

`` `pycon     >>> response.selector.xpath("//span/text()").get()     'good'  Querying responses using XPath and CSS is so common that responses include two ``<span class="title-ref"> more shortcuts: </span><span class="title-ref">response.xpath()</span><span class="title-ref"> and </span><span class="title-ref">response.css()</span>\`:

`` `pycon     >>> response.xpath("//span/text()").get()     'good'     >>> response.css("span::text").get()     'good'  .. skip: end  Scrapy selectors are instances of `~scrapy.Selector` class ``<span class="title-ref"> constructed by passing either </span>\~scrapy.http.TextResponse\` object or markup as a string (in `text` argument).

Usually there is no need to construct Scrapy selectors manually: `response` object is available in Spider callbacks, so in most cases it is more convenient to use `response.css()` and `response.xpath()` shortcuts. By using `response.selector` or one of these shortcuts you can also ensure the response body is parsed only once.

But if required, it is possible to use `Selector` directly. Constructing from text:

`` `pycon     >>> from scrapy.selector import Selector     >>> body = "<html><body><span>good</span></body></html>"     >>> Selector(text=body).xpath("//span/text()").get()     'good'  Constructing from response - `~scrapy.http.HtmlResponse` is one of ``<span class="title-ref"> </span>\~scrapy.http.TextResponse\` subclasses:

`` `pycon     >>> from scrapy.selector import Selector     >>> from scrapy.http import HtmlResponse     >>> response = HtmlResponse(url="http://example.com", body=body, encoding="utf-8")     >>> Selector(response=response).xpath("//span/text()").get()     'good' ``Selector`automatically chooses the best parsing rules`\` (XML vs HTML) based on input type.

### Using selectors

To explain how to use the selectors we'll use the `Scrapy shell` (which provides interactive testing) and an example page located in the Scrapy documentation server:

> <https://docs.scrapy.org/en/latest/_static/selectors-sample1.html>

<div id="topics-selectors-htmlcode">

For the sake of completeness, here's its full HTML code:

</div>

<div class="literalinclude" data-language="html">

../\_static/selectors-sample1.html

</div>

First, let's open the shell:

``` sh
scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
```

Then, after the shell loads, you'll have the response available as `response` shell variable, and its attached selector in `response.selector` attribute.

Since we're dealing with HTML, the selector will automatically use an HTML parser.

So, by looking at the \[HTML code \<topics-selectors-htmlcode\>\](\#html-code-\<topics-selectors-htmlcode\>) of that page, let's construct an XPath for selecting the text inside the title tag:

`` `pycon     >>> response.xpath("//title/text()")     [<Selector query='//title/text()' data='Example website'>]  To actually extract the textual data, you must call the selector ``.get()`  `<span class="title-ref"> or </span><span class="title-ref">.getall()</span>\` methods, as follows:

`` `pycon     >>> response.xpath("//title/text()").getall()     ['Example website']     >>> response.xpath("//title/text()").get()     'Example website' ``.get()`always returns a single result; if there are several matches,`<span class="title-ref"> content of a first match is returned; if there are no matches, None is returned. </span><span class="title-ref">.getall()</span>\` returns a list with all results.

Notice that CSS selectors can select text or attribute nodes using CSS3 pseudo-elements:

`` `pycon     >>> response.css("title::text").get()     'Example website'  As you can see, ``.xpath()`and`.css()`methods return a`<span class="title-ref"> </span>\~scrapy.selector.SelectorList\` instance, which is a list of new selectors. This API can be used for quickly selecting nested data:

`` `pycon     >>> response.css("img").xpath("@src").getall()     ['image1_thumb.jpg',     'image2_thumb.jpg',     'image3_thumb.jpg',     'image4_thumb.jpg',     'image5_thumb.jpg']  If you want to extract only the first matched element, you can call the ``<span class="title-ref"> selector </span><span class="title-ref">.get()</span><span class="title-ref"> (or its alias </span><span class="title-ref">.extract\_first()</span>\` commonly used in previous Scrapy versions):

`` `pycon     >>> response.xpath('//div[@id="images"]/a/text()').get()     'Name: My image 1 '  It returns ``None`if no element was found:  .. code-block:: pycon      >>> response.xpath('//div[@id="not-exists"]/text()').get() is None     True  A default return value can be provided as an argument, to be used instead`<span class="title-ref"> of </span><span class="title-ref">None</span>\`:

`` `pycon     >>> response.xpath('//div[@id="not-exists"]/text()').get(default="not-found")     'not-found'  Instead of using e.g. ``'@src'`XPath it is possible to query for attributes`<span class="title-ref"> using </span><span class="title-ref">.attrib</span><span class="title-ref"> property of a </span>\~scrapy.Selector\`:

`` `pycon     >>> [img.attrib["src"] for img in response.css("img")]     ['image1_thumb.jpg',     'image2_thumb.jpg',     'image3_thumb.jpg',     'image4_thumb.jpg',     'image5_thumb.jpg']  As a shortcut, ``.attrib`is also available on SelectorList directly;`\` it returns attributes for the first matching element:

`` `pycon     >>> response.css("img").attrib["src"]     'image1_thumb.jpg'  This is most useful when only a single result is expected, e.g. when selecting ``\` by id, or selecting unique elements on a web page:

`` `pycon     >>> response.css("base").attrib["href"]     'http://example.com/'  Now we're going to get the base URL and some image links:  .. code-block:: pycon      >>> response.xpath("//base/@href").get()     'http://example.com/'      >>> response.css("base::attr(href)").get()     'http://example.com/'      >>> response.css("base").attrib["href"]     'http://example.com/'      >>> response.xpath('//a[contains(@href, "image")]/@href').getall()     ['image1.html',     'image2.html',     'image3.html',     'image4.html',     'image5.html']      >>> response.css("a[href*=image]::attr(href)").getall()     ['image1.html',     'image2.html',     'image3.html',     'image4.html',     'image5.html']      >>> response.xpath('//a[contains(@href, "image")]/img/@src').getall()     ['image1_thumb.jpg',     'image2_thumb.jpg',     'image3_thumb.jpg',     'image4_thumb.jpg',     'image5_thumb.jpg']      >>> response.css("a[href*=image] img::attr(src)").getall()     ['image1_thumb.jpg',     'image2_thumb.jpg',     'image3_thumb.jpg',     'image4_thumb.jpg',     'image5_thumb.jpg']  .. _topics-selectors-css-extensions:  Extensions to CSS Selectors ``\` ---------------------------

Per W3C standards, [CSS selectors](https://www.w3.org/TR/selectors-3/#selectors) do not support selecting text nodes or attribute values. But selecting these is so essential in a web scraping context that Scrapy (parsel) implements a couple of **non-standard pseudo-elements**:

  - to select text nodes, use `::text`
  - to select attribute values, use `::attr(name)` where *name* is the name of the attribute that you want the value of

<div class="warning">

<div class="title">

Warning

</div>

These pseudo-elements are Scrapy-/Parsel-specific. They will most probably not work with other libraries like [lxml](https://lxml.de/) or [PyQuery](https://pypi.org/project/pyquery/).

</div>

Examples:

  - `title::text` selects children text nodes of a descendant `<title>` element:

`` `pycon     >>> response.css("title::text").get()     'Example website'  * ``\*::text`selects all descendant text nodes of the current selector context:  .. code-block:: pycon      >>> response.css("#images *::text").getall()     ['\n   ',     'Name: My image 1 ',     '\n   ',     'Name: My image 2 ',     '\n   ',     'Name: My image 3 ',     '\n   ',     'Name: My image 4 ',     '\n   ',     'Name: My image 5 ',     '\n  ']  *`foo::text`returns no results if`foo`element exists, but contains   no text (i.e. text is empty):  .. code-block:: pycon    >>> response.css("img::text").getall()   []    This means`.css('foo::text').get()`could return None even if an element   exists. Use`default=''`if you always want a string:  .. code-block:: pycon      >>> response.css("img::text").get()     >>> response.css("img::text").get(default="")     ''  *`a::attr(href)`selects the *href* attribute value of descendant links:  .. code-block:: pycon      >>> response.css("a::attr(href)").getall()     ['image1.html',     'image2.html',     'image3.html',     'image4.html',     'image5.html']  .. note::     See also: [selecting-attributes](#selecting-attributes).  .. note::     You cannot chain these pseudo-elements. But in practice it would not     make much sense: text nodes do not have attributes, and attribute values     are string values already and do not have children nodes.    .. _topics-selectors-nesting-selectors:  Nesting selectors`\` -----------------

The selection methods (`.xpath()` or `.css()`) return a list of selectors of the same type, so you can call the selection methods for those selectors too. Here's an example:

`` `pycon     >>> links = response.xpath('//a[contains(@href, "image")]')     >>> links.getall()     ['<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg" alt="image1"></a>',     '<a href="image2.html">Name: My image 2 <br><img src="image2_thumb.jpg" alt="image2"></a>',     '<a href="image3.html">Name: My image 3 <br><img src="image3_thumb.jpg" alt="image3"></a>',     '<a href="image4.html">Name: My image 4 <br><img src="image4_thumb.jpg" alt="image4"></a>',     '<a href="image5.html">Name: My image 5 <br><img src="image5_thumb.jpg" alt="image5"></a>']      >>> for index, link in enumerate(links):     ...     href_xpath = link.xpath("@href").get()     ...     img_xpath = link.xpath("img/@src").get()     ...     print(f"Link number {index} points to url {href_xpath!r} and image {img_xpath!r}")     ...     Link number 0 points to url 'image1.html' and image 'image1_thumb.jpg'     Link number 1 points to url 'image2.html' and image 'image2_thumb.jpg'     Link number 2 points to url 'image3.html' and image 'image3_thumb.jpg'     Link number 3 points to url 'image4.html' and image 'image4_thumb.jpg'     Link number 4 points to url 'image5.html' and image 'image5_thumb.jpg'  .. _selecting-attributes:  Selecting element attributes ``\` ----------------------------

There are several ways to get a value of an attribute. First, one can use XPath syntax:

`` `pycon     >>> response.xpath("//a/@href").getall()     ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']  XPath syntax has a few advantages: it is a standard XPath feature, and ``<span class="title-ref"> </span><span class="title-ref">@attributes</span>\` can be used in other parts of an XPath expression - e.g. it is possible to filter by attribute value.

Scrapy also provides an extension to CSS selectors (`::attr(...)`) which allows to get attribute values:

`` `pycon     >>> response.css("a::attr(href)").getall()     ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']  In addition to that, there is a ``.attrib`property of Selector.`\` You can use it if you prefer to lookup attributes in Python code, without using XPaths or CSS extensions:

`` `pycon     >>> [a.attrib["href"] for a in response.css("a")]     ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']  This property is also available on SelectorList; it returns a dictionary ``\` with attributes of a first matching element. It is convenient to use when a selector is expected to give a single result (e.g. when selecting by element ID, or when selecting an unique element on a page):

`` `pycon     >>> response.css("base").attrib     {'href': 'http://example.com/'}     >>> response.css("base").attrib["href"]     'http://example.com/' ``.attrib`property of an empty SelectorList is empty:  .. code-block:: pycon      >>> response.css("foo").attrib     {}  Using selectors with regular expressions`\` ----------------------------------------

<span class="title-ref">\~scrapy.Selector</span> also has a `.re()` method for extracting data using regular expressions. However, unlike using `.xpath()` or `.css()` methods, `.re()` returns a list of strings. So you can't construct nested `.re()` calls.

Here's an example used to extract image names from the \[HTML code \<topics-selectors-htmlcode\>\](\#html-code \<topics-selectors-htmlcode\>) above:

`` `pycon     >>> response.xpath('//a[contains(@href, "image")]/text()').re(r"Name:\s*(.*)")     ['My image 1 ',     'My image 2 ',     'My image 3 ',     'My image 4 ',     'My image 5 ']  There's an additional helper reciprocating ``.get()`(and its`<span class="title-ref"> alias </span><span class="title-ref">.extract\_first()</span><span class="title-ref">) for </span><span class="title-ref">.re()</span><span class="title-ref">, named </span><span class="title-ref">.re\_first()</span>\`. Use it to extract just the first matching string:

`` `pycon     >>> response.xpath('//a[contains(@href, "image")]/text()').re_first(r"Name:\s*(.*)")     'My image 1 '  .. _old-extraction-api:  extract() and extract_first() ``\` -----------------------------

If you're a long-time Scrapy user, you're probably familiar with `.extract()` and `.extract_first()` selector methods. Many blog posts and tutorials are using them as well. These methods are still supported by Scrapy, there are **no plans** to deprecate them.

However, Scrapy usage docs are now written using `.get()` and `.getall()` methods. We feel that these new methods result in a more concise and readable code.

The following examples show how these methods map to each other.

1.  `SelectorList.get()` is the same as `SelectorList.extract_first()`:

`` `pycon     >>> response.css("a::attr(href)").get()     'image1.html'     >>> response.css("a::attr(href)").extract_first()     'image1.html'  2. ``SelectorList.getall()`is the same as`SelectorList.extract()`:  .. code-block:: pycon      >>> response.css("a::attr(href)").getall()     ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']     >>> response.css("a::attr(href)").extract()     ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']  3.`Selector.get()`is the same as`Selector.extract()`:  .. code-block:: pycon      >>> response.css("a::attr(href)")[0].get()     'image1.html'     >>> response.css("a::attr(href)")[0].extract()     'image1.html'  4.  For consistency, there is also`Selector.getall()`, which returns a list:  .. code-block:: pycon      >>> response.css("a::attr(href)")[0].getall()     ['image1.html']  So, the main difference is that output of`.get()`and`.getall()`methods`<span class="title-ref"> is more predictable: </span><span class="title-ref">.get()</span><span class="title-ref"> always returns a single result, </span><span class="title-ref">.getall()</span><span class="title-ref"> always returns a list of all extracted results. With </span><span class="title-ref">.extract()</span><span class="title-ref"> method it was not always obvious if a result is a list or not; to get a single result either </span><span class="title-ref">.extract()</span><span class="title-ref"> or </span><span class="title-ref">.extract\_first()</span>\` should be called.

## Working with XPaths

Here are some tips which may help you to use XPath with Scrapy selectors effectively. If you are not much familiar with XPath yet, you may want to take a look first at this [XPath tutorial](http://www.zvon.org/comp/r/tut-XPath_1.html).

<div class="note">

<div class="title">

Note

</div>

Some of the tips are based on [this post from Zyte's blog](https://www.zyte.com/blog/xpath-tips-from-the-web-scraping-trenches/).

</div>

### Working with relative XPaths

Keep in mind that if you are nesting selectors and use an XPath that starts with `/`, that XPath will be absolute to the document and not relative to the `Selector` you're calling it from.

For example, suppose you want to extract all `<p>` elements inside `<div>` elements. First, you would get all `<div>` elements:

`` `pycon     >>> divs = response.xpath("//div")  At first, you may be tempted to use the following approach, which is wrong, as ``<span class="title-ref"> it actually extracts all </span><span class="title-ref">\<p\></span><span class="title-ref"> elements from the document, not only those inside </span><span class="title-ref">\<div\></span>\` elements:

`` `pycon     >>> for p in divs.xpath("//p"):  # this is wrong - gets all <p> from the whole document     ...     print(p.get())     ...  This is the proper way to do it (note the dot prefixing the ``.//p`XPath):  .. code-block:: pycon      >>> for p in divs.xpath(".//p"):  # extracts all <p> inside     ...     print(p.get())     ...  Another common case would be to extract all direct`\<p\>``children:  .. code-block:: pycon      >>> for p in divs.xpath("p"):     ...     print(p.get())     ...  For more details about relative XPaths see the `Location Paths`_ section in the``\` XPath specification.

### When querying by class, consider using CSS

Because an element can contain multiple CSS classes, the XPath way to select elements by class is the rather verbose:

``` python
*[contains(concat(' ', normalize-space(@class), ' '), ' someclass ')]
```

If you use `@class='someclass'` you may end up missing elements that have other classes, and if you just use `contains(@class, 'someclass')` to make up for that you may end up with more elements that you want, if they have a different class name that shares the string `someclass`.

As it turns out, Scrapy selectors allow you to chain selectors, so most of the time you can just select by class using CSS and then switch to XPath when needed:

`` `pycon     >>> from scrapy import Selector     >>> sel = Selector(     ...     text='<div class="hero shout"><time datetime="2014-07-23 19:00">Special date</time></div>'     ... )     >>> sel.css(".shout").xpath("./time/@datetime").getall()     ['2014-07-23 19:00']  This is cleaner than using the verbose XPath trick shown above. Just remember ``<span class="title-ref"> to use the </span><span class="title-ref">.</span>\` in the XPath expressions that will follow.

### Beware of the difference between //node\[1\] and (//node)\[1\]

`//node[1]` selects all the nodes occurring first under their respective parents.

`(//node)[1]` selects all the nodes in the document, and then gets only the first of them.

Example:

`` `pycon     >>> from scrapy import Selector     >>> sel = Selector(     ...     text="""     ...     <ul class="list">     ...         <li>1</li>     ...         <li>2</li>     ...         <li>3</li>     ...     </ul>     ...     <ul class="list">     ...         <li>4</li>     ...         <li>5</li>     ...         <li>6</li>     ...     </ul>"""     ... )     >>> xp = lambda x: sel.xpath(x).getall()  This gets all first ``\<li\>`elements under whatever it is its parent:  .. code-block:: pycon      >>> xp("//li[1]")     ['<li>1</li>', '<li>4</li>']  And this gets the first`\<li\>`element in the whole document:  .. code-block:: pycon      >>> xp("(//li)[1]")     ['<li>1</li>']  This gets all first`\<li\>`elements under an`\<ul\>`parent:  .. code-block:: pycon      >>> xp("//ul/li[1]")     ['<li>1</li>', '<li>4</li>']  And this gets the first`\<li\>`element under an`\<ul\>`parent in the whole document:  .. code-block:: pycon      >>> xp("(//ul/li)[1]")     ['<li>1</li>']  Using text nodes in a condition`\` -------------------------------

When you need to use the text content as argument to an [XPath string function](https://www.w3.org/TR/xpath-10/#section-String-Functions), avoid using `.//text()` and use just `.` instead.

This is because the expression `.//text()` yields a collection of text elements -- a *node-set*. And when a node-set is converted to a string, which happens when it is passed as argument to a string function like `contains()` or `starts-with()`, it results in the text for the first element only.

Example:

`` `pycon     >>> from scrapy import Selector     >>> sel = Selector(     ...     text='<a href="#">Click here to go to the <strong>Next Page</strong></a>'     ... )  Converting a *node-set* to string:  .. code-block:: pycon      >>> sel.xpath("//a//text()").getall()  # take a peek at the node-set     ['Click here to go to the ', 'Next Page']     >>> sel.xpath("string(//a[1]//text())").getall()  # convert it to string     ['Click here to go to the ']  A *node* converted to a string, however, puts together the text of itself plus of all its descendants:  .. code-block:: pycon      >>> sel.xpath("//a[1]").getall()  # select the first node     ['<a href="#">Click here to go to the <strong>Next Page</strong></a>']     >>> sel.xpath("string(//a[1])").getall()  # convert it to string     ['Click here to go to the Next Page']  So, using the ``.//text()`node-set won't select anything in this case:  .. code-block:: pycon      >>> sel.xpath("//a[contains(.//text(), 'Next Page')]").getall()     []  But using the`.`to mean the node, works:  .. code-block:: pycon      >>> sel.xpath("//a[contains(., 'Next Page')]").getall()     ['<a href="#">Click here to go to the <strong>Next Page</strong></a>']    .. _topics-selectors-xpath-variables:  Variables in XPath expressions`\` ------------------------------

XPath allows you to reference variables in your XPath expressions, using the `$somevariable` syntax. This is somewhat similar to parameterized queries or prepared statements in the SQL world where you replace some arguments in your queries with placeholders like `?`, which are then substituted with values passed with the query.

Here's an example to match an element based on its "id" attribute value, without hard-coding it (that was shown previously):

`` `pycon     >>> # `$val` used in the expression, a `val` argument needs to be passed     >>> response.xpath("//div[@id=$val]/a/text()", val="images").get()     'Name: My image 1 '  Here's another example, to find the "id" attribute of a ``\<div\>`tag containing`<span class="title-ref"> five </span><span class="title-ref">\<a\></span><span class="title-ref"> children (here we pass the value </span><span class="title-ref">5</span>\` as an integer):

`` `pycon     >>> response.xpath("//div[count(a)=$cnt]/@id", cnt=5).get()     'images'  All variable references must have a binding value when calling ``.xpath()`  `<span class="title-ref"> (otherwise you'll get a </span><span class="title-ref">ValueError: XPath error:</span>\` exception). This is done by passing as many named arguments as necessary.

[parsel](https://parsel.readthedocs.io/en/latest/), the library powering Scrapy selectors, has more details and examples on [XPath variables](https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions).

### Removing namespaces

When dealing with scraping projects, it is often quite convenient to get rid of namespaces altogether and just work with element names, to write more simple/convenient XPaths. You can use the <span class="title-ref">Selector.remove\_namespaces</span> method for that.

Let's show an example that illustrates this with the Python Insider blog atom feed.

First, we open the shell with the url we want to scrape:

``` sh
$ scrapy shell https://feeds.feedburner.com/PythonInsider
```

This is how the file starts:

``` sh
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet ...
<feed xmlns="http://www.w3.org/2005/Atom"
      xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/"
      xmlns:blogger="http://schemas.google.com/blogger/2008"
      xmlns:georss="http://www.georss.org/georss"
      xmlns:gd="http://schemas.google.com/g/2005"
      xmlns:thr="http://purl.org/syndication/thread/1.0"
      xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">
  ...
```

You can see several namespace declarations including a default `"http://www.w3.org/2005/Atom"` and another one using the `gd:` prefix for `"http://schemas.google.com/g/2005"`.

Once in the shell we can try selecting all `<link>` objects and see that it doesn't work (because the Atom XML namespace is obfuscating those nodes):

`` `pycon     >>> response.xpath("//link")     []  But once we call the `Selector.remove_namespaces` method, all ``\` nodes can be accessed directly by their names:

`` `pycon     >>> response.selector.remove_namespaces()     >>> response.xpath("//link")     [<Selector query='//link' data='<link rel="alternate" type="text/html" h'>,         <Selector query='//link' data='<link rel="next" type="application/atom+'>,         ...  If you wonder why the namespace removal procedure isn't always called by default ``\` instead of having to call it manually, this is because of two reasons, which, in order of relevance, are:

1.  Removing namespaces requires to iterate and modify all nodes in the document, which is a reasonably expensive operation to perform by default for all documents crawled by Scrapy
2.  There could be some cases where using namespaces is actually required, in case some element names clash between namespaces. These cases are very rare though.

### Using EXSLT extensions

Being built atop [lxml](https://lxml.de/), Scrapy selectors support some [EXSLT](http://exslt.org/) extensions and come with these pre-registered namespaces to use in XPath expressions:

| prefix | namespace                            | usage                                                     |
| ------ | ------------------------------------ | --------------------------------------------------------- |
| re     | http://exslt.org/regular-expressions | [regular expressions](http://exslt.org/regexp/index.html) |
| set    | http://exslt.org/sets                | [set manipulation](http://exslt.org/set/index.html)       |

#### Regular expressions

The `test()` function, for example, can prove quite useful when XPath's `starts-with()` or `contains()` are not sufficient.

Example selecting links in list item with a "class" attribute ending with a digit:

`` `pycon     >>> from scrapy import Selector     >>> doc = """     ... <div>     ...     <ul>     ...         <li class="item-0"><a href="link1.html">first item</a></li>     ...         <li class="item-1"><a href="link2.html">second item</a></li>     ...         <li class="item-inactive"><a href="link3.html">third item</a></li>     ...         <li class="item-1"><a href="link4.html">fourth item</a></li>     ...         <li class="item-0"><a href="link5.html">fifth item</a></li>     ...     </ul>     ... </div>     ... """     >>> sel = Selector(text=doc, type="html")     >>> sel.xpath("//li//@href").getall()     ['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']     >>> sel.xpath('//li[re:test(@class, "item-\d$")]//@href').getall()     ['link1.html', 'link2.html', 'link4.html', 'link5.html']  .. warning:: C library ``libxslt``doesn't natively support EXSLT regular     expressions so `lxml`_'s implementation uses hooks to Python's``re`module.     Thus, using regexp functions in your XPath expressions may add a small     performance penalty.  Set operations`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~

These can be handy for excluding parts of a document tree before extracting text elements for example.

Example extracting microdata (sample content taken from <https://schema.org/Product>) with groups of itemscopes and corresponding itemprops:

`` `pycon     >>> doc = """     ... <div itemscope itemtype="http://schema.org/Product">     ...   <span itemprop="name">Kenmore White 17" Microwave</span>     ...   <img src="kenmore-microwave-17in.jpg" alt='Kenmore 17" Microwave' />     ...   <div itemprop="aggregateRating"     ...     itemscope itemtype="http://schema.org/AggregateRating">     ...    Rated <span itemprop="ratingValue">3.5</span>/5     ...    based on <span itemprop="reviewCount">11</span> customer reviews     ...   </div>     ...   <div itemprop="offers" itemscope itemtype="http://schema.org/Offer">     ...     <span itemprop="price">$55.00</span>     ...     <link itemprop="availability" href="http://schema.org/InStock" />In stock     ...   </div>     ...   Product description:     ...   <span itemprop="description">0.7 cubic feet countertop microwave.     ...   Has six preset cooking categories and convenience features like     ...   Add-A-Minute and Child Lock.</span>     ...   Customer reviews:     ...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">     ...     <span itemprop="name">Not a happy camper</span> -     ...     by <span itemprop="author">Ellie</span>,     ...     <meta itemprop="datePublished" content="2011-04-01">April 1, 2011     ...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">     ...       <meta itemprop="worstRating" content = "1">     ...       <span itemprop="ratingValue">1</span>/     ...       <span itemprop="bestRating">5</span>stars     ...     </div>     ...     <span itemprop="description">The lamp burned out and now I have to replace     ...     it. </span>     ...   </div>     ...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">     ...     <span itemprop="name">Value purchase</span> -     ...     by <span itemprop="author">Lucas</span>,     ...     <meta itemprop="datePublished" content="2011-03-25">March 25, 2011     ...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">     ...       <meta itemprop="worstRating" content = "1"/>     ...       <span itemprop="ratingValue">4</span>/     ...       <span itemprop="bestRating">5</span>stars     ...     </div>     ...     <span itemprop="description">Great microwave for the price. It is small and     ...     fits in my apartment.</span>     ...   </div>     ...   ...     ... </div>     ... """     >>> sel = Selector(text=doc, type="html")     >>> for scope in sel.xpath("//div[@itemscope]"):     ...     print("current scope:", scope.xpath("@itemtype").getall())     ...     props = scope.xpath(     ...         """     ...                 set:difference(./descendant::*/@itemprop,     ...                                .//*[@itemscope]/*/@itemprop)"""     ...     )     ...     print(f"    properties: {props.getall()}")     ...     print("")     ...      current scope: ['http://schema.org/Product']         properties: ['name', 'aggregateRating', 'offers', 'description', 'review', 'review']      current scope: ['http://schema.org/AggregateRating']         properties: ['ratingValue', 'reviewCount']      current scope: ['http://schema.org/Offer']         properties: ['price', 'availability']      current scope: ['http://schema.org/Review']         properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']      current scope: ['http://schema.org/Rating']         properties: ['worstRating', 'ratingValue', 'bestRating']      current scope: ['http://schema.org/Review']         properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']      current scope: ['http://schema.org/Rating']         properties: ['worstRating', 'ratingValue', 'bestRating']   Here we first iterate over ``itemscope`elements, and for each one,`<span class="title-ref"> we look for all </span><span class="title-ref">itemprops</span><span class="title-ref"> elements and exclude those that are themselves inside another </span><span class="title-ref">itemscope</span>\`.

### Other XPath extensions

Scrapy selectors also provide a sorely missed XPath extension function `has-class` that returns `True` for nodes that have all of the specified HTML classes.

For the following HTML:

`` `pycon     >>> from scrapy.http import HtmlResponse     >>> response = HtmlResponse(     ...     url="http://example.com",     ...     body="""     ... <html>     ...     <body>     ...         <p class="foo bar-baz">First</p>     ...         <p class="foo">Second</p>     ...         <p class="bar">Third</p>     ...         <p>Fourth</p>     ...     </body>     ... </html>     ... """,     ...     encoding="utf-8",     ... )  You can use it like this:  .. code-block:: pycon      >>> response.xpath('//p[has-class("foo")]')     [<Selector query='//p[has-class("foo")]' data='<p class="foo bar-baz">First</p>'>,     <Selector query='//p[has-class("foo")]' data='<p class="foo">Second</p>'>]     >>> response.xpath('//p[has-class("foo", "bar-baz")]')     [<Selector query='//p[has-class("foo", "bar-baz")]' data='<p class="foo bar-baz">First</p>'>]     >>> response.xpath('//p[has-class("foo", "bar")]')     []  So XPath ``//p\[has-class("foo", "bar-baz")\]`is roughly equivalent to CSS`<span class="title-ref"> </span><span class="title-ref">p.foo.bar-baz</span>\`. Please note, that it is slower in most of the cases, because it's a pure-Python function that's invoked for every node in question whereas the CSS lookup is translated into XPath and thus runs more efficiently, so performance-wise its uses are limited to situations that are not easily described with CSS selectors.

Parsel also simplifies adding your own XPath extensions with <span class="title-ref">\~parsel.xpathfuncs.set\_xpathfunc</span>.

## Built-in Selectors reference

<div class="module" data-synopsis="Selector class">

scrapy.selector

</div>

### Selector objects

<div class="autoclass">

Selector

<div class="automethod">

xpath

\> **Note**

</div>

</div>

  - \>  
    For convenience, this method can be called as `response.xpath()`
    
    <div class="automethod">
    
    css
    
    </div>
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    </div>
    
    For convenience, this method can be called as `response.css()`
    
    <div class="automethod">
    
    jmespath
    
    </div>
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    </div>
    
    For convenience, this method can be called as `response.jmespath()`
    
    <div class="automethod">
    
    get
    
    </div>
    
    See also: \[old-extraction-api\](\#old-extraction-api)
    
    <div class="autoattribute">
    
    attrib
    
    </div>
    
    See also: \[selecting-attributes\](\#selecting-attributes).
    
    <div class="automethod">
    
    re
    
    </div>
    
    <div class="automethod">
    
    re\_first
    
    </div>
    
    <div class="automethod">
    
    register\_namespace
    
    </div>
    
    <div class="automethod">
    
    remove\_namespaces
    
    </div>
    
    <div class="automethod">
    
    \_\_bool\_\_
    
    </div>
    
    <div class="automethod">
    
    getall
    
    </div>
    
    This method is added to Selector for consistency; it is more useful with SelectorList. See also: \[old-extraction-api\](\#old-extraction-api)

### SelectorList objects

<div class="autoclass">

SelectorList

<div class="automethod">

xpath

</div>

<div class="automethod">

css

</div>

<div class="automethod">

jmespath

</div>

<div class="automethod">

getall

See also: \[old-extraction-api\](\#old-extraction-api)

</div>

<div class="automethod">

get

See also: \[old-extraction-api\](\#old-extraction-api)

</div>

<div class="automethod">

re

</div>

<div class="automethod">

re\_first

</div>

<div class="autoattribute">

attrib

See also: \[selecting-attributes\](\#selecting-attributes).

</div>

</div>

## Examples

### Selector examples on HTML response

Here are some <span class="title-ref">Selector</span> examples to illustrate several concepts. In all cases, we assume there is already a <span class="title-ref">Selector</span> instantiated with a <span class="title-ref">\~scrapy.http.HtmlResponse</span> object like this:

`` `python       sel = Selector(html_response)  1. Select all ``\<h1\>``elements from an HTML response body, returning a list of    `Selector` objects (i.e. a `SelectorList` object):     .. code-block:: python        sel.xpath("//h1")  2. Extract the text of all``\<h1\>`elements from an HTML response body,    returning a list of strings:     .. code-block:: python        sel.xpath("//h1").getall()  # this includes the h1 tag       sel.xpath("//h1/text()").getall()  # this excludes the h1 tag  3. Iterate over all`\<p\>`tags and print their class attribute:      .. code-block:: python        for node in sel.xpath("//p"):           print(node.attrib["class"])   .. _selector-examples-xml:  Selector examples on XML response`\` ---------------------------------

Here are some examples to illustrate concepts for <span class="title-ref">Selector</span> objects instantiated with an <span class="title-ref">\~scrapy.http.XmlResponse</span> object:

`` `python       sel = Selector(xml_response)  1. Select all ``\<product\>``elements from an XML response body, returning a list    of `Selector` objects (i.e. a `SelectorList` object):     .. code-block:: python        sel.xpath("//product")  2. Extract all prices from a `Google Base XML feed`_ which requires registering    a namespace:     .. code-block:: python        sel.register_namespace("g", "http://base.google.com/ns/1.0")       sel.xpath("//g:price").getall()  .. skip: end``\`

---

settings.md

---

# Settings

The Scrapy settings allows you to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves.

The infrastructure of the settings provides a global namespace of key-value mappings that the code can use to pull configuration values from. The settings can be populated through different mechanisms, which are described below.

The settings are also the mechanism for selecting the currently active Scrapy project (in case you have many).

For a list of available built-in settings see: \[topics-settings-ref\](\#topics-settings-ref).

## Designating the settings

When you use Scrapy, you have to tell it which settings you're using. You can do this by using an environment variable, `SCRAPY_SETTINGS_MODULE`.

The value of `SCRAPY_SETTINGS_MODULE` should be in Python path syntax, e.g. `myproject.settings`. Note that the settings module should be on the Python \[import search path \<tut-searchpath\>\](\#import-search-path-\<tut-searchpath\>).

## Populating the settings

Settings can be populated using different mechanisms, each of which having a different precedence. Here is the list of them in decreasing order of precedence:

> 1.  Command line options (most precedence)
> 2.  Settings per-spider
> 3.  Project settings module
> 4.  Settings set by add-ons
> 5.  Default settings per-command
> 6.  Default global settings (less precedence)

The population of these settings sources is taken care of internally, but a manual handling is possible using API calls. See the \[topics-api-settings\](\#topics-api-settings) topic for reference.

These mechanisms are described in more detail below.

### 1\. Command line options

Arguments provided by the command line are the ones that take most precedence, overriding any other options. You can explicitly override one (or more) settings using the `-s` (or `--set`) command line option.

Example:

``` sh
scrapy crawl myspider -s LOG_FILE=scrapy.log
```

### 2\. Settings per-spider

Spiders (See the \[topics-spiders\](\#topics-spiders) chapter for reference) can define their own settings that will take precedence and override the project ones. One way to do so is by setting their <span class="title-ref">\~scrapy.Spider.custom\_settings</span> attribute:

`` `python     import scrapy       class MySpider(scrapy.Spider):         name = "myspider"          custom_settings = {             "SOME_SETTING": "some value",         }  It's often better to implement `~scrapy.Spider.update_settings` instead, ``\` and settings set there should use the "spider" priority explicitly:

`` `python     import scrapy       class MySpider(scrapy.Spider):         name = "myspider"          @classmethod         def update_settings(cls, settings):             super().update_settings(settings)             settings.set("SOME_SETTING", "some value", priority="spider")  .. versionadded:: 2.11  It's also possible to modify the settings in the ``<span class="title-ref"> </span>\~scrapy.Spider.from\_crawler\` method, e.g. based on \[spider arguments \<spiderargs\>\](\#spider arguments-\<spiderargs\>) or other logic:

`` `python     import scrapy       class MySpider(scrapy.Spider):         name = "myspider"          @classmethod         def from_crawler(cls, crawler, *args, **kwargs):             spider = super().from_crawler(crawler, *args, **kwargs)             if "some_argument" in kwargs:                 spider.settings.set(                     "SOME_SETTING", kwargs["some_argument"], priority="spider"                 )             return spider  3. Project settings module ``\` --------------------------

The project settings module is the standard configuration file for your Scrapy project, it's where most of your custom settings will be populated. For a standard Scrapy project, this means you'll be adding or changing the settings in the `settings.py` file created for your project.

### 4\. Settings set by add-ons

\[Add-ons \<topics-addons\>\](\#add-ons-\<topics-addons\>) can modify settings. They should do this with this priority, though this is not enforced.

### 5\. Default settings per-command

Each \[Scrapy tool \</topics/commands\>\](Scrapy tool \</topics/commands\>.md) command can have its own default settings, which override the global default settings. Those custom command settings are specified in the `default_settings` attribute of the command class.

### 6\. Default global settings

The global defaults are located in the `scrapy.settings.default_settings` module and documented in the \[topics-settings-ref\](\#topics-settings-ref) section.

## Compatibility with pickle

Setting values must be \[picklable \<pickle-picklable\>\](\#picklable-\<pickle-picklable\>).

## Import paths and classes

<div class="versionadded">

2.4.0

</div>

When a setting references a callable object to be imported by Scrapy, such as a class or a function, there are two different ways you can specify that object:

  - As a string containing the import path of that object
  - As the object itself

For example:

`` `python    from mybot.pipelines.validate import ValidateMyItem     ITEM_PIPELINES = {        # passing the classname...        ValidateMyItem: 300,        # ...equals passing the class path        "mybot.pipelines.validate.ValidateMyItem": 300,    }  .. note:: Passing non-callable objects is not supported.   How to access settings ``\` ======================

In a spider, the settings are available through `self.settings`:

`` `python     class MySpider(scrapy.Spider):         name = "myspider"         start_urls = ["http://example.com"]          def parse(self, response):             print(f"Existing settings: {self.settings.attributes.keys()}")  .. note::     The ``settings`attribute is set in the base Spider class after the spider     is initialized.  If you want to use the settings before the initialization     (e.g., in your spider's`\_\_init\_\_()`` method), you'll need to override the     `~scrapy.Spider.from_crawler` method.  Settings can be accessed through the `scrapy.crawler.Crawler.settings` ``<span class="title-ref"> attribute of the Crawler that is passed to </span><span class="title-ref">from\_crawler</span>\` method in extensions, middlewares and item pipelines:

`` `python     class MyExtension:         def __init__(self, log_is_enabled=False):             if log_is_enabled:                 print("log is enabled!")          @classmethod         def from_crawler(cls, crawler):             settings = crawler.settings             return cls(settings.getbool("LOG_ENABLED"))  The settings object can be used like a dict (e.g., ``<span class="title-ref"> </span><span class="title-ref">settings\['LOG\_ENABLED'\]</span><span class="title-ref">), but it's usually preferred to extract the setting in the format you need it to avoid type errors, using one of the methods provided by the </span>\~scrapy.settings.Settings\` API.

## Rationale for setting names

Setting names are usually prefixed with the component that they configure. For example, proper setting names for a fictional robots.txt extension would be `ROBOTSTXT_ENABLED`, `ROBOTSTXT_OBEY`, `ROBOTSTXT_CACHEDIR`, etc.

## Built-in settings reference

Here's a list of all available Scrapy settings, in alphabetical order, along with their default values and the scope where they apply.

The scope, where available, shows where the setting is being used, if it's tied to any particular component. In that case the module of that component will be shown, typically an extension, middleware or pipeline. It also means that the component must be enabled in order for the setting to have any effect.

<div class="setting">

ADDONS

</div>

### ADDONS

Default: `{}`

A dict containing paths to the add-ons enabled in your project and their priorities. For more information, see \[topics-addons\](\#topics-addons).

<div class="setting">

AWS\_ACCESS\_KEY\_ID

</div>

### AWS\_ACCESS\_KEY\_ID

Default: `None`

The AWS access key used by code that requires access to [Amazon Web services](https://aws.amazon.com/), such as the \[S3 feed storage backend \<topics-feed-storage-s3\>\](\#s3-feed-storage-backend-\<topics-feed-storage-s3\>).

<div class="setting">

AWS\_SECRET\_ACCESS\_KEY

</div>

### AWS\_SECRET\_ACCESS\_KEY

Default: `None`

The AWS secret key used by code that requires access to [Amazon Web services](https://aws.amazon.com/), such as the \[S3 feed storage backend \<topics-feed-storage-s3\>\](\#s3-feed-storage-backend-\<topics-feed-storage-s3\>).

<div class="setting">

AWS\_SESSION\_TOKEN

</div>

### AWS\_SESSION\_TOKEN

Default: `None`

The AWS security token used by code that requires access to [Amazon Web services](https://aws.amazon.com/), such as the \[S3 feed storage backend \<topics-feed-storage-s3\>\](\#s3-feed-storage-backend-\<topics-feed-storage-s3\>), when using [temporary security credentials](https://docs.aws.amazon.com/IAM/latest/UserGuide/security-creds.html).

<div class="setting">

AWS\_ENDPOINT\_URL

</div>

### AWS\_ENDPOINT\_URL

Default: `None`

Endpoint URL used for S3-like storage, for example Minio or s3.scality.

<div class="setting">

AWS\_USE\_SSL

</div>

### AWS\_USE\_SSL

Default: `None`

Use this option if you want to disable SSL connection for communication with S3 or S3-like storage. By default SSL will be used.

<div class="setting">

AWS\_VERIFY

</div>

### AWS\_VERIFY

Default: `None`

Verify SSL connection between Scrapy and S3 or S3-like storage. By default SSL verification will occur.

<div class="setting">

AWS\_REGION\_NAME

</div>

### AWS\_REGION\_NAME

Default: `None`

The name of the region associated with the AWS client.

<div class="setting">

ASYNCIO\_EVENT\_LOOP

</div>

### ASYNCIO\_EVENT\_LOOP

Default: `None`

Import path of a given `asyncio` event loop class.

If the asyncio reactor is enabled (see `TWISTED_REACTOR`) this setting can be used to specify the asyncio event loop to be used with it. Set the setting to the import path of the desired asyncio event loop class. If the setting is set to `None` the default asyncio event loop will be used.

If you are installing the asyncio reactor manually using the <span class="title-ref">\~scrapy.utils.reactor.install\_reactor</span> function, you can use the `event_loop_path` parameter to indicate the import path of the event loop class to be used.

Note that the event loop class must inherit from <span class="title-ref">asyncio.AbstractEventLoop</span>.

<div class="caution">

<div class="title">

Caution

</div>

Please be aware that, when using a non-default event loop (either defined via `ASYNCIO_EVENT_LOOP` or installed with <span class="title-ref">\~scrapy.utils.reactor.install\_reactor</span>), Scrapy will call <span class="title-ref">asyncio.set\_event\_loop</span>, which will set the specified event loop as the current loop for the current OS thread.

</div>

<div class="setting">

BOT\_NAME

</div>

### BOT\_NAME

Default: `'scrapybot'`

The name of the bot implemented by this Scrapy project (also known as the project name). This name will be used for the logging too.

It's automatically populated with your project name when you create your project with the `startproject` command.

<div class="setting">

CONCURRENT\_ITEMS

</div>

### CONCURRENT\_ITEMS

Default: `100`

Maximum number of concurrent items (per response) to process in parallel in \[item pipelines \<topics-item-pipeline\>\](\#item-pipelines-\<topics-item-pipeline\>).

<div class="setting">

CONCURRENT\_REQUESTS

</div>

### CONCURRENT\_REQUESTS

Default: `16`

The maximum number of concurrent (i.e. simultaneous) requests that will be performed by the Scrapy downloader.

<div class="setting">

CONCURRENT\_REQUESTS\_PER\_DOMAIN

</div>

### CONCURRENT\_REQUESTS\_PER\_DOMAIN

Default: `8`

The maximum number of concurrent (i.e. simultaneous) requests that will be performed to any single domain.

See also: \[topics-autothrottle\](\#topics-autothrottle) and its `AUTOTHROTTLE_TARGET_CONCURRENCY` option.

<div class="setting">

CONCURRENT\_REQUESTS\_PER\_IP

</div>

### CONCURRENT\_REQUESTS\_PER\_IP

Default: `0`

The maximum number of concurrent (i.e. simultaneous) requests that will be performed to any single IP. If non-zero, the `CONCURRENT_REQUESTS_PER_DOMAIN` setting is ignored, and this one is used instead. In other words, concurrency limits will be applied per IP, not per domain.

This setting also affects `DOWNLOAD_DELAY` and \[topics-autothrottle\](\#topics-autothrottle): if `CONCURRENT_REQUESTS_PER_IP` is non-zero, download delay is enforced per IP, not per domain.

<div class="setting">

DEFAULT\_ITEM\_CLASS

</div>

### DEFAULT\_ITEM\_CLASS

Default: `'scrapy.Item'`

The default class that will be used for instantiating items in the \[the Scrapy shell \<topics-shell\>\](\#the scrapy-shell-\<topics-shell\>).

<div class="setting">

DEFAULT\_REQUEST\_HEADERS

</div>

### DEFAULT\_REQUEST\_HEADERS

Default:

`` `python     {         "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",         "Accept-Language": "en",     }  The default headers used for Scrapy HTTP Requests. They're populated in the ``<span class="title-ref"> </span>\~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware\`.

<div class="caution">

<div class="title">

Caution

</div>

Cookies set via the `Cookie` header are not considered by the \[cookies-mw\](\#cookies-mw). If you need to set cookies for a request, use the <span class="title-ref">Request.cookies \<scrapy.Request\></span> parameter. This is a known current limitation that is being worked on.

</div>

<div class="setting">

DEPTH\_LIMIT

</div>

### DEPTH\_LIMIT

Default: `0`

Scope: `scrapy.spidermiddlewares.depth.DepthMiddleware`

The maximum depth that will be allowed to crawl for any site. If zero, no limit will be imposed.

<div class="setting">

DEPTH\_PRIORITY

</div>

### DEPTH\_PRIORITY

Default: `0`

Scope: `scrapy.spidermiddlewares.depth.DepthMiddleware`

An integer that is used to adjust the <span class="title-ref">\~scrapy.Request.priority</span> of a <span class="title-ref">\~scrapy.Request</span> based on its depth.

The priority of a request is adjusted as follows:

`` `python     request.priority = request.priority - (depth * DEPTH_PRIORITY)  As depth increases, positive values of ``DEPTH\_PRIORITY`decrease request`\` priority (BFO), while negative values increase request priority (DFO). See also \[faq-bfo-dfo\](\#faq-bfo-dfo).

\> **Note** \> This setting adjusts priority **in the opposite way** compared to other priority settings `REDIRECT_PRIORITY_ADJUST` and `RETRY_PRIORITY_ADJUST`.

<div class="setting">

DEPTH\_STATS\_VERBOSE

</div>

### DEPTH\_STATS\_VERBOSE

Default: `False`

Scope: `scrapy.spidermiddlewares.depth.DepthMiddleware`

Whether to collect verbose depth stats. If this is enabled, the number of requests for each depth is collected in the stats.

<div class="setting">

DNSCACHE\_ENABLED

</div>

### DNSCACHE\_ENABLED

Default: `True`

Whether to enable DNS in-memory cache.

<div class="setting">

DNSCACHE\_SIZE

</div>

### DNSCACHE\_SIZE

Default: `10000`

DNS in-memory cache size.

<div class="setting">

DNS\_RESOLVER

</div>

### DNS\_RESOLVER

<div class="versionadded">

2.0

</div>

Default: `'scrapy.resolver.CachingThreadedResolver'`

The class to be used to resolve DNS names. The default `scrapy.resolver.CachingThreadedResolver` supports specifying a timeout for DNS requests via the `DNS_TIMEOUT` setting, but works only with IPv4 addresses. Scrapy provides an alternative resolver, `scrapy.resolver.CachingHostnameResolver`, which supports IPv4/IPv6 addresses but does not take the `DNS_TIMEOUT` setting into account.

<div class="setting">

DNS\_TIMEOUT

</div>

### DNS\_TIMEOUT

Default: `60`

Timeout for processing of DNS queries in seconds. Float is supported.

<div class="setting">

DOWNLOADER

</div>

### DOWNLOADER

Default: `'scrapy.core.downloader.Downloader'`

The downloader to use for crawling.

<div class="setting">

DOWNLOADER\_HTTPCLIENTFACTORY

</div>

### DOWNLOADER\_HTTPCLIENTFACTORY

Default: `'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'`

Defines a Twisted `protocol.ClientFactory` class to use for HTTP/1.0 connections (for `HTTP10DownloadHandler`).

\> **Note** \> HTTP/1.0 is rarely used nowadays so you can safely ignore this setting, unless you really want to use HTTP/1.0 and override `DOWNLOAD_HANDLERS` for `http(s)` scheme accordingly, i.e. to `'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'`.

<div class="setting">

DOWNLOADER\_CLIENTCONTEXTFACTORY

</div>

### DOWNLOADER\_CLIENTCONTEXTFACTORY

Default: `'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'`

Represents the classpath to the ContextFactory to use.

Here, "ContextFactory" is a Twisted term for SSL/TLS contexts, defining the TLS/SSL protocol version to use, whether to do certificate verification, or even enable client-side authentication (and various other things).

\> **Note** \> Scrapy default context factory **does NOT perform remote server certificate verification**. This is usually fine for web scraping.

> If you do need remote server certificate verification enabled, Scrapy also has another context factory class that you can set, `'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'`, which uses the platform's certificates to validate remote endpoints.

If you do use a custom ContextFactory, make sure its `__init__` method accepts a `method` parameter (this is the `OpenSSL.SSL` method mapping `DOWNLOADER_CLIENT_TLS_METHOD`), a `tls_verbose_logging` parameter (`bool`) and a `tls_ciphers` parameter (see `DOWNLOADER_CLIENT_TLS_CIPHERS`).

<div class="setting">

DOWNLOADER\_CLIENT\_TLS\_CIPHERS

</div>

### DOWNLOADER\_CLIENT\_TLS\_CIPHERS

Default: `'DEFAULT'`

Use this setting to customize the TLS/SSL ciphers used by the default HTTP/1.1 downloader.

The setting should contain a string in the [OpenSSL cipher list format](https://docs.openssl.org/master/man1/openssl-ciphers/#cipher-list-format), these ciphers will be used as client ciphers. Changing this setting may be necessary to access certain HTTPS websites: for example, you may need to use `'DEFAULT:!DH'` for a website with weak DH parameters or enable a specific cipher that is not included in `DEFAULT` if a website requires it.

<div class="setting">

DOWNLOADER\_CLIENT\_TLS\_METHOD

</div>

### DOWNLOADER\_CLIENT\_TLS\_METHOD

Default: `'TLS'`

Use this setting to customize the TLS/SSL method used by the default HTTP/1.1 downloader.

This setting must be one of these string values:

  - `'TLS'`: maps to OpenSSL's `TLS_method()` (a.k.a `SSLv23_method()`), which allows protocol negotiation, starting from the highest supported by the platform; **default, recommended**
  - `'TLSv1.0'`: this value forces HTTPS connections to use TLS version 1.0 ; set this if you want the behavior of Scrapy\<1.1
  - `'TLSv1.1'`: forces TLS version 1.1
  - `'TLSv1.2'`: forces TLS version 1.2

<div class="setting">

DOWNLOADER\_CLIENT\_TLS\_VERBOSE\_LOGGING

</div>

### DOWNLOADER\_CLIENT\_TLS\_VERBOSE\_LOGGING

Default: `False`

Setting this to `True` will enable DEBUG level messages about TLS connection parameters after establishing HTTPS connections. The kind of information logged depends on the versions of OpenSSL and pyOpenSSL.

This setting is only used for the default `DOWNLOADER_CLIENTCONTEXTFACTORY`.

<div class="setting">

DOWNLOADER\_MIDDLEWARES

</div>

### DOWNLOADER\_MIDDLEWARES

Default:: `{}`

A dict containing the downloader middlewares enabled in your project, and their orders. For more info see \[topics-downloader-middleware-setting\](\#topics-downloader-middleware-setting).

<div class="setting">

DOWNLOADER\_MIDDLEWARES\_BASE

</div>

### DOWNLOADER\_MIDDLEWARES\_BASE

Default:

`` `python     {         "scrapy.downloadermiddlewares.offsite.OffsiteMiddleware": 50,         "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware": 100,         "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware": 300,         "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware": 350,         "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware": 400,         "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware": 500,         "scrapy.downloadermiddlewares.retry.RetryMiddleware": 550,         "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware": 560,         "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware": 580,         "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware": 590,         "scrapy.downloadermiddlewares.redirect.RedirectMiddleware": 600,         "scrapy.downloadermiddlewares.cookies.CookiesMiddleware": 700,         "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware": 750,         "scrapy.downloadermiddlewares.stats.DownloaderStats": 850,         "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware": 900,     }  A dict containing the downloader middlewares enabled by default in Scrapy. Low ``<span class="title-ref"> orders are closer to the engine, high orders are closer to the downloader. You should never modify this setting in your project, modify :setting:\`DOWNLOADER\_MIDDLEWARES</span> instead. For more info see \[topics-downloader-middleware-setting\](\#topics-downloader-middleware-setting).

<div class="setting">

DOWNLOADER\_STATS

</div>

### DOWNLOADER\_STATS

Default: `True`

Whether to enable downloader stats collection.

<div class="setting">

DOWNLOAD\_DELAY

</div>

### DOWNLOAD\_DELAY

Default: `0`

Minimum seconds to wait between 2 consecutive requests to the same domain.

Use `DOWNLOAD_DELAY` to throttle your crawling speed, to avoid hitting servers too hard.

Decimal numbers are supported. For example, to send a maximum of 4 requests every 10 seconds:

``` python
DOWNLOAD_DELAY = 2.5
```

This setting is also affected by the `RANDOMIZE_DOWNLOAD_DELAY` setting, which is enabled by default.

When `CONCURRENT_REQUESTS_PER_IP` is non-zero, delays are enforced per IP address instead of per domain.

Note that `DOWNLOAD_DELAY` can lower the effective per-domain concurrency below `CONCURRENT_REQUESTS_PER_DOMAIN`. If the response time of a domain is lower than `DOWNLOAD_DELAY`, the effective concurrency for that domain is 1. When testing throttling configurations, it usually makes sense to lower `CONCURRENT_REQUESTS_PER_DOMAIN` first, and only increase `DOWNLOAD_DELAY` once `CONCURRENT_REQUESTS_PER_DOMAIN` is 1 but a higher throttling is desired.

<div id="spider-download_delay-attribute">

\> **Note** \> This delay can be set per spider using <span class="title-ref">download\_delay</span> spider attribute.

</div>

It is also possible to change this setting per domain, although it requires non-trivial code. See the implementation of the \[AutoThrottle \<topics-autothrottle\>\](\#autothrottle \<topics-autothrottle\>) extension for an example.

<div class="setting">

DOWNLOAD\_HANDLERS

</div>

### DOWNLOAD\_HANDLERS

Default: `{}`

A dict containing the request downloader handlers enabled in your project. See `DOWNLOAD_HANDLERS_BASE` for example format.

<div class="setting">

DOWNLOAD\_HANDLERS\_BASE

</div>

### DOWNLOAD\_HANDLERS\_BASE

Default:

`` `python     {         "data": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler",         "file": "scrapy.core.downloader.handlers.file.FileDownloadHandler",         "http": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler",         "https": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler",         "s3": "scrapy.core.downloader.handlers.s3.S3DownloadHandler",         "ftp": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",     }   A dict containing the request download handlers enabled by default in Scrapy. ``<span class="title-ref"> You should never modify this setting in your project, modify :setting:\`DOWNLOAD\_HANDLERS</span> instead.

You can disable any of these download handlers by assigning `None` to their URI scheme in `DOWNLOAD_HANDLERS`. E.g., to disable the built-in FTP handler (without replacement), place this in your `settings.py`:

`` `python     DOWNLOAD_HANDLERS = {         "ftp": None,     }  .. _http2:  The default HTTPS handler uses HTTP/1.1. To use HTTP/2:  #.  Install ``Twisted\[http2\]\>=17.9.0``to install the packages required to     enable HTTP/2 support in Twisted.  #.  Update :setting:`DOWNLOAD_HANDLERS` as follows:      .. code-block:: python          DOWNLOAD_HANDLERS = {             "https": "scrapy.core.downloader.handlers.http2.H2DownloadHandler",         }  > **Warning** >      HTTP/2 support in Scrapy is experimental, and not yet recommended for     production environments. Future Scrapy versions may introduce related     changes without a deprecation period or warning.  > **Note** >      Known limitations of the current HTTP/2 implementation of Scrapy include:      -   No support for HTTP/2 Cleartext (h2c), since no major browser supports         HTTP/2 unencrypted (refer `http2 faq`_).      -   No setting to specify a maximum `frame size`_ larger than the default         value, 16384. Connections to servers that send a larger frame will         fail.      -   No support for `server pushes`_, which are ignored.      -   No support for the :signal:`bytes_received` and         :signal:`headers_received` signals.``\` .. \_http2 faq: <https://http2.github.io/faq/#does-http2-require-encryption> .. \_server pushes: <https://datatracker.ietf.org/doc/html/rfc7540#section-8.2>

<div class="setting">

DOWNLOAD\_SLOTS

</div>

### DOWNLOAD\_SLOTS

Default: `{}`

Allows to define concurrency/delay parameters on per slot (domain) basis:

>   - \`\`\`python
>     
>       - DOWNLOAD\_SLOTS = {  
>         "quotes.toscrape.com": {"concurrency": 1, "delay": 2, "randomize\_delay": False}, "books.toscrape.com": {"delay": 3, "randomize\_delay": False},
>     
>     }

\> **Note** \> For other downloader slots default settings values will be used:

>   - `DOWNLOAD_DELAY`: `delay`
>   - `CONCURRENT_REQUESTS_PER_DOMAIN`: `concurrency`
>   - `RANDOMIZE_DOWNLOAD_DELAY`: `randomize_delay`

<div class="setting">

DOWNLOAD\_TIMEOUT

</div>

DOWNLOAD\_TIMEOUT `` ` ----------------  Default: ``180``The amount of time (in secs) that the downloader will wait before timing out.  > **Note** >      This timeout can be set per spider using `download_timeout`     spider attribute and per-request using :reqmeta:`download_timeout`     Request.meta key.  .. setting:: DOWNLOAD_MAXSIZE .. reqmeta:: download_maxsize  DOWNLOAD_MAXSIZE ----------------  Default:``1073741824`(1 GiB)  The maximum response body size (in bytes) allowed. Bigger responses are aborted and ignored.  This applies both before and after compression. If decompressing a response body would exceed this limit, decompression is aborted and the response is ignored.  Use`0``to disable this limit.  This limit can be set per spider using the `download_maxsize` spider attribute and per request using the :reqmeta:`download_maxsize` Request.meta key.  .. setting:: DOWNLOAD_WARNSIZE .. reqmeta:: download_warnsize  DOWNLOAD_WARNSIZE -----------------  Default:``33554432`(32 MiB)  If the size of a response exceeds this value, before or after compression, a warning will be logged about it.  Use`0``to disable this limit.  This limit can be set per spider using the `download_warnsize` spider attribute and per request using the :reqmeta:`download_warnsize` Request.meta key.  .. setting:: DOWNLOAD_FAIL_ON_DATALOSS  DOWNLOAD_FAIL_ON_DATALOSS -------------------------  Default:``True`Whether or not to fail on broken responses, that is, declared`Content-Length`does not match content sent by the server or chunked response was not properly finish. If`True`, these responses raise a`ResponseFailed(\[\_DataLoss\])`error. If`False`, these responses are passed through and the flag`dataloss`is added to the response, i.e.:`'dataloss' in response.flags`is`True``.  Optionally, this can be set per-request basis by using the :reqmeta:`download_fail_on_dataloss` Request.meta key to``False``.  > **Note** >    A broken response, or data loss error, may happen under several   circumstances, from server misconfiguration to network errors to data   corruption. It is up to the user to decide if it makes sense to process   broken responses considering they may contain partial or incomplete content.   If :setting:`RETRY_ENABLED` is``True`and this setting is set to`True`,   the`ResponseFailed(\[\_DataLoss\])``failure will be retried as usual.  > **Warning** >      This setting is ignored by the     `~scrapy.core.downloader.handlers.http2.H2DownloadHandler`     download handler (see :setting:`DOWNLOAD_HANDLERS`). In case of a data loss     error, the corresponding HTTP/2 connection may be corrupted, affecting other     requests that use the same connection; hence, a``ResponseFailed(\[InvalidBodyLengthError\])`failure is always raised for every request that was using that connection.  .. setting:: DUPEFILTER_CLASS  DUPEFILTER_CLASS ----------------  Default:`'scrapy.dupefilters.RFPDupeFilter'`The class used to detect and filter duplicate requests.  The default (`RFPDupeFilter``) filters based on the :setting:`REQUEST_FINGERPRINTER_CLASS` setting.  You can disable filtering of duplicate requests by setting :setting:`DUPEFILTER_CLASS` to``'scrapy.dupefilters.BaseDupeFilter'`. Be very careful about this however, because you can get into crawling loops. It's usually a better idea to set the`dont\_filter`parameter to`True``on the specific `~scrapy.Request` that should not be filtered.  .. setting:: DUPEFILTER_DEBUG  DUPEFILTER_DEBUG ----------------  Default:``False`By default,`RFPDupeFilter``only logs the first duplicate request. Setting :setting:`DUPEFILTER_DEBUG` to``True`will make it log all duplicate requests.  .. setting:: EDITOR  EDITOR ------  Default:`vi``(on Unix systems) or the IDLE editor (on Windows)  The editor to use for editing spiders with the :command:`edit` command. Additionally, if the``EDITOR``environment variable is set, the :command:`edit` command will prefer it over the default setting.  .. setting:: EXTENSIONS  EXTENSIONS ----------  Default::``{}`A dict containing the extensions enabled in your project, and their orders.  .. setting:: EXTENSIONS_BASE  EXTENSIONS_BASE ---------------  Default:`\`python { "scrapy.extensions.corestats.CoreStats": 0, "scrapy.extensions.telnet.TelnetConsole": 0, "scrapy.extensions.memusage.MemoryUsage": 0, "scrapy.extensions.memdebug.MemoryDebugger": 0, "scrapy.extensions.closespider.CloseSpider": 0, "scrapy.extensions.feedexport.FeedExporter": 0, "scrapy.extensions.logstats.LogStats": 0, "scrapy.extensions.spiderstate.SpiderState": 0, "scrapy.extensions.throttle.AutoThrottle": 0, }

A dict containing the extensions available by default in Scrapy, and their `` ` orders. This setting contains all stable built-in extensions. Keep in mind that some of them need to be enabled through a setting.  For more information See the [extensions user guide  <topics-extensions>](#extensions-user-guide--<topics-extensions>) and the [list of available extensions <topics-extensions-ref>](#list-of-available-extensions-<topics-extensions-ref>).  .. setting:: FEED_TEMPDIR  FEED_TEMPDIR ------------  The Feed Temp dir allows you to set a custom folder to save crawler temporary files before uploading with [FTP feed storage <topics-feed-storage-ftp>](#ftp-feed-storage-<topics-feed-storage-ftp>) and [Amazon S3 <topics-feed-storage-s3>](#amazon-s3-<topics-feed-storage-s3>).  .. setting:: FEED_STORAGE_GCS_ACL  FEED_STORAGE_GCS_ACL --------------------  The Access Control List (ACL) used when storing items to [Google Cloud Storage <topics-feed-storage-gcs>](#google-cloud-storage-<topics-feed-storage-gcs>). For more information on how to set this value, please refer to the column *JSON API* in `Google Cloud documentation <https://cloud.google.com/storage/docs/access-control/lists>`_.  .. setting:: FTP_PASSIVE_MODE  FTP_PASSIVE_MODE ----------------  Default: ``True`Whether or not to use passive mode when initiating FTP transfers.  .. reqmeta:: ftp_password .. setting:: FTP_PASSWORD  FTP_PASSWORD ------------  Default:`"guest"`The password to use for FTP connections when there is no`"ftp\_password"`in`Request``meta.  .. note::     Paraphrasing `RFC 1635`_, although it is common to use either the password     "guest" or one's e-mail address for anonymous FTP,     some FTP servers explicitly ask for the user's e-mail address     and will not allow login with the "guest" password.    .. reqmeta:: ftp_user .. setting:: FTP_USER  FTP_USER --------  Default:``"anonymous"`The username to use for FTP connections when there is no`"ftp\_user"`in`Request`meta.  .. setting:: GCS_PROJECT_ID  GCS_PROJECT_ID -----------------  Default:`None``The Project ID that will be used when storing data on `Google Cloud Storage`_.  .. setting:: ITEM_PIPELINES  ITEM_PIPELINES --------------  Default:``{}`A dict containing the item pipelines to use, and their orders. Order values are arbitrary, but it is customary to define them in the 0-1000 range. Lower orders process before higher orders.  Example:`\`python ITEM\_PIPELINES = { "mybot.pipelines.validate.ValidateMyItem": 300, "mybot.pipelines.validate.StoreMyItem": 800, }

<div class="setting">

ITEM\_PIPELINES\_BASE

</div>

ITEM\_PIPELINES\_BASE `` ` -------------------  Default: ``{}``A dict containing the pipelines enabled by default in Scrapy. You should never modify this setting in your project, modify :setting:`ITEM_PIPELINES` instead.  .. setting:: JOBDIR  JOBDIR ------  Default:``None`A string indicating the directory for storing the state of a crawl when [pausing and resuming crawls <topics-jobs>](#pausing-and-resuming-crawls-<topics-jobs>).  .. setting:: LOG_ENABLED  LOG_ENABLED -----------  Default:`True`Whether to enable logging.  .. setting:: LOG_ENCODING  LOG_ENCODING ------------  Default:`'utf-8'`The encoding to use for logging.  .. setting:: LOG_FILE  LOG_FILE --------  Default:`None`File name to use for logging output. If`None`, standard error will be used.  .. setting:: LOG_FILE_APPEND  LOG_FILE_APPEND ---------------  Default:`True`If`False``, the log file specified with :setting:`LOG_FILE` will be overwritten (discarding the output from previous runs, if any).  .. setting:: LOG_FORMAT  LOG_FORMAT ----------  Default:``'%(asctime)s \[%(name)s\] %(levelname)s: %(message)s'`String for formatting log messages. Refer to the [Python logging documentation <logrecord-attributes>](#python-logging-documentation-<logrecord-attributes>) for the whole list of available placeholders.  .. setting:: LOG_DATEFORMAT  LOG_DATEFORMAT --------------  Default:`'%Y-%m-%d %H:%M:%S'`String for formatting date/time, expansion of the`%(asctime)s``placeholder in :setting:`LOG_FORMAT`. Refer to the [Python datetime documentation <strftime-strptime-behavior>](#python-datetime-documentation-<strftime-strptime-behavior>) for the whole list of available directives.  .. setting:: LOG_FORMATTER  LOG_FORMATTER -------------  Default: `scrapy.logformatter.LogFormatter`  The class to use for [formatting log messages <custom-log-formats>](#formatting-log-messages-<custom-log-formats>) for different actions.  .. setting:: LOG_LEVEL  LOG_LEVEL ---------  Default:``'DEBUG'`Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING, INFO, DEBUG. For more info see [topics-logging](#topics-logging).  .. setting:: LOG_STDOUT  LOG_STDOUT ----------  Default:`False`If`True`, all standard output (and error) of your process will be redirected to the log. For example if you`print('hello')`it will appear in the Scrapy log.  .. setting:: LOG_SHORT_NAMES  LOG_SHORT_NAMES ---------------  Default:`False`If`True`, the logs will just contain the root path. If it is set to`False`then it displays the component responsible for the log output  .. setting:: LOGSTATS_INTERVAL  LOGSTATS_INTERVAL -----------------  Default:`60.0``The interval (in seconds) between each logging printout of the stats by `~scrapy.extensions.logstats.LogStats`.  .. setting:: MEMDEBUG_ENABLED  MEMDEBUG_ENABLED ----------------  Default:``False`Whether to enable memory debugging.  .. setting:: MEMDEBUG_NOTIFY  MEMDEBUG_NOTIFY ---------------  Default:`\[\]`When memory debugging is enabled a memory report will be sent to the specified addresses if this setting is not empty, otherwise the report will be written to the log.  Example::      MEMDEBUG_NOTIFY = ['user@example.com']  .. setting:: MEMUSAGE_ENABLED  MEMUSAGE_ENABLED ----------------  Default:`True`Scope:`scrapy.extensions.memusage``Whether to enable the memory usage extension. This extension keeps track of a peak memory used by the process (it writes it to stats). It can also optionally shutdown the Scrapy process when it exceeds a memory limit (see :setting:`MEMUSAGE_LIMIT_MB`), and notify by email when that happened (see :setting:`MEMUSAGE_NOTIFY_MAIL`).  See [topics-extensions-ref-memusage](#topics-extensions-ref-memusage).  .. setting:: MEMUSAGE_LIMIT_MB  MEMUSAGE_LIMIT_MB -----------------  Default:``0`Scope:`scrapy.extensions.memusage`The maximum amount of memory to allow (in megabytes) before shutting down Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.  See [topics-extensions-ref-memusage](#topics-extensions-ref-memusage).  .. setting:: MEMUSAGE_CHECK_INTERVAL_SECONDS  MEMUSAGE_CHECK_INTERVAL_SECONDS -------------------------------  Default:`60.0`Scope:`scrapy.extensions.memusage``The [Memory usage extension <topics-extensions-ref-memusage>](#memory-usage-extension-<topics-extensions-ref-memusage>) checks the current memory usage, versus the limits set by :setting:`MEMUSAGE_LIMIT_MB` and :setting:`MEMUSAGE_WARNING_MB`, at fixed time intervals.  This sets the length of these intervals, in seconds.  See [topics-extensions-ref-memusage](#topics-extensions-ref-memusage).  .. setting:: MEMUSAGE_NOTIFY_MAIL  MEMUSAGE_NOTIFY_MAIL --------------------  Default:``False`Scope:`scrapy.extensions.memusage`A list of emails to notify if the memory limit has been reached.  Example::      MEMUSAGE_NOTIFY_MAIL = ['user@example.com']  See [topics-extensions-ref-memusage](#topics-extensions-ref-memusage).  .. setting:: MEMUSAGE_WARNING_MB  MEMUSAGE_WARNING_MB -------------------  Default:`0`Scope:`scrapy.extensions.memusage`The maximum amount of memory to allow (in megabytes) before sending a warning email notifying about it. If zero, no warning will be produced.  .. setting:: NEWSPIDER_MODULE  NEWSPIDER_MODULE ----------------  Default:`''``Module where to create new spiders using the :command:`genspider` command.  Example::      NEWSPIDER_MODULE = 'mybot.spiders_dev'  .. setting:: RANDOMIZE_DOWNLOAD_DELAY  RANDOMIZE_DOWNLOAD_DELAY ------------------------  Default:``True``If enabled, Scrapy will wait a random amount of time (between 0.5 * :setting:`DOWNLOAD_DELAY` and 1.5 * :setting:`DOWNLOAD_DELAY`) while fetching requests from the same website.  This randomization decreases the chance of the crawler being detected (and subsequently blocked) by sites which analyze requests looking for statistically significant similarities in the time between their requests.  The randomization policy is the same used by `wget`_``--random-wait``option.  If :setting:`DOWNLOAD_DELAY` is zero (default) this option has no effect.    .. setting:: REACTOR_THREADPOOL_MAXSIZE  REACTOR_THREADPOOL_MAXSIZE --------------------------  Default:``10`The maximum limit for Twisted Reactor thread pool size. This is common multi-purpose thread pool used by various Scrapy components. Threaded DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few. Increase this value if you're experiencing problems with insufficient blocking IO.  .. setting:: REDIRECT_PRIORITY_ADJUST  REDIRECT_PRIORITY_ADJUST ------------------------  Default:`+2`Scope:`scrapy.downloadermiddlewares.redirect.RedirectMiddleware`Adjust redirect request priority relative to original request:  - **a positive priority adjust (default) means higher priority.** - a negative priority adjust means lower priority.  .. setting:: ROBOTSTXT_OBEY  ROBOTSTXT_OBEY --------------  Default:`False`Scope:`scrapy.downloadermiddlewares.robotstxt`If enabled, Scrapy will respect robots.txt policies. For more information see [topics-dlmw-robots](#topics-dlmw-robots).  > **Note** >      While the default value is`False`for historical reasons,     this option is enabled by default in settings.py file generated     by`scrapy startproject`command.  .. setting:: ROBOTSTXT_PARSER  ROBOTSTXT_PARSER ----------------  Default:`'scrapy.robotstxt.ProtegoRobotParser'`The parser backend to use for parsing`robots.txt`files. For more information see [topics-dlmw-robots](#topics-dlmw-robots).  .. setting:: ROBOTSTXT_USER_AGENT  ROBOTSTXT_USER_AGENT ^^^^^^^^^^^^^^^^^^^^  Default:`None`The user agent string to use for matching in the robots.txt file. If`None``, the User-Agent header you are sending with the request or the :setting:`USER_AGENT` setting (in that order) will be used for determining the user agent to use in the robots.txt file.  .. setting:: SCHEDULER  SCHEDULER ---------  Default:``'scrapy.core.scheduler.Scheduler'`The scheduler class to be used for crawling. See the [topics-scheduler](#topics-scheduler) topic for details.  .. setting:: SCHEDULER_DEBUG  SCHEDULER_DEBUG ---------------  Default:`False`Setting to`True`will log debug information about the requests scheduler. This currently logs (only once) if the requests cannot be serialized to disk. Stats counter (`scheduler/unserializable`) tracks the number of times this happens.  Example entry in logs::      1956-01-31 00:00:00+0800 [scrapy.core.scheduler] ERROR: Unable to serialize request:     <GET http://example.com> - reason: cannot serialize <Request at 0x9a7c7ec>     (type Request)> - no more unserializable requests will be logged     (see 'scheduler/unserializable' stats counter)   .. setting:: SCHEDULER_DISK_QUEUE  SCHEDULER_DISK_QUEUE --------------------  Default:`'scrapy.squeues.PickleLifoDiskQueue'`Type of disk queue that will be used by scheduler. Other available types are`scrapy.squeues.PickleFifoDiskQueue`,`scrapy.squeues.MarshalFifoDiskQueue`,`scrapy.squeues.MarshalLifoDiskQueue`.  .. setting:: SCHEDULER_MEMORY_QUEUE  SCHEDULER_MEMORY_QUEUE ---------------------- Default:`'scrapy.squeues.LifoMemoryQueue'`Type of in-memory queue used by scheduler. Other available type is:`scrapy.squeues.FifoMemoryQueue`.  .. setting:: SCHEDULER_PRIORITY_QUEUE  SCHEDULER_PRIORITY_QUEUE ------------------------ Default:`'scrapy.pqueues.ScrapyPriorityQueue'`Type of priority queue used by the scheduler. Another available type is`scrapy.pqueues.DownloaderAwarePriorityQueue`.`scrapy.pqueues.DownloaderAwarePriorityQueue`works better than`scrapy.pqueues.ScrapyPriorityQueue`when you crawl many different domains in parallel. But currently`scrapy.pqueues.DownloaderAwarePriorityQueue``does not work together with :setting:`CONCURRENT_REQUESTS_PER_IP`.  .. setting:: SCRAPER_SLOT_MAX_ACTIVE_SIZE  SCRAPER_SLOT_MAX_ACTIVE_SIZE ----------------------------  .. versionadded:: 2.0  Default:``5\_000\_000`Soft limit (in bytes) for response data being processed.  While the sum of the sizes of all responses being processed is above this value, Scrapy does not process new requests.  .. setting:: SPIDER_CONTRACTS  SPIDER_CONTRACTS ----------------  Default::`{}`A dict containing the spider contracts enabled in your project, used for testing spiders. For more info see [topics-contracts](#topics-contracts).  .. setting:: SPIDER_CONTRACTS_BASE  SPIDER_CONTRACTS_BASE ---------------------  Default:`\`python { "scrapy.contracts.default.UrlContract": 1, "scrapy.contracts.default.ReturnsContract": 2, "scrapy.contracts.default.ScrapesContract": 3, }

A dict containing the Scrapy contracts enabled by default in Scrapy. You should `` ` never modify this setting in your project, modify :setting:`SPIDER_CONTRACTS` instead. For more info see [topics-contracts](#topics-contracts).  You can disable any of these contracts by assigning ``None``to their class path in :setting:`SPIDER_CONTRACTS`. E.g., to disable the built-in``ScrapesContract`, place this in your`settings.py`:`\`python SPIDER\_CONTRACTS = { "scrapy.contracts.default.ScrapesContract": None, }

<div class="setting">

SPIDER\_LOADER\_CLASS

</div>

SPIDER\_LOADER\_CLASS `` ` -------------------  Default: ``'scrapy.spiderloader.SpiderLoader'`The class that will be used for loading spiders, which must implement the [topics-api-spiderloader](#topics-api-spiderloader).  .. setting:: SPIDER_LOADER_WARN_ONLY  SPIDER_LOADER_WARN_ONLY -----------------------  Default:`False``By default, when Scrapy tries to import spider classes from :setting:`SPIDER_MODULES`, it will fail loudly if there is any``ImportError`or`SyntaxError`exception. But you can choose to silence this exception and turn it into a simple warning by setting`SPIDER\_LOADER\_WARN\_ONLY = True`.  .. note::     Some [scrapy commands <topics-commands>](#scrapy-commands-<topics-commands>) run with this setting to`True``already (i.e. they will only issue a warning and will not fail)     since they do not actually need to load spider classes to work:     :command:`scrapy runspider <runspider>`,     :command:`scrapy settings <settings>`,     :command:`scrapy startproject <startproject>`,     :command:`scrapy version <version>`.  .. setting:: SPIDER_MIDDLEWARES  SPIDER_MIDDLEWARES ------------------  Default::``{}`A dict containing the spider middlewares enabled in your project, and their orders. For more info see [topics-spider-middleware-setting](#topics-spider-middleware-setting).  .. setting:: SPIDER_MIDDLEWARES_BASE  SPIDER_MIDDLEWARES_BASE -----------------------  Default:`\`python { "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware": 50, "scrapy.spidermiddlewares.referer.RefererMiddleware": 700, "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware": 800, "scrapy.spidermiddlewares.depth.DepthMiddleware": 900, }

A dict containing the spider middlewares enabled by default in Scrapy, and `` ` their orders. Low orders are closer to the engine, high orders are closer to the spider. For more info see [topics-spider-middleware-setting](#topics-spider-middleware-setting).  .. setting:: SPIDER_MODULES  SPIDER_MODULES --------------  Default: ``\[\]`A list of modules where Scrapy will look for spiders.  Example:`\`python SPIDER\_MODULES = \["mybot.spiders\_prod", "mybot.spiders\_dev"\]

<div class="setting">

STATS\_CLASS

</div>

STATS\_CLASS `` ` -----------  Default: ``'scrapy.statscollectors.MemoryStatsCollector'`The class to use for collecting stats, who must implement the [topics-api-stats](#topics-api-stats).  .. setting:: STATS_DUMP  STATS_DUMP ----------  Default:`True`Dump the [Scrapy stats <topics-stats>](#scrapy-stats-<topics-stats>) (to the Scrapy log) once the spider finishes.  For more info see: [topics-stats](#topics-stats).  .. setting:: STATSMAILER_RCPTS  STATSMAILER_RCPTS -----------------  Default:`\[\]``(empty list)  Send Scrapy stats after spiders finish scraping. See `~scrapy.extensions.statsmailer.StatsMailer` for more info.  .. setting:: TELNETCONSOLE_ENABLED  TELNETCONSOLE_ENABLED ---------------------  Default:``True`A boolean which specifies if the [telnet console <topics-telnetconsole>](#telnet-console-<topics-telnetconsole>) will be enabled (provided its extension is also enabled).  .. setting:: TEMPLATES_DIR  TEMPLATES_DIR -------------  Default:`templates``dir inside scrapy module  The directory where to look for templates when creating new projects with :command:`startproject` command and new spiders with :command:`genspider` command.  The project name must not conflict with the name of custom files or directories in the``project`subdirectory.  .. setting:: TWISTED_REACTOR  TWISTED_REACTOR ---------------  .. versionadded:: 2.0  Default:`None``Import path of a given :mod:`~twisted.internet.reactor`.  Scrapy will install this reactor if no other reactor is installed yet, such as when the``scrapy``CLI program is invoked or when using the `~scrapy.crawler.CrawlerProcess` class.  If you are using the `~scrapy.crawler.CrawlerRunner` class, you also need to install the correct reactor manually. You can do that using `~scrapy.utils.reactor.install_reactor`:  .. autofunction:: scrapy.utils.reactor.install_reactor  If a reactor is already installed, `~scrapy.utils.reactor.install_reactor` has no effect.  `CrawlerRunner.__init__ <scrapy.crawler.CrawlerRunner.__init__>` raises `Exception` if the installed reactor does not match the :setting:`TWISTED_REACTOR` setting; therefore, having top-level :mod:`~twisted.internet.reactor` imports in project files and imported third-party libraries will make Scrapy raise `Exception` when it checks which reactor is installed.  In order to use the reactor installed by Scrapy:``\`python import scrapy from twisted.internet import reactor

>   - class QuotesSpider(scrapy.Spider):  
>     name = "quotes"
>     
>     def \_\_init\_\_(self, *args,kwargs): self.timeout = int(kwargs.pop("timeout", "60")) super(QuotesSpider, self).\_\_init\_\_(*args, \*\*kwargs)
>     
>       - def start\_requests(self):  
>         reactor.callLater(self.timeout, self.stop)
>         
>         urls = \["<https://quotes.toscrape.com/page/1>"\] for url in urls: yield scrapy.Request(url=url, callback=self.parse)
>     
>       - def parse(self, response):
>         
>           - for quote in response.css("div.quote"):  
>             yield {"text": quote.css("span.text::text").get()}
>     
>       - def stop(self):  
>         self.crawler.engine.close\_spider(self, "timeout")

which raises <span class="title-ref">Exception</span>, becomes:

``` python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def __init__(self, *args, **kwargs):
        self.timeout = int(kwargs.pop("timeout", "60"))
        super(QuotesSpider, self).__init__(*args, **kwargs)

    def start_requests(self):
        from twisted.internet import reactor

        reactor.callLater(self.timeout, self.stop)

        urls = ["https://quotes.toscrape.com/page/1"]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {"text": quote.css("span.text::text").get()}

    def stop(self):
        self.crawler.engine.close_spider(self, "timeout")
```

The default value of the `TWISTED_REACTOR` setting is `None`, which `` ` means that Scrapy will use the existing reactor if one is already installed, or install the default reactor defined by Twisted for the current platform. This is to maintain backward compatibility and avoid possible problems caused by using a non-default reactor.  .. versionchanged:: 2.7    The :command:`startproject` command now sets this setting to ``twisted.internet.asyncioreactor.AsyncioSelectorReactor`in the generated`settings.py`file.  For additional information, see [core/howto/choosing-reactor](core/howto/choosing-reactor.md).   .. setting:: URLLENGTH_LIMIT  URLLENGTH_LIMIT ---------------  Default:`2083`Scope:`spidermiddlewares.urllength``The maximum URL length to allow for crawled URLs.  This setting can act as a stopping condition in case of URLs of ever-increasing length, which may be caused for example by a programming error either in the target server or in your code. See also :setting:`REDIRECT_MAX_TIMES` and :setting:`DEPTH_LIMIT`.  Use``0``to allow URLs of any length.  The default value is copied from the `Microsoft Internet Explorer maximum URL length`_, even though this setting exists for different reasons.    .. setting:: USER_AGENT  USER_AGENT ----------  Default:``"Scrapy/VERSION (+https://scrapy.org)"``The default User-Agent to use when crawling, unless overridden. This user agent is also used by `~scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware` if :setting:`ROBOTSTXT_USER_AGENT` setting is``None\`\` and there is no overriding User-Agent header specified for the request.

### Settings documented elsewhere:

The following settings are documented elsewhere, please check each specific case to see how to enable and use them.

<div class="settingslist">

</div>

---

shell.md

---

# Scrapy shell

The Scrapy shell is an interactive shell where you can try and debug your scraping code very quickly, without having to run the spider. It's meant to be used for testing data extraction code, but you can actually use it for testing any kind of code as it is also a regular Python shell.

The shell is used for testing XPath or CSS expressions and see how they work and what data they extract from the web pages you're trying to scrape. It allows you to interactively test your expressions while you're writing your spider, without having to run the spider to test every change.

Once you get familiarized with the Scrapy shell, you'll see that it's an invaluable tool for developing and debugging your spiders.

## Configuring the shell

If you have [IPython](https://ipython.org/) installed, the Scrapy shell will use it (instead of the standard Python console). The [IPython](https://ipython.org/) console is much more powerful and provides smart auto-completion and colorized output, among other things.

We highly recommend you install [IPython](https://ipython.org/), specially if you're working on Unix systems (where [IPython](https://ipython.org/) excels). See the [IPython installation guide](https://ipython.org/install.html) for more info.

Scrapy also has support for [bpython](https://bpython-interpreter.org/), and will try to use it where [IPython](https://ipython.org/) is unavailable.

Through Scrapy's settings you can configure it to use any one of `ipython`, `bpython` or the standard `python` shell, regardless of which are installed. This is done by setting the `SCRAPY_PYTHON_SHELL` environment variable; or by defining it in your \[scrapy.cfg \<topics-config-settings\>\](\#scrapy.cfg-\<topics-config-settings\>):

    [settings]
    shell = bpython

## Launch the shell

To launch the Scrapy shell you can use the `shell` command like this:

    scrapy shell <url>

Where the `<url>` is the URL you want to scrape.

`shell` also works for local files. This can be handy if you want to play around with a local copy of a web page. `shell` understands the following syntaxes for local files:

    # UNIX-style
    scrapy shell ./path/to/file.html
    scrapy shell ../other/path/to/file.html
    scrapy shell /absolute/path/to/file.html
    
    # File URI
    scrapy shell file:///absolute/path/to/file.html

<div class="note">

<div class="title">

Note

</div>

When using relative file paths, be explicit and prepend them with `./` (or `../` when relevant). `scrapy shell index.html` will not work as one might expect (and this is by design, not a bug).

Because `shell` favors HTTP URLs over File URIs, and `index.html` being syntactically similar to `example.com`, `shell` will treat `index.html` as a domain name and trigger a DNS lookup error:

    $ scrapy shell index.html
    [ ... scrapy shell starts ... ]
    [ ... traceback ... ]
    twisted.internet.error.DNSLookupError: DNS lookup failed:
    address 'index.html' not found: [Errno -5] No address associated with hostname.

`shell` will not test beforehand if a file called `index.html` exists in the current directory. Again, be explicit.

</div>

## Using the shell

The Scrapy shell is just a regular Python console (or [IPython](https://ipython.org/) console if you have it available) which provides some additional shortcut functions for convenience.

### Available Shortcuts

  - `shelp()` - print a help with the list of available objects and shortcuts
  - `fetch(url[, redirect=True])` - fetch a new response from the given URL and update all related objects accordingly. You can optionally ask for HTTP 3xx redirections to not be followed by passing `redirect=False`
  - `fetch(request)` - fetch a new response from the given request and update all related objects accordingly.
  - `view(response)` - open the given response in your local web browser, for inspection. This will add a [\\\<base\\\> tag]() to the response body in order for external links (such as images and style sheets) to display properly. Note, however, that this will create a temporary file in your computer, which won't be removed automatically.

### Available Scrapy objects

The Scrapy shell automatically creates some convenient objects from the downloaded page, like the <span class="title-ref">\~scrapy.http.Response</span> object and the <span class="title-ref">\~scrapy.Selector</span> objects (for both HTML and XML content).

Those objects are:

  - `crawler` - the current <span class="title-ref">\~scrapy.crawler.Crawler</span> object.
  - `spider` - the Spider which is known to handle the URL, or a <span class="title-ref">\~scrapy.Spider</span> object if there is no spider found for the current URL
  - `request` - a <span class="title-ref">\~scrapy.Request</span> object of the last fetched page. You can modify this request using <span class="title-ref">\~scrapy.Request.replace</span> or fetch a new request (without leaving the shell) using the `fetch` shortcut.
  - `response` - a <span class="title-ref">\~scrapy.http.Response</span> object containing the last fetched page
  - `settings` - the current \[Scrapy settings \<topics-settings\>\](\#scrapy-settings-\<topics-settings\>)

## Example of shell session

Here's an example of a typical shell session where we start by scraping the <https://scrapy.org> page, and then proceed to scrape the <https://old.reddit.com/> page. Finally, we modify the (Reddit) request method to POST and re-fetch it getting an error. We end the session by typing Ctrl-D (in Unix systems) or Ctrl-Z in Windows.

Keep in mind that the data extracted here may not be the same when you try it, as those pages are not static and could have changed by the time you test this. The only purpose of this example is to get you familiarized with how the Scrapy shell works.

First, we launch the shell:

    scrapy shell 'https://scrapy.org' --nolog

\> **Note** \> Remember to always enclose URLs in quotes when running the Scrapy shell from the command line, otherwise URLs containing arguments (i.e. the `&` character) will not work.

> On Windows, use double quotes instead:
> 
>     scrapy shell "https://scrapy.org" --nolog

Then, the shell fetches the URL (using the Scrapy downloader) and prints the list of available objects and useful shortcuts (you'll notice that these lines all start with the `[s]` prefix):

    [s] Available Scrapy objects:
    [s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
    [s]   crawler    <scrapy.crawler.Crawler object at 0x7f07395dd690>
    [s]   item       {}
    [s]   request    <GET https://scrapy.org>
    [s]   response   <200 https://scrapy.org/>
    [s]   settings   <scrapy.settings.Settings object at 0x7f07395dd710>
    [s]   spider     <DefaultSpider 'default' at 0x7f0735891690>
    [s] Useful shortcuts:
    [s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
    [s]   fetch(req)                  Fetch a scrapy.Request and update local objects
    [s]   shelp()           Shell help (print this help)
    [s]   view(response)    View response in a browser
    
    >>>

After that, we can start playing with the objects:

`` `pycon     >>> response.xpath("//title/text()").get()     'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework'      >>> fetch("https://old.reddit.com/")      >>> response.xpath("//title/text()").get()     'reddit: the front page of the internet'      >>> request = request.replace(method="POST")      >>> fetch(request)      >>> response.status     404      >>> from pprint import pprint      >>> pprint(response.headers)     {'Accept-Ranges': ['bytes'],     'Cache-Control': ['max-age=0, must-revalidate'],     'Content-Type': ['text/html; charset=UTF-8'],     'Date': ['Thu, 08 Dec 2016 16:21:19 GMT'],     'Server': ['snooserv'],     'Set-Cookie': ['loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',                     'loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',                     'loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',                     'loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure'],     'Vary': ['accept-encoding'],     'Via': ['1.1 varnish'],     'X-Cache': ['MISS'],     'X-Cache-Hits': ['0'],     'X-Content-Type-Options': ['nosniff'],     'X-Frame-Options': ['SAMEORIGIN'],     'X-Moose': ['majestic'],     'X-Served-By': ['cache-cdg8730-CDG'],     'X-Timer': ['S1481214079.394283,VS0,VE159'],     'X-Ua-Compatible': ['IE=edge'],     'X-Xss-Protection': ['1; mode=block']}   .. _topics-shell-inspect-response:  Invoking the shell from spiders to inspect responses ``\` ====================================================

Sometimes you want to inspect the responses that are being processed in a certain point of your spider, if only to check that response you expect is getting there.

This can be achieved by using the `scrapy.shell.inspect_response` function.

Here's an example of how you would call it from your spider:

`` `python     import scrapy       class MySpider(scrapy.Spider):         name = "myspider"         start_urls = [             "http://example.com",             "http://example.org",             "http://example.net",         ]          def parse(self, response):             # We want to inspect one specific response.             if ".org" in response.url:                 from scrapy.shell import inspect_response                  inspect_response(response, self)              # Rest of parsing code.  When you run the spider, you will get something similar to this::      2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com> (referer: None)     2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.org> (referer: None)     [s] Available Scrapy objects:     [s]   crawler    <scrapy.crawler.Crawler object at 0x1e16b50>     ...      >>> response.url     'http://example.org'  Then, you can check if the extraction code is working:  .. code-block:: pycon      >>> response.xpath('//h1[@class="fn"]')     []  Nope, it doesn't. So you can open the response in your web browser and see if ``\` it's the response you were expecting:

`` `pycon     >>> view(response)     True  Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and resume the ``\` crawling:

    >>> ^D
    2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.net> (referer: None)
    ...

Note that you can't use the `fetch` shortcut here since the Scrapy engine is blocked by the shell. However, after you leave the shell, the spider will continue crawling where it stopped, as shown above.

---

signals.md

---

# Signals

Scrapy uses signals extensively to notify when certain events occur. You can catch some of those signals in your Scrapy project (using an \[extension \<topics-extensions\>\](\#extension \<topics-extensions\>), for example) to perform additional tasks or extend Scrapy to add functionality not provided out of the box.

Even though signals provide several arguments, the handlers that catch them don't need to accept all of them - the signal dispatching mechanism will only deliver the arguments that the handler receives.

You can connect to signals (or send your own) through the \[topics-api-signals\](\#topics-api-signals).

Here is a simple example showing how you can catch signals and perform some action:

`` `python     from scrapy import signals     from scrapy import Spider       class DmozSpider(Spider):         name = "dmoz"         allowed_domains = ["dmoz.org"]         start_urls = [             "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",             "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/",         ]          @classmethod         def from_crawler(cls, crawler, *args, **kwargs):             spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)             crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)             return spider          def spider_closed(self, spider):             spider.logger.info("Spider closed: %s", spider.name)          def parse(self, response):             pass  .. _signal-deferred:  Deferred signal handlers ``\` ========================

Some signals support returning <span class="title-ref">\~twisted.internet.defer.Deferred</span> or `awaitable objects <awaitable>` from their handlers, allowing you to run asynchronous code that does not block Scrapy. If a signal handler returns one of these objects, Scrapy waits for that asynchronous operation to finish.

Let's take an example using \[coroutines \<topics-coroutines\>\](\#coroutines-\<topics-coroutines\>):

`` `python     import scrapy       class SignalSpider(scrapy.Spider):         name = "signals"         start_urls = ["https://quotes.toscrape.com/page/1/"]          @classmethod         def from_crawler(cls, crawler, *args, **kwargs):             spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)             crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)             return spider          async def item_scraped(self, item):             # Send the scraped item to the server             response = await treq.post(                 "http://example.com/post",                 json.dumps(item).encode("ascii"),                 headers={b"Content-Type": [b"application/json"]},             )              return response          def parse(self, response):             for quote in response.css("div.quote"):                 yield {                     "text": quote.css("span.text::text").get(),                     "author": quote.css("small.author::text").get(),                     "tags": quote.css("div.tags a.tag::text").getall(),                 }  See the [topics-signals-ref](#topics-signals-ref) below to know which signals support ``<span class="title-ref"> </span>\~twisted.internet.defer.Deferred\` and `awaitable objects <awaitable>`.

## Built-in signals reference

<div class="module" data-synopsis="Signals definitions">

scrapy.signals

</div>

Here's the list of Scrapy built-in signals and their meaning.

### Engine signals

#### engine\_started

<div class="signal">

engine\_started

</div>

<div class="function">

engine\_started()

Sent when the Scrapy engine has started crawling.

This signal supports returning deferreds from its handlers.

</div>

<div class="note">

<div class="title">

Note

</div>

This signal may be fired *after* the `spider_opened` signal, depending on how the spider was started. So **don't** rely on this signal getting fired before `spider_opened`.

</div>

#### engine\_stopped

<div class="signal">

engine\_stopped

</div>

<div class="function">

engine\_stopped()

Sent when the Scrapy engine is stopped (for example, when a crawling process has finished).

This signal supports returning deferreds from its handlers.

</div>

### Item signals

<div class="note">

<div class="title">

Note

</div>

As at max `CONCURRENT_ITEMS` items are processed in parallel, many deferreds are fired together using <span class="title-ref">\~twisted.internet.defer.DeferredList</span>. Hence the next batch waits for the <span class="title-ref">\~twisted.internet.defer.DeferredList</span> to fire and then runs the respective item signal handler for the next batch of scraped items.

</div>

#### item\_scraped

<div class="signal">

item\_scraped

</div>

<div class="function">

item\_scraped(item, response, spider)

Sent when an item has been scraped, after it has passed all the \[topics-item-pipeline\](\#topics-item-pipeline) stages (without being dropped).

This signal supports returning deferreds from its handlers.

  - param item  
    the scraped item

  - type item  
    \[item object \<item-types\>\](\#item-object-\<item-types\>)

  - param spider  
    the spider which scraped the item

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

  - param response  
    the response from where the item was scraped, or `None` if it was yielded from <span class="title-ref">\~scrapy.Spider.start\_requests</span>.

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> | `None`

</div>

#### item\_dropped

<div class="signal">

item\_dropped

</div>

<div class="function">

item\_dropped(item, response, exception, spider)

Sent after an item has been dropped from the \[topics-item-pipeline\](\#topics-item-pipeline) when some stage raised a <span class="title-ref">\~scrapy.exceptions.DropItem</span> exception.

This signal supports returning deferreds from its handlers.

  - param item  
    the item dropped from the \[topics-item-pipeline\](\#topics-item-pipeline)

  - type item  
    \[item object \<item-types\>\](\#item-object-\<item-types\>)

  - param spider  
    the spider which scraped the item

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

  - param response  
    the response from where the item was dropped, or `None` if it was yielded from <span class="title-ref">\~scrapy.Spider.start\_requests</span>.

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> | `None`

  - param exception  
    the exception (which must be a <span class="title-ref">\~scrapy.exceptions.DropItem</span> subclass) which caused the item to be dropped

  - type exception  
    <span class="title-ref">\~scrapy.exceptions.DropItem</span> exception

</div>

#### item\_error

<div class="signal">

item\_error

</div>

<div class="function">

item\_error(item, response, spider, failure)

Sent when a \[topics-item-pipeline\](\#topics-item-pipeline) generates an error (i.e. raises an exception), except <span class="title-ref">\~scrapy.exceptions.DropItem</span> exception.

This signal supports returning deferreds from its handlers.

  - param item  
    the item that caused the error in the \[topics-item-pipeline\](\#topics-item-pipeline)

  - type item  
    \[item object \<item-types\>\](\#item-object-\<item-types\>)

  - param response  
    the response being processed when the exception was raised, or `None` if it was yielded from <span class="title-ref">\~scrapy.Spider.start\_requests</span>.

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> | `None`

  - param spider  
    the spider which raised the exception

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

  - param failure  
    the exception raised

  - type failure  
    twisted.python.failure.Failure

</div>

### Spider signals

#### spider\_closed

<div class="signal">

spider\_closed

</div>

<div class="function">

spider\_closed(spider, reason)

Sent after a spider has been closed. This can be used to release per-spider resources reserved on `spider_opened`.

This signal supports returning deferreds from its handlers.

  - param spider  
    the spider which has been closed

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

  - param reason  
    a string which describes the reason why the spider was closed. If it was closed because the spider has completed scraping, the reason is `'finished'`. Otherwise, if the spider was manually closed by calling the `close_spider` engine method, then the reason is the one passed in the `reason` argument of that method (which defaults to `'cancelled'`). If the engine was shutdown (for example, by hitting Ctrl-C to stop it) the reason will be `'shutdown'`.

  - type reason  
    str

</div>

#### spider\_opened

<div class="signal">

spider\_opened

</div>

<div class="function">

spider\_opened(spider)

Sent after a spider has been opened for crawling. This is typically used to reserve per-spider resources, but can be used for any task that needs to be performed when a spider is opened.

This signal supports returning deferreds from its handlers.

  - param spider  
    the spider which has been opened

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

#### spider\_idle

<div class="signal">

spider\_idle

</div>

<div class="function">

spider\_idle(spider)

Sent when a spider has gone idle, which means the spider has no further:

>   - requests waiting to be downloaded
>   - requests scheduled
>   - items being processed in the item pipeline

If the idle state persists after all handlers of this signal have finished, the engine starts closing the spider. After the spider has finished closing, the `spider_closed` signal is sent.

You may raise a <span class="title-ref">\~scrapy.exceptions.DontCloseSpider</span> exception to prevent the spider from being closed.

Alternatively, you may raise a <span class="title-ref">\~scrapy.exceptions.CloseSpider</span> exception to provide a custom spider closing reason. An idle handler is the perfect place to put some code that assesses the final spider results and update the final closing reason accordingly (e.g. setting it to 'too\_few\_results' instead of 'finished').

This signal does not support returning deferreds from its handlers.

  - param spider  
    the spider which has gone idle

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="note">

<div class="title">

Note

</div>

Scheduling some requests in your `spider_idle` handler does **not** guarantee that it can prevent the spider from being closed, although it sometimes can. That's because the spider may still remain idle if all the scheduled requests are rejected by the scheduler (e.g. filtered due to duplication).

</div>

#### spider\_error

<div class="signal">

spider\_error

</div>

<div class="function">

spider\_error(failure, response, spider)

Sent when a spider callback generates an error (i.e. raises an exception).

This signal does not support returning deferreds from its handlers.

  - param failure  
    the exception raised

  - type failure  
    twisted.python.failure.Failure

  - param response  
    the response being processed when the exception was raised

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> object

  - param spider  
    the spider which raised the exception

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

#### feed\_slot\_closed

<div class="signal">

feed\_slot\_closed

</div>

<div class="function">

feed\_slot\_closed(slot)

Sent when a \[feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) slot is closed.

This signal supports returning deferreds from its handlers.

  - param slot  
    the slot closed

  - type slot  
    scrapy.extensions.feedexport.FeedSlot

</div>

#### feed\_exporter\_closed

<div class="signal">

feed\_exporter\_closed

</div>

<div class="function">

feed\_exporter\_closed()

Sent when the \[feed exports \<topics-feed-exports\>\](\#feed-exports-\<topics-feed-exports\>) extension is closed, during the handling of the `spider_closed` signal by the extension, after all feed exporting has been handled.

This signal supports returning deferreds from its handlers.

</div>

### Request signals

#### request\_scheduled

<div class="signal">

request\_scheduled

</div>

<div class="function">

request\_scheduled(request, spider)

Sent when the engine is asked to schedule a <span class="title-ref">\~scrapy.Request</span>, to be downloaded later, before the request reaches the \[scheduler \<topics-scheduler\>\](\#scheduler

</div>

\----\<topics-scheduler\>).

> Raise <span class="title-ref">\~scrapy.exceptions.IgnoreRequest</span> to drop a request before it reaches the scheduler.
> 
> This signal does not support returning deferreds from its handlers.
> 
> <div class="versionadded">
> 
> 2.11.2 Allow dropping requests with <span class="title-ref">\~scrapy.exceptions.IgnoreRequest</span>.
> 
> </div>
> 
>   - param request  
>     the request that reached the scheduler
> 
>   - type request  
>     <span class="title-ref">\~scrapy.Request</span> object
> 
>   - param spider  
>     the spider that yielded the request
> 
>   - type spider  
>     <span class="title-ref">\~scrapy.Spider</span> object

#### request\_dropped

<div class="signal">

request\_dropped

</div>

<div class="function">

request\_dropped(request, spider)

Sent when a <span class="title-ref">\~scrapy.Request</span>, scheduled by the engine to be downloaded later, is rejected by the scheduler.

This signal does not support returning deferreds from its handlers.

  - param request  
    the request that reached the scheduler

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider that yielded the request

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

#### request\_reached\_downloader

<div class="signal">

request\_reached\_downloader

</div>

<div class="function">

request\_reached\_downloader(request, spider)

Sent when a <span class="title-ref">\~scrapy.Request</span> reached downloader.

This signal does not support returning deferreds from its handlers.

  - param request  
    the request that reached downloader

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider that yielded the request

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

#### request\_left\_downloader

<div class="signal">

request\_left\_downloader

</div>

<div class="function">

request\_left\_downloader(request, spider)

<div class="versionadded">

2.0

</div>

Sent when a <span class="title-ref">\~scrapy.Request</span> leaves the downloader, even in case of failure.

This signal does not support returning deferreds from its handlers.

  - param request  
    the request that reached the downloader

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider that yielded the request

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

#### bytes\_received

<div class="versionadded">

2.2

</div>

<div class="signal">

bytes\_received

</div>

<div class="function">

bytes\_received(data, request, spider)

Sent by the HTTP 1.1 and S3 download handlers when a group of bytes is received for a specific request. This signal might be fired multiple times for the same request, with partial data each time. For instance, a possible scenario for a 25 kb response would be two signals fired with 10 kb of data, and a final one with 5 kb of data.

Handlers for this signal can stop the download of a response while it is in progress by raising the <span class="title-ref">\~scrapy.exceptions.StopDownload</span> exception. Please refer to the \[topics-stop-response-download\](\#topics-stop-response-download) topic for additional information and examples.

This signal does not support returning deferreds from its handlers.

  - param data  
    the data received by the download handler

  - type data  
    <span class="title-ref">bytes</span> object

  - param request  
    the request that generated the download

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider associated with the response

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

#### headers\_received

<div class="versionadded">

2.5

</div>

<div class="signal">

headers\_received

</div>

<div class="function">

headers\_received(headers, body\_length, request, spider)

Sent by the HTTP 1.1 and S3 download handlers when the response headers are available for a given request, before downloading any additional content.

Handlers for this signal can stop the download of a response while it is in progress by raising the <span class="title-ref">\~scrapy.exceptions.StopDownload</span> exception. Please refer to the \[topics-stop-response-download\](\#topics-stop-response-download) topic for additional information and examples.

This signal does not support returning deferreds from its handlers.

  - param headers  
    the headers received by the download handler

  - type headers  
    <span class="title-ref">scrapy.http.headers.Headers</span> object

  - param body\_length  
    expected size of the response body, in bytes

  - type body\_length  
    <span class="title-ref">int</span>

  - param request  
    the request that generated the download

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider associated with the response

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

### Response signals

#### response\_received

<div class="signal">

response\_received

</div>

<div class="function">

response\_received(response, request, spider)

Sent when the engine receives a new <span class="title-ref">\~scrapy.http.Response</span> from the downloader.

This signal does not support returning deferreds from its handlers.

  - param response  
    the response received

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> object

  - param request  
    the request that generated the response

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider for which the response is intended

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="note">

<div class="title">

Note

</div>

The `request` argument might not contain the original request that reached the downloader, if a \[topics-downloader-middleware\](\#topics-downloader-middleware) modifies the <span class="title-ref">\~scrapy.http.Response</span> object and sets a specific `request` attribute.

</div>

#### response\_downloaded

<div class="signal">

response\_downloaded

</div>

<div class="function">

response\_downloaded(response, request, spider)

Sent by the downloader right after a `HTTPResponse` is downloaded.

This signal does not support returning deferreds from its handlers.

  - param response  
    the response downloaded

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> object

  - param request  
    the request that generated the response

  - type request  
    <span class="title-ref">\~scrapy.Request</span> object

  - param spider  
    the spider for which the response is intended

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

---

spider-middleware.md

---

# Spider Middleware

The spider middleware is a framework of hooks into Scrapy's spider processing mechanism where you can plug custom functionality to process the responses that are sent to \[topics-spiders\](\#topics-spiders) for processing and to process the requests and items that are generated from spiders.

## Activating a spider middleware

To activate a spider middleware component, add it to the `SPIDER_MIDDLEWARES` setting, which is a dict whose keys are the middleware class path and their values are the middleware orders.

Here's an example:

`` `python     SPIDER_MIDDLEWARES = {         "myproject.middlewares.CustomSpiderMiddleware": 543,     }  The :setting:`SPIDER_MIDDLEWARES` setting is merged with the ``<span class="title-ref"> :setting:\`SPIDER\_MIDDLEWARES\_BASE</span> setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled middlewares: the first middleware is the one closer to the engine and the last is the one closer to the spider. In other words, the <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_spider\_input</span> method of each middleware will be invoked in increasing middleware order (100, 200, 300, ...), and the <span class="title-ref">\~scrapy.spidermiddlewares.SpiderMiddleware.process\_spider\_output</span> method of each middleware will be invoked in decreasing order.

To decide which order to assign to your middleware see the `SPIDER_MIDDLEWARES_BASE` setting and pick a value according to where you want to insert the middleware. The order does matter because each middleware performs a different action and your middleware could depend on some previous (or subsequent) middleware being applied.

If you want to disable a builtin middleware (the ones defined in `SPIDER_MIDDLEWARES_BASE`, and enabled by default) you must define it in your project `SPIDER_MIDDLEWARES` setting and assign `None` as its value. For example, if you want to disable the off-site middleware:

`` `python     SPIDER_MIDDLEWARES = {         "scrapy.spidermiddlewares.referer.RefererMiddleware": None,         "myproject.middlewares.CustomRefererSpiderMiddleware": 700,     }  Finally, keep in mind that some middlewares may need to be enabled through a ``\` particular setting. See each middleware documentation for more info.

## Writing your own spider middleware

Each spider middleware is a Python class that defines one or more of the methods defined below.

The main entry point is the `from_crawler` class method, which receives a <span class="title-ref">\~scrapy.crawler.Crawler</span> instance. The <span class="title-ref">\~scrapy.crawler.Crawler</span> object gives you access, for example, to the \[settings \<topics-settings\>\](\#settings-\<topics-settings\>).

<div class="module">

scrapy.spidermiddlewares

</div>

<div class="SpiderMiddleware">

<div class="method">

process\_spider\_input(response, spider)

This method is called for each response that goes through the spider middleware and into the spider, for processing.

<span class="title-ref">process\_spider\_input</span> should return `None` or raise an exception.

If it returns `None`, Scrapy will continue processing this response, executing all other middlewares until, finally, the response is handed to the spider for processing.

If it raises an exception, Scrapy won't bother calling any other spider middleware <span class="title-ref">process\_spider\_input</span> and will call the request errback if there is one, otherwise it will start the <span class="title-ref">process\_spider\_exception</span> chain. The output of the errback is chained back in the other direction for <span class="title-ref">process\_spider\_output</span> to process it, or <span class="title-ref">process\_spider\_exception</span> if it raised an exception.

  - param response  
    the response being processed

  - type response  
    <span class="title-ref">\~scrapy.http.Response</span> object

  - param spider  
    the spider for which this response is intended

  - type spider  
    <span class="title-ref">\~scrapy.Spider</span> object

</div>

<div class="method">

process\_spider\_output(response, result, spider)

This method is called with the results returned from the Spider, after it has processed the response.

<span class="title-ref">process\_spider\_output</span> must return an iterable of <span class="title-ref">\~scrapy.Request</span> objects and \[item objects \<topics-items\>\](\#item-objects

</div>

</div>

\--------\<topics-items\>).

> 
> 
> <div class="versionchanged">
> 
> 2.7 This method may be defined as an `asynchronous generator`, in which case `result` is an `asynchronous iterable`.
> 
> </div>
> 
> Consider defining this method as an `asynchronous generator`, which will be a requirement in a future version of Scrapy. However, if you plan on sharing your spider middleware with other people, consider either \[enforcing Scrapy 2.7 \<enforce-component-requirements\>\](\#enforcing-scrapy-2.7-\<enforce-component-requirements\>) as a minimum requirement of your spider middleware, or \[making your spider middleware universal \<universal-spider-middleware\>\](\#making

  - \--------your-spider-middleware-universal-\<universal-spider-middleware\>) so that  
    it works with Scrapy versions earlier than Scrapy 2.7.
    
      - param response  
        the response which generated this output from the spider
    
      - type response  
        <span class="title-ref">\~scrapy.http.Response</span> object
    
      - param result  
        the result returned by the spider
    
      - type result  
        an iterable of <span class="title-ref">\~scrapy.Request</span> objects and \[item objects \<topics-items\>\](\#item-objects-\<topics-items\>)
    
      - param spider  
        the spider whose result is being processed
    
      - type spider  
        <span class="title-ref">\~scrapy.Spider</span> object
    
    <div class="method">
    
    process\_spider\_output\_async(response, result, spider)
    
    </div>
    
    <div class="versionadded">
    
    2.7
    
    </div>
    
    If defined, this method must be an `asynchronous generator`, which will be called instead of <span class="title-ref">process\_spider\_output</span> if `result` is an `asynchronous iterable`.
    
    <div class="method">
    
    process\_spider\_exception(response, exception, spider)
    
    </div>
    
    This method is called when a spider or <span class="title-ref">process\_spider\_output</span> method (from a previous spider middleware) raises an exception.
    
    <span class="title-ref">process\_spider\_exception</span> should return either `None` or an iterable of <span class="title-ref">\~scrapy.Request</span> or \[item \<topics-items\>\](\#item-\<topics-items\>) objects.
    
    If it returns `None`, Scrapy will continue processing this exception, executing any other <span class="title-ref">process\_spider\_exception</span> in the following middleware components, until no middleware components are left and the exception reaches the engine (where it's logged and discarded).
    
    If it returns an iterable the <span class="title-ref">process\_spider\_output</span> pipeline kicks in, starting from the next spider middleware, and no other <span class="title-ref">process\_spider\_exception</span> will be called.
    
      - param response  
        the response being processed when the exception was raised
    
      - type response  
        <span class="title-ref">\~scrapy.http.Response</span> object
    
      - param exception  
        the exception raised
    
      - type exception  
        <span class="title-ref">Exception</span> object
    
      - param spider  
        the spider which raised the exception
    
      - type spider  
        <span class="title-ref">\~scrapy.Spider</span> object
    
    <div class="method">
    
    process\_start\_requests(start\_requests, spider)
    
    </div>
    
    This method is called with the start requests of the spider, and works similarly to the <span class="title-ref">process\_spider\_output</span> method, except that it doesn't have a response associated and must return only requests (not items).
    
    It receives an iterable (in the `start_requests` parameter) and must return another iterable of <span class="title-ref">\~scrapy.Request</span> objects and/or \[item objects \<topics-items\>\](\#item-objects-\<topics-items\>).
    
    <div class="note">
    
    <div class="title">
    
    Note
    
    </div>
    
    When implementing this method in your spider middleware, you should always return an iterable (that follows the input one) and not consume all `start_requests` iterator because it can be very large (or even unbounded) and cause a memory overflow. The Scrapy engine is designed to pull start requests while it has capacity to process them, so the start requests iterator can be effectively endless where there is some other condition for stopping the spider (like a time limit or item/page count).
    
    </div>
    
      - param start\_requests  
        the start requests
    
      - type start\_requests  
        an iterable of <span class="title-ref">\~scrapy.Request</span>
    
      - param spider  
        the spider to whom the start requests belong
    
      - type spider  
        <span class="title-ref">\~scrapy.Spider</span> object
    
    <div class="method">
    
    from\_crawler(cls, crawler)
    
    </div>
    
    If present, this classmethod is called to create a middleware instance from a <span class="title-ref">\~scrapy.crawler.Crawler</span>. It must return a new instance of the middleware. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for middleware to access them and hook its functionality into Scrapy.
    
      - param crawler  
        crawler that uses this middleware
    
      - type crawler  
        <span class="title-ref">\~scrapy.crawler.Crawler</span> object

## Built-in spider middleware reference

This page describes all spider middleware components that come with Scrapy. For information on how to use them and how to write your own spider middleware, see the \[spider middleware usage guide \<topics-spider-middleware\>\](\#spider-middleware-usage-guide-\<topics-spider-middleware\>).

For a list of the components enabled by default (and their orders) see the `SPIDER_MIDDLEWARES_BASE` setting.

### DepthMiddleware

<div class="module" data-synopsis="Depth Spider Middleware">

scrapy.spidermiddlewares.depth

</div>

<div class="DepthMiddleware">

DepthMiddleware is used for tracking the depth of each Request inside the site being scraped. It works by setting `request.meta['depth'] = 0` whenever there is no value previously set (usually just the first Request) and incrementing it by 1 otherwise.

It can be used to limit the maximum depth to scrape, control Request priority based on their depth, and things like that.

The <span class="title-ref">DepthMiddleware</span> can be configured through the following settings (see the settings documentation for more info):

>   - `DEPTH_LIMIT` - The maximum depth that will be allowed to crawl for any site. If zero, no limit will be imposed.
>   - `DEPTH_STATS_VERBOSE` - Whether to collect the number of requests for each depth.
>   - `DEPTH_PRIORITY` - Whether to prioritize the requests based on their depth.

</div>

### HttpErrorMiddleware

<div class="module" data-synopsis="HTTP Error Spider Middleware">

scrapy.spidermiddlewares.httperror

</div>

<div class="HttpErrorMiddleware">

Filter out unsuccessful (erroneous) HTTP responses so that spiders don't have to deal with them, which (most of the time) imposes an overhead, consumes more resources, and makes the spider logic more complex.

</div>

According to the [HTTP standard](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html), successful responses are those whose status codes are in the 200-300 range.

If you still want to process response codes outside that range, you can specify which response codes the spider is able to handle using the `handle_httpstatus_list` spider attribute or `HTTPERROR_ALLOWED_CODES` setting.

For example, if you want your spider to handle 404 responses you can do this:

`` `python     from scrapy.spiders import CrawlSpider       class MySpider(CrawlSpider):         handle_httpstatus_list = [404]  .. reqmeta:: handle_httpstatus_list  .. reqmeta:: handle_httpstatus_all  The ``handle\_httpstatus\_list``key of `Request.meta``<span class="title-ref"> \<scrapy.Request.meta\></span> can also be used to specify which response codes to allow on a per-request basis. You can also set the meta key `handle_httpstatus_all` to `True` if you want to allow any response code for a request, and `False` to disable the effects of the `handle_httpstatus_all` key.

Keep in mind, however, that it's usually a bad idea to handle non-200 responses, unless you really know what you're doing.

For more information see: [HTTP Status Code Definitions](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html).

#### HttpErrorMiddleware settings

<div class="setting">

HTTPERROR\_ALLOWED\_CODES

</div>

##### HTTPERROR\_ALLOWED\_CODES

Default: `[]`

Pass all responses with non-200 status codes contained in this list.

<div class="setting">

HTTPERROR\_ALLOW\_ALL

</div>

##### HTTPERROR\_ALLOW\_ALL

Default: `False`

Pass all responses, regardless of its status code.

### RefererMiddleware

<div class="module" data-synopsis="Referer Spider Middleware">

scrapy.spidermiddlewares.referer

</div>

<div class="RefererMiddleware">

Populates Request `Referer` header, based on the URL of the Response which generated it.

</div>

#### RefererMiddleware settings

<div class="setting">

REFERER\_ENABLED

</div>

##### REFERER\_ENABLED

Default: `True`

Whether to enable referer middleware.

<div class="setting">

REFERRER\_POLICY

</div>

##### REFERRER\_POLICY

Default: `'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'`

<div class="reqmeta">

referrer\_policy

</div>

[Referrer Policy](https://www.w3.org/TR/referrer-policy) to apply when populating Request "Referer" header.

<div class="note">

<div class="title">

Note

</div>

You can also set the Referrer Policy per request, using the special `"referrer_policy"` \[Request.meta \<topics-request-meta\>\](\#request.meta-\<topics-request-meta\>) key, with the same acceptable values as for the `REFERRER_POLICY` setting.

</div>

###### Acceptable values for REFERRER\_POLICY

  - either a path to a `scrapy.spidermiddlewares.referer.ReferrerPolicy` subclass — a custom policy or one of the built-in ones (see classes below),
  - or one of the standard W3C-defined string values,
  - or the special `"scrapy-default"`.

| String value                                                                                                                | Class name (as a string)                                                                          |
| --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |
| `"scrapy-default"` (default)                                                                                                | <span class="title-ref">scrapy.spidermiddlewares.referer.DefaultReferrerPolicy</span>             |
| ["no-referrer"](https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer)                                         | <span class="title-ref">scrapy.spidermiddlewares.referer.NoReferrerPolicy</span>                  |
| ["no-referrer-when-downgrade"](https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade)           | <span class="title-ref">scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy</span>     |
| ["same-origin"](https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin)                                         | <span class="title-ref">scrapy.spidermiddlewares.referer.SameOriginPolicy</span>                  |
| ["origin"](https://www.w3.org/TR/referrer-policy/#referrer-policy-origin)                                                   | <span class="title-ref">scrapy.spidermiddlewares.referer.OriginPolicy</span>                      |
| ["strict-origin"](https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin)                                     | <span class="title-ref">scrapy.spidermiddlewares.referer.StrictOriginPolicy</span>                |
| ["origin-when-cross-origin"](https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin)               | <span class="title-ref">scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy</span>       |
| ["strict-origin-when-cross-origin"](https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin) | <span class="title-ref">scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy</span> |
| ["unsafe-url"](https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url)                                           | <span class="title-ref">scrapy.spidermiddlewares.referer.UnsafeUrlPolicy</span>                   |

<div class="autoclass">

DefaultReferrerPolicy

</div>

<div class="warning">

<div class="title">

Warning

</div>

Scrapy's default referrer policy — just like ["no-referrer-when-downgrade"](https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade), the W3C-recommended value for browsers — will send a non-empty "Referer" header from any `http(s)://` to any `https://` URL, even if the domain is different.

["same-origin"](https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin) may be a better choice if you want to remove referrer information for cross-domain requests.

</div>

<div class="autoclass">

NoReferrerPolicy

</div>

<div class="autoclass">

NoReferrerWhenDowngradePolicy

</div>

<div class="note">

<div class="title">

Note

</div>

"no-referrer-when-downgrade" policy is the W3C-recommended default, and is used by major web browsers.

However, it is NOT Scrapy's default referrer policy (see <span class="title-ref">DefaultReferrerPolicy</span>).

</div>

<div class="autoclass">

SameOriginPolicy

</div>

<div class="autoclass">

OriginPolicy

</div>

<div class="autoclass">

StrictOriginPolicy

</div>

<div class="autoclass">

OriginWhenCrossOriginPolicy

</div>

<div class="autoclass">

StrictOriginWhenCrossOriginPolicy

</div>

<div class="autoclass">

UnsafeUrlPolicy

</div>

<div class="warning">

<div class="title">

Warning

</div>

"unsafe-url" policy is NOT recommended.

</div>

### UrlLengthMiddleware

<div class="module" data-synopsis="URL Length Spider Middleware">

scrapy.spidermiddlewares.urllength

</div>

<div class="UrlLengthMiddleware">

Filters out requests with URLs longer than URLLENGTH\_LIMIT

The <span class="title-ref">UrlLengthMiddleware</span> can be configured through the following settings (see the settings documentation for more info):

>   - `URLLENGTH_LIMIT` - The maximum URL length to allow for crawled URLs.

</div>

---

spiders.md

---

# Spiders

Spiders are classes which define how a certain site (or a group of sites) will be scraped, including how to perform the crawl (i.e. follow links) and how to extract structured data from their pages (i.e. scraping items). In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites).

For spiders, the scraping cycle goes through something like this:

1.  You start by generating the initial Requests to crawl the first URLs, and specify a callback function to be called with the response downloaded from those requests.
    
    The first requests to perform are obtained by calling the <span class="title-ref">\~scrapy.Spider.start\_requests</span> method which (by default) generates <span class="title-ref">\~scrapy.Request</span> for the URLs specified in the <span class="title-ref">\~scrapy.Spider.start\_urls</span> and the <span class="title-ref">\~scrapy.Spider.parse</span> method as callback function for the Requests.

2.  In the callback function, you parse the response (web page) and return \[item objects \<topics-items\>\](\#item-objects-\<topics-items\>), <span class="title-ref">\~scrapy.Request</span> objects, or an iterable of these objects. Those Requests will also contain a callback (maybe the same) and will then be downloaded by Scrapy and then their response handled by the specified callback.

3.  In callback functions, you parse the page contents, typically using \[topics-selectors\](\#topics-selectors) (but you can also use BeautifulSoup, lxml or whatever mechanism you prefer) and generate items with the parsed data.

4.  Finally, the items returned from the spider will be typically persisted to a database (in some \[Item Pipeline \<topics-item-pipeline\>\](\#item-pipeline-\<topics-item-pipeline\>)) or written to a file using \[topics-feed-exports\](\#topics-feed-exports).

Even though this cycle applies (more or less) to any kind of spider, there are different kinds of default spiders bundled into Scrapy for different purposes. We will talk about those types here.

## scrapy.Spider

<div class="scrapy.spiders.Spider">

<div class="scrapy.Spider()">

This is the simplest spider, and the one from which every other spider must inherit (including spiders that come bundled with Scrapy, as well as spiders that you write yourself). It doesn't provide any special functionality. It just provides a default <span class="title-ref">start\_requests</span> implementation which sends requests from the <span class="title-ref">start\_urls</span> spider attribute and calls the spider's method `parse` for each of the resulting responses.

<div class="attribute">

name

A string which defines the name for this spider. The spider name is how the spider is located (and instantiated) by Scrapy, so it must be unique. However, nothing prevents you from instantiating more than one instance of the same spider. This is the most important spider attribute and it's required.

If the spider scrapes a single domain, a common practice is to name the spider after the domain, with or without the [TLD](). So, for example, a spider that crawls `mywebsite.com` would often be called `mywebsite`.

</div>

<div class="attribute">

allowed\_domains

An optional list of strings containing domains that this spider is allowed to crawl. Requests for URLs not belonging to the domain names specified in this list (or their subdomains) won't be followed if <span class="title-ref">\~scrapy.downloadermiddlewares.offsite.OffsiteMiddleware</span> is enabled.

Let's say your target url is `https://www.example.com/1.html`, then add `'example.com'` to the list.

</div>

<div class="attribute">

start\_urls

A list of URLs where the spider will begin to crawl from, when no particular URLs are specified. So, the first pages downloaded will be those listed here. The subsequent <span class="title-ref">\~scrapy.Request</span> will be generated successively from data contained in the start URLs.

</div>

<div class="attribute">

custom\_settings

A dictionary of settings that will be overridden from the project wide configuration when running this spider. It must be defined as a class attribute since the settings are updated before instantiation.

For a list of available built-in settings see: \[topics-settings-ref\](\#topics-settings-ref).

</div>

<div class="attribute">

crawler

This attribute is set by the <span class="title-ref">from\_crawler</span> class method after initializing the class, and links to the <span class="title-ref">\~scrapy.crawler.Crawler</span> object to which this spider instance is bound.

Crawlers encapsulate a lot of components in the project for their single entry access (such as extensions, middlewares, signals managers, etc). See \[topics-api-crawler\](\#topics-api-crawler) to know more about them.

</div>

<div class="attribute">

settings

Configuration for running this spider. This is a <span class="title-ref">\~scrapy.settings.Settings</span> instance, see the \[topics-settings\](\#topics-settings) topic for a detailed introduction on this subject.

</div>

<div class="attribute">

logger

Python logger created with the Spider's <span class="title-ref">name</span>. You can use it to send log messages through it as described on \[topics-logging-from-spiders\](\#topics-logging-from-spiders).

</div>

<div class="attribute">

state

A dict you can use to persist some spider state between batches. See \[topics-keeping-persistent-state-between-batches\](\#topics-keeping-persistent-state-between-batches) to know more about it.

</div>

<div class="method">

from\_crawler(crawler, *args,*\*kwargs)

This is the class method used by Scrapy to create your spiders.

You probably won't need to override this directly because the default implementation acts as a proxy to the <span class="title-ref">\_\_init\_\_</span> method, calling it with the given arguments `args` and named arguments `kwargs`.

Nonetheless, this method sets the <span class="title-ref">crawler</span> and <span class="title-ref">settings</span> attributes in the new instance so they can be accessed later inside the spider's code.

<div class="versionchanged">

2.11

The settings in `crawler.settings` can now be modified in this method, which is handy if you want to modify them based on arguments. As a consequence, these settings aren't the final values as they can be modified later by e.g. \[add-ons \<topics-addons\>\](\#add-ons

</div>

</div>

</div>

</div>

  - \-----------\<topics-addons\>). For the same reason, most of the  
    <span class="title-ref">\~scrapy.crawler.Crawler</span> attributes aren't initialized at this point.
    
    The final settings and the initialized <span class="title-ref">\~scrapy.crawler.Crawler</span> attributes are available in the <span class="title-ref">start\_requests</span> method, handlers of the `engine_started` signal and later.
    
      - param crawler  
        crawler to which the spider will be bound
    
      - type crawler  
        <span class="title-ref">\~scrapy.crawler.Crawler</span> instance
    
      - param args  
        arguments passed to the <span class="title-ref">\_\_init\_\_</span> method
    
      - type args  
        list
    
      - param kwargs  
        keyword arguments passed to the <span class="title-ref">\_\_init\_\_</span> method
    
      - type kwargs  
        dict
    
    <div class="classmethod">
    
    update\_settings(settings)
    
    </div>
    
    The `update_settings()` method is used to modify the spider's settings and is called during initialization of a spider instance.
    
    It takes a <span class="title-ref">\~scrapy.settings.Settings</span> object as a parameter and can add or update the spider's configuration values. This method is a class method, meaning that it is called on the <span class="title-ref">\~scrapy.Spider</span> class and allows all instances of the spider to share the same configuration.
    
    While per-spider settings can be set in <span class="title-ref">\~scrapy.Spider.custom\_settings</span>, using `update_settings()` allows you to dynamically add, remove or change settings based on other settings, spider attributes or other factors and use setting priorities other than `'spider'`. Also, it's easy to extend `update_settings()` in a subclass by overriding it, while doing the same with <span class="title-ref">\~scrapy.Spider.custom\_settings</span> can be hard.
    
    For example, suppose a spider needs to modify `FEEDS`:
    
    \`\`\`python import scrapy
    
      - class MySpider(scrapy.Spider):  
        name = "myspider" custom\_feed = { "/home/user/documents/items.json": { "format": "json", "indent": 4, } }
        
        @classmethod def update\_settings(cls, settings): super().update\_settings(settings) settings.setdefault("FEEDS", {}).update(cls.custom\_feed)
    
    <div class="method">
    
    start\_requests()
    
    </div>
    
    This method must return an iterable with the first Requests to crawl and/or with \[item objects \<topics-items\>\](\#item-objects

  - \-------\<topics-items\>) for  
    this spider. It is called by Scrapy when the spider is opened for scraping. Scrapy calls it only once, so it is safe to implement <span class="title-ref">start\_requests</span> as a generator.
    
    The default implementation generates `Request(url, dont_filter=True)` for each url in <span class="title-ref">start\_urls</span>.
    
    If you want to change the Requests used to start scraping a domain, this is the method to override. For example, if you need to start by logging in using a POST request, you could do:
    
    ``` python
    import scrapy

    class MySpider(scrapy.Spider):
        name = "myspider"
    
        def start_requests(self):
            return [
                scrapy.FormRequest(
                    "http://www.example.com/login",
                    formdata={"user": "john", "pass": "secret"},
                    callback=self.logged_in,
                )
            ]
    
        def logged_in(self, response):
            # here you would extract links to follow and return Requests for
            # each of them, with another callback
            pass
    ```
    
    <div class="method">
    
    parse(response)
    
    </div>
    
    This is the default callback used by Scrapy to process downloaded responses, when their requests don't specify a callback.
    
    The `parse` method is in charge of processing the response and returning scraped data and/or more URLs to follow. Other Requests callbacks have the same requirements as the <span class="title-ref">Spider</span> class.
    
    This method, as well as any other Request callback, must return a <span class="title-ref">\~scrapy.Request</span> object, an \[item object \<topics-items\>\](\#item-object-\<topics-items\>), an iterable of <span class="title-ref">\~scrapy.Request</span> objects and/or \[item objects \<topics-items\>\](\#item-objects

\-------\<topics-items\>), or `None`.

>   - param response  
>     the response to parse
> 
>   - type response  
>     <span class="title-ref">\~scrapy.http.Response</span>
> 
> <div class="method">
> 
> log(message, \[level, component\])
> 
> </div>
> 
> Wrapper that sends a log message through the Spider's <span class="title-ref">logger</span>, kept for backward compatibility. For more information see \[topics-logging-from-spiders\](\#topics-logging-from-spiders).
> 
> <div class="method">
> 
> closed(reason)
> 
> </div>
> 
> Called when the spider closes. This method provides a shortcut to signals.connect() for the `spider_closed` signal.

Let's see an example:

``` python
import scrapy

class MySpider(scrapy.Spider):
    name = "example.com"
    allowed_domains = ["example.com"]
    start_urls = [
        "http://www.example.com/1.html",
        "http://www.example.com/2.html",
        "http://www.example.com/3.html",
    ]

    def parse(self, response):
        self.logger.info("A response from %s just arrived!", response.url)
```

Return multiple Requests and items from a single callback:

``` python
import scrapy

class MySpider(scrapy.Spider):
    name = "example.com"
    allowed_domains = ["example.com"]
    start_urls = [
        "http://www.example.com/1.html",
        "http://www.example.com/2.html",
        "http://www.example.com/3.html",
    ]

    def parse(self, response):
        for h3 in response.xpath("//h3").getall():
            yield {"title": h3}

        for href in response.xpath("//a/@href").getall():
            yield scrapy.Request(response.urljoin(href), self.parse)
```

Instead of <span class="title-ref">\~.start\_urls</span> you can use <span class="title-ref">\~.start\_requests</span> directly; `` ` to give data more structure you can use `~scrapy.Item` objects:  .. skip: next ``\`python import scrapy from myproject.items import MyItem

>   - class MySpider(scrapy.Spider):  
>     name = "example.com" allowed\_domains = \["example.com"\]
>     
>       - def start\_requests(self):  
>         yield scrapy.Request("<http://www.example.com/1.html>", self.parse) yield scrapy.Request("<http://www.example.com/2.html>", self.parse) yield scrapy.Request("<http://www.example.com/3.html>", self.parse)
>     
>       - def parse(self, response):
>         
>           - for h3 in response.xpath("//h3").getall():  
>             yield MyItem(title=h3)
>         
>           - for href in response.xpath("//<a/@href>").getall():  
>             yield scrapy.Request(response.urljoin(href), self.parse)

<div id="spiderargs">

Spider arguments `` ` ================  Spiders can receive arguments that modify their behaviour. Some common uses for spider arguments are to define the start URLs or to restrict the crawl to certain sections of the site, but they can be used to configure any functionality of the spider.  Spider arguments are passed through the :command:`crawl` command using the ``-a``option. For example::      scrapy crawl myspider -a category=electronics  Spiders can access arguments in their `__init__` methods:``\`python import scrapy

</div>

>   - class MySpider(scrapy.Spider):  
>     name = "myspider"
>     
>       - def \_\_init\_\_(self, category=None, *args,kwargs): super(MySpider, self).\_\_init\_\_(*args, \*\*kwargs)  
>         self.start\_urls = \[f"[http://www.example.com/categories/{category}](http://www.example.com/categories/%7Bcategory%7D)"\] \# ...

The default <span class="title-ref">\_\_init\_\_</span> method will take any spider arguments `` ` and copy them to the spider as attributes. The above example can also be written as follows: ``\`python import scrapy

>   - class MySpider(scrapy.Spider):  
>     name = "myspider"
>     
>       - def start\_requests(self):  
>         yield scrapy.Request(f"[http://www.example.com/categories/{self.category](http://www.example.com/categories/%7Bself.category)}")

If you are \[running Scrapy from a script \<run-from-script\>\](\#running-scrapy-from-a-script-\<run-from-script\>), you can `` ` specify spider arguments when calling  `CrawlerProcess.crawl <scrapy.crawler.CrawlerProcess.crawl>` or `CrawlerRunner.crawl <scrapy.crawler.CrawlerRunner.crawl>`:  .. skip: next ``\`python process = CrawlerProcess() process.crawl(MySpider, category="electronics")

Keep in mind that spider arguments are only strings. `` ` The spider will not do any parsing on its own. If you were to set the ``start\_urls``attribute from the command line, you would have to parse it on your own into a list using something like `ast.literal_eval` or `json.loads` and then set it as an attribute. Otherwise, you would cause iteration over a``start\_urls``string (a very common python pitfall) resulting in each character being seen as a separate url.  A valid use case is to set the http auth credentials used by `~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware` or the user agent used by `~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`::      scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot  Spider arguments can also be passed through the Scrapyd``schedule.json``API. See `Scrapyd documentation`_.  .. _builtin-spiders:  Generic Spiders ===============  Scrapy comes with some useful generic spiders that you can use to subclass your spiders from. Their aim is to provide convenient functionality for a few common scraping cases, like following all links on a site based on certain rules, crawling from `Sitemaps`_, or parsing an XML/CSV feed.  For the examples used in the following spiders, we'll assume you have a project with a``TestItem`declared in a`myproject.items`module:`\`python import scrapy

>   - class TestItem(scrapy.Item):  
>     id = scrapy.Field() name = scrapy.Field() description = scrapy.Field()

<div class="currentmodule">

scrapy.spiders

</div>

CrawlSpider `` ` -----------  .. class:: CrawlSpider     This is the most commonly used spider for crawling regular websites, as it    provides a convenient mechanism for following links by defining a set of rules.    It may not be the best suited for your particular web sites or project, but    it's generic enough for several cases, so you can start from it and override it    as needed for more custom functionality, or just implement your own spider.     Apart from the attributes inherited from Spider (that you must    specify), this class supports a new attribute:     .. attribute:: rules         Which is a list of one (or more) `Rule` objects.  Each `Rule`        defines a certain behaviour for crawling the site. Rules objects are        described below. If multiple rules match the same link, the first one        will be used, according to the order they're defined in this attribute.     This spider also exposes an overridable method:     .. method:: parse_start_url(response, **kwargs)        This method is called for each response produced for the URLs in       the spider's ``start\_urls``attribute. It allows to parse       the initial responses and must return either an       [item object <topics-items>](#item-object-<topics-items>), a `~scrapy.Request`       object, or an iterable containing any of them.  Crawling rules ~~~~~~~~~~~~~~  .. autoclass:: Rule``link\_extractor``is a [Link Extractor <topics-link-extractors>](#link-extractor-<topics-link-extractors>) object which    defines how links will be extracted from each crawled page. Each produced link will    be used to generate a `~scrapy.Request` object, which will contain the    link's text in its``meta`dictionary (under the`link\_text`key).    If omitted, a default link extractor created with no arguments will be used,    resulting in all links being extracted.`callback``is a callable or a string (in which case a method from the spider    object with that name will be used) to be called for each link extracted with    the specified link extractor. This callback receives a `~scrapy.http.Response`    as its first argument and must return either a single instance or an iterable of    [item objects <topics-items>](#item-objects-<topics-items>) and/or `~scrapy.Request` objects    (or any subclass of them). As mentioned above, the received `~scrapy.http.Response`    object will contain the text of the link that produced the `~scrapy.Request`    in its``meta`dictionary (under the`link\_text`key)`cb\_kwargs`is a dict containing the keyword arguments to be passed to the    callback function.`follow`is a boolean which specifies if links should be followed from each    response extracted with this rule. If`callback`is None`follow`defaults    to`True`, otherwise it defaults to`False`.`process\_links`is a callable, or a string (in which case a method from the    spider object with that name will be used) which will be called for each list    of links extracted from each response using the specified`link\_extractor`.    This is mainly used for filtering purposes.`process\_request``is a callable (or a string, in which case a method from    the spider object with that name will be used) which will be called for every    `~scrapy.Request` extracted by this rule. This callable should    take said request as first argument and the `~scrapy.http.Response`    from which the request originated as second argument. It must return a``Request`object or`None`(to filter out the request).`errback``is a callable or a string (in which case a method from the spider    object with that name will be used) to be called if any exception is    raised while processing a request generated by the rule.    It receives a `Twisted Failure <twisted.python.failure.Failure>`    instance as first parameter.     .. warning:: Because of its internal implementation, you must explicitly set       callbacks for new requests when writing `CrawlSpider`-based spiders;       unexpected behaviour can occur otherwise.     .. versionadded:: 2.0       The *errback* parameter.  CrawlSpider example ~~~~~~~~~~~~~~~~~~~  Let's now take a look at an example CrawlSpider with rules:``\`python import scrapy from scrapy.spiders import CrawlSpider, Rule from scrapy.linkextractors import LinkExtractor

>   - class MySpider(CrawlSpider):  
>     name = "example.com" allowed\_domains = \["example.com"\] start\_urls = \["<http://www.example.com>"\]
>     
>       - rules = (  
>         \# Extract links matching 'category.php' (but not matching 'subsection.php') \# and follow links from them (since no callback means follow=True by default). Rule(LinkExtractor(allow=(r"category.php",), deny=(r"subsection.php",))), \# Extract links matching 'item.php' and parse them with the spider's method parse\_item Rule(LinkExtractor(allow=(r"item.php",)), callback="parse\_item"),
>     
>     )
>     
>       - def parse\_item(self, response):  
>         self.logger.info("Hi, this is an item page\! %s", response.url) item = scrapy.Item() item\["id"\] = response.xpath('//td\[@id="item\_id"\]/text()').re(r"ID: (d+)") item\["name"\] = response.xpath('//td\[@id="item\_name"\]/text()').get() item\["description"\] = response.xpath( '//td\[@id="item\_description"\]/text()' ).get() item\["link\_text"\] = response.meta\["link\_text"\] url = response.xpath('//td\[@id="additional\_data"\]/@href').get() return response.follow( url, self.parse\_additional\_page, cb\_kwargs=dict(item=item) )
>     
>       - def parse\_additional\_page(self, response, item):
>         
>           - item\["additional\_data"\] = response.xpath(  
>             '//p\[@id="additional\_data"\]/text()'
>         
>         ).get() return item

This spider would start crawling example.com's home page, collecting category `` ` links, and item links, parsing the latter with the ``parse\_item``method. For each item response, some data will be extracted from the HTML using XPath, and an `~scrapy.Item` will be filled with it.  XMLFeedSpider -------------  .. class:: XMLFeedSpider      XMLFeedSpider is designed for parsing XML feeds by iterating through them by a     certain node name.  The iterator can be chosen from:``iternodes`,`xml`,     and`html`.  It's recommended to use the`iternodes`iterator for     performance reasons, since the`xml`and`html`iterators generate the     whole DOM at once in order to parse it.  However, using`html`as the     iterator may be useful when parsing XML with bad markup.      To set the iterator and the tag name, you must define the following class     attributes:      .. attribute:: iterator          A string which defines the iterator to use. It can be either:             -`'iternodes'`- a fast iterator based on regular expressions             -`'html'``- an iterator which uses `~scrapy.Selector`.              Keep in mind this uses DOM parsing and must load all DOM in memory              which could be a problem for big feeds             -``'xml'``- an iterator which uses `~scrapy.Selector`.              Keep in mind this uses DOM parsing and must load all DOM in memory              which could be a problem for big feeds          It defaults to:``'iternodes'`.      .. attribute:: itertag          A string with the name of the node (or element) to iterate in. Example::              itertag = 'product'      .. attribute:: namespaces          A list of`(prefix, uri)`tuples which define the namespaces         available in that document that will be processed with this spider. The`prefix`and`uri``will be used to automatically register         namespaces using the         `~scrapy.Selector.register_namespace` method.          You can then specify nodes with namespaces in the `itertag`         attribute.          Example::              class YourSpider(XMLFeedSpider):                  namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')]                 itertag = 'n:url'                 # ...      Apart from these new attributes, this spider has the following overridable     methods too:      .. method:: adapt_response(response)          A method that receives the response as soon as it arrives from the spider         middleware, before the spider starts parsing it. It can be used to modify         the response body before parsing it. This method receives a response and         also returns a response (it could be the same or another one).      .. method:: parse_node(response, selector)          This method is called for the nodes matching the provided tag name         (``itertag``).  Receives the response and an         `~scrapy.Selector` for each node.  Overriding this         method is mandatory. Otherwise, you spider won't work.  This method         must return an [item object <topics-items>](#item-object-<topics-items>), a         `~scrapy.Request` object, or an iterable containing any of         them.      .. method:: process_results(response, results)          This method is called for each result (item or request) returned by the         spider, and it's intended to perform any last time processing required         before returning the results to the framework core, for example setting the         item IDs. It receives a list of results and the response which originated         those results. It must return a list of results (items or requests).      .. warning:: Because of its internal implementation, you must explicitly set        callbacks for new requests when writing `XMLFeedSpider`-based spiders;        unexpected behaviour can occur otherwise.   XMLFeedSpider example ~~~~~~~~~~~~~~~~~~~~~  These spiders are pretty easy to use, let's have a look at one example:  .. skip: next``\`python from scrapy.spiders import XMLFeedSpider from myproject.items import TestItem

>   - class MySpider(XMLFeedSpider):  
>     name = "example.com" allowed\_domains = \["example.com"\] start\_urls = \["<http://www.example.com/feed.xml>"\] iterator = "iternodes" \# This is actually unnecessary, since it's the default value itertag = "item"
>     
>       - def parse\_node(self, response, node):
>         
>           - self.logger.info(  
>             "Hi, this is a \<%s\> node\!: %s", self.itertag, "".join(node.getall())
>         
>         )
>         
>         item = TestItem() item\["id"\] = node.xpath("@id").get() item\["name"\] = node.xpath("name").get() item\["description"\] = node.xpath("description").get() return item

Basically what we did up there was to create a spider that downloads a feed from `` ` the given ``start\_urls`, and then iterates through each of its`item``tags, prints them out, and stores some random data in an `~scrapy.Item`.  CSVFeedSpider -------------  .. class:: CSVFeedSpider     This spider is very similar to the XMLFeedSpider, except that it iterates    over rows, instead of nodes. The method that gets called in each iteration    is `parse_row`.     .. attribute:: delimiter         A string with the separator character for each field in the CSV file        Defaults to``','`(comma).     .. attribute:: quotechar         A string with the enclosure character for each field in the CSV file        Defaults to`'"'`(quotation mark).     .. attribute:: headers         A list of the column names in the CSV file.     .. method:: parse_row(response, row)         Receives a response and a dict (representing each row) with a key for each        provided (or detected) header of the CSV file.  This spider also gives the        opportunity to override`adapt\_response`and`process\_results``methods        for pre- and post-processing purposes.  CSVFeedSpider example ~~~~~~~~~~~~~~~~~~~~~  Let's see an example similar to the previous one, but using a `CSVFeedSpider`:  .. skip: next``\`python from scrapy.spiders import CSVFeedSpider from myproject.items import TestItem

>   - class MySpider(CSVFeedSpider):  
>     name = "example.com" allowed\_domains = \["example.com"\] start\_urls = \["<http://www.example.com/feed.csv>"\] delimiter = ";" quotechar = "'" headers = \["id", "name", "description"\]
>     
>       - def parse\_row(self, response, row):  
>         self.logger.info("Hi, this is a row\!: %r", row)
>         
>         item = TestItem() item\["id"\] = row\["id"\] item\["name"\] = row\["name"\] item\["description"\] = row\["description"\] return item

SitemapSpider `` ` -------------  .. class:: SitemapSpider      SitemapSpider allows you to crawl a site by discovering the URLs using     `Sitemaps`_.      It supports nested sitemaps and discovering sitemap urls from     `robots.txt`_.      .. attribute:: sitemap_urls          A list of urls pointing to the sitemaps whose urls you want to crawl.          You can also point to a `robots.txt`_ and it will be parsed to extract         sitemap urls from it.      .. attribute:: sitemap_rules          A list of tuples ``(regex, callback)`where:          *`regex`is a regular expression to match urls extracted from sitemaps.`regex`can be either a str or a compiled regex object.          * callback is the callback to use for processing the urls that match           the regular expression.`callback`can be a string (indicating the           name of a spider method) or a callable.          For example::              sitemap_rules = [('/product/', 'parse_product')]          Rules are applied in order, and only the first one that matches will be         used.          If you omit this attribute, all urls found in sitemaps will be         processed with the`parse``callback.      .. attribute:: sitemap_follow          A list of regexes of sitemap that should be followed. This is only         for sites that use `Sitemap index files`_ that point to other sitemap         files.          By default, all sitemaps are followed.      .. attribute:: sitemap_alternate_links          Specifies if alternate links for one``url`should be followed. These         are links for the same website in another language passed within         the same`url`block.          For example::              <url>                 <loc>http://example.com/</loc>                 <xhtml:link rel="alternate" hreflang="de" href="http://example.com/de"/>             </url>          With`sitemap\_alternate\_links`set, this would retrieve both URLs. With`sitemap\_alternate\_links`disabled, only`<http://example.com/>`would be         retrieved.          Default is`sitemap\_alternate\_links`disabled.      .. method:: sitemap_filter(entries)          This is a filter function that could be overridden to select sitemap entries         based on their attributes.          For example::              <url>                 <loc>http://example.com/</loc>                 <lastmod>2005-01-01</lastmod>             </url>          We can define a`sitemap\_filter`function to filter`entries`by date:`\`python from datetime import datetime from scrapy.spiders import SitemapSpider

>   - class FilteredSitemapSpider(SitemapSpider):  
>     name = "filtered\_sitemap\_spider" allowed\_domains = \["example.com"\] sitemap\_urls = \["<http://example.com/sitemap.xml>"\]
>     
>       - def sitemap\_filter(self, entries):
>         
>           - for entry in entries:  
>             date\_time = datetime.strptime(entry\["lastmod"\], "%Y-%m-%d") if date\_time.year \>= 2005: yield entry
> 
> This would retrieve only `entries` modified on 2005 and the following years.
> 
> Entries are dict objects extracted from the sitemap document. Usually, the key is the tag name and the value is the text inside it.
> 
> It's important to notice that:
> 
>   - as the loc attribute is required, entries without this tag are discarded
> 
> \- alternate links are stored in a list with the key `alternate` (see `sitemap_alternate_links`) - namespaces are removed, so lxml tags named as `{namespace}tagname` become only `tagname`
> 
> If you omit this method, all entries found in sitemaps will be processed, observing other attributes and their settings.

SitemapSpider examples `` ` ~~~~~~~~~~~~~~~~~~~~~~  Simplest example: process all urls discovered through sitemaps using the ``parse`callback:`\`python from scrapy.spiders import SitemapSpider

>   - class MySpider(SitemapSpider):  
>     sitemap\_urls = \["<http://www.example.com/sitemap.xml>"\]
>     
>       - def parse(self, response):  
>         pass \# ... scrape item here ...

Process some urls with certain callback and other urls with a different `` ` callback: ``\`python from scrapy.spiders import SitemapSpider

>   - class MySpider(SitemapSpider):  
>     sitemap\_urls = \["<http://www.example.com/sitemap.xml>"\] sitemap\_rules = \[ ("/product/", "parse\_product"), ("/category/", "parse\_category"), \]
>     
>       - def parse\_product(self, response):  
>         pass \# ... scrape product ...
>     
>       - def parse\_category(self, response):  
>         pass \# ... scrape category ...

Follow sitemaps defined in the [robots.txt]() file and only follow sitemaps `` ` whose url contains ``/sitemap\_shop`:`\`python from scrapy.spiders import SitemapSpider

>   - class MySpider(SitemapSpider):  
>     sitemap\_urls = \["<http://www.example.com/robots.txt>"\] sitemap\_rules = \[ ("/shop/", "parse\_shop"), \] sitemap\_follow = \["/sitemap\_shops"\]
>     
>       - def parse\_shop(self, response):  
>         pass \# ... scrape shop here ...

Combine SitemapSpider with other sources of urls:

``` python
from scrapy.spiders import SitemapSpider

class MySpider(SitemapSpider):
    sitemap_urls = ["http://www.example.com/robots.txt"]
    sitemap_rules = [
        ("/shop/", "parse_shop"),
    ]

    other_urls = ["http://www.example.com/about"]

    def start_requests(self):
        requests = list(super(MySpider, self).start_requests())
        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]
        return requests

    def parse_shop(self, response):
        pass  # ... scrape shop here ...

    def parse_other(self, response):
        pass  # ... scrape other here ...
```

\`\`\` .. \_Sitemap index files: <https://www.sitemaps.org/protocol.html#index> .. \_robots.txt: <https://www.robotstxt.org/> .. \_TLD: <https://en.wikipedia.org/wiki/Top-level_domain> .. \_Scrapyd documentation: <https://scrapyd.readthedocs.io/en/latest/>

---

stats.md

---

# Stats Collection

Scrapy provides a convenient facility for collecting stats in the form of key/values, where values are often counters. The facility is called the Stats Collector, and can be accessed through the <span class="title-ref">\~scrapy.crawler.Crawler.stats</span> attribute of the \[topics-api-crawler\](\#topics-api-crawler), as illustrated by the examples in the \[topics-stats-usecases\](\#topics-stats-usecases) section below.

However, the Stats Collector is always available, so you can always import it in your module and use its API (to increment or set new stat keys), regardless of whether the stats collection is enabled or not. If it's disabled, the API will still work but it won't collect anything. This is aimed at simplifying the stats collector usage: you should spend no more than one line of code for collecting stats in your spider, Scrapy extension, or whatever code you're using the Stats Collector from.

Another feature of the Stats Collector is that it's very efficient (when enabled) and extremely efficient (almost unnoticeable) when disabled.

The Stats Collector keeps a stats table per open spider which is automatically opened when the spider is opened, and closed when the spider is closed.

## Common Stats Collector uses

Access the stats collector through the <span class="title-ref">\~scrapy.crawler.Crawler.stats</span> attribute. Here is an example of an extension that access stats:

`` `python     class ExtensionThatAccessStats:         def __init__(self, stats):             self.stats = stats          @classmethod         def from_crawler(cls, crawler):             return cls(crawler.stats)  Set stat value:  .. code-block:: python      stats.set_value("hostname", socket.gethostname())  Increment stat value:  .. code-block:: python      stats.inc_value("custom_count")  Set stat value only if greater than previous:  .. code-block:: python      stats.max_value("max_items_scraped", value)  Set stat value only if lower than previous:  .. code-block:: python      stats.min_value("min_free_memory_percent", value)  Get stat value:  .. code-block:: pycon      >>> stats.get_value("custom_count")     1  Get all stats:  .. code-block:: pycon      >>> stats.get_stats()     {'custom_count': 1, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}  Available Stats Collectors ``\` ==========================

Besides the basic <span class="title-ref">StatsCollector</span> there are other Stats Collectors available in Scrapy which extend the basic Stats Collector. You can select which Stats Collector to use through the `STATS_CLASS` setting. The default Stats Collector used is the <span class="title-ref">MemoryStatsCollector</span>.

<div class="currentmodule">

scrapy.statscollectors

</div>

### MemoryStatsCollector

<div class="MemoryStatsCollector">

A simple stats collector that keeps the stats of the last scraping run (for each spider) in memory, after they're closed. The stats can be accessed through the <span class="title-ref">spider\_stats</span> attribute, which is a dict keyed by spider domain name.

This is the default Stats Collector used in Scrapy.

<div class="attribute">

spider\_stats

A dict of dicts (keyed by spider name) containing the stats of the last scraping run for each spider.

</div>

</div>

### DummyStatsCollector

<div class="DummyStatsCollector">

A Stats collector which does nothing but is very efficient (because it does nothing). This stats collector can be set via the `STATS_CLASS` setting, to disable stats collect in order to improve performance. However, the performance penalty of stats collection is usually marginal compared to other Scrapy workload like parsing pages.

</div>

---

telnetconsole.md

---

<div class="currentmodule">

scrapy.extensions.telnet

</div>

# Telnet Console

Scrapy comes with a built-in telnet console for inspecting and controlling a Scrapy running process. The telnet console is just a regular python shell running inside the Scrapy process, so you can do literally anything from it.

The telnet console is a \[built-in Scrapy extension \<topics-extensions-ref\>\](\#built-in-scrapy-extension \<topics-extensions-ref\>) which comes enabled by default, but you can also disable it if you want. For more information about the extension itself see \[topics-extensions-ref-telnetconsole\](\#topics-extensions-ref-telnetconsole).

<div class="warning">

<div class="title">

Warning

</div>

It is not secure to use telnet console via public networks, as telnet doesn't provide any transport-layer security. Having username/password authentication doesn't change that.

Intended usage is connecting to a running Scrapy spider locally (spider process and telnet client are on the same machine) or over a secure connection (VPN, SSH tunnel). Please avoid using telnet console over insecure connections, or disable it completely using `TELNETCONSOLE_ENABLED` option.

</div>

## How to access the telnet console

The telnet console listens in the TCP port defined in the `TELNETCONSOLE_PORT` setting, which defaults to `6023`. To access the console you need to type:

``` none
telnet localhost 6023
Trying localhost...
Connected to localhost.
Escape character is '^]'.
Username:
Password:
>>>
```

By default Username is `scrapy` and Password is autogenerated. The autogenerated Password can be seen on Scrapy logs like the example below:

``` none
2018-10-16 14:35:21 [scrapy.extensions.telnet] INFO: Telnet Password: 16f92501e8a59326
```

Default Username and Password can be overridden by the settings `TELNETCONSOLE_USERNAME` and `TELNETCONSOLE_PASSWORD`.

<div class="warning">

<div class="title">

Warning

</div>

Username and password provide only a limited protection, as telnet is not using secure transport - by default traffic is not encrypted even if username and password are set.

</div>

You need the telnet program which comes installed by default in Windows, and most Linux distros.

## Available variables in the telnet console

The telnet console is like a regular Python shell running inside the Scrapy process, so you can do anything from it including importing new modules, etc.

However, the telnet console comes with some default variables defined for convenience:

| Shortcut     | Description                                                                       |
| ------------ | --------------------------------------------------------------------------------- |
| `crawler`    | the Scrapy Crawler (<span class="title-ref">scrapy.crawler.Crawler</span> object) |
| `engine`     | Crawler.engine attribute                                                          |
| `spider`     | the active spider                                                                 |
| `slot`       | the engine slot                                                                   |
| `extensions` | the Extension Manager (Crawler.extensions attribute)                              |
| `stats`      | the Stats Collector (Crawler.stats attribute)                                     |
| `settings`   | the Scrapy settings object (Crawler.settings attribute)                           |
| `est`        | print a report of the engine status                                               |
| `prefs`      | for memory debugging (see \[topics-leaks\](\#topics-leaks))                       |
| `p`          | a shortcut to the <span class="title-ref">pprint.pprint</span> function           |
| `hpy`        | for memory debugging (see \[topics-leaks\](\#topics-leaks))                       |

## Telnet console usage examples

Here are some example tasks you can do with the telnet console:

### View engine status

You can use the `est()` method of the Scrapy engine to quickly show its state using the telnet console:

``` none
telnet localhost 6023
>>> est()
Execution engine status

time()-engine.start_time                        : 8.62972998619
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False
engine.spider.name                              : followall
engine.spider_is_idle()                         : False
engine.slot.closing                             : False
len(engine.slot.inprogress)                     : 16
len(engine.slot.scheduler.dqs or [])            : 0
len(engine.slot.scheduler.mqs)                  : 92
len(engine.scraper.slot.queue)                  : 0
len(engine.scraper.slot.active)                 : 0
engine.scraper.slot.active_size                 : 0
engine.scraper.slot.itemproc_size               : 0
engine.scraper.slot.needs_backout()             : False
```

### Pause, resume and stop the Scrapy engine

To pause:

``` none
telnet localhost 6023
>>> engine.pause()
>>>
```

To resume:

``` none
telnet localhost 6023
>>> engine.unpause()
>>>
```

To stop:

``` none
telnet localhost 6023
>>> engine.stop()
Connection closed by foreign host.
```

## Telnet Console signals

<div class="signal">

update\_telnet\_vars

</div>

<div class="function">

update\_telnet\_vars(telnet\_vars)

Sent just before the telnet console is opened. You can hook up to this signal to add, remove or update the variables that will be available in the telnet local namespace. In order to do that, you need to update the `telnet_vars` dict in your handler.

  - param telnet\_vars  
    the dict of telnet variables

  - type telnet\_vars  
    dict

</div>

## Telnet settings

These are the settings that control the telnet console's behaviour:

<div class="setting">

TELNETCONSOLE\_PORT

</div>

### TELNETCONSOLE\_PORT

Default: `[6023, 6073]`

The port range to use for the telnet console. If set to `None`, a dynamically assigned port is used.

<div class="setting">

TELNETCONSOLE\_HOST

</div>

### TELNETCONSOLE\_HOST

Default: `'127.0.0.1'`

The interface the telnet console should listen on

<div class="setting">

TELNETCONSOLE\_USERNAME

</div>

### TELNETCONSOLE\_USERNAME

Default: `'scrapy'`

The username used for the telnet console

<div class="setting">

TELNETCONSOLE\_PASSWORD

</div>

### TELNETCONSOLE\_PASSWORD

Default: `None`

The password used for the telnet console, default behaviour is to have it autogenerated

---

versioning.md

---

# Versioning and API stability

## Versioning

There are 3 numbers in a Scrapy version: *A.B.C*

  - *A* is the major version. This will rarely change and will signify very large changes.
  - *B* is the release number. This will include many changes including features and things that possibly break backward compatibility, although we strive to keep these cases at a minimum.
  - *C* is the bugfix release number.

Backward-incompatibilities are explicitly mentioned in the \[release notes \<news\>\](\#release-notes-\<news\>), and may require special attention before upgrading.

Development releases do not follow 3-numbers version and are generally released as `dev` suffixed versions, e.g. `1.3dev`.

<div class="note">

<div class="title">

Note

</div>

With Scrapy 0.\* series, Scrapy used [odd-numbered versions for development releases](https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases). This is not the case anymore from Scrapy 1.0 onwards.

Starting with Scrapy 1.0, all releases should be considered production-ready.

</div>

For example:

  - *1.1.1* is the first bugfix release of the *1.1* series (safe to use in production)

## API stability

API stability was one of the major goals for the *1.0* release.

Methods or functions that start with a single dash (`_`) are private and should never be relied as stable.

Also, keep in mind that stable doesn't mean complete: stable APIs could grow new methods or functionality but the existing methods should keep working the same way.

## Deprecation policy

We aim to maintain support for deprecated Scrapy features for at least 1 year.

For example, if a feature is deprecated in a Scrapy version released on June 15th 2020, that feature should continue to work in versions released on June 14th 2021 or before that.

Any new Scrapy release after a year *may* remove support for that deprecated feature.

All deprecated features removed in a Scrapy release are explicitly mentioned in the \[release notes \<news\>\](\#release-notes-\<news\>).

---

CODE_OF_CONDUCT.md

---


# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official e-mail address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
opensource@zyte.com.
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations


---

CONTRIBUTING.md

---

The guidelines for contributing are available here:
https://docs.scrapy.org/en/master/contributing.html

Please do not abuse the issue tracker for support questions.
If your issue topic can be rephrased to "How to ...?", please use the
support channels to get it answered: https://scrapy.org/community/


---

INSTALL.md

---

For information about installing Scrapy see:

* [Local docs](docs/intro/install.rst)
* [Online docs](https://docs.scrapy.org/en/latest/intro/install.html)


---

README.rst

---

[![image](https://scrapy.org/img/scrapylogo.png)](https://scrapy.org/)

Scrapy
======

[![PyPI Version](https://img.shields.io/pypi/v/Scrapy.svg)](https://pypi.org/pypi/Scrapy)

[![Supported Python Versions](https://img.shields.io/pypi/pyversions/Scrapy.svg)](https://pypi.org/pypi/Scrapy)

[![Ubuntu](https://github.com/scrapy/scrapy/workflows/Ubuntu/badge.svg)](https://github.com/scrapy/scrapy/actions?query=workflow%3AUbuntu)

[![Windows](https://github.com/scrapy/scrapy/workflows/Windows/badge.svg)](https://github.com/scrapy/scrapy/actions?query=workflow%3AWindows)

[![Wheel Status](https://img.shields.io/badge/wheel-yes-brightgreen.svg)](https://pypi.org/pypi/Scrapy)

[![Coverage report](https://img.shields.io/codecov/c/github/scrapy/scrapy/master.svg)](https://codecov.io/github/scrapy/scrapy?branch=master)

[![Conda Version](https://anaconda.org/conda-forge/scrapy/badges/version.svg)](https://anaconda.org/conda-forge/scrapy)

Overview
--------

Scrapy is a BSD-licensed fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.

Scrapy is maintained by [Zyte](https://www.zyte.com/) (formerly Scrapinghub) and [many other contributors](https://github.com/scrapy/scrapy/graphs/contributors).

Check the Scrapy homepage at <https://scrapy.org> for more information, including a list of features.

Requirements
------------

-   Python 3.9+
-   Works on Linux, Windows, macOS, BSD

Install
-------

The quick way:

``` {.bash}
pip install scrapy
```

See the install section in the documentation at <https://docs.scrapy.org/en/latest/intro/install.html> for more details.

Documentation
-------------

Documentation is available online at <https://docs.scrapy.org/> and in the `docs` directory.

Releases
--------

You can check <https://docs.scrapy.org/en/latest/news.html> for the release notes.

Community (blog, twitter, mail list, IRC)
-----------------------------------------

See <https://scrapy.org/community/> for details.

Contributing
------------

See <https://docs.scrapy.org/en/master/contributing.html> for details.

### Code of Conduct

Please note that this project is released with a Contributor [Code of Conduct](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md).

By participating in this project you agree to abide by its terms. Please report unacceptable behavior to <opensource@zyte.com>.

Companies using Scrapy
----------------------

See <https://scrapy.org/companies/> for a list.

Commercial Support
------------------

See <https://scrapy.org/support/> for details.


---

SECURITY.md

---

# Security Policy

## Supported Versions

| Version | Supported          |
| ------- | ------------------ |
| 2.11.x     | :white_check_mark: |
| < 2.11.x   | :x:                |

## Reporting a Vulnerability

Please report the vulnerability using https://github.com/scrapy/scrapy/security/advisories/new.
